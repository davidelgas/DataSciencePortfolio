{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFoYUicRnbMqVHHNIlVr6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Neural%20Network%20with%20Knowledge%20Graph/KG_DNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "B74IjdlZP8q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I will be creating a small NN and a knowledge graph with an open source e-Commerce dataset.\n",
        "\n",
        "\n",
        "My use case:<br>\n",
        "A language interface (chatbot or search engine) that answers user queries.<br>\n",
        "A knowledge graph to structure relationships between products, conversations, and recommendations.<br>\n",
        "A PyTorch-based neural network for embeddings, retrieval, or ranking.<br>\n",
        "Retrieval-Augmented Generation (RAG) to enhance responses with knowledge graph lookups.<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "m_PtksQiY6Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MO2kbk7afp3",
        "outputId": "a72a7484-b68e-4081-e338-137e8770bdd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workfow\n",
        "\n",
        "\n",
        "1.   Load dataset\n",
        "2.   Preprocess and Clean the Data\n",
        "3.   Convert Data into a Knowledge Graph Structure\n",
        "4.   Generate Knowledge Graph Embeddings\n",
        "5.   Build a Graph-Based Recommendation Model\n",
        "6.   Integrate Conversational AI (Natural Language Model)\n",
        "7.   Train & Optimize the Full System\n",
        "8.   Implement Real-Time Inference for Recommendations\n",
        "9.   Deploy as an Interactive API or Chatbot\n",
        "10.  Apppendx of code and notes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TZGx4I6FHCHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "OljZpwOhukMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# I tried the Grocerie dataset, but dial_gt_context_train is corrupt.\n",
        "\n",
        "dial_gt_context_test = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_gt_context_test.npz\", allow_pickle=True)\n",
        "\n",
        "dial_gt_context_train = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_gt_context_train.npz\", allow_pickle=True)\n",
        "\n",
        "dial_gt_context_val = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_gt_context_val.npz\", allow_pickle=True)\n",
        "\n",
        "dial_utter_resp_test = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_utter_resp_test.npz\", allow_pickle=True)\n",
        "\n",
        "dial_utter_resp_train = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_utter_resp_train.npz\", allow_pickle=True)\n",
        "\n",
        "dial_utter_resp_val = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_utter_resp_val.npz\", allow_pickle=True)\n",
        "\n",
        "dial_word_embed = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/dial_word_embed.npz\", allow_pickle=True)\n",
        "\n",
        "rec_test_candidate100 = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/rec_test_candidate100.npz\", allow_pickle=True)\n",
        "\n",
        "rec_val_candidate100 = np.load(\"/content/drive/MyDrive/Colab Notebooks/Datasets/rec_val_candidate100.npz\", allow_pickle=True)\n",
        "\n",
        "\n",
        "# /content/vocab_and_embeddings.pkl"
      ],
      "metadata": {
        "id": "3a1AJuaaaaep"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    \"dial_gt_context_test\": dial_gt_context_test,\n",
        "    \"dial_gt_context_train\": dial_gt_context_train,\n",
        "    \"dial_gt_context_val\": dial_gt_context_val,\n",
        "    \"dial_utter_resp_test\": dial_utter_resp_test,\n",
        "    \"dial_utter_resp_train\": dial_utter_resp_train,\n",
        "    \"dial_utter_resp_val\": dial_utter_resp_val,\n",
        "    \"dial_word_embed\": dial_word_embed,\n",
        "    \"rec_test_candidate100\": rec_test_candidate100,\n",
        "    \"rec_val_candidate100\": rec_val_candidate100\n",
        "}\n",
        "\n",
        "\n",
        "def look_at_files(files):\n",
        "    for name, data in files.items():  # Loop through dictionary items\n",
        "        print(f\"\\nInspecting {name}\")\n",
        "        print(f\"Stored arrays: {data.files}\")  # List arrays in each NPZ file\n",
        "\n",
        "        for key in data.files:\n",
        "            array = data[key]\n",
        "            print(f\"Array Name: {key}\")\n",
        "            print(f\"   Type: {type(array)}\")\n",
        "            print(f\"   Shape: {array.shape}\")\n",
        "            print(f\"   Data Type: {array.dtype}\")\n",
        "\n",
        "            if array.dtype.names:  # This checks for structured NumPy arrays with named columns\n",
        "                print(f\"   Columns: {array.dtype.names}\")\n",
        "            print(\"-\" * 50)  # Separator\n",
        "\n",
        "look_at_files(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRIpbz1rMAnV",
        "outputId": "d153ae3a-61e3-4738-8df6-699b02106cf3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inspecting dial_gt_context_test\n",
            "Stored arrays: ['utter_gt', 'utter_context', 'resp_gt', 'resp_contexst', 'dial_length']\n",
            "Array Name: utter_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (127712, 10, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: utter_context\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (127712, 10, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1277120, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_contexst\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1277120, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: dial_length\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (127712,)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_gt_context_train\n",
            "Stored arrays: ['utter_gt', 'utter_context', 'resp_gt', 'resp_context']\n",
            "Array Name: utter_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (361590, 10, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: utter_context\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (361590, 10, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (723180, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_context\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (723180, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_gt_context_val\n",
            "Stored arrays: ['utter_gt', 'utter_context', 'resp_gt', 'resp_context']\n",
            "Array Name: utter_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (119421, 10, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: utter_context\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (119421, 10, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_gt\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1194210, 1)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: resp_context\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1194210, 24)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_utter_resp_test\n",
            "Stored arrays: ['utterance', 'response', 'label']\n",
            "Array Name: utterance\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (127712, 10, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: response\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1277120, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: label\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1277120,)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_utter_resp_train\n",
            "Stored arrays: ['utterance', 'response', 'label']\n",
            "Array Name: utterance\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (361590, 10, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: response\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (723180, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: label\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (723180,)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_utter_resp_val\n",
            "Stored arrays: ['utterance', 'response', 'label']\n",
            "Array Name: utterance\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (119421, 10, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: response\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1194210, 50)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "Array Name: label\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (1194210,)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting dial_word_embed\n",
            "Stored arrays: ['word_emb']\n",
            "Array Name: word_emb\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (19071, 200)\n",
            "   Data Type: float64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting rec_test_candidate100\n",
            "Stored arrays: ['candidates']\n",
            "Array Name: candidates\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (127712, 102)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n",
            "\n",
            "Inspecting rec_val_candidate100\n",
            "Stored arrays: ['candidates']\n",
            "Array Name: candidates\n",
            "   Type: <class 'numpy.ndarray'>\n",
            "   Shape: (119421, 102)\n",
            "   Data Type: int64\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess and Clean the Data"
      ],
      "metadata": {
        "id": "nRoQyVWn6_nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expose the arrays from each NPZ\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "extracted_arrays = {}\n",
        "\n",
        "# Iterate through each NPZ file and extract its arrays\n",
        "for file_name, npz_obj in files.items():\n",
        "    extracted_arrays[file_name] = {}\n",
        "    for array_name in npz_obj.files:\n",
        "        extracted_arrays[file_name][array_name] = npz_obj[array_name]\n",
        "        print(f\"Extracted {array_name} from {file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXWRzve5QvNw",
        "outputId": "711a8cad-c7b7-48ac-e752-9365e49f523d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted utter_gt from dial_gt_context_test\n",
            "Extracted utter_context from dial_gt_context_test\n",
            "Extracted resp_gt from dial_gt_context_test\n",
            "Extracted resp_contexst from dial_gt_context_test\n",
            "Extracted dial_length from dial_gt_context_test\n",
            "Extracted utter_gt from dial_gt_context_train\n",
            "Extracted utter_context from dial_gt_context_train\n",
            "Extracted resp_gt from dial_gt_context_train\n",
            "Extracted resp_context from dial_gt_context_train\n",
            "Extracted utter_gt from dial_gt_context_val\n",
            "Extracted utter_context from dial_gt_context_val\n",
            "Extracted resp_gt from dial_gt_context_val\n",
            "Extracted resp_context from dial_gt_context_val\n",
            "Extracted utterance from dial_utter_resp_test\n",
            "Extracted response from dial_utter_resp_test\n",
            "Extracted label from dial_utter_resp_test\n",
            "Extracted utterance from dial_utter_resp_train\n",
            "Extracted response from dial_utter_resp_train\n",
            "Extracted label from dial_utter_resp_train\n",
            "Extracted utterance from dial_utter_resp_val\n",
            "Extracted response from dial_utter_resp_val\n",
            "Extracted label from dial_utter_resp_val\n",
            "Extracted word_emb from dial_word_embed\n",
            "Extracted candidates from rec_test_candidate100\n",
            "Extracted candidates from rec_val_candidate100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes and sample data for conversation context\n",
        "print(\"utter_gt_train shape:\", extracted_arrays[\"dial_gt_context_train\"][\"utter_gt\"].shape)\n",
        "print(\"Sample:\", extracted_arrays[\"dial_gt_context_train\"][\"utter_gt\"][:2])\n",
        "\n",
        "print(\"resp_gt_train shape:\", extracted_arrays[\"dial_gt_context_train\"][\"resp_gt\"].shape)\n",
        "print(\"Sample:\", extracted_arrays[\"dial_gt_context_train\"][\"resp_gt\"][:2])\n",
        "\n",
        "print(\"utter_context_train shape:\", extracted_arrays[\"dial_gt_context_train\"][\"utter_context\"].shape)\n",
        "print(\"Sample:\", extracted_arrays[\"dial_gt_context_train\"][\"utter_context\"][:2])\n",
        "\n",
        "print(\"resp_context_train shape:\", extracted_arrays[\"dial_gt_context_train\"][\"resp_context\"].shape)\n",
        "print(\"Sample:\", extracted_arrays[\"dial_gt_context_train\"][\"resp_context\"][:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SF7L1IHUzUJ",
        "outputId": "963d1e14-809a-41eb-bb31-9b8b1fe9c578"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utter_gt_train shape: (361590, 10, 1)\n",
            "Sample: [[[169331]\n",
            "  [169331]\n",
            "  [169331]\n",
            "  [169331]\n",
            "  [169331]\n",
            "  [169331]\n",
            "  [135181]\n",
            "  [169331]\n",
            "  [128901]\n",
            "  [169331]]\n",
            "\n",
            " [[139397]\n",
            "  [169331]\n",
            "  [168871]\n",
            "  [169331]\n",
            "  [163173]\n",
            "  [169331]\n",
            "  [160426]\n",
            "  [169331]\n",
            "  [115820]\n",
            "  [169331]]]\n",
            "resp_gt_train shape: (723180, 1)\n",
            "Sample: [[155243]\n",
            " [153058]]\n",
            "utter_context_train shape: (361590, 10, 24)\n",
            "Sample: [[[169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [     0      0 108416      0      0  93588      0      0  79580      0\n",
            "        0 106201  79580      3 128901 106201      3 128901 108416      3\n",
            "   128901  93588      3 128901]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]]\n",
            "\n",
            " [[     0      0  90652  90652      8 139397      0      0  90652      0\n",
            "        0  90652 108416      8 139397      0      0  90652 108416      8\n",
            "   139397      0      0 108416]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [     0     10 160426      0     10 160426      0     10 160426      0\n",
            "       10 160426      0     10 160426      0     10 160426      0     10\n",
            "   160426      0     10 160426]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]\n",
            "  [     0      1 115820      0      1 115820      0      1 115820      0\n",
            "        1 115820      0      1 115820      0      1 115820      0      1\n",
            "   115820      0      1 115820]\n",
            "  [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "       45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "   169331 169331     45 169331]]]\n",
            "resp_context_train shape: (723180, 24)\n",
            "Sample: [[     0     10 155243      0     10 155243      0     10 155243      0\n",
            "      10 155243      0     10 155243      0     10 155243      0     10\n",
            "  155243      0     10 155243]\n",
            " [169331     45 169331 169331     45 169331 169331     45 169331 169331\n",
            "      45 169331 169331     45 169331 169331     45 169331 169331     45\n",
            "  169331 169331     45 169331]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, these have already been encoded. I'll proceed with them as is, but then start with raw text, convert to tensors, encode, etc. afterwards."
      ],
      "metadata": {
        "id": "-lAcC0QGVEIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = {}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Datasets/word_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()  # Remove any leading or trailing spaces\n",
        "        parts = line.rsplit(\" \", 1)  # Split at the LAST space\n",
        "\n",
        "        if len(parts) == 2 and parts[1].isdigit():  # Ensure the last part is an index\n",
        "            word, idx = parts\n",
        "            word_dict[int(idx)] = word  # Store as {index: word}\n",
        "\n",
        "print(list(word_dict.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY8uaFD0Vkxm",
        "outputId": "79287d91-d617-4342-f3aa-e2f73cf41058"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(786, 'militaryshield'), (3464, '6.6ft'), (8135, 'circuitri'), (12798, 'c-pattern-4'), (15378, 'zedari'), (7476, '20000mah-black'), (16861, 'rvh-ac03'), (3592, 'hanlesi'), (709, 'yellow'), (5354, 'interchang')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Are these tripples ?\n",
        "# Extract a small sample\n",
        "utter_gt_sample = extracted_arrays[\"dial_gt_context_train\"][\"utter_gt\"][:2]\n",
        "resp_gt_sample = extracted_arrays[\"dial_gt_context_train\"][\"resp_gt\"][:2]\n",
        "utter_context_sample = extracted_arrays[\"dial_gt_context_train\"][\"utter_context\"][:2]\n",
        "\n",
        "# Decode using the word dictionary\n",
        "decoded_utterances = [\n",
        "    [word_dict.get(int(token[0]), \"[UNK]\") for token in utterance] for utterance in utter_gt_sample\n",
        "]\n",
        "decoded_responses = [\n",
        "    [word_dict.get(int(token[0]), \"[UNK]\") for token in response] for response in resp_gt_sample\n",
        "]\n",
        "decoded_contexts = [\n",
        "    [[word_dict.get(int(token), \"[UNK]\") for token in context] for context in utterance]\n",
        "    for utterance in utter_context_sample\n",
        "]\n",
        "\n",
        "# Print a few examples\n",
        "print(\"Decoded Utterances:\", decoded_utterances)\n",
        "print(\"Decoded Responses:\", decoded_responses)\n",
        "print(\"Decoded Contexts:\", decoded_contexts[:1])  # Print only one for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "70Am-L6B1B-6",
        "outputId": "c45af4f3-0afc-43b7-9aff-9581301fcd5a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2c343c71e383>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutterance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutterance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutter_gt_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 11\u001b[0;31m decoded_responses = [\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp_gt_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-14-2c343c71e383>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m decoded_responses = [\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp_gt_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m decoded_contexts = [\n",
            "\u001b[0;32m<ipython-input-14-2c343c71e383>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m decoded_responses = [\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp_gt_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m decoded_contexts = [\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Data into a Knowledge Graph Structure"
      ],
      "metadata": {
        "id": "64B8Z8Zc6_qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Knowledge Graph Embeddings"
      ],
      "metadata": {
        "id": "t8tsltRK6_sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Graph-Based Recommendation Model"
      ],
      "metadata": {
        "id": "iiUUMoqW6_u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrate Conversational AI (Natural Language Model)"
      ],
      "metadata": {
        "id": "jC3997sK6_xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Optimize the Full System"
      ],
      "metadata": {
        "id": "8tHREmv66_0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Real-Time Inference for Recommendations"
      ],
      "metadata": {
        "id": "slEeYQzK6_3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy as an Interactive API or Chatbot"
      ],
      "metadata": {
        "id": "Z82Fb6r_6_55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "GRLnTAAAuxUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scoring Criteria for Selecting an Encoder\n",
        "\n",
        "\n",
        "| **Factor**                 | **Description** |\n",
        "|---------------------------|----------------|\n",
        "| **Computational Efficiency** | How fast is the encoding on CPU/GPU? |\n",
        "| **Memory Usage**          | How much memory does it require? |\n",
        "| **Scalability**           | Can it handle large datasets like OpenBG500? |\n",
        "| **Preserves Semantic Meaning** | Does the encoding capture relationships between entities? |\n",
        "| **Compatibility with PyTorch** | How well does it integrate into PyTorch models? |\n",
        "| **Ease of Implementation** | How difficult is it to set up? |\n",
        "\n",
        "Each encoding method gets a **score from 1 to 5** for each factor.\n",
        "\n",
        "---\n",
        "\n",
        "## Scoring Different Encoding Methods\n",
        "\n",
        "| Encoding Method  | Computational Efficiency | Memory Usage | Scalability | Semantic Meaning | PyTorch Compatibility | Ease of Implementation | **Total Score** |\n",
        "|-----------------|------------------------|--------------|-------------|------------------|----------------------|--------------------|--------------|\n",
        "| **Label Encoding** (Integer Mapping) | **5** (Very fast) | **5** (Very low) | **5** (Handles millions of nodes) | **1** (No meaning captured) | **5** (PyTorch works with integers easily) | **5** (Simple `map()`) | **26** |\n",
        "| **One-Hot Encoding** | **2** (Slow for large datasets) | **1** (Consumes huge memory) | **1** (Bad for large graphs) | **3** (Some structure captured) | **3** (Can be used, but not ideal) | **3** (Easy but inefficient) | **13** |\n",
        "| **BERT Embeddings** (Text-Based) | **2** (Slow on CPU) | **3** (Moderate) | **3** (Can use pre-trained models) | **5** (Captures meaning well) | **4** (PyTorch supports it, but needs preprocessing) | **2** (Requires NLP model) | **19** |\n",
        "| **Word2Vec/FastText** | **3** (Faster than BERT) | **3** (Moderate) | **4** (Good for large datasets) | **4** (Captures word meaning) | **4** (PyTorch supports it) | **3** (Requires preprocessing) | **21** |\n",
        "| **Knowledge Graph Embeddings (TransE, RotatE)** | **4** (Moderate) | **4** (Efficient for large graphs) | **5** (Scales well) | **5** (Captures graph meaning) | **5** (Designed for PyTorch models) | **3** (Requires model training) | **26** |\n",
        "\n"
      ],
      "metadata": {
        "id": "w1WMfj7DuMfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique entities (from both head and tail)\n",
        "all_entities = set(triples_df_train[\"head\"]).union(set(triples_df_train[\"tail\"]))\n",
        "\n",
        "# Get all unique relations\n",
        "all_relations = set(triples_df_train[\"relation\"])\n",
        "\n",
        "# Create mapping dictionaries\n",
        "entity2id = {entity: idx for idx, entity in enumerate(all_entities)}\n",
        "relation2id = {relation: idx for idx, relation in enumerate(all_relations)}\n",
        "\n",
        "def encode_triples(df):\n",
        "    df[\"head\"] = df[\"head\"].map(entity2id)\n",
        "    df[\"relation\"] = df[\"relation\"].map(relation2id)\n",
        "    df[\"tail\"] = df[\"tail\"].map(entity2id)\n",
        "    return df\n",
        "\n",
        "# Encode train, test, and validation sets\n",
        "triples_df_train = encode_triples(triples_df_train)\n",
        "triples_df_test = encode_triples(triples_df_test)\n",
        "triples_df_val = encode_triples(triples_df_val)\n"
      ],
      "metadata": {
        "id": "pHs0Bg06wJEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert to tensor format\n",
        "train_tensor = torch.tensor(triples_df_train.values, dtype=torch.long)\n",
        "test_tensor = torch.tensor(triples_df_test.values, dtype=torch.long)\n",
        "val_tensor = torch.tensor(triples_df_val.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "0OT9yU2Kwh7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check the shape of the tensors\n",
        "print(\"Train Tensor Shape:\", train_tensor.shape)\n",
        "print(\"Test Tensor Shape:\", test_tensor.shape)\n",
        "print(\"Validation Tensor Shape:\", val_tensor.shape)\n",
        "\n",
        "# Access the first 5 samples\n",
        "print(\"First 5 Training Samples:\\n\", train_tensor[:5])\n",
        "\n",
        "# Get specific columns\n",
        "heads = train_tensor[:, 0]  # Head entities\n",
        "relations = train_tensor[:, 1]  # Relations\n",
        "tails = train_tensor[:, 2]  # Tail entities\n",
        "\n",
        "print(\"First 5 Head Entities:\\n\", heads[:5])\n",
        "print(\"First 5 Relations:\\n\", relations[:5])\n",
        "print(\"First 5 Tail Entities:\\n\", tails[:5])\n",
        "\n",
        "# Perform simple operations\n",
        "sum_tensor = heads + tails  # Example tensor addition\n",
        "print(\"Sum of Head & Tail Entities:\\n\", sum_tensor[:5])\n",
        "\n",
        "# Get unique values\n",
        "unique_heads = torch.unique(heads)\n",
        "print(f\"Unique Head Entities Count: {unique_heads.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4eelogAxJIe",
        "outputId": "55f64804-fb53-4e26-9ca1-70e43dc90324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Tensor Shape: torch.Size([1242550, 3])\n",
            "Test Tensor Shape: torch.Size([5000, 3])\n",
            "Validation Tensor Shape: torch.Size([5000, 3])\n",
            "First 5 Training Samples:\n",
            " tensor([[158292,    282,  79197],\n",
            "        [193190,    490, 184642],\n",
            "        [243732,     56,  86323],\n",
            "        [248311,    134,  78130],\n",
            "        [ 34938,    253, 231834]])\n",
            "First 5 Head Entities:\n",
            " tensor([158292, 193190, 243732, 248311,  34938])\n",
            "First 5 Relations:\n",
            " tensor([282, 490,  56, 134, 253])\n",
            "First 5 Tail Entities:\n",
            " tensor([ 79197, 184642,  86323,  78130, 231834])\n",
            "Sum of Head & Tail Entities:\n",
            " tensor([237489, 377832, 330055, 326441, 266772])\n",
            "Unique Head Entities Count: 116721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")  # Force CPU mode for now\n",
        "\n",
        "print(\"Using Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5kgl1K566zF",
        "outputId": "8b83bad6-f269-4880-aecf-fbe1603497c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple MLP model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "# Three layer network\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set dimensions\n",
        "input_dim = 3  # (head, relation, tail)\n",
        "hidden_dim = 16\n",
        "output_dim = 1  # Binary classification or regression\n",
        "\n",
        "# Initialize model\n",
        "model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.MSELoss()  # Example: MSE loss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Dummy training loop\n",
        "for epoch in range(5):  # Short training example\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(train_tensor.float())  # Convert tensor to float for Linear layers\n",
        "    loss = criterion(outputs, torch.rand_like(outputs))  # Dummy target values\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDdRgsUR6tST",
        "outputId": "3f4fe4e9-44d1-4ea5-aea7-77152d45276e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1196703744.0\n",
            "Epoch 2, Loss: 737774528.0\n",
            "Epoch 3, Loss: 400586816.0\n",
            "Epoch 4, Loss: 173411088.0\n",
            "Epoch 5, Loss: 46432520.0\n"
          ]
        }
      ]
    }
  ]
}