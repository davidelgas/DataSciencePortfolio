{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/NLP_with_RAG/notebooks%20/NLP_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2vTigd3lYAM"
      },
      "source": [
        "# 1 Project Objective and Limitations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEVpPV9NmPK2"
      },
      "source": [
        "## 1.1 Project Overview\n",
        "The advent of modern automobile manufacturing has led to increased technical complexity, often resulting in mechanics opting to replace parts rather than diagnose and fix issues. This approach, while convenient for contemporary vehicles, poses a significant challenge for classic cars built 30 to 40 years ago, where replacement parts are scarce or non-existent.\n",
        "\n",
        "To address this problem, this project aims to leverage Generative AI to create a \"virtual mechanic.\" By utilizing a corpus of text gathered from a classic car forum, this AI-driven tool will be capable of understanding unstructured questions and providing relevant answers. This innovative solution aims to assist classic car enthusiasts and mechanics by offering expert guidance, thereby preserving the heritage and functionality of vintage automobiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jOHmOOCkEvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1324ec-9285-4af1-c594-a2082a3e0e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ7ngp93CY18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "94d08038-3c42-441b-8004-f0f2b80119d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('/content/drive/MyDrive/Colab Notebooks/NLP/BMW_E9.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YK6zzgBCySF"
      },
      "source": [
        "1972 - 1974 BMW 3.0 CS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFqXbkd_mWjv"
      },
      "source": [
        "## 1.2 Objectives\n",
        "The primary objective of this project is the development of a Natural Language Processing (NLP) model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparision and selection of architectures, libraries and methods. This is a complement to my pursuit of a Masters Degree in Data Science.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGUdzGfwgzo"
      },
      "source": [
        "## 1.3 Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are a result of previous similar questions from the forum used to create the corpus. The answers will also include results from pre-trained models, ensuring a rich and informed response. Users will see these answers in plain English. As a programmer, you have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model. However, users will not see the verbatim source text used to generate the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlctA2pymZYv"
      },
      "source": [
        "## 1.4 Limitations and Challenges\n",
        "Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs. Data storage will be done in a Snowflake database. Where possible, a combination of open source and free resource will be used.\n",
        "\n",
        "Budgetary constraints will play a part in several decisions on the specific workflow. Most noteably, these are known to include the use of CPUs over more powerful compute options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqO0ZIpBCGrt"
      },
      "source": [
        "## 1.4 Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are a result of previous similar questions from the forum used to create the corpus. The answers will also include results from pre-trained models, ensuring a rich and informed response. Users will see these answers in plain English. As a programmer, you have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model. However, users will not see the verbatim source text used to generate the answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHJWehqmrr6a"
      },
      "source": [
        "## 1.5 Workflow for NLP RAG Model\n",
        "**Natural Language Processing Architectures and Models**\n",
        "   - Choose a pre-trained model such as DistilBERT for embeddings and T5 or GPT for generation. Possibly employ a strategy of using both for specific tasks (embeddings vs generation)\n",
        "\n",
        "**Develop Corpus**\n",
        "   - Gather and organize text data from sources such as a forum or published material.\n",
        "\n",
        "**Remove Unnecessary Characters**\n",
        "   - Clean the text by removing HTML tags, extra whitespace, non-printable characters, and other irrelevant elements.\n",
        "\n",
        "**Convert Text to Lowercase**\n",
        "   - Standardize the text by converting all characters to lowercase to ensure uniformity.\n",
        "\n",
        "**Remove Stop Words**\n",
        "   - Filter out common stop words to focus on more meaningful content.\n",
        "\n",
        "**Deduplication**\n",
        "   - Remove duplicate entries to ensure the uniqueness of the data.\n",
        "\n",
        "**Lemmatization**\n",
        "   - Convert words to their base or dictionary form to consolidate similar forms of a word.\n",
        "\n",
        "**Entity Recognition and Anonymization**\n",
        "   - Identify and anonymize personal information or specific entity names to maintain privacy.\n",
        "\n",
        "**Filter Irrelevant Sections**\n",
        "   - Remove sections of the text that do not contribute to the knowledge base or are off-topic.\n",
        "\n",
        "**Consolidate Threads**\n",
        "   - Combine related threads or posts to create a comprehensive view of discussions on similar topics.\n",
        "\n",
        "**Clustering**\n",
        "   - Group text segments by their semantic similarities to enhance the structure of the dataset.\n",
        "\n",
        "**Summarize Corpus Content**\n",
        "   - Condense your text data to highlight the most important information and insights from your corpus.\n",
        "\n",
        "**Format into Questions and Answers**\n",
        "   - Structure your text into a question-answer format suitable for training your RAG model.\n",
        "   - Ensure the question string ends with a question mark for clarity.\n",
        "\n",
        "**Tokenization**\n",
        "   - Break down the text into smaller units called tokens. Use a tokenizer compatible with your chosen model, such as the BERT tokenizer.\n",
        "\n",
        "**Embedding**\n",
        "   - Convert tokens into numerical representations using embeddings. Use pre-trained embeddings from transformer models like BERT, DistilBERT, or T5.\n",
        "\n",
        "**Build FAISS Index**\n",
        "   - Create an index of the embeddings using FAISS for fast similarity searches.\n",
        "\n",
        "**Query Processing and Search**\n",
        "   - Generate embeddings for new queries and use the FAISS index to find the most similar questions in the corpus.\n",
        "\n",
        "**Retrieve and Rank**\n",
        "   - Fetch the top-N most similar questions and their corresponding answers from the corpus.\n",
        "   - Concatenate the retrieved contexts to form a comprehensive input for the generative model.\n",
        "\n",
        "**Answer Generation**\n",
        "   - Employ a generative model like T5 or GPT to generate an answer based on the concatenated context and the query.\n",
        "\n",
        "**Evaluation and Tuning**\n",
        "   - Assess the model's performance using metrics like precision, recall, F1-score, and accuracy.\n",
        "   - Fine-tune the pre-trained models on your specific dataset if necessary.\n",
        "\n",
        "**Deploying the Demo**\n",
        "   - (Optional) Create an interactive UI using tools like Flask or Streamlit.\n",
        "   - Deploy the model to a server or cloud service for remote access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRMLNtZXGeya"
      },
      "source": [
        "# 2 Natural Language Processing Architectures and Models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This document provides an overview of various architectures, models, and tools used in natural language processing tasks. Understanding the strengths and weaknesses of different approaches is crucial for designing effective NLP systems tailored to specific use cases and requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4-gxpMOvYn2"
      },
      "source": [
        "\n",
        "## 2.1 Traditional Machine Learning Architectures\n",
        "\n",
        "### 2.1.1 Bag-of-Words (BoW)\n",
        "- **Description:** Represents text data as a collection of unique words and their frequencies.\n",
        "- **Example:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "- **Pros:**\n",
        "  - Simple and efficient representation.\n",
        "  - Works well for tasks like sentiment analysis and document classification.\n",
        "- **Cons:**\n",
        "  - Ignores word order and context.\n",
        "  - Doesn't capture semantic meanings well.\n",
        "\n",
        "### 2.1.2 Word Embeddings\n",
        "- **Description:** Represent words as dense vectors in a continuous vector space.\n",
        "- **Examples:** Word2Vec, GloVe\n",
        "- **Pros:**\n",
        "  - Captures semantic meanings and relationships between words.\n",
        "  - Provides dense vector representations suitable for downstream tasks.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of data for training.\n",
        "  - Struggles with out-of-vocabulary words.\n",
        "\n",
        "### 2.1.3 Sequence Models\n",
        "- **Description:** Models that capture the sequential nature of text data.\n",
        "- **Examples:** Hidden Markov Models (HMM), Conditional Random Fields (CRF)\n",
        "- **Pros:**\n",
        "  - Captures sequential dependencies in data.\n",
        "  - Suitable for tasks like named entity recognition and part-of-speech tagging.\n",
        "- **Cons:**\n",
        "  - Requires labeled sequential data for training.\n",
        "  - Can be computationally intensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8yTPH0xwGYV"
      },
      "source": [
        "\n",
        "## 2.2 Deep Learning Architectures\n",
        "\n",
        "### 2.2.1 Convolutional Neural Networks (CNN)\n",
        "- **Description:** Deep learning models that use convolutional layers for feature extraction.\n",
        "- **Examples:** TextCNN\n",
        "- **Pros:**\n",
        "  - Effective for capturing local dependencies in text data.\n",
        "  - Can capture hierarchical patterns in data.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of training data.\n",
        "  - Limited ability to capture long-range dependencies.\n",
        "\n",
        "### 2.2.2 Recurrent Neural Networks (RNN)\n",
        "- **Description:** Neural networks that process sequences by iterating through elements.\n",
        "- **Examples:** Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)\n",
        "- **Pros:**\n",
        "  - Effective for capturing sequential dependencies in data.\n",
        "  - Suitable for tasks like language modeling and machine translation.\n",
        "- **Cons:**\n",
        "  - Vulnerable to vanishing and exploding gradient problems.\n",
        "  - Computationally expensive to train.\n",
        "\n",
        "### 2.2.3 Transformers\n",
        "- **Description:** Neural network architecture based entirely on self-attention mechanisms.\n",
        "- **Examples:** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer)\n",
        "- **Pros:**\n",
        "  - Captures long-range dependencies effectively.\n",
        "  - Parallelizable training process.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of computational resources.\n",
        "  - Limited interpretability compared to traditional models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R1AMUZwPAG"
      },
      "source": [
        "## 2.3 Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "### 2.3.1 Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)\n",
        "- **Description:** A variant of RAG that incorporates knowledge graphs to enhance retrieval and generation.\n",
        "- **Examples:** Graph-BERT\n",
        "- **Pros:**\n",
        "  - Integrates structured knowledge for improved understanding and generation.\n",
        "  - Enables more coherent and contextually relevant responses.\n",
        "- **Cons:**\n",
        "  - Requires high-quality and curated knowledge graphs.\n",
        "  - Increased computational complexity compared to standard RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRrycYowbPl"
      },
      "source": [
        "## 2.4 Other Architectures and Models\n",
        "\n",
        "### 2.4.1 Pre-trained Models\n",
        "- **Description:** Models pre-trained on large corpora and fine-tuned for specific tasks.\n",
        "- **Examples:** BERT, GPT, T5\n",
        "- **Pros:**\n",
        "  - Leverage large amounts of unlabeled data for pre-training.\n",
        "  - Achieve state-of-the-art performance on various NLP tasks.\n",
        "- **Cons:**\n",
        "  - Resource-intensive pre-training process.\n",
        "  - May require substantial computational resources for fine-tuning.\n",
        "\n",
        "### 2.4.2 Retriever-Generator Models\n",
        "- **Description:** Models that combine retrieval and generation components for text generation tasks.\n",
        "- **Examples:** RAG, KERAG\n",
        "- **Pros:**\n",
        "  - Incorporates both structured and unstructured information for generation.\n",
        "  - Produces more diverse and contextually relevant responses.\n",
        "- **Cons:**\n",
        "  - Requires efficient retrieval mechanisms.\n",
        "  - Increased complexity in model architecture.\n",
        "\n",
        "### 2.4.3 Knowledge Graphs\n",
        "- **Description:** Graph-based structures that represent knowledge and relationships between entities.\n",
        "- **Examples:** ConceptNet, WordNet\n",
        "- **Pros:**\n",
        "  - Organizes knowledge in a structured format.\n",
        "  - Enables reasoning and inference for downstream tasks.\n",
        "- **Cons:**\n",
        "  - Requires manual curation and maintenance.\n",
        "  - Limited coverage and scalability.\n",
        "\n",
        "### 2.4.4 Dense Passage Retrieval (DPR)\n",
        "- **Description:** Technique for retrieving relevant passages from a large corpus for question answering.\n",
        "- **Examples:** DenseRetrieval, TANDA\n",
        "- **Pros:**\n",
        "  - Efficient retrieval of contextually relevant information.\n",
        "  - Suitable for large-scale question answering systems.\n",
        "- **Cons:**\n",
        "  - Computationally intensive for indexing large corpora.\n",
        "  - May suffer from noise in retrieved passages.\n",
        "\n",
        "### 2.4.5 Elastic Search\n",
        "- **Description:** Distributed search and analytics engine for indexing and searching large volumes of data.\n",
        "- **Examples:** Elasticsearch, Apache Solr\n",
        "- **Pros:**\n",
        "  - Scalable and distributed architecture.\n",
        "  - Supports full-text search and complex query structures.\n",
        "- **Cons:**\n",
        "  - Requires infrastructure for deployment and maintenance.\n",
        "  - Indexing and search performance may degrade with large datasets.\n",
        "\n",
        "### 2.4.6 Anserini\n",
        "- **Description:** Information retrieval toolkit built on Apache Lucene for research purposes.\n",
        "- **Examples:** Pyserini, Anserini\n",
        "- **Pros:**\n",
        "  - Provides efficient indexing and retrieval capabilities.\n",
        "  - Supports integration with various retrieval models.\n",
        "- **Cons:**\n",
        "  - Requires expertise in information retrieval concepts.\n",
        "  - Limited documentation and community support.\n",
        "\n",
        "### 2.4.7 BART (Bidirectional and Auto-Regressive Transformers)\n",
        "- **Description:** Transformer-based model architecture capable of bidirectional and auto-regressive generation.\n",
        "- **Examples:** Facebook BART\n",
        "- **Pros:**\n",
        "  - Supports both conditional and unconditional text generation.\n",
        "  - Achieves state-of-the-art performance on various NLP tasks.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of computational resources for training.\n",
        "  - Limited interpretability of generated outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlVbFaTmwmTd"
      },
      "source": [
        "## 2.5 Machine Learning Frameworks and Tools\n",
        "\n",
        "### 2.5.1 TensorFlow\n",
        "- **Description:** Open-source machine learning framework developed by Google for building and deploying ML models.\n",
        "- **Pros:**\n",
        "  - Comprehensive ecosystem with support for various deep learning architectures.\n",
        "  - Scalable and efficient execution on both CPUs and GPUs.\n",
        "- **Cons:**\n",
        "  - Steeper learning curve compared to some other frameworks.\n",
        "  - Limited support for dynamic computation graphs.\n",
        "\n",
        "### 2.5.2 PyTorch\n",
        "- **Description:** Open-source deep learning framework developed by Facebook's AI Research lab.\n",
        "- **Pros:**\n",
        "  - Pythonic and intuitive interface for model development.\n",
        "  - Dynamic computation graph enables easier debugging and experimentation.\n",
        "- **Cons:**\n",
        "  - Less optimized for deployment in production compared to TensorFlow.\n",
        "  - Limited built-in support for distributed training.\n",
        "\n",
        "### 2.5.3 scikit-learn\n",
        "- **Description:** Simple and efficient machine learning library built on NumPy, SciPy, and matplotlib.\n",
        "- **Pros:**\n",
        "  - Easy-to-use API for common machine learning tasks.\n",
        "  - Comprehensive documentation and community support.\n",
        "- **Cons:**\n",
        "  - Limited support for deep learning models and architectures.\n",
        "  - Less flexibility for customization compared to deep learning frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "# 3 Develop Corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHg-WlWRLIL"
      },
      "source": [
        "## 3.1 Data Ethics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyBofE4Cu1Vr"
      },
      "source": [
        "The data collected here is a collection of posts from widely avaialble public sources. However, should this project move into a public forum additional steps will be necessary to endure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the projects goals and data gathering process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otWnw_6_mgNg"
      },
      "source": [
        "## 3.2 Data Collection\n",
        "\n",
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### 3.2.1 Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "\n",
        "### 3.2.2 Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "## 3.4 Summarization Strategies\n",
        "\n",
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through multiple options. For this effort, the following solutions were scored to reduce the potential solution set:\n",
        "\n",
        "| Provider                        | OpenAI               | Anthropic           | Meta (Facebook)                              | Google Research     | Amazon            |\n",
        "|---------------------------------|----------------------|---------------------|---------------------------------------------|---------------------|-------------------|\n",
        "| **Specific Package**            | GPT (GPT-2 )               | Claude (Claude 3)            | DistilBART (sshleifer/distilbart-cnn-12-6)  | T5 (t5-small)       | AWS Comprehend    |\n",
        "| **Strengths**                     | Optimal for CPUs, free access | Latest version, enhanced safety features | Optimized for CPU usage, efficient resource usage | Efficient CPU usage, flexible, and powerful | Integrated AWS service, scalability |\n",
        "| 1. CPU Performance              | 2 (GPT-2 optimized for CPUs) | 2 (Enhanced capabilities in Claude 3) | 2 (DistilBART optimized for CPUs)           | 2 (Efficient on CPUs) | 1 (General cloud performance) |\n",
        "| 2. Free                         | 2 (GPT-2 is free)    | 0 (Research access mainly) | 2 (Open-source and free)                  | 2 (Open-source and free) | 0 (Paid service)  |\n",
        "| 3. API Independence             | 1 (Can be run locally, no API needed) | 1 (Research and conditional access) | 2 (Completely API independent) | 2 (Can be run locally) | 0 (API-dependent service) |\n",
        "| 4. Handling Varying Text Length | 2 (Good at handling different lengths) | 2 (Claude 3 designed for robust handling) | 2 (Handles varying lengths well) | 2 (Handles varying lengths efficiently) | 2 (Designed to scale) |\n",
        "| 5. Ease of Integration          | 1 (Integration flexibility varies) | 1 (Varies by deployment) | 2 (Good documentation and community support) | 2 (Good documentation and support) | 2 (Tightly integrated with AWS ecosystem) |\n",
        "| **Total Score**                 | 8                    | 6                   | 10                                          | 10                    | 5                 |\n",
        "\n",
        "### Criteria Explanations\n",
        "1. **CPU Performance:** Measures how well the solution performs on standard CPUs.\n",
        "2. **Free:** Indicates if the solution is free to use.\n",
        "3. **API Independence:** Indicates if the solution can be used without relying on an external API.\n",
        "4. **Handling Varying Text Length:** Assesses how well the solution can manage different lengths of text input.\n",
        "5. **Ease of Integration:** Measures the flexibility and support for integrating the solution into various environments.\n",
        "\n",
        "### Scoring Explanations\n",
        "0: Does not meet\n",
        "2: Fully meets\n",
        "1: Partially meets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A test of solutions can be found in the Appenix. T5 was chosen based on bettter ROUGE scores."
      ],
      "metadata": {
        "id": "Bief4v5ztx5-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fnf6I4FXNYi"
      },
      "source": [
        "## 3.4 Converting Corpus into Questions and Answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rtgtaGw-q-C"
      },
      "source": [
        "## 3.5 Tokenization Strategies\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "### Word-level Tokenization\n",
        "- **Tools:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers\n",
        "    - **Pros:**\n",
        "        - Preserves word integrity and semantic meaning, crucial for comprehension tasks.\n",
        "        - Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "    - **Cons:**\n",
        "        - Can result in a large vocabulary size, increasing memory and processing requirements.\n",
        "        - May overlook nuances in character-level variations.\n",
        "\n",
        "### Character-level Tokenization\n",
        "- **Tools:** Supported by deep learning frameworks like TensorFlow and Keras\n",
        "    - **Pros:**\n",
        "        - Captures morphological nuances at the character level, aiding languages with rich morphology.\n",
        "        - Simplifies the vocabulary to a set of unique characters, reducing model complexity.\n",
        "    - **Cons:**\n",
        "        - Leads to longer input sequences, which can increase computational costs.\n",
        "        - Loses direct access to semantic information encoded in words or phrases.\n",
        "\n",
        "### Subword Tokenization (BPE and Hugging Face Tokenizers)\n",
        "- **Tools** A blend of word-level and character-level tokenization, aiming to balance vocabulary size and semantic richness.\n",
        "    - **Pros:**\n",
        "        - Offers a middle ground, effectively managing vocabulary size while preserving semantic information.\n",
        "        - Facilitates handling of rare or unknown words by breaking them down into recognizable subwords.\n",
        "    - **Cons:**\n",
        "        - Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "        - Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "\n",
        "\n",
        "### Model-Specific Tokenization\n",
        "- **Tools:** Hugging Face's transformers library provides access to pre-built tokenizers corresponding to each pre-trained model, ensuring that tokenization is consistent with the model's original training data.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **For BERT:** AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "- **For GPT-2:** AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "- **For T5:** AutoTokenizer.from_pretrained('t5-small')\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Hugging Face for tokenization, sequencing and padding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDHfJH1avzcn"
      },
      "source": [
        "\n",
        "## 3.6 Embedding Strategies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWZX-eZellc5"
      },
      "source": [
        "# 4 Training and Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh6QKCnFK2ul"
      },
      "source": [
        "\n",
        "1. **PyTorch and Hugging Face Transformers**\n",
        "   - **Pros:** PyTorch offers dynamic computation graphs that are intuitive for RAG model development. Hugging Face's Transformers library provides easy access to pre-trained models and tokenizers, facilitating both training and evaluation with extensive support for RAG architectures.\n",
        "   - **Cons:** While highly flexible, this combination might require a steep learning curve for those not familiar with PyTorch or the Transformers library.\n",
        "\n",
        "2. **TensorFlow and T5**\n",
        "   - **Pros:** TensorFlow provides robust tools for model development and deployment, with T5 being a versatile model for text-to-text tasks, adaptable for RAG purposes. TensorFlow's extensive ecosystem includes TensorBoard for monitoring training processes.\n",
        "   - **Cons:** TensorFlow's static computation graph can be less intuitive than PyTorch's dynamic graphs. T5's text-to-text format might require additional preprocessing steps.\n",
        "\n",
        "3. **JAX and Flax/Haiku**\n",
        "   - **Pros:** JAX offers accelerated NumPy operations and automatic differentiation, making it efficient for large-scale model training. Flax and Haiku provide neural network libraries for JAX, supporting complex RAG model architectures.\n",
        "   - **Cons:** JAX's ecosystem is less mature, with fewer pre-trained models and community resources available compared to PyTorch and TensorFlow. This can make development and troubleshooting more challenging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ1Ha13mpeGE"
      },
      "source": [
        "## 4.1 Hyperparameter Tuning\n",
        "\n",
        "4.1 Hyperparameter Tuning\n",
        "Tuning Strategy: Outline your strategy for hyperparameter tuning, including the tools or techniques (like grid search or random search) you plan to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNXeUAspgU4"
      },
      "source": [
        "## 4.2 Model Evaluation\n",
        "\n",
        "Evaluation Metrics: Detail the metrics you will use to evaluate your model, such as F1 score, precision, recall, and explain why each is important for your project’s success.\n",
        "\n",
        "\n",
        "The project's success will be assessed based on the accuracy and speed of responses generated by the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRp4JbC7KyX5"
      },
      "source": [
        "\n",
        "1. **PyTorch and Hugging Face Transformers**\n",
        "   - **Pros:** PyTorch offers dynamic computation graphs that are intuitive for RAG model development. Hugging Face's Transformers library provides easy access to pre-trained models and tokenizers, facilitating both training and evaluation with extensive support for RAG architectures.\n",
        "   - **Cons:** While highly flexible, this combination might require a steep learning curve for those not familiar with PyTorch or the Transformers library.\n",
        "\n",
        "2. **TensorFlow and T5**\n",
        "   - **Pros:** TensorFlow provides robust tools for model development and deployment, with T5 being a versatile model for text-to-text tasks, adaptable for RAG purposes. TensorFlow's extensive ecosystem includes TensorBoard for monitoring training processes.\n",
        "   - **Cons:** TensorFlow's static computation graph can be less intuitive than PyTorch's dynamic graphs. T5's text-to-text format might require additional preprocessing steps.\n",
        "\n",
        "3. **JAX and Flax/Haiku**\n",
        "   - **Pros:** JAX offers accelerated NumPy operations and automatic differentiation, making it efficient for large-scale model training. Flax and Haiku provide neural network libraries for JAX, supporting complex RAG model architectures.\n",
        "   - **Cons:** JAX's ecosystem is less mature, with fewer pre-trained models and community resources available compared to PyTorch and TensorFlow. This can make development and troubleshooting more challenging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxRaOdwlrWY"
      },
      "source": [
        "# 5 Results and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGiqPFHIpsn6"
      },
      "source": [
        "## 5.1 Experimental Results\n",
        "5.1 Experimental Results\n",
        "Visualizations: Include charts or graphs that visualize the results, such as learning curves or performance benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56b36gepvMG"
      },
      "source": [
        "## 5.2 Performance Analysis\n",
        "\n",
        "5.2 Performance Analysis\n",
        "Comparative Analysis: If applicable, compare the performance of your model against baseline models or previous benchmarks in similar tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 6 Deployment and Integration\n",
        "6.1 Deployment Tools\n",
        "Deployment Plan: Provide a step-by-step plan for deploying the model, including any cloud resources or services used.\n",
        "6.2 Integration Strategies\n",
        "API Specifications: If your model will be accessed via an API, provide the API specifications including endpoint descriptions, request format, and response format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "## 6.1 Deployment Tools\n",
        "# Deployment and Serving Infrastructure Selection\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.\n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n",
        "## Decision\n",
        "For deploying a RAG model, especially within an academic or portfolio context where ease of use, accessibility, and cost-effectiveness are key considerations, Hugging Face Spaces is highly recommended. It allows you to quickly deploy your models with minimal setup and offers a user-friendly platform for showcasing your work to a wide audience. For projects that might evolve into more scalable or commercial applications, starting with Docker for containerization and then moving to a Kubernetes-based deployment as needs grow could be a strategic approach. This path provides a balance between initial simplicity and long-term scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRe4qbx5p1nj"
      },
      "source": [
        "## 6.2 Integration Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KGSPx5olwxJ"
      },
      "source": [
        "# 7 Reflection and Learning\n",
        "\n",
        "7.1 Challenges Faced\n",
        "Problem-Solving Approaches: Discuss specific problems you anticipate and outline the approaches you plan to take to solve them.\n",
        "7.2 Lessons Learned\n",
        "Documentation of Insights: Plan to document insights and lessons learned throughout the project to guide future projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWoXGpWRp53s"
      },
      "source": [
        "## 7.1 Challenges Faced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmm_IZYBp7Wr"
      },
      "source": [
        "## 7.2 Lessons Learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cXEIxsnly0H"
      },
      "source": [
        "# 8 Future Work and Improvements\n",
        "\n",
        "8.1 Potential Enhancements\n",
        "Technology Upgrades: Identify areas for technological upgrades or enhancements, like implementing more advanced NLP techniques or exploring newer models.\n",
        "8.2 Areas for Further Research\n",
        "Extended Applications: Suggest areas for extending the application of your project to other types of vehicles or different domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9bwbXcuqADQ"
      },
      "source": [
        "## 8.1 Potential Enhancements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 3.3 Data Storage and Database\n",
        "\n",
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### 3.3.1 Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "\n",
        "### 3.3.2 Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lOHxB-hqB1n"
      },
      "source": [
        "## 8.2 Areas for Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 Code"
      ],
      "metadata": {
        "id": "HfNsp0sI-n_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step # | Step Name                 | Step Functionality                                | Step Input File Name                      | Step Output File Name                         |\n",
        "|--------|---------------------------|---------------------------------------------------|-------------------------------------------|-----------------------------------------------|\n",
        "| 9      | Prepare Enviornment            | Load libraries and methods                  | None                                      | None\n",
        "| 1      | Create Corpus             | Fetch and process forum threads                  | None                                      | e9_forum_corpus_dirty.csv                     |\n",
        "| 2      | Clean Corpus              | Clean and preprocess forum data                  | e9_forum_corpus_dirty.csv                 | e9_forum_corpus_clean.csv                     |\n",
        "| 3      | Summarize Corpus          | Summarize forum data using T5 model              | e9_forum_corpus_clean.csv                 | e9_forum_corpus_summarized.csv                |\n",
        "| 4      | Create QA Schema          | Transform summarized data into QA format         | e9_forum_corpus_summarized.csv            | e9_forum_corpus_qa.csv                        |\n",
        "| 5      | Tokenize and Embed        | Tokenize and embed QA data using T5 model        | e9_forum_corpus_qa.csv                    | e9_forum_corpus_tok.csv                       |\n",
        "| 6      | Generate FAISS Embeddings | Generate and save FAISS embeddings               | e9_forum_corpus_tok.csv                   | question_embeddings_t5.npy, answer_embeddings_t5.npy |                         |\n",
        "| 7      | Save Corpus to Snowflake  | Save cleaned and processed corpus to Snowflake   | e9_forum_corpus_clean.csv                 | Data saved to Snowflake database   \n",
        "| 8      | Query Processing and Search | Process and search queries using FAISS index     | faiss_index_t5.index, e9_forum_corpus_tok.csv | None (process queries and display results)    |\n",
        "           |\n"
      ],
      "metadata": {
        "id": "pW3gq6C4C8_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0: Prepare Enviornment"
      ],
      "metadata": {
        "id": "5MswvEFq-3in"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8RFftacbJ5S",
        "outputId": "a26d83c7-8b4f-4d5c-d9f4-950d5eef9061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.7)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.12.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.14.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.2.2)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n",
            "Installing collected packages: asn1crypto, tomlkit, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.10.1 tomlkit-0.12.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=04c3cfe1633fe5348e35a36a7049e4ecc5e889e00c945b0fb3e027739009c102\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Prepare Enviornment\n",
        "# Install libraries and methods\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Suppress DeprecationWarnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# transformers\n",
        "try:\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "except ImportError:\n",
        "    !pip install transformers\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "\n",
        "# gensim\n",
        "try:\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "except ImportError:\n",
        "    !pip install gensim\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "# sumy\n",
        "try:\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "except ImportError:\n",
        "    !pip install sumy\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "\n",
        "# datasets\n",
        "try:\n",
        "    from datasets import load_metric\n",
        "except ImportError:\n",
        "    !pip install datasets\n",
        "    from datasets import load_metric\n",
        "\n",
        "# evaluate\n",
        "try:\n",
        "    import evaluate\n",
        "except ImportError:\n",
        "    !pip install evaluate\n",
        "    import evaluate\n",
        "\n",
        "# pyspellchecker\n",
        "try:\n",
        "    from spellchecker import SpellChecker\n",
        "except ImportError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install pyspellchecker\n",
        "    from spellchecker import SpellChecker\n",
        "\n",
        "# faiss\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    !pip install faiss-cpu\n",
        "    import faiss\n",
        "\n",
        "# snowflake.connector\n",
        "try:\n",
        "    import snowflake.connector\n",
        "except ImportError:\n",
        "    !pip install snowflake-connector-python\n",
        "    import snowflake.connector\n",
        "\n",
        "# pandas\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    !pip install pandas\n",
        "    import pandas as pd\n",
        "\n",
        "# requests\n",
        "try:\n",
        "    import requests\n",
        "except ImportError:\n",
        "    !pip install requests\n",
        "    import requests\n",
        "\n",
        "# BeautifulSoup\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "except ImportError:\n",
        "    !pip install beautifulsoup4\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "# nltk\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "except ImportError:\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# re\n",
        "try:\n",
        "    import re\n",
        "except ImportError:\n",
        "    !pip install re\n",
        "    import re\n",
        "\n",
        "# langdetect\n",
        "try:\n",
        "    from langdetect import detect\n",
        "except ImportError:\n",
        "    !pip install langdetect\n",
        "    from langdetect import detect\n",
        "\n",
        "# torch\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    !pip install torch\n",
        "    import torch\n",
        "\n",
        "# numpy\n",
        "try:\n",
        "    import numpy as np\n",
        "except ImportError:\n",
        "    !pip install numpy\n",
        "    import numpy as np\n",
        "\n",
        "# pyLDAvis\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError:\n",
        "    !pip install pyLDAvis\n",
        "    import pyLDAvis\n",
        "\n",
        "# pickle\n",
        "try:\n",
        "    import pickle\n",
        "except ImportError:\n",
        "    !pip install pickle\n",
        "    import pickle\n",
        "\n",
        "# sklearn\n",
        "try:\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except ImportError:\n",
        "    !pip install scikit-learn\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Other necessary imports\n",
        "import time\n",
        "import itertools\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1a Create corpus"
      ],
      "metadata": {
        "id": "5r9iMu3F_DkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y75VMgRttCo",
        "outputId": "0693a73e-08a5-48a7-937f-ce8924f7d4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 0\n",
            "Processing additional 1 thread(s)\n",
            "Ending with thread_id 1\n",
            "Output saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_dirty.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 1 Create Corpus\n",
        "# Fetch and process forum threads\n",
        "# Corpus created in LDA notebook can be used.\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "def forum_thread_ids():\n",
        "    threads = 1  # Set the number of incremental threads to process here\n",
        "\n",
        "    file_path = os.path.join(BASE_PATH, 'e9_forum_thread_ids.csv')\n",
        "\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(e9_forum_thread_ids['thread_id'].iloc[-1])\n",
        "    else:\n",
        "        e9_forum_thread_ids = pd.DataFrame(columns=['thread_id'])\n",
        "        last_thread_id = 0\n",
        "\n",
        "    next_thread_id = last_thread_id + 1\n",
        "    new_urls = [{'thread_id': thread_id} for thread_id in range(next_thread_id, next_thread_id + threads)]\n",
        "\n",
        "    new_df = pd.DataFrame(new_urls)\n",
        "    e9_forum_thread_ids = pd.concat([e9_forum_thread_ids, new_df], ignore_index=True)\n",
        "    e9_forum_thread_ids.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Starting with thread_id {last_thread_id}\")\n",
        "    print(f\"Processing additional {threads} thread(s)\")\n",
        "    print(f\"Ending with thread_id {next_thread_id + threads - 1}\")\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def forum_thread_url(df):\n",
        "    if df.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "\n",
        "    df.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_url.csv'), index=False)\n",
        "    return df\n",
        "\n",
        "def forum_thread_first_post(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        post_content = first_post.get_text(strip=True) if first_post else \"No content found\"\n",
        "        data.append({'thread_id': thread_id, 'thread_first_post': post_content})\n",
        "\n",
        "    forum_first_post = pd.DataFrame(data)\n",
        "    forum_first_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_first_post.csv'), index=False)\n",
        "    return forum_first_post\n",
        "\n",
        "def forum_thread_all_post(df):\n",
        "    post_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "            post_data.append({'thread_id': row['thread_id'], 'post_raw': content})\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "    e9_forum_posts['thread_all_posts'] = e9_forum_posts['post_raw'].astype(str)\n",
        "    e9_forum_thread_all_post = e9_forum_posts.groupby('thread_id')['thread_all_posts'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "    e9_forum_thread_all_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_all_post.csv'), index=False)\n",
        "    return e9_forum_thread_all_post\n",
        "\n",
        "def forum_corpus(e9_forum_thread_url, e9_forum_thread_first_post, e9_forum_thread_all_post):\n",
        "    agg_df_1 = pd.merge(e9_forum_thread_url, e9_forum_thread_first_post, on='thread_id', how='left')\n",
        "    agg_df_2 = pd.merge(agg_df_1, e9_forum_thread_all_post, on='thread_id', how='left')\n",
        "\n",
        "    e9_forum_corpus = agg_df_2.dropna()\n",
        "    corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "    if os.path.exists(corpus_path) and os.path.getsize(corpus_path) > 0:\n",
        "        existing_corpus = pd.read_csv(corpus_path)\n",
        "        e9_forum_corpus = pd.concat([existing_corpus, e9_forum_corpus]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    e9_forum_corpus.columns = e9_forum_corpus.columns.str.upper()\n",
        "    e9_forum_corpus.to_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv'), index=False)\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    e9_forum_thread_ids = forum_thread_ids()\n",
        "    e9_forum_thread_url_df = forum_thread_url(e9_forum_thread_ids)\n",
        "    e9_forum_thread_first_post_df = forum_thread_first_post(e9_forum_thread_url_df)\n",
        "    e9_forum_thread_all_post_df = forum_thread_all_post(e9_forum_thread_url_df)\n",
        "    e9_forum_corpus_df = forum_corpus(e9_forum_thread_url_df, e9_forum_thread_first_post_df, e9_forum_thread_all_post_df)\n",
        "    print(f\"Output saved to {os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1b Ingest corpus from LDA workbook"
      ],
      "metadata": {
        "id": "65J-_EM5DTlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data here is from LDA workbook stored in Snowflake\n",
        "\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT'),\n",
        "    )\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT THREAD_TITLE, THREAD_FIRST_POST FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS_DIRTY\";\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    # Print the count of records retrieved\n",
        "    print(f\"Number of records retrieved: {len(e9_forum_corpus)}\")\n",
        "    return e9_forum_corpus\n",
        "\n",
        "# Main sequence\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "# Load credentials\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Fetch data from Snowflake and print the count of records retrieved\n",
        "e9_forum_corpus = fetch_data_from_snowflake()\n"
      ],
      "metadata": {
        "id": "dJ2e5CwqDYl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9009e7-8eb6-40f1-aa8f-3ff2a75b4574"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records retrieved: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 Clean Corpus"
      ],
      "metadata": {
        "id": "uQRbwaqI_Kc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 Clean Corpus\n",
        "# Clean and preprocess forum data\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT'),\n",
        "    )\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS\n",
        "    FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS_DIRTY\";\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    # Print the count of records retrieved\n",
        "    print(f\"Number of records retrieved: {len(e9_forum_corpus)}\")\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def remove_urls(df):\n",
        "    \"\"\"Removes URLs from the text.\"\"\"\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: url_pattern.sub(r'', str(text)))\n",
        "    return df\n",
        "\n",
        "def alpha_numeric(df):\n",
        "    \"\"\"Removes non-alphanumeric characters and unwanted patterns from text.\"\"\"\n",
        "    pattern_email = re.compile(r'\\S*@\\S*\\s?')\n",
        "    pattern_non_alpha = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: pattern_non_alpha.sub('', pattern_email.sub('', str(text))))\n",
        "        df[column] = df[column].apply(lambda text: re.sub(r'\\s+', ' ', text).strip())  # Remove extra spaces\n",
        "    return df\n",
        "\n",
        "def remove_stop_words(df):\n",
        "    \"\"\"Removes stop words from the text.\"\"\"\n",
        "    stop_words_set = set(stopwords.words('english')).union({'car', 'csi', 'cs', 'csl', 'e9', 'coupe', 'http', 'https', 'www', 'ebay', 'bmw', 'html'})\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in stop_words_set and len(word) > 2]))\n",
        "    return df\n",
        "\n",
        "def tokenize_and_lemmatize(df):\n",
        "    \"\"\"Tokenizes and lemmatizes the text in specified columns.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]))\n",
        "    return df\n",
        "\n",
        "def spell_check(df):\n",
        "    \"\"\"Corrects spelling errors in the text with caching.\"\"\"\n",
        "    spell = SpellChecker()\n",
        "    cache = {}\n",
        "\n",
        "    def correct_word(word):\n",
        "        if word in cache:\n",
        "            return cache[word]\n",
        "        else:\n",
        "            correction = spell.correction(word) or word\n",
        "            cache[word] = correction\n",
        "            return correction\n",
        "\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([correct_word(word) for word in text.split()]))\n",
        "    return df\n",
        "\n",
        "def clean_nan_values(df):\n",
        "    \"\"\"Removes or replaces NaN values in the dataset and converts all entries to strings.\"\"\"\n",
        "    df.fillna('', inplace=True)\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].astype(str)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data processing pipeline.\"\"\"\n",
        "    # Load credentials\n",
        "    path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "    load_credentials(path_to_credentials)\n",
        "\n",
        "    # Fetch data from Snowflake\n",
        "    e9_forum_corpus = fetch_data_from_snowflake()\n",
        "\n",
        "    # Process the data\n",
        "    df = remove_urls(e9_forum_corpus)\n",
        "    df = alpha_numeric(df)\n",
        "    df = remove_stop_words(df)\n",
        "    df = tokenize_and_lemmatize(df)\n",
        "    df = spell_check(df)  # Apply spell check after tokenization and lemmatization\n",
        "    df = clean_nan_values(df)  # Final NaN cleaning and type conversion step\n",
        "    df.columns = df.columns.str.upper()  # Convert column names to uppercase\n",
        "\n",
        "    # Save the cleaned data\n",
        "    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_clean.csv'\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Cleaned data saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO22BY4_rScA",
        "outputId": "5eebcbd8-ff50-4253-8390-1a4969a6578a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records retrieved: 10\n",
            "Cleaned data saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 Summarize Corpus"
      ],
      "metadata": {
        "id": "u3vObk0s_O8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 Summarize Corpus\n",
        "# Summarize forum data using T5 model\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Load e9_forum_corpus_clean DataFrame from the CSV\n",
        "e9_forum_corpus_clean = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv'))\n",
        "\n",
        "# Load the existing summarized corpus if it exists, otherwise create it\n",
        "summarized_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv')\n",
        "e9_forum_corpus_summarized = pd.read_csv(summarized_corpus_path) if os.path.exists(summarized_corpus_path) and os.path.getsize(summarized_corpus_path) > 0 else pd.DataFrame(columns=e9_forum_corpus_clean.columns)\n",
        "\n",
        "# Calculate the starting THREAD_ID of the summarized corpus\n",
        "starting_thread_id = e9_forum_corpus_summarized['THREAD_ID'].max() if not e9_forum_corpus_summarized.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = e9_forum_corpus_clean[~e9_forum_corpus_clean['THREAD_ID'].isin(e9_forum_corpus_summarized['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def T5_summarize(text):\n",
        "    \"\"\"Summarization using T5.\"\"\"\n",
        "    try:\n",
        "        if text.strip() == \"\":\n",
        "            return text\n",
        "\n",
        "        unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            unformatted_text,\n",
        "            max_length=900,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=50,\n",
        "            min_length=10,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary if summary else text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def main():\n",
        "    # Check if 'THREAD_ALL_POSTS' column exists in new entries\n",
        "    if 'THREAD_ALL_POSTS' in new_entries.columns:\n",
        "        unique_texts = new_entries['THREAD_ALL_POSTS'].drop_duplicates()\n",
        "        summaries = unique_texts.apply(T5_summarize)\n",
        "        summary_map = dict(zip(unique_texts, summaries))\n",
        "        new_entries.loc[:, 'Summarized_Thread'] = new_entries['THREAD_ALL_POSTS'].map(summary_map)\n",
        "\n",
        "        # Append the new summarized data to the existing summarized corpus\n",
        "        updated_summarized_corpus = pd.concat([e9_forum_corpus_summarized, new_entries], ignore_index=True)\n",
        "\n",
        "        # Save the results with the new summarized column\n",
        "        updated_summarized_corpus.to_csv(summarized_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Summarization completed and saved to {summarized_corpus_path}\")\n",
        "    else:\n",
        "        print(\"Error: Column 'THREAD_ALL_POSTS' does not exist in the dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aftZyM87vbS_",
        "outputId": "dd4fe13e-495f-4852-a273-2c7c8164b7bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 0\n",
            "Processing additional 10 thread(s)\n",
            "Ending with thread_id 10\n",
            "Summarization completed and saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_summarized.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 Create QA Schema"
      ],
      "metadata": {
        "id": "TTMSFr3w_VUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2n-Pg-XSTa-",
        "outputId": "3e25e794-93cc-4ce2-e3e7-172b27a336f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 0\n",
            "Processing additional 10 thread(s)\n",
            "Ending with thread_id 10\n",
            "Output saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_qa.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Step 4 Create QA Schema\n",
        "# Transform summarized data into QA format\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the dataset from Step 2\n",
        "df_summarized = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv'))\n",
        "\n",
        "# Load the existing QA corpus if it exists\n",
        "qa_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv')\n",
        "df_qa = pd.read_csv(qa_corpus_path) if os.path.exists(qa_corpus_path) and os.path.getsize(qa_corpus_path) > 0 else pd.DataFrame(columns=['THREAD_ID', 'QUESTION', 'ANSWER'])\n",
        "\n",
        "# Calculate the starting THREAD_ID of the QA corpus\n",
        "starting_thread_id = df_qa['THREAD_ID'].max() if not df_qa.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_summarized[~df_summarized['THREAD_ID'].isin(df_qa['THREAD_ID'])]\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def create_qa_schema(df):\n",
        "    \"\"\"Creates a QA schema by renaming and dropping specific columns.\"\"\"\n",
        "    df.rename(columns={'Summarized_Thread': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "    df.drop(['THREAD_TITLE', 'THREAD_ALL_POSTS'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        # Process the new entries to create QA schema\n",
        "        df_qa_new = create_qa_schema(new_entries.dropna())\n",
        "\n",
        "        # Append the new QA data to the existing QA corpus\n",
        "        updated_qa_corpus = pd.concat([df_qa, df_qa_new], ignore_index=True)\n",
        "\n",
        "        # Save the updated QA corpus\n",
        "        updated_qa_corpus.to_csv(qa_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Output saved to {qa_corpus_path}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 Tokenize and Embed"
      ],
      "metadata": {
        "id": "-3RpQ4jL_oEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsmN8LxHRHDp",
        "outputId": "3ace599a-ff05-4456-8db6-57586490be39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 0\n",
            "Processing additional 10 thread(s)\n",
            "Ending with thread_id 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding completed and saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_tok.csv\n"
          ]
        }
      ],
      "source": [
        "## Step 5 Tokenize and Embed\n",
        "# Tokenize and embed QA data using T5 mode\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the DataFrame from your CSV file\n",
        "df_qa = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv'))\n",
        "\n",
        "# Load the existing tokenized and embedded data if it exists, otherwise create an empty DataFrame\n",
        "embedded_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_tok.csv')\n",
        "df_embedded = pd.read_csv(embedded_corpus_path) if os.path.exists(embedded_corpus_path) and os.path.getsize(embedded_corpus_path) > 0 else pd.DataFrame(columns=df_qa.columns)\n",
        "\n",
        "# Calculate the starting THREAD_ID of the embedded corpus\n",
        "starting_thread_id = df_embedded['THREAD_ID'].max() if not df_embedded.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_qa[~df_qa['THREAD_ID'].isin(df_embedded['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to embed text using T5 encoder model\n",
        "def embed_text(tokens):\n",
        "    inputs = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "def process_new_entries(entries):\n",
        "    entries[\"Question_Tokens\"] = entries[\"QUESTION\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Answer_Tokens\"] = entries[\"ANSWER\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Question_Embeddings\"] = entries[\"Question_Tokens\"].apply(embed_text)\n",
        "    entries[\"Answer_Embeddings\"] = entries[\"Answer_Tokens\"].apply(embed_text)\n",
        "    return entries\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        processed_entries = process_new_entries(new_entries)\n",
        "\n",
        "        # Append the new processed data to the existing embedded corpus\n",
        "        updated_embedded_corpus = pd.concat([df_embedded, processed_entries], ignore_index=True)\n",
        "\n",
        "        # Save the updated corpus\n",
        "        updated_embedded_corpus.to_csv(embedded_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Embedding completed and saved to {embedded_corpus_path}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 FAISS"
      ],
      "metadata": {
        "id": "2pUg18ZY_srO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 FAISS\n",
        "# Generate FAISS Embeddings and Index\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the DataFrame from your CSV file\n",
        "df_tok = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_tok.csv'))\n",
        "\n",
        "# Load the existing FAISS corpus if it exists, otherwise create an empty DataFrame\n",
        "faiss_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv')\n",
        "df_faiss = pd.read_csv(faiss_corpus_path) if os.path.exists(faiss_corpus_path) and os.path.getsize(faiss_corpus_path) > 0 else pd.DataFrame(columns=df_tok.columns)\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_tok[~df_tok['THREAD_ID'].isin(df_faiss['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate the starting THREAD_ID of the FAISS corpus\n",
        "starting_thread_id = df_faiss['THREAD_ID'].max() if not df_faiss.empty else 0\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to embed text using T5 encoder model\n",
        "def embed_text(tokens):\n",
        "    inputs = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "def process_new_entries(entries):\n",
        "    entries[\"Question_Tokens\"] = entries[\"QUESTION\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Answer_Tokens\"] = entries[\"ANSWER\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Question_Embeddings\"] = entries[\"Question_Tokens\"].apply(embed_text)\n",
        "    entries[\"Answer_Embeddings\"] = entries[\"Answer_Tokens\"].apply(embed_text)\n",
        "    return entries\n",
        "\n",
        "def filter_embeddings(embeddings_list, expected_shape):\n",
        "    \"\"\"Filter out embeddings that do not match the expected shape.\"\"\"\n",
        "    return [embedding for embedding in embeddings_list if len(embedding) == expected_shape]\n",
        "\n",
        "def build_faiss_index(embeddings, index_path):\n",
        "    embeddings_np = np.array(embeddings).astype('float32')  # Convert to NumPy array of type float32\n",
        "    d = embeddings_np.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(d)  # Build the index\n",
        "    index.add(embeddings_np)  # Add vectors to the index\n",
        "\n",
        "    # Save the index\n",
        "    faiss.write_index(index, index_path)\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        processed_entries = process_new_entries(new_entries)\n",
        "\n",
        "        # Append the new processed data to the existing FAISS corpus\n",
        "        updated_faiss_corpus = pd.concat([df_faiss, processed_entries], ignore_index=True)\n",
        "\n",
        "        # Save the updated FAISS corpus\n",
        "        updated_faiss_corpus.to_csv(faiss_corpus_path, index=False)\n",
        "\n",
        "        # Ensure all embeddings are of the same shape before saving\n",
        "        question_embeddings_list = updated_faiss_corpus[\"Question_Embeddings\"].to_list()\n",
        "        answer_embeddings_list = updated_faiss_corpus[\"Answer_Embeddings\"].to_list()\n",
        "\n",
        "        expected_shape = 512  # Expected embedding size (512 for T5 model)\n",
        "\n",
        "        question_embeddings_filtered = filter_embeddings(question_embeddings_list, expected_shape)\n",
        "        answer_embeddings_filtered = filter_embeddings(answer_embeddings_list, expected_shape)\n",
        "\n",
        "        question_embeddings = np.array(question_embeddings_filtered)\n",
        "        answer_embeddings = np.array(answer_embeddings_filtered)\n",
        "\n",
        "        np.save(os.path.join(BASE_PATH, 'question_embeddings_t5.npy'), question_embeddings)\n",
        "        np.save(os.path.join(BASE_PATH, 'answer_embeddings_t5.npy'), answer_embeddings)\n",
        "\n",
        "        # Build and save the FAISS index using the new answer embeddings\n",
        "        faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "        index = build_faiss_index(answer_embeddings, faiss_index_path)\n",
        "\n",
        "        print(f\"FAISS index has been rebuilt and saved to {faiss_index_path}\")\n",
        "        print(f\"Embeddings have been generated and saved to {BASE_PATH}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK6L99nQKA_-",
        "outputId": "644ddd09-17b2-4439-a08c-eaa091623390"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 10\n",
            "Processing additional 0 thread(s)\n",
            "Ending with thread_id 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new entries to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 Save corpus"
      ],
      "metadata": {
        "id": "vlMMPDZc_x5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Save corpus\n",
        "# Create and populate data in Snowflake\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the e9_forum_corpus DataFrame from the CSV file\n",
        "e9_forum_corpus = pd.read_csv(BASE_PATH + 'e9_forum_corpus_tok.csv')\n",
        "\n",
        "def load_credentials(credentials_path):\n",
        "    \"\"\"Load Snowflake credentials from a file and set them as environment variables.\"\"\"\n",
        "    with open(credentials_path, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "def create_db_and_schema(cur):\n",
        "    \"\"\"Create the database and schema in Snowflake.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        print(\"Database and schema created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database and schema: {e}\")\n",
        "\n",
        "def create_table_if_not_exists(cur):\n",
        "    \"\"\"Create the e9_forum_corpus table if it does not exist.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            THREAD_ID NUMBER(38,0),\n",
        "            QUESTION VARCHAR(16777216),\n",
        "            ANSWER VARCHAR(16777216),\n",
        "            Question_Tokens STRING,\n",
        "            Answer_Tokens STRING,\n",
        "            Question_Embeddings STRING,\n",
        "            Answer_Embeddings STRING\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table: {e}\")\n",
        "\n",
        "def fetch_existing_thread_ids(cur):\n",
        "    \"\"\"Fetch existing THREAD_IDs from the e9_forum_corpus table.\"\"\"\n",
        "    query = \"SELECT THREAD_ID FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    cur.execute(query)\n",
        "    existing_ids = cur.fetch_pandas_all()\n",
        "    return existing_ids['THREAD_ID'].tolist()\n",
        "\n",
        "def insert_data_into_table(cur, df):\n",
        "    \"\"\"Insert data from the DataFrame into the e9_forum_corpus table.\"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        row = row.where(pd.notnull(row), None)\n",
        "        insert_command = f\"\"\"\n",
        "        INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "        (THREAD_ID, QUESTION, ANSWER, Question_Tokens, Answer_Tokens, Question_Embeddings, Answer_Embeddings)\n",
        "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cur.execute(insert_command, (\n",
        "                row['THREAD_ID'], row['QUESTION'], row['ANSWER'],\n",
        "                row['Question_Tokens'], row['Answer_Tokens'],\n",
        "                row['Question_Embeddings'], row['Answer_Embeddings']\n",
        "            ))\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting data: {e}\")\n",
        "\n",
        "def fetch_data_from_table(cur):\n",
        "    \"\"\"Fetch all data from the e9_forum_corpus table.\"\"\"\n",
        "    query = \"SELECT * FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    cur.execute(query)\n",
        "    return cur.fetch_pandas_all()\n",
        "\n",
        "def main():\n",
        "    # Load Snowflake credentials\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "\n",
        "    # Connect to Snowflake\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create the database, schema, and table if they don't exist\n",
        "    create_db_and_schema(cur)\n",
        "    create_table_if_not_exists(cur)\n",
        "\n",
        "    # Fetch existing thread IDs\n",
        "    existing_ids = fetch_existing_thread_ids(cur)\n",
        "\n",
        "    # Identify new entries\n",
        "    new_entries = e9_forum_corpus[~e9_forum_corpus['THREAD_ID'].isin(existing_ids)]\n",
        "\n",
        "    # Calculate the starting THREAD_ID of the Snowflake table\n",
        "    starting_thread_id = max(existing_ids) if existing_ids else 0\n",
        "\n",
        "    # Calculate ending_thread_id and threads_processed\n",
        "    ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "    threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "    print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "    print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "    print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "    # Insert only new entries into the table\n",
        "    insert_data_into_table(cur, new_entries)\n",
        "    conn.commit()\n",
        "    print(\"New data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "    # Fetch data from the table\n",
        "    e9_forum_corpus_df = fetch_data_from_table(cur)\n",
        "    print(\"Additional entries: \" + str(len(new_entries)))\n",
        "    print(\"Total entries: \" + str(len(e9_forum_corpus_df)))\n",
        "\n",
        "    # Close cursor and connection\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwL4Kst1McoD",
        "outputId": "46e88193-0c16-4d2d-bea7-23ed0ed3147c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n",
            "e9_forum_corpus table created successfully.\n",
            "Starting with thread_id 0\n",
            "Processing additional 10 thread(s)\n",
            "Ending with thread_id 10\n",
            "New data inserted into e9_forum_corpus table.\n",
            "Additional entries: 10\n",
            "Total entries: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7a: User entered Query Processing and Search\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "similarity_threshold = 0.7  # Set your threshold value here\n",
        "\n",
        "def load_credentials(credentials_path):\n",
        "    \"\"\"Load Snowflake credentials from a file and set them as environment variables.\"\"\"\n",
        "    with open(credentials_path, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    cur.execute(query)\n",
        "    answers = cur.fetch_pandas_all()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "# Process a new query\n",
        "def process_query(query, index, k=1):\n",
        "    # Generate query embedding\n",
        "    query_embedding = generate_query_embedding(query)\n",
        "\n",
        "    # Ensure the dimension matches\n",
        "    if query_embedding.shape[1] != index.d:\n",
        "        raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "    # Search FAISS index\n",
        "    similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=k)\n",
        "\n",
        "    if not similar_indices:\n",
        "        print(\"No valid matches found.\")\n",
        "        print(f\"Similarity Score: {similarity_scores[0]:.4f}\" if similarity_scores else \"No similarity score available.\")\n",
        "        return\n",
        "\n",
        "    # Fetch answers and embeddings from Snowflake\n",
        "    answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "    if not answers.empty:\n",
        "        for idx, score in zip(similar_indices, similarity_scores):\n",
        "            matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "            if len(matching_answers) > 0:\n",
        "                answer = matching_answers[0]\n",
        "                print(f\"Answer: {answer}\")\n",
        "                print(f\"Similarity Score: {score:.4f}\")\n",
        "                break\n",
        "    else:\n",
        "        print(\"No valid matches found above the similarity threshold.\")\n",
        "        print(f\"Similarity Score: {similarity_scores[0]:.4f}\")\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        query = input(\"Please enter your question: \")\n",
        "        process_query(query, index, k=3)\n",
        "        another_question = input(\"Do you have another question? (Y/N): \").strip().lower()\n",
        "        if another_question != 'y':\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLcEgzv_McqW",
        "outputId": "7f9501e3-f17f-40db-963a-348c814005a5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your question: air\n",
            "Answer: suck Ill let know go get back find bookshop isn't guy endorse strategy heartily help shed bit full moment window serious note think applies country Word mouth usually best way finding somebody good often hole wall place lot better shiny\n",
            "Similarity Score: 0.3604\n",
            "Do you have another question? (Y/N): N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7b: Query Processing and Search of LDA derived topics\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(credentials_path):\n",
        "    \"\"\"Load Snowflake credentials from a file and set them as environment variables.\"\"\"\n",
        "    with open(credentials_path, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ir9PZxznFQ",
        "outputId": "496f204b-2c03-4cf4-e14e-53e460edde3e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved.\n",
            "Representative Sentence: My Maiden Voyage.... and I ALMOST made it home..........After a fevered weekend of buttonin' up (getting all the lights to work & be legal, had some Plexi to install, plus discovering some PO had the disty in 180 degrees backwards...argh!) and a year and a half of work on my 2800cs project (dual purpose car w/ a complete re-wire), I got to enjoy a blissful voyage from Monterey to San francisco (about 100 miles), watching the sunset, and the fog come over the hills, coming up via HWY 280....very fine day indeed!!........until I hit FOG like I hadn't scene in a LONG TIME on HWY 35 above Pacifica. We ran out of time and could not hook up the wipers (have new switch in hand), so I was reduced to a crawl along with everyone else up there, driving blind, all cruising about 15-20mph on 50mph Skyline HWY 35...a bit stressful, as the CS was starting to act up a bit, not having been driven 100 miles+ in years.So as I crawled along looking for the Headless Horsemen to come outta nowhere, my sweetly aligned, complete new track suspension suddenly darted to the right and jumped me into the slow lane. Thankfully I was traveling slower than those w/ wipers and didn't bump anyone. Spooked and w/ steering wacked, I was suddenly thankful for the fog and that it didn't happen 20 minutes sooner doing 80 on 280........... Thankfully I was able to nurse my grey primered car (ultimate fog camo) out of the left lane (sitting duck in the fog!) and into a mall's parking lot where it is now. After being 'rescued by the wife and retrieving some tools from home and returning to my beached baby,  my sudden dart in movement was caused by a snapped right front axel/spindel caused by a new bearring gone bad and it decided to SEIZE with no noise or warning of going out and snapped my spindel!!!  My wheel was held on by the caliper. (BTW: yes I adjusted the castle nut snug and then backed it off...wheel spun well after new bearring install)Here's where I need some help, I need to source a right front axle spindle (don't need a whole strut) ASAP!! Any leads greatly appreciated so I can get my baby home or to a shop with a sourced part greatly appreciated. Its less than ten miles away (so frustrating my friends), if it wasn't foggy, I could probably see my house from where it is...............Hey TJ, sounds like your drive ended better than mine..... :wink:tired.....gotta pass out now........-shanon'70 2800cs (BTW: it sounds fantastic!!)\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 14.817038536071777\n",
            "\n",
            "Representative Sentence: My Maiden Voyage.... and I ALMOST made it home..........After a fevered weekend of buttonin' up (getting all the lights to work & be legal, had some Plexi to install, plus discovering some PO had the disty in 180 degrees backwards...argh!) and a year and a half of work on my 2800cs project (dual purpose car w/ a complete re-wire), I got to enjoy a blissful voyage from Monterey to San francisco (about 100 miles), watching the sunset, and the fog come over the hills, coming up via HWY 280....very fine day indeed!!........until I hit FOG like I hadn't scene in a LONG TIME on HWY 35 above Pacifica. We ran out of time and could not hook up the wipers (have new switch in hand), so I was reduced to a crawl along with everyone else up there, driving blind, all cruising about 15-20mph on 50mph Skyline HWY 35...a bit stressful, as the CS was starting to act up a bit, not having been driven 100 miles+ in years.So as I crawled along looking for the Headless Horsemen to come outta nowhere, my sweetly aligned, complete new track suspension suddenly darted to the right and jumped me into the slow lane. Thankfully I was traveling slower than those w/ wipers and didn't bump anyone. Spooked and w/ steering wacked, I was suddenly thankful for the fog and that it didn't happen 20 minutes sooner doing 80 on 280........... Thankfully I was able to nurse my grey primered car (ultimate fog camo) out of the left lane (sitting duck in the fog!) and into a mall's parking lot where it is now. After being 'rescued by the wife and retrieving some tools from home and returning to my beached baby,  my sudden dart in movement was caused by a snapped right front axel/spindel caused by a new bearring gone bad and it decided to SEIZE with no noise or warning of going out and snapped my spindel!!!  My wheel was held on by the caliper. (BTW: yes I adjusted the castle nut snug and then backed it off...wheel spun well after new bearring install)Here's where I need some help, I need to source a right front axle spindle (don't need a whole strut) ASAP!! Any leads greatly appreciated so I can get my baby home or to a shop with a sourced part greatly appreciated. Its less than ten miles away (so frustrating my friends), if it wasn't foggy, I could probably see my house from where it is...............Hey TJ, sounds like your drive ended better than mine..... :wink:tired.....gotta pass out now........-shanon'70 2800cs (BTW: it sounds fantastic!!)\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 14.817038536071777\n",
            "\n",
            "Representative Sentence: One Step Forward, Three Steps Back (Loooong 3.5 update) Between work, kids, family, vacation, heat waves and more work, I actually found some time to devote to my coupe and get the 3.5 conversion one step closer to completion.  You may recall I’m using a custom injection system running off of a GM ECU.  Since my last report, I’ve finished hanging all the accessories on the motor and all of the engine electrical (starter, alternator, etc.).  Got the 265 back in, exhaust hooked up, etc.  That was easy.  Now the hard stuff.Since there’s no “out of the box” harness for the sensors and ECU I’m using, I decided I would build one and run it in the original locations starting over by the starter, running across the valve cover (picking up the injector and sensor wiring along the way) and then down through the frame rail into the passenger compartment.  I bought a universal harness – basically a bunch of pre-marked wires – intended for Megasquirt conversions and set to work.  Approximately FIFTEEN hours later, I ended up with a beautiful harness with all the connectors assembled using the correct professional crimper tools and pins – there are NO splices – I even used new connectors and pins for the 30+ wires into the ECU.  In the engine compartment, it’s a dead-ringer for the original – everything is shrink wrapped in incrementally larger sizes as more wires are added to the bundle – very close to the original harness construction.  I didn’t have enough wire to make it to the rear seat like the original D-Jet, so I drilled a small hole in the floor just below the firewall and brought the harness up through there.  The ECU, relays and fuse board are mounted right above the glove box for easy access.  So far, so good.  I was getting excited!That ended up being the easy part; the rest has been nothing short of challenging.  In the past, I always used a separate chip burner which worked flawlessly, but created a time consuming tuning process (collect data, pull chip, erase chip, burn chip, install chip, repeat).  This time, I am trying out some new hardware that lets me program the ECU on the fly while monitoring all the vital stats from the diagnostic port.  Good stuff!  But it wouldn’t work.  After several frustrating hours, I finally got it to work with some help from the seller – my fault, of course – something about bits and parity settings on my COM port.  Then I spent a few hours surfing the web to locate an appropriate .bin file to start programming with.  I found several, and made appropriate changes to the injector constants and engine size to at least get it to start.  The excitement is building.Saturday night.  Check all my wiring – run the fuel pump to prime the system – crank the motor over with no spark to get oil pumping through it – set the base timing as close as I can – fire up the laptop and connect it to the ECU – they are now communicating nicely – check everything again.  Finally, after 16 weeks of parts chasing all over town (and the web), research into every sensor, wire and connector, late nights in the garage playing with cams, pistons, bearings, gaskets, hoses, etc. – its finally the MOMENT OF TRUTH – I turn the key, and. . . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, pop, pfitzzzzz.Hmm.  Not what I had hoped for.  Check the timing, close enough.  Crank over, same result.  Try again, and again, and again.  Same results.  I went to bed.Next day after the usual errands, kids birthday parties, dinner, baths, bedtime stories, etc. I’m back in the garage.  Check the firing order.  Reset the timing.  Try again.  Same result.  I start thinking about it some more and suppose it could be the 180 degrees out of time.  That could be it!  Now, mind you, I really knew that’s NOT it since it installed the dang dizzy with the valve cover off and I KNOW cylinder no. 1 was at TDC on the compression stroke, but sometimes desperation has a way of twisting things in your head, so I tried that –Grrr, pop, grrr, pop, grrr, pop, grrr, pop.  Not even close.  Pack it in for the night.Monday at work.  I’m obsessed.   What could be the problem?  It must be running way too lean.  I gotta get home.  After the usual family obligations, I’m back in the garage at 9 p.m.  This time, I pull out one of my most useful diagnostic tools – the propane bottle.  Huh?  Yes, that’s right, propane bottle.  If you drill out the orifice from a small bernz-o-matic torch and attach a hose to the end, it makes a very useful tuning tool.  Its quite handy for finding vacuum leaks, but even better to figure out whether you’re running rich or lean.  If its worse with propane, you’re already rich.  If its better, then you’re lean.  Easy and intuitive.  I hooked it into the manifold and turned the knob.  Went to start it and. . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.WOW – progress!  I took off the bottle and jumped back into the computer programming.  My injector constant was set for 24lb/hr injectors (which is what I’m using) so I knocked it down to 19lb/hr to richen up the mixture.  I know this isn’t right, but I figure I can deal with it later after its running.  Same result as the propane.  After three or four tries, I can keep it sputtering just long enough to get to the throttle, but as soon as I try to gas it, it dies.  It’s late, I’m packing it in for the night.Two days in San Francisco on business kept me away from the coupe, but gave me time to think.  Why is it so lean?  Why won’t it run clean and rev?  I gotta get it running at 3k rpm with a good mixture to break in the cam.  There’s just too much time and money in it to screw it up now.  Hmmm.  Wait!  D-Jet runs at a lower fuel pressure and my injectors are spec’d at 3 bar!  That means I’m not getting nearly enough gas.  That’s gotta be it!  I got home late on Wednesday and was too tired to even look in the garage.Yesterday was a loooong day at work waiting to get home.  This time, I coerced my son into “helping” me in the garage before dinner since there’s no way I could wait any longer – I know this is it!  I adjusted the D-Jet regulator in all the way and ended up with 40 psi – close enough!  Then I adjusted the injector constant back where it belongs – checked everything twice – contact!Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.Arrrggghhhh!   Take a break, eat some dinner, put the kids to bed.  Back in the garage at 9 p.m.  I try going way lean on the injectors.  No improvement.  I try way rich.  Same result.  I try a completely different .bin file.  Even worse.  I enjoy a good challenge every now and then, but at this point I’m pretty frustrated.  Take a break.  Have a glass of wine.  Then, a flash of inspiration!  I’m running an L-Jet dizzy with a VR sensor which goes straight into an MSD 6-A.  Then the ECU takes its trigger off of the MSD tach signal.  When I set it all up the first time, I had the VR sensor set so the MSD would trigger on the leading edge as the star wheel in the distributor passes the magnet.  That made sense at the time, but maybe it really wants to see the trigger on the trailing edge. Maybe, just maybe, I have the VR sensor wires reversed. . . could it be that simple?A quick wire swap, injector constant set back to 24lb/hr, double check everything again, turn the key, and ------Vroooom!Its actually running!  I jump out and hit the throttle to keep it going and it actually catches!  Revs to 3k and stays there while I’m holding the throttle open!  Finally.  Its surging, so I know its running really lean, and I shut her down for now.  I’m going to swipe the wide-band O2 sensor off my Jeep and put the cooling system back on before I go any further.  But it runs – and it should run well after several hours with the WBO2 and the laptop.  What a relief!  So now I just need to find some time this weekend to break in the cam, re-torque the head bolts, experiment with the tuning and hopefully it’ll be back on the road next week.  Its been too long.  Unfortunately, there’s a brake job on my wife’s Grand Cherokee in line for Saturday morning, then my mother in law’s window regulator, then the exhaust on my brother in law’s 535i . . . I should just open a shop.Sorry for the long musings – but I had to share with people who may appreciate these trials and tribulations.  I tried to share my excitement with my wife and got a simple “that’s nice – when is it done?” in response.   I’ll get some pictures of the finished product posted when (should that be “if”?) its done.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 15.027259826660156\n",
            "\n",
            "Representative Sentence: One Step Forward, Three Steps Back (Loooong 3.5 update) Between work, kids, family, vacation, heat waves and more work, I actually found some time to devote to my coupe and get the 3.5 conversion one step closer to completion.  You may recall I’m using a custom injection system running off of a GM ECU.  Since my last report, I’ve finished hanging all the accessories on the motor and all of the engine electrical (starter, alternator, etc.).  Got the 265 back in, exhaust hooked up, etc.  That was easy.  Now the hard stuff.Since there’s no “out of the box” harness for the sensors and ECU I’m using, I decided I would build one and run it in the original locations starting over by the starter, running across the valve cover (picking up the injector and sensor wiring along the way) and then down through the frame rail into the passenger compartment.  I bought a universal harness – basically a bunch of pre-marked wires – intended for Megasquirt conversions and set to work.  Approximately FIFTEEN hours later, I ended up with a beautiful harness with all the connectors assembled using the correct professional crimper tools and pins – there are NO splices – I even used new connectors and pins for the 30+ wires into the ECU.  In the engine compartment, it’s a dead-ringer for the original – everything is shrink wrapped in incrementally larger sizes as more wires are added to the bundle – very close to the original harness construction.  I didn’t have enough wire to make it to the rear seat like the original D-Jet, so I drilled a small hole in the floor just below the firewall and brought the harness up through there.  The ECU, relays and fuse board are mounted right above the glove box for easy access.  So far, so good.  I was getting excited!That ended up being the easy part; the rest has been nothing short of challenging.  In the past, I always used a separate chip burner which worked flawlessly, but created a time consuming tuning process (collect data, pull chip, erase chip, burn chip, install chip, repeat).  This time, I am trying out some new hardware that lets me program the ECU on the fly while monitoring all the vital stats from the diagnostic port.  Good stuff!  But it wouldn’t work.  After several frustrating hours, I finally got it to work with some help from the seller – my fault, of course – something about bits and parity settings on my COM port.  Then I spent a few hours surfing the web to locate an appropriate .bin file to start programming with.  I found several, and made appropriate changes to the injector constants and engine size to at least get it to start.  The excitement is building.Saturday night.  Check all my wiring – run the fuel pump to prime the system – crank the motor over with no spark to get oil pumping through it – set the base timing as close as I can – fire up the laptop and connect it to the ECU – they are now communicating nicely – check everything again.  Finally, after 16 weeks of parts chasing all over town (and the web), research into every sensor, wire and connector, late nights in the garage playing with cams, pistons, bearings, gaskets, hoses, etc. – its finally the MOMENT OF TRUTH – I turn the key, and. . . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, pop, pfitzzzzz.Hmm.  Not what I had hoped for.  Check the timing, close enough.  Crank over, same result.  Try again, and again, and again.  Same results.  I went to bed.Next day after the usual errands, kids birthday parties, dinner, baths, bedtime stories, etc. I’m back in the garage.  Check the firing order.  Reset the timing.  Try again.  Same result.  I start thinking about it some more and suppose it could be the 180 degrees out of time.  That could be it!  Now, mind you, I really knew that’s NOT it since it installed the dang dizzy with the valve cover off and I KNOW cylinder no. 1 was at TDC on the compression stroke, but sometimes desperation has a way of twisting things in your head, so I tried that –Grrr, pop, grrr, pop, grrr, pop, grrr, pop.  Not even close.  Pack it in for the night.Monday at work.  I’m obsessed.   What could be the problem?  It must be running way too lean.  I gotta get home.  After the usual family obligations, I’m back in the garage at 9 p.m.  This time, I pull out one of my most useful diagnostic tools – the propane bottle.  Huh?  Yes, that’s right, propane bottle.  If you drill out the orifice from a small bernz-o-matic torch and attach a hose to the end, it makes a very useful tuning tool.  Its quite handy for finding vacuum leaks, but even better to figure out whether you’re running rich or lean.  If its worse with propane, you’re already rich.  If its better, then you’re lean.  Easy and intuitive.  I hooked it into the manifold and turned the knob.  Went to start it and. . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.WOW – progress!  I took off the bottle and jumped back into the computer programming.  My injector constant was set for 24lb/hr injectors (which is what I’m using) so I knocked it down to 19lb/hr to richen up the mixture.  I know this isn’t right, but I figure I can deal with it later after its running.  Same result as the propane.  After three or four tries, I can keep it sputtering just long enough to get to the throttle, but as soon as I try to gas it, it dies.  It’s late, I’m packing it in for the night.Two days in San Francisco on business kept me away from the coupe, but gave me time to think.  Why is it so lean?  Why won’t it run clean and rev?  I gotta get it running at 3k rpm with a good mixture to break in the cam.  There’s just too much time and money in it to screw it up now.  Hmmm.  Wait!  D-Jet runs at a lower fuel pressure and my injectors are spec’d at 3 bar!  That means I’m not getting nearly enough gas.  That’s gotta be it!  I got home late on Wednesday and was too tired to even look in the garage.Yesterday was a loooong day at work waiting to get home.  This time, I coerced my son into “helping” me in the garage before dinner since there’s no way I could wait any longer – I know this is it!  I adjusted the D-Jet regulator in all the way and ended up with 40 psi – close enough!  Then I adjusted the injector constant back where it belongs – checked everything twice – contact!Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.Arrrggghhhh!   Take a break, eat some dinner, put the kids to bed.  Back in the garage at 9 p.m.  I try going way lean on the injectors.  No improvement.  I try way rich.  Same result.  I try a completely different .bin file.  Even worse.  I enjoy a good challenge every now and then, but at this point I’m pretty frustrated.  Take a break.  Have a glass of wine.  Then, a flash of inspiration!  I’m running an L-Jet dizzy with a VR sensor which goes straight into an MSD 6-A.  Then the ECU takes its trigger off of the MSD tach signal.  When I set it all up the first time, I had the VR sensor set so the MSD would trigger on the leading edge as the star wheel in the distributor passes the magnet.  That made sense at the time, but maybe it really wants to see the trigger on the trailing edge. Maybe, just maybe, I have the VR sensor wires reversed. . . could it be that simple?A quick wire swap, injector constant set back to 24lb/hr, double check everything again, turn the key, and ------Vroooom!Its actually running!  I jump out and hit the throttle to keep it going and it actually catches!  Revs to 3k and stays there while I’m holding the throttle open!  Finally.  Its surging, so I know its running really lean, and I shut her down for now.  I’m going to swipe the wide-band O2 sensor off my Jeep and put the cooling system back on before I go any further.  But it runs – and it should run well after several hours with the WBO2 and the laptop.  What a relief!  So now I just need to find some time this weekend to break in the cam, re-torque the head bolts, experiment with the tuning and hopefully it’ll be back on the road next week.  Its been too long.  Unfortunately, there’s a brake job on my wife’s Grand Cherokee in line for Saturday morning, then my mother in law’s window regulator, then the exhaust on my brother in law’s 535i . . . I should just open a shop.Sorry for the long musings – but I had to share with people who may appreciate these trials and tribulations.  I tried to share my excitement with my wife and got a simple “that’s nice – when is it done?” in response.   I’ll get some pictures of the finished product posted when (should that be “if”?) its done.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 15.027259826660156\n",
            "\n",
            "Representative Sentence: E24 6 series seats, is it possible to fit them? Hi all,Have any of you fitted seats from a 6 series into an E9? Whats involved and how long does it take. Are there any sites on the web that describe the process. Only found this forum last week and was blown away by the work that Malc is doing on his car. Bought my first E9 off a certain expert on these vehicles, whos written about them and has owned several. Works as a motoring journalist for a Classic car magazine. Described as in good condition with 1 years MOT. I picked the car up on a Sunday in Hounslow and coming into London had to stop at a set of traffic lights. Imagine my horror when the brakes decided to give up! It had only pasted its test the day before. Had that car about  3 years before I broke it for spares. I now own a CSA which needs work but but the main structure is suprisingly good. This car I want to rebuild to my own specs. 2 years ago I bought a CSL off ebay that was being sold by Gus V.R. He told me that it had the wrong engine (out of a CSI) Inspired by Malcs car I decided at the beginning of the week to go over it and imagine my suprise when the engine number matched the chassis #. Chassis # 2285032. Alloy panels are missing and about 2 foot has been cut off the rear with about a foot off the front. It will be a while before I can get stuck into either of the cars as I'm moving to Spain in the near future. Looking for 1 Scheel bucket seat for the CSL. If anyone has a spare to sell or knows of one please let me know. Great site and Malc, keep those pictures coming, they make my mouth waterPeter\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 9.112518310546875\n",
            "\n",
            "Representative Sentence: One Step Forward, Three Steps Back (Loooong 3.5 update) Between work, kids, family, vacation, heat waves and more work, I actually found some time to devote to my coupe and get the 3.5 conversion one step closer to completion.  You may recall I’m using a custom injection system running off of a GM ECU.  Since my last report, I’ve finished hanging all the accessories on the motor and all of the engine electrical (starter, alternator, etc.).  Got the 265 back in, exhaust hooked up, etc.  That was easy.  Now the hard stuff.Since there’s no “out of the box” harness for the sensors and ECU I’m using, I decided I would build one and run it in the original locations starting over by the starter, running across the valve cover (picking up the injector and sensor wiring along the way) and then down through the frame rail into the passenger compartment.  I bought a universal harness – basically a bunch of pre-marked wires – intended for Megasquirt conversions and set to work.  Approximately FIFTEEN hours later, I ended up with a beautiful harness with all the connectors assembled using the correct professional crimper tools and pins – there are NO splices – I even used new connectors and pins for the 30+ wires into the ECU.  In the engine compartment, it’s a dead-ringer for the original – everything is shrink wrapped in incrementally larger sizes as more wires are added to the bundle – very close to the original harness construction.  I didn’t have enough wire to make it to the rear seat like the original D-Jet, so I drilled a small hole in the floor just below the firewall and brought the harness up through there.  The ECU, relays and fuse board are mounted right above the glove box for easy access.  So far, so good.  I was getting excited!That ended up being the easy part; the rest has been nothing short of challenging.  In the past, I always used a separate chip burner which worked flawlessly, but created a time consuming tuning process (collect data, pull chip, erase chip, burn chip, install chip, repeat).  This time, I am trying out some new hardware that lets me program the ECU on the fly while monitoring all the vital stats from the diagnostic port.  Good stuff!  But it wouldn’t work.  After several frustrating hours, I finally got it to work with some help from the seller – my fault, of course – something about bits and parity settings on my COM port.  Then I spent a few hours surfing the web to locate an appropriate .bin file to start programming with.  I found several, and made appropriate changes to the injector constants and engine size to at least get it to start.  The excitement is building.Saturday night.  Check all my wiring – run the fuel pump to prime the system – crank the motor over with no spark to get oil pumping through it – set the base timing as close as I can – fire up the laptop and connect it to the ECU – they are now communicating nicely – check everything again.  Finally, after 16 weeks of parts chasing all over town (and the web), research into every sensor, wire and connector, late nights in the garage playing with cams, pistons, bearings, gaskets, hoses, etc. – its finally the MOMENT OF TRUTH – I turn the key, and. . . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, pop, pfitzzzzz.Hmm.  Not what I had hoped for.  Check the timing, close enough.  Crank over, same result.  Try again, and again, and again.  Same results.  I went to bed.Next day after the usual errands, kids birthday parties, dinner, baths, bedtime stories, etc. I’m back in the garage.  Check the firing order.  Reset the timing.  Try again.  Same result.  I start thinking about it some more and suppose it could be the 180 degrees out of time.  That could be it!  Now, mind you, I really knew that’s NOT it since it installed the dang dizzy with the valve cover off and I KNOW cylinder no. 1 was at TDC on the compression stroke, but sometimes desperation has a way of twisting things in your head, so I tried that –Grrr, pop, grrr, pop, grrr, pop, grrr, pop.  Not even close.  Pack it in for the night.Monday at work.  I’m obsessed.   What could be the problem?  It must be running way too lean.  I gotta get home.  After the usual family obligations, I’m back in the garage at 9 p.m.  This time, I pull out one of my most useful diagnostic tools – the propane bottle.  Huh?  Yes, that’s right, propane bottle.  If you drill out the orifice from a small bernz-o-matic torch and attach a hose to the end, it makes a very useful tuning tool.  Its quite handy for finding vacuum leaks, but even better to figure out whether you’re running rich or lean.  If its worse with propane, you’re already rich.  If its better, then you’re lean.  Easy and intuitive.  I hooked it into the manifold and turned the knob.  Went to start it and. . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.WOW – progress!  I took off the bottle and jumped back into the computer programming.  My injector constant was set for 24lb/hr injectors (which is what I’m using) so I knocked it down to 19lb/hr to richen up the mixture.  I know this isn’t right, but I figure I can deal with it later after its running.  Same result as the propane.  After three or four tries, I can keep it sputtering just long enough to get to the throttle, but as soon as I try to gas it, it dies.  It’s late, I’m packing it in for the night.Two days in San Francisco on business kept me away from the coupe, but gave me time to think.  Why is it so lean?  Why won’t it run clean and rev?  I gotta get it running at 3k rpm with a good mixture to break in the cam.  There’s just too much time and money in it to screw it up now.  Hmmm.  Wait!  D-Jet runs at a lower fuel pressure and my injectors are spec’d at 3 bar!  That means I’m not getting nearly enough gas.  That’s gotta be it!  I got home late on Wednesday and was too tired to even look in the garage.Yesterday was a loooong day at work waiting to get home.  This time, I coerced my son into “helping” me in the garage before dinner since there’s no way I could wait any longer – I know this is it!  I adjusted the D-Jet regulator in all the way and ended up with 40 psi – close enough!  Then I adjusted the injector constant back where it belongs – checked everything twice – contact!Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.Arrrggghhhh!   Take a break, eat some dinner, put the kids to bed.  Back in the garage at 9 p.m.  I try going way lean on the injectors.  No improvement.  I try way rich.  Same result.  I try a completely different .bin file.  Even worse.  I enjoy a good challenge every now and then, but at this point I’m pretty frustrated.  Take a break.  Have a glass of wine.  Then, a flash of inspiration!  I’m running an L-Jet dizzy with a VR sensor which goes straight into an MSD 6-A.  Then the ECU takes its trigger off of the MSD tach signal.  When I set it all up the first time, I had the VR sensor set so the MSD would trigger on the leading edge as the star wheel in the distributor passes the magnet.  That made sense at the time, but maybe it really wants to see the trigger on the trailing edge. Maybe, just maybe, I have the VR sensor wires reversed. . . could it be that simple?A quick wire swap, injector constant set back to 24lb/hr, double check everything again, turn the key, and ------Vroooom!Its actually running!  I jump out and hit the throttle to keep it going and it actually catches!  Revs to 3k and stays there while I’m holding the throttle open!  Finally.  Its surging, so I know its running really lean, and I shut her down for now.  I’m going to swipe the wide-band O2 sensor off my Jeep and put the cooling system back on before I go any further.  But it runs – and it should run well after several hours with the WBO2 and the laptop.  What a relief!  So now I just need to find some time this weekend to break in the cam, re-torque the head bolts, experiment with the tuning and hopefully it’ll be back on the road next week.  Its been too long.  Unfortunately, there’s a brake job on my wife’s Grand Cherokee in line for Saturday morning, then my mother in law’s window regulator, then the exhaust on my brother in law’s 535i . . . I should just open a shop.Sorry for the long musings – but I had to share with people who may appreciate these trials and tribulations.  I tried to share my excitement with my wife and got a simple “that’s nice – when is it done?” in response.   I’ll get some pictures of the finished product posted when (should that be “if”?) its done.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 15.027259826660156\n",
            "\n",
            "Representative Sentence: 3.5 Update -- Malc -- You'll want to look at this! I finally made some time to focus on getting my 3.5 rebuild completed and my coupe back together.  All of the manifold pieces, covers, oil pan, etc. were all stripped and painted with Endura.  I also started to sort out the various fuel injection bits for the conversion I'm doing.  Some of you may recall I'm converting it to a GM based ECU, but I'm trying to keep it as stock looking as possible (no bulky AFM or other crap).  I'm very pleased with the results.  It will take a very sharp eye to notice this is anything but stock D-Jet.  So far, I've figured out how to mount a throttle position sensor, the coolant temp sensor, the intake air temp sensor and an idle air bypass valve.  This info should be useful for anyone contemplating any type of aftermarket ECU.  Here are a few pictures:Overall view:Here's the TPS mounted inside the stock D-Jet throttle body.  This will be hidden under the stock D-Jet TPS cover:Here's how the TPS mounts:The Idle Air Control Valve in the stock warm-up valve location:Finally, here's the Intake Air Sensor mounted in place of the stock cold-start injector:I'm going to be running a stock L-Jet distributor which will trigger an MSD ignition box.  That, in turn, will trigger the fuel injection.  I'll be running fuel only to start out (just like stock D-Jet), but can add ignition control in later once I get the fuel curve programmed.  Hopefully I'll have the motor in tonight, then its just a matter of making the wiring harness and getting it all wired up.  I'll keep you guys posted.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 8.711413383483887\n",
            "\n",
            "Representative Sentence: Anyone tried Five O Motorsports injectors on 3.5L? I bought a set based on the claim that they're Bosch Design III, supposedly the correct OEM replacement for the original injectors for a 1987 M30 3.5L which now resides in the engine bay of my E9.  My motor does not pull like it should (especially for a high compression 3.5L), but it could be so many things causing it.  Chris at La Jolla Independent said the Bosch Design III may not be optimal for the hybrid M30/Motronic/L Jet set up I'm running but was coy about what he would put in (except to say not BMW OEM, but it's a secret).  Just wondering if anyone else ever tried these injectors and had any problems or good results.http://www.fiveomotorsport.com/Injector_SetsBMW.aspGarrett\n",
            "Answer: None\n",
            "Similarity Score: None\n",
            "\n",
            "Representative Sentence: One Step Forward, Three Steps Back (Loooong 3.5 update) Between work, kids, family, vacation, heat waves and more work, I actually found some time to devote to my coupe and get the 3.5 conversion one step closer to completion.  You may recall I’m using a custom injection system running off of a GM ECU.  Since my last report, I’ve finished hanging all the accessories on the motor and all of the engine electrical (starter, alternator, etc.).  Got the 265 back in, exhaust hooked up, etc.  That was easy.  Now the hard stuff.Since there’s no “out of the box” harness for the sensors and ECU I’m using, I decided I would build one and run it in the original locations starting over by the starter, running across the valve cover (picking up the injector and sensor wiring along the way) and then down through the frame rail into the passenger compartment.  I bought a universal harness – basically a bunch of pre-marked wires – intended for Megasquirt conversions and set to work.  Approximately FIFTEEN hours later, I ended up with a beautiful harness with all the connectors assembled using the correct professional crimper tools and pins – there are NO splices – I even used new connectors and pins for the 30+ wires into the ECU.  In the engine compartment, it’s a dead-ringer for the original – everything is shrink wrapped in incrementally larger sizes as more wires are added to the bundle – very close to the original harness construction.  I didn’t have enough wire to make it to the rear seat like the original D-Jet, so I drilled a small hole in the floor just below the firewall and brought the harness up through there.  The ECU, relays and fuse board are mounted right above the glove box for easy access.  So far, so good.  I was getting excited!That ended up being the easy part; the rest has been nothing short of challenging.  In the past, I always used a separate chip burner which worked flawlessly, but created a time consuming tuning process (collect data, pull chip, erase chip, burn chip, install chip, repeat).  This time, I am trying out some new hardware that lets me program the ECU on the fly while monitoring all the vital stats from the diagnostic port.  Good stuff!  But it wouldn’t work.  After several frustrating hours, I finally got it to work with some help from the seller – my fault, of course – something about bits and parity settings on my COM port.  Then I spent a few hours surfing the web to locate an appropriate .bin file to start programming with.  I found several, and made appropriate changes to the injector constants and engine size to at least get it to start.  The excitement is building.Saturday night.  Check all my wiring – run the fuel pump to prime the system – crank the motor over with no spark to get oil pumping through it – set the base timing as close as I can – fire up the laptop and connect it to the ECU – they are now communicating nicely – check everything again.  Finally, after 16 weeks of parts chasing all over town (and the web), research into every sensor, wire and connector, late nights in the garage playing with cams, pistons, bearings, gaskets, hoses, etc. – its finally the MOMENT OF TRUTH – I turn the key, and. . . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, pop, pfitzzzzz.Hmm.  Not what I had hoped for.  Check the timing, close enough.  Crank over, same result.  Try again, and again, and again.  Same results.  I went to bed.Next day after the usual errands, kids birthday parties, dinner, baths, bedtime stories, etc. I’m back in the garage.  Check the firing order.  Reset the timing.  Try again.  Same result.  I start thinking about it some more and suppose it could be the 180 degrees out of time.  That could be it!  Now, mind you, I really knew that’s NOT it since it installed the dang dizzy with the valve cover off and I KNOW cylinder no. 1 was at TDC on the compression stroke, but sometimes desperation has a way of twisting things in your head, so I tried that –Grrr, pop, grrr, pop, grrr, pop, grrr, pop.  Not even close.  Pack it in for the night.Monday at work.  I’m obsessed.   What could be the problem?  It must be running way too lean.  I gotta get home.  After the usual family obligations, I’m back in the garage at 9 p.m.  This time, I pull out one of my most useful diagnostic tools – the propane bottle.  Huh?  Yes, that’s right, propane bottle.  If you drill out the orifice from a small bernz-o-matic torch and attach a hose to the end, it makes a very useful tuning tool.  Its quite handy for finding vacuum leaks, but even better to figure out whether you’re running rich or lean.  If its worse with propane, you’re already rich.  If its better, then you’re lean.  Easy and intuitive.  I hooked it into the manifold and turned the knob.  Went to start it and. . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.WOW – progress!  I took off the bottle and jumped back into the computer programming.  My injector constant was set for 24lb/hr injectors (which is what I’m using) so I knocked it down to 19lb/hr to richen up the mixture.  I know this isn’t right, but I figure I can deal with it later after its running.  Same result as the propane.  After three or four tries, I can keep it sputtering just long enough to get to the throttle, but as soon as I try to gas it, it dies.  It’s late, I’m packing it in for the night.Two days in San Francisco on business kept me away from the coupe, but gave me time to think.  Why is it so lean?  Why won’t it run clean and rev?  I gotta get it running at 3k rpm with a good mixture to break in the cam.  There’s just too much time and money in it to screw it up now.  Hmmm.  Wait!  D-Jet runs at a lower fuel pressure and my injectors are spec’d at 3 bar!  That means I’m not getting nearly enough gas.  That’s gotta be it!  I got home late on Wednesday and was too tired to even look in the garage.Yesterday was a loooong day at work waiting to get home.  This time, I coerced my son into “helping” me in the garage before dinner since there’s no way I could wait any longer – I know this is it!  I adjusted the D-Jet regulator in all the way and ended up with 40 psi – close enough!  Then I adjusted the injector constant back where it belongs – checked everything twice – contact!Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.Arrrggghhhh!   Take a break, eat some dinner, put the kids to bed.  Back in the garage at 9 p.m.  I try going way lean on the injectors.  No improvement.  I try way rich.  Same result.  I try a completely different .bin file.  Even worse.  I enjoy a good challenge every now and then, but at this point I’m pretty frustrated.  Take a break.  Have a glass of wine.  Then, a flash of inspiration!  I’m running an L-Jet dizzy with a VR sensor which goes straight into an MSD 6-A.  Then the ECU takes its trigger off of the MSD tach signal.  When I set it all up the first time, I had the VR sensor set so the MSD would trigger on the leading edge as the star wheel in the distributor passes the magnet.  That made sense at the time, but maybe it really wants to see the trigger on the trailing edge. Maybe, just maybe, I have the VR sensor wires reversed. . . could it be that simple?A quick wire swap, injector constant set back to 24lb/hr, double check everything again, turn the key, and ------Vroooom!Its actually running!  I jump out and hit the throttle to keep it going and it actually catches!  Revs to 3k and stays there while I’m holding the throttle open!  Finally.  Its surging, so I know its running really lean, and I shut her down for now.  I’m going to swipe the wide-band O2 sensor off my Jeep and put the cooling system back on before I go any further.  But it runs – and it should run well after several hours with the WBO2 and the laptop.  What a relief!  So now I just need to find some time this weekend to break in the cam, re-torque the head bolts, experiment with the tuning and hopefully it’ll be back on the road next week.  Its been too long.  Unfortunately, there’s a brake job on my wife’s Grand Cherokee in line for Saturday morning, then my mother in law’s window regulator, then the exhaust on my brother in law’s 535i . . . I should just open a shop.Sorry for the long musings – but I had to share with people who may appreciate these trials and tribulations.  I tried to share my excitement with my wife and got a simple “that’s nice – when is it done?” in response.   I’ll get some pictures of the finished product posted when (should that be “if”?) its done.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 15.027259826660156\n",
            "\n",
            "Representative Sentence: One Step Forward, Three Steps Back (Loooong 3.5 update) Between work, kids, family, vacation, heat waves and more work, I actually found some time to devote to my coupe and get the 3.5 conversion one step closer to completion.  You may recall I’m using a custom injection system running off of a GM ECU.  Since my last report, I’ve finished hanging all the accessories on the motor and all of the engine electrical (starter, alternator, etc.).  Got the 265 back in, exhaust hooked up, etc.  That was easy.  Now the hard stuff.Since there’s no “out of the box” harness for the sensors and ECU I’m using, I decided I would build one and run it in the original locations starting over by the starter, running across the valve cover (picking up the injector and sensor wiring along the way) and then down through the frame rail into the passenger compartment.  I bought a universal harness – basically a bunch of pre-marked wires – intended for Megasquirt conversions and set to work.  Approximately FIFTEEN hours later, I ended up with a beautiful harness with all the connectors assembled using the correct professional crimper tools and pins – there are NO splices – I even used new connectors and pins for the 30+ wires into the ECU.  In the engine compartment, it’s a dead-ringer for the original – everything is shrink wrapped in incrementally larger sizes as more wires are added to the bundle – very close to the original harness construction.  I didn’t have enough wire to make it to the rear seat like the original D-Jet, so I drilled a small hole in the floor just below the firewall and brought the harness up through there.  The ECU, relays and fuse board are mounted right above the glove box for easy access.  So far, so good.  I was getting excited!That ended up being the easy part; the rest has been nothing short of challenging.  In the past, I always used a separate chip burner which worked flawlessly, but created a time consuming tuning process (collect data, pull chip, erase chip, burn chip, install chip, repeat).  This time, I am trying out some new hardware that lets me program the ECU on the fly while monitoring all the vital stats from the diagnostic port.  Good stuff!  But it wouldn’t work.  After several frustrating hours, I finally got it to work with some help from the seller – my fault, of course – something about bits and parity settings on my COM port.  Then I spent a few hours surfing the web to locate an appropriate .bin file to start programming with.  I found several, and made appropriate changes to the injector constants and engine size to at least get it to start.  The excitement is building.Saturday night.  Check all my wiring – run the fuel pump to prime the system – crank the motor over with no spark to get oil pumping through it – set the base timing as close as I can – fire up the laptop and connect it to the ECU – they are now communicating nicely – check everything again.  Finally, after 16 weeks of parts chasing all over town (and the web), research into every sensor, wire and connector, late nights in the garage playing with cams, pistons, bearings, gaskets, hoses, etc. – its finally the MOMENT OF TRUTH – I turn the key, and. . . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, pop, pfitzzzzz.Hmm.  Not what I had hoped for.  Check the timing, close enough.  Crank over, same result.  Try again, and again, and again.  Same results.  I went to bed.Next day after the usual errands, kids birthday parties, dinner, baths, bedtime stories, etc. I’m back in the garage.  Check the firing order.  Reset the timing.  Try again.  Same result.  I start thinking about it some more and suppose it could be the 180 degrees out of time.  That could be it!  Now, mind you, I really knew that’s NOT it since it installed the dang dizzy with the valve cover off and I KNOW cylinder no. 1 was at TDC on the compression stroke, but sometimes desperation has a way of twisting things in your head, so I tried that –Grrr, pop, grrr, pop, grrr, pop, grrr, pop.  Not even close.  Pack it in for the night.Monday at work.  I’m obsessed.   What could be the problem?  It must be running way too lean.  I gotta get home.  After the usual family obligations, I’m back in the garage at 9 p.m.  This time, I pull out one of my most useful diagnostic tools – the propane bottle.  Huh?  Yes, that’s right, propane bottle.  If you drill out the orifice from a small bernz-o-matic torch and attach a hose to the end, it makes a very useful tuning tool.  Its quite handy for finding vacuum leaks, but even better to figure out whether you’re running rich or lean.  If its worse with propane, you’re already rich.  If its better, then you’re lean.  Easy and intuitive.  I hooked it into the manifold and turned the knob.  Went to start it and. . .Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.WOW – progress!  I took off the bottle and jumped back into the computer programming.  My injector constant was set for 24lb/hr injectors (which is what I’m using) so I knocked it down to 19lb/hr to richen up the mixture.  I know this isn’t right, but I figure I can deal with it later after its running.  Same result as the propane.  After three or four tries, I can keep it sputtering just long enough to get to the throttle, but as soon as I try to gas it, it dies.  It’s late, I’m packing it in for the night.Two days in San Francisco on business kept me away from the coupe, but gave me time to think.  Why is it so lean?  Why won’t it run clean and rev?  I gotta get it running at 3k rpm with a good mixture to break in the cam.  There’s just too much time and money in it to screw it up now.  Hmmm.  Wait!  D-Jet runs at a lower fuel pressure and my injectors are spec’d at 3 bar!  That means I’m not getting nearly enough gas.  That’s gotta be it!  I got home late on Wednesday and was too tired to even look in the garage.Yesterday was a loooong day at work waiting to get home.  This time, I coerced my son into “helping” me in the garage before dinner since there’s no way I could wait any longer – I know this is it!  I adjusted the D-Jet regulator in all the way and ended up with 40 psi – close enough!  Then I adjusted the injector constant back where it belongs – checked everything twice – contact!Grrr, grrr, grrr, grrr, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, bhlapt, pop, pfitzzzzz.Arrrggghhhh!   Take a break, eat some dinner, put the kids to bed.  Back in the garage at 9 p.m.  I try going way lean on the injectors.  No improvement.  I try way rich.  Same result.  I try a completely different .bin file.  Even worse.  I enjoy a good challenge every now and then, but at this point I’m pretty frustrated.  Take a break.  Have a glass of wine.  Then, a flash of inspiration!  I’m running an L-Jet dizzy with a VR sensor which goes straight into an MSD 6-A.  Then the ECU takes its trigger off of the MSD tach signal.  When I set it all up the first time, I had the VR sensor set so the MSD would trigger on the leading edge as the star wheel in the distributor passes the magnet.  That made sense at the time, but maybe it really wants to see the trigger on the trailing edge. Maybe, just maybe, I have the VR sensor wires reversed. . . could it be that simple?A quick wire swap, injector constant set back to 24lb/hr, double check everything again, turn the key, and ------Vroooom!Its actually running!  I jump out and hit the throttle to keep it going and it actually catches!  Revs to 3k and stays there while I’m holding the throttle open!  Finally.  Its surging, so I know its running really lean, and I shut her down for now.  I’m going to swipe the wide-band O2 sensor off my Jeep and put the cooling system back on before I go any further.  But it runs – and it should run well after several hours with the WBO2 and the laptop.  What a relief!  So now I just need to find some time this weekend to break in the cam, re-torque the head bolts, experiment with the tuning and hopefully it’ll be back on the road next week.  Its been too long.  Unfortunately, there’s a brake job on my wife’s Grand Cherokee in line for Saturday morning, then my mother in law’s window regulator, then the exhaust on my brother in law’s 535i . . . I should just open a shop.Sorry for the long musings – but I had to share with people who may appreciate these trials and tribulations.  I tried to share my excitement with my wife and got a simple “that’s nice – when is it done?” in response.   I’ll get some pictures of the finished product posted when (should that be “if”?) its done.\n",
            "Answer: hello cry custom saidHelloIt cry expand sorry butseventytwo thousand eurosTell typo keen most roll don't think CSLAnybody know differentially CSLAnybody know differentially CSLAnybody know differential\n",
            "Similarity Score: 15.027259826660156\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0WNzGEEdLbe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Stop the execution of code\n",
        "sys.exit(\"Execution stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 10. Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization Srategies"
      ],
      "metadata": {
        "id": "7bvouULWteB2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H0Xnn_aMnYq"
      },
      "outputs": [],
      "source": [
        "# Summarize corpus\n",
        "\n",
        "# Visual comparison of leading options\n",
        "\n",
        "# Sample text for summarization\n",
        "sample_text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board\".\"I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it\".\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours\".God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\".It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does\".\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "def t5_summarize(text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load the tokenizer and model for T5\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,  # Tokens\n",
        "        min_length=30,  # Optional: setting a minimum length for the output\n",
        "        length_penalty=2.0,  # Optional: encourage longer summaries if needed\n",
        "        num_beams=4,  # Optional: number of beams for beam search\n",
        "        early_stopping=True  # Stop when at least num_beams sentences are finished\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return summary, elapsed_time\n",
        "\n",
        "def distilbart_summarize(text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load the tokenizer and model for DistilBART\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "    # Remove new lines to treat the text as one long unformatted piece of text\n",
        "    unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        unformatted_text,\n",
        "        max_length=900,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=500,  # Tokens\n",
        "        min_length=30,  # Optional: setting a minimum length for the output\n",
        "        length_penalty=2.0,  # Optional: encourage longer summaries if needed\n",
        "        num_beams=4,  # Optional: number of beams for beam search\n",
        "        early_stopping=True  # Stop when at least num_beams sentences are finished\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Return the summary text and the time taken\n",
        "    return summary, elapsed_time\n",
        "\n",
        "def main():\n",
        "    print(\"Original Text:\\n\", sample_text)\n",
        "\n",
        "    t5_summary, t5_time = t5_summarize(sample_text)\n",
        "    print(\"\\nT5 Summary:\\n\", t5_summary)\n",
        "    print(f\"T5 Time Taken: {t5_time:.2f} seconds\")\n",
        "\n",
        "    distilbart_summary, distilbart_time = distilbart_summarize(sample_text)\n",
        "    print(\"\\nDistilBART Summary:\\n\", distilbart_summary)\n",
        "    print(f\"DistilBART Time Taken: {distilbart_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0r8gc0eJQtF"
      },
      "outputs": [],
      "source": [
        "# Score summarization: T5\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def t5_summarize(text, max_length, min_length, num_beams):\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model_t5.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Define the reference summary (ground truth)\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = t5_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zal-aEkL_BFj"
      },
      "outputs": [],
      "source": [
        "# Score summarization: BART\n",
        "\n",
        "# Load the tokenizer and model for DistilBART\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def distilbart_summarize(text, max_length, min_length, num_beams):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,  # Adjusted to fit within the model's constraints\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text and reference summary\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = distilbart_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RdIXKt4LPUB"
      },
      "source": [
        "Compare ROGE Summarization Scores:\n",
        "\n",
        "T5\n",
        "\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1778\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.1290\n",
        "*   Best parameters: max_length=50, min_length=10, num_beams=2\n",
        "\n",
        "\n",
        "\n",
        "BART\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1270\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.0816\n",
        "*   Best parameters: max_length=100, min_length=10, num_beams=4\n",
        "\n",
        "\n",
        "\n",
        "Based on these scores, Ill be using T5 for training due to its higher Precision and no discernable differnce in Recall."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbAsEfOhs8cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VHT3TQuos3kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parking lot of code"
      ],
      "metadata": {
        "id": "jfFFh6IJs3m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHrVJfmAZA1N"
      },
      "outputs": [],
      "source": [
        "# Embeddings V1\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, DistilBertModel\n",
        "import torch\n",
        "import pickle\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "def prepare_and_save_data(df, model='bert-base-uncased', max_length=512, save_path='/content/drive/MyDrive/TensorBoardLogs/tokenized_data.pkl'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    df = df.head(100)\n",
        "    tokenized_questions = tokenizer(list(df['QUESTION'].fillna('')), max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    tokenized_answers = tokenizer(list(df['ANSWER'].fillna('')), max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump((tokenized_questions, tokenized_answers), f)\n",
        "\n",
        "def load_data_and_generate_embeddings(model_path='distilbert-base-uncased', data_path='/content/drive/MyDrive/TensorBoardLogs/tokenized_data.pkl', log_dir='/content/drive/MyDrive/TensorBoardLogs'):\n",
        "    model = DistilBertModel.from_pretrained(model_path)\n",
        "    with open(data_path, 'rb') as f:\n",
        "        tokenized_questions, tokenized_answers = pickle.load(f)\n",
        "    with torch.no_grad():\n",
        "        question_embeddings = model(input_ids=tokenized_questions['input_ids'], attention_mask=tokenized_questions['attention_mask']).last_hidden_state.mean(dim=1)\n",
        "        answer_embeddings = model(input_ids=tokenized_answers['input_ids'], attention_mask=tokenized_answers['attention_mask']).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Save embeddings to a file for later use\n",
        "    np.save(os.path.join(log_dir, 'question_embeddings.npy'), question_embeddings.cpu().numpy())\n",
        "\n",
        "    return question_embeddings, answer_embeddings\n",
        "\n",
        "def find_most_similar(query_embedding, answer_embeddings):\n",
        "    query_embedding_np = query_embedding.detach().cpu().numpy().reshape(1, -1)\n",
        "    answer_embeddings_np = answer_embeddings.detach().cpu().numpy()\n",
        "    similarities = cosine_similarity(query_embedding_np, answer_embeddings_np)\n",
        "    return np.argmax(similarities)\n",
        "\n",
        "def generate_predictions(question_embeddings, answer_embeddings):\n",
        "    predictions = []\n",
        "    for i in range(len(question_embeddings)):\n",
        "        index_of_most_similar = find_most_similar(question_embeddings[i], answer_embeddings)\n",
        "        predictions.append(index_of_most_similar)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_model(predictions, ground_truth):\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def main():\n",
        "    # Load DataFrame\n",
        "    #df = pd.read_csv('/content/drive/MyDrive/your_data_file.csv')\n",
        "\n",
        "    # Process and save tokenized data\n",
        "    prepare_and_save_data(df)\n",
        "\n",
        "    # Load data and generate embeddings\n",
        "    question_embeddings, answer_embeddings = load_data_and_generate_embeddings()\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = generate_predictions(question_embeddings, answer_embeddings)\n",
        "\n",
        "    # Define ground truth\n",
        "    ground_truth = list(range(len(question_embeddings)))\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy, precision, recall, f1 = evaluate_model(predictions, ground_truth)\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UuWmxSnXtYd"
      },
      "outputs": [],
      "source": [
        "# Embeddings Visualizations 2D\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load question_embeddings from the file\n",
        "question_embeddings = np.load('/content/drive/MyDrive/TensorBoardLogs/question_embeddings.npy')\n",
        "\n",
        "# Initialize t-SNE with the desired number of components and random state for reproducibility\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "embeddings_2d = tsne.fit_transform(question_embeddings)  # No need for .detach() or .cpu()\n",
        "\n",
        "# Plotting the 2D embeddings\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='blue', cmap=plt.cm.get_cmap(\"jet\", 10))\n",
        "plt.colorbar()  # Optional: adds a color bar to the plot\n",
        "plt.title('Question Embeddings Visualization')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKOdpgCLYH3O"
      },
      "outputs": [],
      "source": [
        "# Embeddings Visualizations 3D\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "# Load your embeddings and optional metadata\n",
        "embeddings = np.load('/content/drive/MyDrive/TensorBoardLogs/question_embeddings.npy')\n",
        "metadata_path = '/content/drive/MyDrive/TensorBoardLogs/metadata.tsv'\n",
        "\n",
        "# Set up the log directory\n",
        "log_dir = '/content/drive/MyDrive/TensorBoardLogs'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save embeddings to a file for TensorBoard\n",
        "embeddings_tensor_name = 'question_embeddings'\n",
        "embeddings_file_path = os.path.join(log_dir, embeddings_tensor_name + '.ckpt')\n",
        "\n",
        "# Initialize a TensorFlow variable with the embeddings\n",
        "embedding_var = tf.Variable(embeddings, name=embeddings_tensor_name)\n",
        "\n",
        "# Use a TensorFlow checkpoint to save the embeddings\n",
        "checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
        "checkpoint_path = checkpoint.save(embeddings_file_path)\n",
        "\n",
        "# Set up the TensorBoard projector\n",
        "config = projector.ProjectorConfig()\n",
        "embedding_config = config.embeddings.add()\n",
        "embedding_config.tensor_name = embedding_var.name\n",
        "embedding_config.metadata_path = metadata_path  # Ensure this points to a valid .tsv file\n",
        "\n",
        "# Save the projector config file\n",
        "projector_file = os.path.join(log_dir, 'projector_config.pbtxt')\n",
        "with open(projector_file, 'w') as f:\n",
        "    f.write(str(config))\n",
        "\n",
        "# Ensure TensorFlow knows to track this variable\n",
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "    tf.summary.scalar('example_scalar', 1, step=0)\n",
        "    projector.visualize_embeddings(log_dir, config)\n",
        "\n",
        "# In Jupyter Notebook, use this to start TensorBoard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/TensorBoardLogs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLmkUtIa8-Lz"
      },
      "outputs": [],
      "source": [
        "# LDA Visualizations\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Sample data\n",
        "#df = df\n",
        "\n",
        "# Vectorize the text data\n",
        "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "count_data = count_vectorizer.fit_transform(df['QUESTION'])\n",
        "\n",
        "# Run LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
        "lda.fit(count_data)\n",
        "\n",
        "# Get the topic distribution for each document\n",
        "# 10, 5 vector summaries but after tokenization...\n",
        "\n",
        "lda_topics = lda.transform(count_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyQlwNyRG7bG"
      },
      "outputs": [],
      "source": [
        "# LDA Visualizations\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Assuming your preprocessed text is already in the 'QUESTION' column of your DataFrame\n",
        "texts = [text.split() for text in df['QUESTION']]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Filter out tokens that appear in less than 5 documents or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
        "\n",
        "# Create a Bag-of-Words representation of the documents\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "num_topics = 2  # Adjust the number of topics as needed\n",
        "lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "# Visualize the LDA results\n",
        "panel = gensimvis.prepare(lda, corpus, dictionary)\n",
        "pyLDAvis.display(panel)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A9uW-2OyR9fa3DZlxEvVAAVJPf7jpGrw",
      "authorship_tag": "ABX9TyNkMKKzJrndYT7swALp4WBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}