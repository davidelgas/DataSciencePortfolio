{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/NLP_with_RAG/notebooks/NLP_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2vTigd3lYAM"
      },
      "source": [
        "# 1. Project Scope and Limitations\n",
        "\n",
        "\n",
        "Use Case Detailing: Elaborate on how the virtual mechanic will interact with users. Describe example queries and responses to illustrate practical usage.\n",
        "Impact on Stakeholders: Discuss the expected impact this project could have on various stakeholders, including car owners and BMW clubs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEVpPV9NmPK2"
      },
      "source": [
        "## 1.1 Project Overview\n",
        "This project aims to build a generative language model capable of processing written, unstructured questions in English from users and providing targeted written answers. The  use case for this project is a virtual mechanic to help owners maintain a specific make and model of classic car: the BMW 3.0 CS. The results of this effort will be provided to several international BMW clubs to solicity feedback on usability and potential extension to other makes and models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jOHmOOCkEvX",
        "outputId": "78313d7c-67f3-48b0-cd4f-d1db6f11512e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "UJ7ngp93CY18",
        "outputId": "ed649f10-d0c3-4058-efea-c104c2d0cf73"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('/content/drive/MyDrive/Colab Notebooks/NLP/BMW_E9.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YK6zzgBCySF"
      },
      "source": [
        "1972 - 1974 BMW 3.0 CS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFqXbkd_mWjv"
      },
      "source": [
        "## 1.2 Objectives\n",
        "The primary objective of this project is the development of a Natural Language Processing (NLP) model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparision and selection of architectures, libraries and methods. This is a complement to my pursuit of a Masters Degree in Data Science.\n",
        "\n",
        "Measurable Outcomes: Define specific, measurable outcomes to gauge success, such as accuracy metrics or user satisfaction rates.\n",
        "Workflow Diagram: Include a flowchart or diagram that outlines the entire project workflow for visual representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlctA2pymZYv"
      },
      "source": [
        "## 1.3 Limitations and Challenges\n",
        "Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs. Data storage will be done in a Snowflake database. Where possible, a combination of open source and free resource will be used.\n",
        "\n",
        "Resource Constraints: Discuss the impact of CPU-only resources on model training and inference times.\n",
        "Data Privacy: Address how data privacy will be ensured when handling user-generated data, particularly concerning personally identifiable information (PII)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqO0ZIpBCGrt"
      },
      "source": [
        "## 1.4 Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are a result of previous similar questions from the forum used to create the corpus. The answers will also include results from pre-trained models, ensuring a rich and informed response. Users will see these answers in plain English. As a programmer, you have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model. However, users will not see the verbatim source text used to generate the answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHJWehqmrr6a"
      },
      "source": [
        "## 1.5 Workflow for Building a Retrieval-Augmented Generation (RAG) Model\n",
        "\n",
        "1. **Select a Model**\n",
        "   - Choose a suitable pre-trained model for your task, such as DistilBERT for embeddings and T5 or GPT for generation.\n",
        "\n",
        "2. **Compile Corpus**\n",
        "   - Gather and organize your text data from sources such as a classic car forum.\n",
        "\n",
        "3. **Create and Store into a Database**\n",
        "   - Store your structured data into a database for easy access and management.\n",
        "\n",
        "4. **Remove Unnecessary Characters**\n",
        "   - Clean the text by removing HTML tags, extra whitespace, non-printable characters, and other irrelevant elements.\n",
        "\n",
        "5. **Convert Text to Lowercase**\n",
        "   - Standardize the text by converting all characters to lowercase to ensure uniformity.\n",
        "\n",
        "6. **Remove Stop Words**\n",
        "   - Filter out common stop words to focus on more meaningful content.\n",
        "\n",
        "7. **Deduplication**\n",
        "   - Remove duplicate entries to ensure the uniqueness of the data.\n",
        "\n",
        "8. **Lemmatization**\n",
        "   - Convert words to their base or dictionary form to consolidate similar forms of a word.\n",
        "\n",
        "9. **Entity Recognition and Anonymization**\n",
        "   - Identify and anonymize personal information or specific entity names to maintain privacy.\n",
        "\n",
        "10. **Filter Irrelevant Sections**\n",
        "    - Remove sections of the text that do not contribute to the knowledge base or are off-topic.\n",
        "\n",
        "11. **Consolidate Threads**\n",
        "    - Combine related threads or posts to create a comprehensive view of discussions on similar topics.\n",
        "\n",
        "12. **Semantic Clustering**\n",
        "    - Group text segments by their semantic similarities to enhance the structure of the dataset.\n",
        "\n",
        "13. **Summarize Corpus Content**\n",
        "    - Condense your text data to highlight the most important information and insights from your corpus.\n",
        "\n",
        "14. **Format into Questions and Answers**\n",
        "    - Structure your text into a question-answer format suitable for training your RAG model.\n",
        "    - Ensure the question string ends with a question mark for clarity.\n",
        "\n",
        "15. **Tokenization**\n",
        "    - Break down the text into smaller units called tokens. Use a tokenizer compatible with your chosen model, such as the BERT tokenizer.\n",
        "\n",
        "16. **Embedding**\n",
        "    - Convert tokens into numerical representations using embeddings. Use pre-trained embeddings from transformer models like BERT, DistilBERT, or T5.\n",
        "\n",
        "17. **Build FAISS Index**\n",
        "    - Create an index of the embeddings using FAISS for fast similarity searches.\n",
        "\n",
        "18. **Query Processing and Search**\n",
        "    - Generate embeddings for new queries and use the FAISS index to find the most similar questions in the corpus.\n",
        "\n",
        "19. **Retrieve and Rank**\n",
        "    - Fetch the top-N most similar questions and their corresponding answers from the corpus.\n",
        "    - Concatenate the retrieved contexts to form a comprehensive input for the generative model.\n",
        "\n",
        "20. **Answer Generation**\n",
        "    - Employ a generative model like T5 or GPT to generate an answer based on the concatenated context and the query.\n",
        "\n",
        "21. **Evaluation and Tuning**\n",
        "    - Assess the model's performance using metrics like precision, recall, F1-score, and accuracy.\n",
        "    - Fine-tune the pre-trained models on your specific dataset if necessary.\n",
        "\n",
        "22. **Deploying the Demo**\n",
        "    - (Optional) Create an interactive UI using tools like Flask or Streamlit.\n",
        "    - Deploy the model to a server or cloud service for remote access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svb8UxyCkC7O",
        "outputId": "32ac89a0-b452-4538-e237-5a5df5d50a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.7)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.11.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.14.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.2.1)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (0.12.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "# Data Collection\n",
        "import os\n",
        "\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "\n",
        "!pip3 install requests\n",
        "import requests\n",
        "\n",
        "!pip3 install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "\n",
        "# Data Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer as lemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip3 install numpy\n",
        "import numpy as np\n",
        "\n",
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRMLNtZXGeya"
      },
      "source": [
        "# 2. Natural Language Processing Architectures and Models\n",
        "\n",
        "## 2.1 Traditional Machine Learning Architectures\n",
        "\n",
        "### 2.1.1 Bag-of-Words (BoW)\n",
        "- **Description:** Represents text data as a collection of unique words and their frequencies.\n",
        "- **Example:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "- **Pros:**\n",
        "  - Simple and efficient representation.\n",
        "  - Works well for tasks like sentiment analysis and document classification.\n",
        "- **Cons:**\n",
        "  - Ignores word order and context.\n",
        "  - Doesn't capture semantic meanings well.\n",
        "\n",
        "### 2.1.2 Word Embeddings\n",
        "- **Description:** Represent words as dense vectors in a continuous vector space.\n",
        "- **Examples:** Word2Vec, GloVe\n",
        "- **Pros:**\n",
        "  - Captures semantic meanings and relationships between words.\n",
        "  - Provides dense vector representations suitable for downstream tasks.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of data for training.\n",
        "  - Struggles with out-of-vocabulary words.\n",
        "\n",
        "### 2.1.3 Sequence Models\n",
        "- **Description:** Models that capture the sequential nature of text data.\n",
        "- **Examples:** Hidden Markov Models (HMM), Conditional Random Fields (CRF)\n",
        "- **Pros:**\n",
        "  - Captures sequential dependencies in data.\n",
        "  - Suitable for tasks like named entity recognition and part-of-speech tagging.\n",
        "- **Cons:**\n",
        "  - Requires labeled sequential data for training.\n",
        "  - Can be computationally intensive.\n",
        "\n",
        "## 2.2 Deep Learning Architectures\n",
        "\n",
        "### 2.2.1 Convolutional Neural Networks (CNN)\n",
        "- **Description:** Deep learning models that use convolutional layers for feature extraction.\n",
        "- **Examples:** TextCNN\n",
        "- **Pros:**\n",
        "  - Effective for capturing local dependencies in text data.\n",
        "  - Can capture hierarchical patterns in data.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of training data.\n",
        "  - Limited ability to capture long-range dependencies.\n",
        "\n",
        "### 2.2.2 Recurrent Neural Networks (RNN)\n",
        "- **Description:** Neural networks that process sequences by iterating through elements.\n",
        "- **Examples:** Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)\n",
        "- **Pros:**\n",
        "  - Effective for capturing sequential dependencies in data.\n",
        "  - Suitable for tasks like language modeling and machine translation.\n",
        "- **Cons:**\n",
        "  - Vulnerable to vanishing and exploding gradient problems.\n",
        "  - Computationally expensive to train.\n",
        "\n",
        "### 2.2.3 Transformers\n",
        "- **Description:** Neural network architecture based entirely on self-attention mechanisms.\n",
        "- **Examples:** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer)\n",
        "- **Pros:**\n",
        "  - Captures long-range dependencies effectively.\n",
        "  - Parallelizable training process.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of computational resources.\n",
        "  - Limited interpretability compared to traditional models.\n",
        "\n",
        "## 2.3 Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "### 2.3.1 Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)\n",
        "- **Description:** A variant of RAG that incorporates knowledge graphs to enhance retrieval and generation.\n",
        "- **Examples:** Graph-BERT\n",
        "- **Pros:**\n",
        "  - Integrates structured knowledge for improved understanding and generation.\n",
        "  - Enables more coherent and contextually relevant responses.\n",
        "- **Cons:**\n",
        "  - Requires high-quality and curated knowledge graphs.\n",
        "  - Increased computational complexity compared to standard RAG.\n",
        "\n",
        "## 2.4 Other Architectures and Models\n",
        "\n",
        "### 2.4.1 Pre-trained Models\n",
        "- **Description:** Models pre-trained on large corpora and fine-tuned for specific tasks.\n",
        "- **Examples:** BERT, GPT, T5\n",
        "- **Pros:**\n",
        "  - Leverage large amounts of unlabeled data for pre-training.\n",
        "  - Achieve state-of-the-art performance on various NLP tasks.\n",
        "- **Cons:**\n",
        "  - Resource-intensive pre-training process.\n",
        "  - May require substantial computational resources for fine-tuning.\n",
        "\n",
        "### 2.4.2 Retriever-Generator Models\n",
        "- **Description:** Models that combine retrieval and generation components for text generation tasks.\n",
        "- **Examples:** RAG, KERAG\n",
        "- **Pros:**\n",
        "  - Incorporates both structured and unstructured information for generation.\n",
        "  - Produces more diverse and contextually relevant responses.\n",
        "- **Cons:**\n",
        "  - Requires efficient retrieval mechanisms.\n",
        "  - Increased complexity in model architecture.\n",
        "\n",
        "### 2.4.3 Knowledge Graphs\n",
        "- **Description:** Graph-based structures that represent knowledge and relationships between entities.\n",
        "- **Examples:** ConceptNet, WordNet\n",
        "- **Pros:**\n",
        "  - Organizes knowledge in a structured format.\n",
        "  - Enables reasoning and inference for downstream tasks.\n",
        "- **Cons:**\n",
        "  - Requires manual curation and maintenance.\n",
        "  - Limited coverage and scalability.\n",
        "\n",
        "### 2.4.4 Dense Passage Retrieval (DPR)\n",
        "- **Description:** Technique for retrieving relevant passages from a large corpus for question answering.\n",
        "- **Examples:** DenseRetrieval, TANDA\n",
        "- **Pros:**\n",
        "  - Efficient retrieval of contextually relevant information.\n",
        "  - Suitable for large-scale question answering systems.\n",
        "- **Cons:**\n",
        "  - Computationally intensive for indexing large corpora.\n",
        "  - May suffer from noise in retrieved passages.\n",
        "\n",
        "### 2.4.5 Elastic Search\n",
        "- **Description:** Distributed search and analytics engine for indexing and searching large volumes of data.\n",
        "- **Examples:** Elasticsearch, Apache Solr\n",
        "- **Pros:**\n",
        "  - Scalable and distributed architecture.\n",
        "  - Supports full-text search and complex query structures.\n",
        "- **Cons:**\n",
        "  - Requires infrastructure for deployment and maintenance.\n",
        "  - Indexing and search performance may degrade with large datasets.\n",
        "\n",
        "### 2.4.6 Anserini\n",
        "- **Description:** Information retrieval toolkit built on Apache Lucene for research purposes.\n",
        "- **Examples:** Pyserini, Anserini\n",
        "- **Pros:**\n",
        "  - Provides efficient indexing and retrieval capabilities.\n",
        "  - Supports integration with various retrieval models.\n",
        "- **Cons:**\n",
        "  - Requires expertise in information retrieval concepts.\n",
        "  - Limited documentation and community support.\n",
        "\n",
        "### 2.4.7 BART (Bidirectional and Auto-Regressive Transformers)\n",
        "- **Description:** Transformer-based model architecture capable of bidirectional and auto-regressive generation.\n",
        "- **Examples:** Facebook BART\n",
        "- **Pros:**\n",
        "  - Supports both conditional and unconditional text generation.\n",
        "  - Achieves state-of-the-art performance on various NLP tasks.\n",
        "- **Cons:**\n",
        "  - Requires large amounts of computational resources for training.\n",
        "  - Limited interpretability of generated outputs.\n",
        "\n",
        "## 2.5 Machine Learning Frameworks and Tools\n",
        "\n",
        "### 2.5.1 TensorFlow\n",
        "- **Description:** Open-source machine learning framework developed by Google for building and deploying ML models.\n",
        "- **Pros:**\n",
        "  - Comprehensive ecosystem with support for various deep learning architectures.\n",
        "  - Scalable and efficient execution on both CPUs and GPUs.\n",
        "- **Cons:**\n",
        "  - Steeper learning curve compared to some other frameworks.\n",
        "  - Limited support for dynamic computation graphs.\n",
        "\n",
        "### 2.5.2 PyTorch\n",
        "- **Description:** Open-source deep learning framework developed by Facebook's AI Research lab.\n",
        "- **Pros:**\n",
        "  - Pythonic and intuitive interface for model development.\n",
        "  - Dynamic computation graph enables easier debugging and experimentation.\n",
        "- **Cons:**\n",
        "  - Less optimized for deployment in production compared to TensorFlow.\n",
        "  - Limited built-in support for distributed training.\n",
        "\n",
        "### 2.5.3 scikit-learn\n",
        "- **Description:** Simple and efficient machine learning library built on NumPy, SciPy, and matplotlib.\n",
        "- **Pros:**\n",
        "  - Easy-to-use API for common machine learning tasks.\n",
        "  - Comprehensive documentation and community support.\n",
        "- **Cons:**\n",
        "  - Limited support for deep learning models and architectures.\n",
        "  - Less flexibility for customization compared to deep learning frameworks.\n",
        "\n",
        "## 2.6 Topic Modeling and Dimensionality Reduction\n",
        "\n",
        "### 2.6.1 Latent Dirichlet Allocation (LDA)\n",
        "- **Description:** Probabilistic generative model for topic modeling in text corpora.\n",
        "- **Pros:**\n",
        "  - Provides interpretable topics from document collections.\n",
        "  - Flexible and scalable for large datasets.\n",
        "- **Cons:**\n",
        "  - Assumes documents are mixtures of topics, which may not always hold true.\n",
        "  - Requires manual tuning of hyperparameters for optimal performance.\n",
        "\n",
        "### 2.6.2 Principal Component Analysis (PCA)\n",
        "- **Description:** Technique for dimensionality reduction by projecting data onto a lower-dimensional subspace.\n",
        "- **Pros:**\n",
        "  - Reduces dimensionality while preserving most of the variance in the data.\n",
        "  - Computationally efficient and widely applicable.\n",
        "- **Cons:**\n",
        "  - Linear transformation may not capture complex nonlinear relationships.\n",
        "  - Assumes linear relationships between variables.\n",
        "\n",
        "### 2.6.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "- **Description:** Nonlinear dimensionality reduction technique for visualizing high-dimensional data in low-dimensional space.\n",
        "- **Pros:**\n",
        "  - Preserves local structure and clusters in the data.\n",
        "  - Effective for visualizing complex datasets in two or three dimensions.\n",
        "- **Cons:**\n",
        "  - Computationally expensive for large datasets.\n",
        "  - Interpretation of results can be subjective and dependent on hyperparameters.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This document provides an overview of various architectures, models, and tools used in natural language processing tasks. Understanding the strengths and weaknesses of different approaches is crucial for designing effective NLP systems tailored to specific use cases and requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "# 3. Data Collection and Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHg-WlWRLIL"
      },
      "source": [
        "## 3.1 Data Ethics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyBofE4Cu1Vr"
      },
      "source": [
        "The data collected here is a collection of posts from widely avaialble public sources. However, should this project move into a public forum additional steps will be necessary to endure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the projects goals and data gathering process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otWnw_6_mgNg"
      },
      "source": [
        "## 3.2 Data Collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVgpk9xJyba"
      },
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cjtRklP2AjE",
        "outputId": "c736dfb5-52a4-421f-b25b-e11e515ad642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 300\n",
            "Processing additional 100 threads\n",
            "Ending with thread_id 400\n"
          ]
        }
      ],
      "source": [
        "# Create Corpus\n",
        "\n",
        "# Set the base path to save files\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Create URLs from the thread_ids and save to a CSV\n",
        "def create_urls():\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_thread_ids.csv'\n",
        "\n",
        "    # Set the number of incremental thread_ids to process\n",
        "    threads = 100\n",
        "\n",
        "    # Check if the file exists and has content. If it does, update last_thread_id\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = e9_forum_thread_ids['thread_id'].iloc[-1]\n",
        "        last_thread_id = int(last_thread_id)  # Convert to integer\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "\n",
        "    urls = []\n",
        "    for thread_id in range(last_thread_id + 1, last_thread_id + threads + 1):\n",
        "        urls.append({'thread_id': thread_id})\n",
        "\n",
        "    last_thread_id_processed = urls[-1]['thread_id']\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    e9_forum_thread_ids = pd.DataFrame(urls)\n",
        "\n",
        "    # Save DataFrame to CSV file\n",
        "    e9_forum_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    print(\"Starting with thread_id \" + str(last_thread_id))\n",
        "    print(\"Processing additional \" + str(threads) + \" threads\")\n",
        "    print(\"Ending with thread_id \" + str(last_thread_id_processed))\n",
        "\n",
        "\n",
        "    return last_thread_id, last_thread_id_processed, e9_forum_thread_ids\n",
        "\n",
        "# Ingest thread_ids and return title, id and URL\n",
        "def fetch_thread_data(df):\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_threads.csv'\n",
        "\n",
        "    # Set the number of pages to process\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "\n",
        "    df.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Find the first post in the thread creation\n",
        "def fetch_first_post_content(df):\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_threads_decorated.csv'\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        if first_post:\n",
        "            post_content = first_post.get_text(strip=True)\n",
        "        else:\n",
        "            post_content = \"No content found\"  # Handle case where no post content is found\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_first_post': post_content})\n",
        "\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    e9_forum_threads_decorated = pd.DataFrame(data)\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_threads_decorated.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    return e9_forum_threads_decorated\n",
        "\n",
        "\n",
        "# Original UDF to fetch and parse thread posts\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "            post_data.append({\n",
        "                'thread_id': row['thread_id'],\n",
        "                'post_timestamp': post_timestamp,\n",
        "                'post_raw': content\n",
        "            })\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "\n",
        "    e9_forum_posts['post_raw'] = e9_forum_posts['post_raw'].astype(str)\n",
        "\n",
        "    # Define the output path\n",
        "    output_path = os.path.join(BASE_PATH, 'e9_forum_posts.csv')\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_posts.to_csv(output_path, index=False)\n",
        "\n",
        "    return e9_forum_posts\n",
        "\n",
        "\n",
        "# Define the UDF to process the data\n",
        "def create_forum_corpus(e9_forum_posts, e9_forum_threads_decorated):\n",
        "\n",
        "    # Group by THREAD_ID and concatenate the POST_RAW values\n",
        "    aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # Rename the column to indicate that it contains concatenated post content\n",
        "    aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    # Ensure thread_id columns are of type int64\n",
        "    e9_forum_threads_decorated['thread_id'] = e9_forum_threads_decorated['thread_id'].astype('int64')\n",
        "    aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "    # Define the output path\n",
        "    output_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_corpus.to_csv(output_path, index=False)\n",
        "\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    # Execute the function and print results\n",
        "    last_thread_id, last_thread_id_processed, e9_forum_thread_ids = create_urls()\n",
        "\n",
        "    # Fetch thread URLs and title\n",
        "    e9_forum_threads = fetch_thread_data(e9_forum_thread_ids)\n",
        "\n",
        "    # Fetch first post content\n",
        "    e9_forum_threads_decorated = fetch_first_post_content(e9_forum_threads)\n",
        "\n",
        "    # Fetch all thread post content\n",
        "    e9_forum_posts = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "    e9_forum_corpus = create_forum_corpus(e9_forum_posts, e9_forum_threads_decorated)\n",
        "\n",
        "\n",
        "# Ensure the main function is called\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 3.3 Data Storage and Database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL1MSXuDKXbx"
      },
      "source": [
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "### Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQYy-5Y4y7gA",
        "outputId": "8fd3e85a-aeb1-4f14-cc2a-58b209370d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n",
            "e9_forum_corpus table created successfully.\n",
            "Data inserted into e9_forum_corpus table.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   THREAD_ID          400 non-null    int16 \n",
            " 1   THREAD_TITLE       400 non-null    object\n",
            " 2   THREAD_FIRST_POST  400 non-null    object\n",
            " 3   THREAD_ALL_POSTS   400 non-null    object\n",
            "dtypes: int16(1), object(3)\n",
            "memory usage: 10.3+ KB\n"
          ]
        }
      ],
      "source": [
        "# Store Corpus\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the e9_forum_corpus DataFrame from the CSV file\n",
        "e9_forum_corpus = pd.read_csv(BASE_PATH + 'e9_forum_corpus.csv')\n",
        "\n",
        "def load_credentials(credentials_path):\n",
        "    \"\"\"Load Snowflake credentials from a file and set them as environment variables.\"\"\"\n",
        "    with open(credentials_path, 'r') as file:\n",
        "        for line in file:\n",
        "            key, value = line.strip().split('=')\n",
        "            os.environ[key] = value\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "def create_db_and_schema(cur):\n",
        "    \"\"\"Create the database and schema in Snowflake.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        print(\"Database and schema created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database and schema: {e}\")\n",
        "\n",
        "def create_table_if_not_exists(cur):\n",
        "    \"\"\"Create the e9_forum_corpus table if it does not exist.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            thread_id NUMBER(38,0),\n",
        "            thread_title VARCHAR(16777216),\n",
        "            thread_first_post VARCHAR(16777216),\n",
        "            thread_all_posts VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table: {e}\")\n",
        "\n",
        "def insert_data_into_table(cur, df):\n",
        "    \"\"\"Insert data from the DataFrame into the e9_forum_corpus table.\"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        row = row.where(pd.notnull(row), None)\n",
        "        insert_command = f\"\"\"\n",
        "        INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "        (thread_id, thread_title, thread_first_post, thread_all_posts)\n",
        "        VALUES (%s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cur.execute(insert_command, (\n",
        "                row['thread_id'], row['thread_title'],\n",
        "                row['thread_first_post'], row['thread_all_posts']\n",
        "            ))\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting data: {e}\")\n",
        "\n",
        "def fetch_data_from_table(cur):\n",
        "    \"\"\"Fetch all data from the e9_forum_corpus table.\"\"\"\n",
        "    query = \"SELECT * FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    cur.execute(query)\n",
        "    return cur.fetch_pandas_all()\n",
        "\n",
        "def main():\n",
        "    # Load Snowflake credentials\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "\n",
        "    # Connect to Snowflake\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create the database, schema, and table if they don't exist\n",
        "    create_db_and_schema(cur)\n",
        "    create_table_if_not_exists(cur)\n",
        "\n",
        "    # Insert data into the table\n",
        "    insert_data_into_table(cur, e9_forum_corpus)\n",
        "    conn.commit()\n",
        "    print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "    # Fetch data from the table\n",
        "    e9_forum_corpus_df = fetch_data_from_table(cur)\n",
        "    e9_forum_corpus_df.info()\n",
        "\n",
        "    # Close cursor and connection\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbFswHVvn7Ny",
        "outputId": "e7409331-2a11-4901-a0f1-f481ddf5fea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Clean corpus\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "import snowflake.connector\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish connection to the Snowflake database.\"\"\"\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ['USER'],\n",
        "        password=os.environ['PASSWORD'],\n",
        "        account=os.environ['ACCOUNT'],\n",
        "    )\n",
        "    return conn\n",
        "\n",
        "def fetch_data(conn):\n",
        "    \"\"\"Fetch data from the Snowflake database.\"\"\"\n",
        "    cur = conn.cursor()\n",
        "    query = \"\"\"\n",
        "    SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    df = cur.fetch_pandas_all()\n",
        "    cur.close()\n",
        "    return df\n",
        "\n",
        "def alpha_numeric(df):\n",
        "    \"\"\"Removes non-alphanumeric characters and unwanted patterns from text.\"\"\"\n",
        "    pattern_email = re.compile(r'\\S*@\\S*\\s?')\n",
        "    pattern_url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    pattern_non_alpha = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: pattern_non_alpha.sub('', pattern_url.sub('', pattern_email.sub('', str(text)))))\n",
        "    return df\n",
        "\n",
        "def remove_stop_words(df):\n",
        "    \"\"\"Removes stop words from the text.\"\"\"\n",
        "    stop_words_set = set(stopwords.words('english')).union({'car', 'csi', 'cs', 'csl', 'e9', 'coupe', 'http', 'https', 'www', 'ebay', 'bmw', 'html'})\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in stop_words_set]))\n",
        "    return df\n",
        "\n",
        "def tokenize_and_lemmatize(df):\n",
        "    \"\"\"Tokenizes and lemmatizes the text in specified columns.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]))\n",
        "    return df\n",
        "\n",
        "def clean_nan_values(df):\n",
        "    \"\"\"Removes or replaces NaN values in the dataset.\"\"\"\n",
        "    # Option to replace NaN values with an empty string or other placeholder\n",
        "    df.fillna('', inplace=True)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data processing pipeline.\"\"\"\n",
        "    conn = connect_to_snowflake()\n",
        "    df = fetch_data(conn)\n",
        "    df = alpha_numeric(df)\n",
        "    df = remove_stop_words(df)\n",
        "    df = tokenize_and_lemmatize(df)\n",
        "    df = clean_nan_values(df)  # Final NaN cleaning step\n",
        "    df.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/cleaned_corpus.csv', index=False)\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "### 2.4.1 Summarization Strategies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JGc6LreKKyu"
      },
      "source": [
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through extractive or abstractive methods, or a combination of both.\n",
        "\n",
        "### Extractive Summarization\n",
        "- **Tools:** OpenNMT, Sumy, Gensim\n",
        "    - **Pros:**\n",
        "        - **Good with Raw Text:** Works directly with raw text, selecting key sentences without deep processing.\n",
        "        - **Straightforward Implementation:** Simplifies implementation without needing complex preprocessing.\n",
        "    - **Cons:**\n",
        "        - **Limited Depth in Understanding:** May not fully grasp nuanced meanings in complex texts.\n",
        "        - **Less Effective with Poorly Structured Text:** Struggles with identifying main points in unstructured or informal text.\n",
        "\n",
        "### Abstractive Summarization\n",
        "- **Tools:** sshleifer/distilbart-cnn-12-6, T5, BERTSum\n",
        "    - **Pros:**\n",
        "        - **Advanced Processing Capabilities:** Designed to interpret and rephrase raw text, capturing contextual nuances.\n",
        "        - **Higher Tolerance for Unstructured Text:** Manages and refines unstructured or informal text into coherent summaries.\n",
        "    - **Cons:**\n",
        "        - **Dependence on Preprocessing:** The output's quality can be improved with proper preprocessing for complex texts.\n",
        "        - **Potential Overhead:** More computational resources required for understanding raw text.\n",
        "\n",
        "### Hybrid Summarization\n",
        "- **Combines extractive and abstractive methods for a balanced approach to summarization.**\n",
        "    - **Pros:**\n",
        "        - **Flexibility in Text Processing:** Handles both raw and preprocessed text, adapting to text complexity.\n",
        "        - **Balanced Approach:** Leverages strengths of both methods for identifying key points and generating summaries.\n",
        "    - **Cons:**\n",
        "        - **Complex Preprocessing Requirements:** Integrating both methods may require sophisticated preprocessing.\n",
        "        - **Potential for Processing Inefficiencies:** Could lead to redundancies or inefficiencies if not carefully managed.\n",
        "\n",
        "*Note: After attempting sshleifer/distilbart-cnn-12-6, which has a character limit too restrictive for my needs, T5 was chosen for its lack of character limits and broader applicability to the project's goals.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJtWgaQAQPQH",
        "outputId": "4b9e9820-c7e4-4996-9f3c-96fc5d888333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'New owner Goin NC drive home NJ cant wait post experience read laugh cry critiqueI look forward coming lurker statusRegards Congrats NJ New OwnerCongrats pecsokfrom one new owner another javascriptemoticonBack left coast last week wrap thing Coupeking due arrive 2nd week MayWill look forward meeting ristate coupstersChuckL Hi ChuckHow thing go imagine look great Agave CoupeHi LenCar way look great cant wait start active ownership CoupeKing experience immersive Take look latest Bimmer Magazine ge', 1: 'looksfamiliarI suspected might board went MIA day mye28com DNS problem time 8So DanG carving board forum Consider vote General conversation Tech talk Tech FAQ would usefulBroken External Image ForumsWe break anyway want limited amount traffic get coupe dont need break muchGeneral DiscussionOffTopicTechnical ArticlesSite suggestion questionsClassifiedsNot sure need Tech Talk area since General Discussion area probably wont get busy always add laterDan good back chatting amongst owner world Ive ma', 4: 'purchased discovered tranny ran dry purchase 5speed tranny would work current Getrag 225 speedo cable attached fact soft plug place gear source fluid leak would Getrag number beThanksDoug TrannyHi DougYou want 265 Getrag Bullet proof reliable fairly easy find Youll need shorten driveshaft hard job youre mechanically inclined TrannyHi DougYou want 265 Getrag Bullet proof reliable fairly easy find Youll need shorten driveshaft hard job youre mechanically inclined TrannyWill need something special ', 3: 'anyone know 364 lsd sale thx mike Open LSD Opens everywhere need LSD might take ratio lsd purchase 364 crown pinion rebuild ideal cost wise whole rear end inn part looking getting unit 73 euro probably 325 lsdI Located Santa CruzCheersPJ PJI would buy unit lsd much would want itps tell lsd either looking white painted case trying twist one shaft opposite direction otherThx Mike sound great email offline get going side note dealer trader breaker proceeds going back 73 namely turbo 35 motor Mikeem', 2: 'possible cause hiccup Im driving road happens speed tach fluctuate wildly act momentarily getting fuel filter new leaksOne time actually cut entirely drivingalmost like ran gas minute able start againMany thanksBrooks type ofcarbs FIclogged jet somewhereshanon Clogged fuel filter dying fuel pump perhaps 1972 30 sorry made clear initiallyBrooks tach telling something Especially carburetted car fuel issue cause tach move smoothly twitch ignition problem low voltage side probably connection problem'}\n"
          ]
        }
      ],
      "source": [
        "# Summarization and clustering\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def consolidate_threads(df, n_topics=10):\n",
        "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    data_vectorized = vectorizer.fit_transform(df['THREAD_ALL_POSTS'])\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
        "    lda.fit(data_vectorized)\n",
        "    df['Topic'] = lda.transform(data_vectorized).argmax(axis=1)\n",
        "    return df\n",
        "\n",
        "def semantic_clustering(df, n_clusters=5):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['THREAD_ALL_POSTS'])\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    df['Cluster'] = kmeans.labels_\n",
        "    return df\n",
        "\n",
        "def summarize_clusters(df):\n",
        "    summaries = {}\n",
        "    for cluster in df['Cluster'].unique():\n",
        "        cluster_text = \" \".join(df[df['Cluster'] == cluster]['THREAD_ALL_POSTS'])\n",
        "        # Placeholder for summarization logic\n",
        "        summaries[cluster] = cluster_text[:500]  # Simplified example\n",
        "    return summaries\n",
        "\n",
        "def main():\n",
        "    # Load the preprocessed data\n",
        "    lem_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/cleaned_corpus.csv')\n",
        "\n",
        "    # Handle NaN values explicitly\n",
        "    lem_df['THREAD_ALL_POSTS'].fillna('', inplace=True)  # Replace NaN with empty strings\n",
        "\n",
        "    # Apply the processing functions\n",
        "    lem_df = consolidate_threads(lem_df)\n",
        "    lem_df = semantic_clustering(lem_df)\n",
        "    cluster_summaries = summarize_clusters(lem_df)\n",
        "\n",
        "    # Optionally, save or display the results\n",
        "    print(cluster_summaries)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "Z4MkQ7DMQPSl",
        "outputId": "df0bc235-a5c8-404f-9478-004b2b0c8f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1068831350300487398882439700399\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1068831350300487398882439700399_data = {\"mdsDat\": {\"x\": [-11.943994522094727, 14.958772659301758, -81.74794006347656, -48.136749267578125, 50.656837463378906, -53.14125442504883, -17.03994369506836, 9.434487342834473, 79.3235092163086, 46.26103591918945], \"y\": [90.2683334350586, 30.69865608215332, -1.0470157861709595, 45.647613525390625, 77.89014434814453, -58.06718826293945, -11.306497573852539, -70.90225219726562, 20.867860794067383, -26.39545440673828], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [23.532277217093466, 13.912262073406456, 11.634299924767026, 9.91333188344743, 9.66230040561496, 9.46930485058099, 7.182897277733781, 5.510393271023226, 4.749699978229756, 4.433233118102907]}, \"tinfo\": {\"Term\": [\"injector\", \"ecu\", \"switch\", \"fuel\", \"steering\", \"sensor\", \"plug\", \"engine\", \"window\", \"light\", \"gm\", \"stock\", \"air\", \"wheel\", \"seat\", \"motronic\", \"box\", \"problem\", \"color\", \"key\", \"oil\", \"door\", \"motor\", \"16\", \"black\", \"fuse\", \"cam\", \"idle\", \"jet\", \"im\", \"brace\", \"911\", \"clunk\", \"technology\", \"linda\", \"heading\", \"bodyshop\", \"fiat\", \"replica\", \"theyve\", \"magazine\", \"distribution\", \"industry\", \"30k\", \"lap\", \"molex\", \"giving\", \"twist\", \"morris\", \"strut\", \"coupeking\", \"gt\", \"clunking\", \"cvs\", \"124\", \"civic\", \"tep\", \"ramp\", \"ski\", \"525i\", \"motorcycle\", \"turbo\", \"estate\", \"family\", \"optima\", \"porsche\", \"sale\", \"insert\", \"rally\", \"bushing\", \"ford\", \"wife\", \"country\", \"battery\", \"vw\", \"bucket\", \"sold\", \"year\", \"daily\", \"event\", \"garage\", \"great\", \"track\", \"friend\", \"home\", \"got\", \"subframe\", \"suspension\", \"day\", \"good\", \"end\", \"car\", \"time\", \"work\", \"nice\", \"like\", \"rear\", \"look\", \"old\", \"know\", \"fun\", \"new\", \"point\", \"ive\", \"im\", \"really\", \"way\", \"make\", \"engine\", \"want\", \"lot\", \"think\", \"dont\", \"right\", \"going\", \"caliper\", \"364\", \"ebrake\", \"metric\", \"vented\", \"bidding\", \"getrag\", \"propshaft\", \"39\", \"345\", \"experiencing\", \"e12s\", \"od\", \"ratio\", \"lsd\", \"taller\", \"mechanically\", \"633\", \"diffs\", \"2655\", \"worthwhile\", \"5th\", \"enginetrans\", \"manual\", \"interchange\", \"tighten\", \"5spd\", \"diff\", \"betweeen\", \"derived\", \"steering\", \"dogleg\", \"box\", \"e3\", \"5speed\", \"cr\", \"tranny\", \"265\", \"transmission\", \"seat\", \"centre\", \"brake\", \"speed\", \"chain\", \"rail\", \"s38\", \"rear\", \"need\", \"interior\", \"power\", \"series\", \"original\", \"unit\", \"used\", \"new\", \"know\", \"like\", \"want\", \"e12\", \"fit\", \"use\", \"engine\", \"time\", \"im\", \"dont\", \"think\", \"right\", \"look\", \"good\", \"make\", \"way\", \"drive\", \"mechanism\", \"release\", \"propane\", \"regulator\", \"entry\", \"audio\", \"soaking\", \"freed\", \"riley\", \"tilt\", \"window\", \"barrel\", \"continuity\", \"diagram\", \"switch\", \"lube\", \"prevents\", \"etype\", \"1011\", \"extinguisher\", \"protectedoremail\", \"shit\", \"hans\", \"wd40\", \"engages\", \"penetrating\", \"messing\", \"rattling\", \"iron\", \"btw\", \"blow\", \"wired\", \"offending\", \"voltage\", \"veloce\", \"breaker\", \"choke\", \"wire\", \"temperature\", \"glue\", \"ground\", \"65\", \"pop\", \"motor\", \"fuse\", \"electrical\", \"light\", \"circuit\", \"sender\", \"gauge\", \"causing\", \"instrument\", \"gear\", \"remove\", \"connection\", \"problem\", \"relay\", \"throttle\", \"post\", \"work\", \"tank\", \"start\", \"rear\", \"like\", \"thing\", \"time\", \"im\", \"running\", \"make\", \"close\", \"bad\", \"engine\", \"door\", \"good\", \"sure\", \"look\", \"year\", \"know\", \"way\", \"need\", \"think\", \"shifter\", \"loop\", \"stamped\", \"horizontal\", \"autocross\", \"autocrossed\", \"ice\", \"tan\", \"sanding\", \"m1\", \"cabin\", \"wet\", \"vin\", \"bbq\", \"blaster\", \"wharf\", \"autocrossing\", \"grit\", \"removable\", \"redone\", \"soda\", \"cup\", \"blasting\", \"cloth\", \"pinched\", \"nozzle\", \"imo\", \"knowjoseph\", \"omg\", \"transformation\", \"528i\", \"pn\", \"heater\", \"tow\", \"gray\", \"carpet\", \"leather\", \"grey\", \"carrier\", \"blue\", \"layer\", \"dust\", \"wheel\", \"interior\", \"floor\", \"shift\", \"hi\", \"fit\", \"plate\", \"seat\", \"fender\", \"panel\", \"dark\", \"alpinas\", \"hot\", \"free\", \"look\", \"thanks\", \"use\", \"like\", \"new\", \"good\", \"need\", \"got\", \"car\", \"think\", \"black\", \"set\", \"know\", \"used\", \"year\", \"paint\", \"want\", \"rear\", \"time\", \"dont\", \"work\", \"thing\", \"octane\", \"gps\", \"beam\", \"gasoline\", \"density\", \"maf\", \"forged\", \"toluene\", \"integrated\", \"btdc\", \"illuminate\", \"b35\", \"opposed\", \"5series\", \"advanced\", \"repeater\", \"observation\", \"b34\", \"premium\", \"flywheel\", \"afm\", \"stronger\", \"89\", \"87k\", \"fooling\", \"ping\", \"drilling\", \"harmonic\", \"lites\", \"m53535\", \"pinging\", \"significantly\", \"signal\", \"map\", \"parking\", \"m30b35\", \"retard\", \"91\", \"oil\", \"ignition\", \"spoiler\", \"compression\", \"pertronix\", \"head\", \"piston\", \"light\", \"euro\", \"boost\", \"high\", \"engine\", \"motronic\", \"unit\", \"air\", \"gas\", \"pressure\", \"headlight\", \"use\", \"pump\", \"low\", \"like\", \"turn\", \"im\", \"fuse\", \"need\", \"run\", \"way\", \"dont\", \"work\", \"car\", \"think\", \"going\", \"ive\", \"thing\", \"know\", \"venturi\", \"foam\", \"injected\", \"dynamat\", \"carbs\", \"steady\", \"headliner\", \"mesa\", \"pitted\", \"oxygen\", \"core\", \"145\", \"availability\", \"jet\", \"airdam\", \"priming\", \"screwy\", \"champ\", \"6505740561\", \"evan\", \"bath\", \"complaining\", \"primary\", \"solenoid\", \"register\", \"needle\", \"dizzy\", \"diaphram\", \"satisfaction\", \"wegweiser\", \"korman\", \"faq\", \"runs\", \"remaining\", \"idle\", \"underhood\", \"secondary\", \"fuel\", \"linkage\", \"185\", \"pump\", \"material\", \"cold\", \"weber\", \"heat\", \"badge\", \"air\", \"valve\", \"main\", \"carb\", \"stumble\", \"problem\", \"check\", \"tank\", \"stuff\", \"manifold\", \"point\", \"running\", \"way\", \"start\", \"need\", \"like\", \"thanks\", \"good\", \"leak\", \"run\", \"dont\", \"engine\", \"year\", \"work\", \"new\", \"car\", \"right\", \"look\", \"know\", \"time\", \"im\", \"injector\", \"thermostat\", \"impedance\", \"motronics\", \"043\", \"csr\", \"036\", \"pot\", \"wbo2\", \"gerrit\", \"jag\", \"january\", \"finest\", \"march\", \"sorn\", \"creating\", \"iii\", \"armrest\", \"troubleshooting\", \"dagmars\", \"brilliant\", \"ms\", \"rapidly\", \"lowimpedence\", \"years\", \"flowed\", \"hibernation\", \"ensure\", \"pinking\", \"analog\", \"john\", \"taxed\", \"headlamp\", \"pack\", \"530\", \"ohm\", \"flow\", \"bosch\", \"ljet\", \"resistance\", \"ecu\", \"current\", \"rate\", \"resistor\", \"lean\", \"motronic\", \"list\", \"scope\", \"coil\", \"djet\", \"fuel\", \"checked\", \"problem\", \"run\", \"sensor\", \"set\", \"think\", \"intake\", \"air\", \"engine\", \"good\", \"email\", \"running\", \"dont\", \"used\", \"line\", \"time\", \"know\", \"need\", \"car\", \"number\", \"going\", \"like\", \"make\", \"im\", \"gm\", \"programming\", \"module\", \"reservior\", \"programmable\", \"motivation\", \"stuffi\", \"wolf\", \"constant\", \"msd\", \"crimp\", \"programmed\", \"premarked\", \"splice\", \"hindsight\", \"debating\", \"specialize\", \"converting\", \"3100\", \"quoted\", \"manually\", \"trigger\", \"confused\", \"parameter\", \"ripped\", \"triangular\", \"bleed\", \"lajolla\", \"vr\", \"ferrari\", \"verona\", \"key\", \"tuning\", \"connector\", \"aftermarket\", \"windshield\", \"sensor\", \"megasquirt\", \"ecu\", \"fully\", \"master\", \"chip\", \"monica\", \"paste\", \"tool\", \"stock\", \"malc\", \"aluminum\", \"dealer\", \"fluid\", \"im\", \"glass\", \"using\", \"like\", \"paint\", \"mile\", \"need\", \"make\", \"time\", \"control\", \"work\", \"know\", \"try\", \"harness\", \"air\", \"sure\", \"look\", \"going\", \"ignition\", \"dont\", \"use\", \"original\", \"really\", \"year\", \"available\", \"think\", \"new\", \"mario\", \"wing\", \"r134a\", \"3033\", \"resonator\", \"mechanics\", \"visually\", \"bmwcca\", \"4310316\", \"marios\", \"cheeky\", \"climate\", \"dryer\", \"branch\", \"york\", \"compressor\", \"turbos\", \"touching\", \"tooth\", \"doable\", \"condenser\", \"tdc\", \"rig\", \"stupid\", \"roundel\", \"grill\", \"row\", \"widely\", \"stahl\", \"undertaking\", \"lhd\", \"stroke\", \"bored\", \"factory\", \"color\", \"ac\", \"black\", \"cam\", \"bore\", \"chain\", \"vent\", \"newer\", \"hood\", \"adjustment\", \"timing\", \"exhaust\", \"rhd\", \"engine\", \"german\", \"secondary\", \"muffler\", \"replace\", \"know\", \"dont\", \"valve\", \"stock\", \"idea\", \"panel\", \"right\", \"think\", \"new\", \"like\", \"original\", \"good\", \"set\", \"need\", \"door\", \"head\", \"post\", \"number\", \"year\", \"ive\", \"r12\", \"wr9ls\", \"silbers\", \"r134\", \"refrigerant\", \"bilstien\", \"electrode\", \"et24\", \"freon\", \"shim\", \"15mm\", \"2255016\", \"tires\", \"recharge\", \"gap\", \"rim\", \"offset\", \"rake\", \"traction\", \"yoko\", \"w7dc\", \"2055516rear\", \"14x7\", \"interfere\", \"subwoofer\", \"survey\", \"restos\", \"af\", \"oversized\", \"interference\", \"sway\", \"et11\", \"3200cs\", \"lug\", \"staggered\", \"st\", \"plug\", \"chamber\", \"16\", \"combustion\", \"bilsteins\", \"psi\", \"size\", \"ride\", \"hinge\", \"alpina\", \"spring\", \"tire\", \"bar\", \"inch\", \"carl\", \"door\", \"14\", \"compression\", \"number\", \"alpinas\", \"look\", \"new\", \"wheel\", \"set\", \"right\", \"problem\", \"stock\", \"use\", \"want\", \"thanks\", \"know\", \"need\", \"engine\", \"thing\"], \"Freq\": [128.0, 99.0, 112.0, 138.0, 108.0, 92.0, 59.0, 332.0, 80.0, 138.0, 36.0, 111.0, 114.0, 123.0, 125.0, 82.0, 127.0, 218.0, 63.0, 47.0, 77.0, 97.0, 158.0, 45.0, 64.0, 67.0, 55.0, 70.0, 40.0, 285.0, 31.17562837554481, 14.665166949748967, 11.751560241585933, 8.837836889958075, 7.866739198146786, 7.866718934924243, 7.866400156037669, 15.636371958064373, 6.895508858196889, 6.895495922903031, 6.8954466840327, 6.895421464978931, 5.924338841420388, 5.92433543350519, 5.924313562984912, 5.924235099265074, 5.9241284466303075, 5.9223913672521755, 5.903359074016676, 49.394831880661705, 10.780331903320638, 10.78026469232293, 4.953137269254473, 4.95313726924226, 4.95313542954321, 4.953135429540413, 4.953135077356731, 4.953133589433054, 4.953129908154855, 4.953129908152198, 15.553716498034492, 16.402395337762492, 9.800532133269831, 10.667428190910897, 9.809149395481526, 15.635755795305647, 30.175739038471736, 18.449314895525497, 17.358227035038542, 11.70703004589934, 17.578779144488788, 33.474346104777716, 12.998671350126616, 43.65294170043008, 22.43479302393141, 28.259487201146417, 33.43299585129693, 138.81537690407254, 14.393011677095588, 18.877452441481196, 35.21033880293697, 75.54105827645368, 41.45256277513883, 28.22768685973039, 33.81705611562989, 73.29069063263155, 15.48137748464006, 31.285392175420938, 50.84255579841646, 99.44671949162986, 55.28325133734359, 81.45494132908134, 93.03297290866948, 82.21215092537193, 47.962708352084846, 100.60474722687293, 71.4812545649502, 74.4586501884835, 51.248207672908116, 75.2566243662889, 27.685566683709748, 65.59903475142119, 40.104714441476894, 47.590309743258935, 63.75764805176911, 48.60915762529096, 53.11469071061712, 52.240056523412115, 60.34755119975843, 44.960807957374726, 41.135883802112154, 45.366177219522534, 45.16278702552859, 42.7635759749851, 41.60128791445282, 25.453346510887705, 12.311957304572015, 9.492430630516179, 9.492427450869885, 8.552590878645947, 5.7330425010524175, 11.26112392754797, 4.793211649683292, 4.793206123370935, 4.793200594651945, 4.793179023731447, 4.7931709714704, 16.00819544843801, 46.91593176248695, 19.48338072984539, 3.8533641132779075, 3.8533592687686538, 3.85335939161172, 3.8533586850865946, 3.8533577052243806, 3.8533358072388206, 3.8533090774950165, 4.603813433514386, 25.08393315772566, 7.612747551969198, 6.672893248420301, 5.871236647267895, 18.408560021110343, 2.913520806680448, 2.9135185729926634, 82.43011723108249, 9.374981245589911, 82.81843712649324, 25.970382262540085, 9.284169896080178, 23.369886855142994, 14.192254707911498, 13.040893422661838, 23.509286795898735, 62.848732810637934, 12.613556920050534, 46.73285327043997, 41.89859530312113, 15.131502748162308, 22.124769406796446, 15.787406987741184, 70.22669165880797, 80.34697722206354, 30.961666350249196, 30.691076227803467, 20.433696786370323, 36.9382677046777, 23.845777527456963, 38.95910295927441, 48.76591616292497, 51.2756950278361, 59.54761861299487, 38.862949829789066, 20.504026498848944, 27.07249938567817, 40.931687246336054, 44.52338869904322, 42.21298168925716, 40.05161104370872, 37.70262724649441, 37.39420320606643, 29.299017693087684, 30.548052586669005, 30.47944370700195, 28.919302281751467, 27.640823801481428, 26.802750612555414, 6.590924495806741, 6.590899805597685, 5.662555082624562, 36.60017349211575, 4.73427612842774, 3.8060361301275116, 3.8060115519210673, 3.8059996237887623, 3.8059603248302682, 3.8059459313120003, 66.44811632228273, 3.795534084446811, 8.129028495728608, 10.153455123345498, 88.13885643095419, 2.877734634947543, 2.877731779094739, 2.8777308490013374, 2.8777289216395587, 2.877723246502531, 2.877720981992102, 2.8777200198707975, 2.877703585390708, 2.877703541396987, 2.877693875679581, 2.8776906126917807, 2.87768571990424, 2.87767335719371, 2.877661075920538, 2.8776558735060855, 5.662550373147453, 5.657448918126699, 5.662639824549954, 29.418502684628052, 5.650951635579889, 6.992204228334073, 8.201707033532873, 47.96333979220832, 12.79413790953153, 6.590935629020023, 33.537565030275644, 4.73432320469049, 13.483851899724144, 77.09036439827351, 35.79680564148253, 15.846307961362974, 58.13777122980009, 21.46638173088022, 10.884340319466128, 13.68415581554531, 9.919949312917028, 9.248646775968126, 26.617854741415787, 29.251075661546523, 13.301278580800846, 54.74009251040862, 12.89397182046206, 19.032708902911494, 31.89178576467107, 49.05966113690583, 16.5251693477454, 26.693049454893764, 42.039974314276265, 56.293502990190525, 34.94520835167645, 44.72657211683097, 42.51942349933076, 29.098680261571307, 33.29703550654397, 19.8221720192328, 21.055735730691918, 37.84191920832543, 23.171956370952216, 35.67205329232257, 26.287437289445737, 32.20210635024845, 32.090160301537, 31.53224109676737, 27.701063868569317, 27.641390539082497, 26.71444462663945, 27.861054610804178, 19.530542341345356, 13.051163474791117, 5.646264497558505, 5.646243801573462, 5.646243801573462, 5.646236319854813, 15.130657824435097, 4.720644354954105, 4.720640718814108, 4.7205951355692655, 8.423095120537836, 22.307385684525503, 3.7950319222740965, 3.7950296163051247, 3.7950204061484936, 3.7949930992096923, 3.7949271354701892, 3.794912167002092, 7.598555669443036, 10.27435709272133, 6.571889336647194, 6.571886649654766, 7.139920529944963, 2.8694159547752145, 2.8694159547660805, 2.8694144661140797, 2.8694026056897544, 2.869400469920375, 2.869398436230335, 5.646217324712757, 5.646274133074674, 12.123461736139701, 15.1370936271568, 5.495365901846195, 22.859600347045966, 17.447905788341945, 14.36095636253083, 8.366196660949441, 30.121916164830584, 7.497506141971031, 6.948597799744117, 44.400198283658945, 28.033141747756627, 13.53489228242561, 11.294592748905197, 23.619863420157063, 26.583648244002315, 18.38487404443441, 32.79097134623368, 15.523850158411724, 20.552410437364145, 10.069607986678161, 11.637796268447302, 15.247400050144076, 14.22632665851932, 38.94902527266234, 27.078853892636385, 34.389910687019274, 45.72347848100134, 36.51726308283485, 38.702830744746564, 35.13299439273447, 26.731108813932437, 28.448055369410636, 28.327022409064803, 17.718768575942573, 23.922892001250137, 27.66694101851388, 22.95526936535506, 26.63795301332974, 18.627499189453925, 21.82920288778626, 22.048324747505323, 22.694847607295333, 20.764163293891443, 20.664131762402434, 19.857890967870656, 26.833209280328028, 21.300633641376663, 24.831016763521088, 6.546929408879504, 6.546293753758531, 7.286488360530374, 5.62484594557392, 5.624836083815641, 4.702732204251815, 4.702732171408303, 9.242548546666379, 3.780634160104365, 3.7806324522373043, 3.7806298414777553, 3.780621471660011, 3.780609778414651, 3.7806022944239808, 3.780598265290677, 3.7805873418405276, 11.151550032540284, 11.51398911423192, 2.8585282673934636, 2.8585246393041466, 2.858524639294932, 2.858522303692658, 2.858520566673241, 2.8585202816898856, 2.85851730077208, 2.8585114432019054, 2.858504860093098, 8.391163783873822, 8.321383975429768, 24.969617295687293, 18.664778577572516, 23.42170492086735, 9.313132787064074, 4.702716791751729, 11.998814346946672, 39.66450010222821, 30.971254404142396, 10.384883022887635, 18.907746865365013, 7.287683684778493, 38.60093093673205, 17.175327502703304, 43.30889787888797, 25.19964238639884, 7.469059985024106, 28.473493533942186, 69.66121857995945, 27.80326448236424, 22.83666349874481, 30.941776151947977, 23.13433633603406, 20.45513297037683, 14.430541420671124, 39.76557633252603, 21.146172548894736, 18.98192802236169, 48.27141428936497, 22.807083855341215, 35.54062025748424, 19.24936988603405, 34.30750899813795, 28.266327973401392, 25.83916674725484, 27.04780676091613, 27.90324014410759, 25.590347581852214, 25.256784113972603, 23.812552961377854, 22.02513207201396, 22.128679600092735, 22.417071198310936, 9.248501658100507, 8.33700467774707, 6.504611258739495, 5.588526652315858, 23.840518250893137, 4.576063443453598, 17.477756748334414, 12.00168717990612, 3.7562426161780014, 3.756230612977616, 3.7562114336308277, 3.7561969797197716, 3.7561705158519363, 33.04076410814003, 7.420844326948065, 2.8400858804849434, 2.840080000899711, 2.840073906299293, 2.8400324766357596, 2.8400324766357596, 2.840008605760022, 2.8366261962886052, 7.42086626028128, 4.672379186596626, 7.420841412055135, 4.672388272683337, 7.093273937848965, 1.9239291448320892, 1.9239291447971731, 1.9239273719230952, 3.7562279481805954, 3.756244378016746, 3.7562286945607073, 5.588554460969233, 39.05209981635617, 6.504683665729415, 9.25318531508117, 53.90590025481878, 6.453132369435819, 5.380527759698351, 27.91410817297505, 13.285479624670103, 11.041299011932265, 15.29754903888385, 20.898735541107925, 10.169341866922363, 34.69178433461922, 25.871182500347853, 18.486937843707505, 9.237043632736137, 7.340965913665803, 41.372327104158174, 26.55645874866676, 14.228036509675965, 18.995222929922964, 15.215302156955469, 22.795514090836228, 26.372878755882713, 29.41524201955365, 20.85646599757366, 32.978237792874026, 36.18947182450678, 22.54141976395493, 30.328976158491567, 14.611208619142692, 24.554393366243406, 26.5402685566332, 28.939153401289644, 26.69348217883605, 25.850056801413974, 25.595448797403375, 24.38171092211783, 22.375281257966478, 23.262091609826857, 23.248355503563243, 19.56563486460926, 19.138940937655075, 123.09546701816717, 9.104715259756496, 7.319544340745914, 7.319530684971041, 5.512262660283293, 12.674340637722379, 3.7049582313425002, 3.704958275715136, 3.704914497219884, 7.319574218874095, 2.8013168077634694, 2.8013155345965264, 2.801315481719327, 2.801304496122653, 2.801304496122653, 2.8013046088011713, 2.8012746071456545, 2.801246442701241, 2.7980942245613556, 6.415899931938205, 1.8976662246375475, 1.89766622462986, 1.8976662246228484, 1.8976639698569722, 1.8976516866604274, 1.8976458384463237, 1.8976440118707658, 1.897631412178829, 1.89761888772071, 1.8975975549429889, 4.4369284445956865, 6.267898810736909, 5.512270600008227, 5.454738582381149, 3.101069252976108, 5.767630037829244, 22.682087508248873, 16.931166557348224, 19.20089165816377, 11.980176139295066, 39.5575861069994, 16.207278715207128, 12.491963452570854, 12.58010401172788, 11.819839036815125, 27.449867507302887, 18.336713961687092, 6.137705904427046, 13.79472836854965, 18.564885043589243, 29.92848770382264, 13.58875242369426, 31.471181066078575, 28.426902378647704, 18.731551719359324, 24.892987755141824, 28.805490856466353, 10.464964226786243, 19.003996238460758, 29.233125902559582, 27.811559458492265, 15.600694898139391, 19.87845128739136, 24.18903125514774, 19.954698446234445, 16.18753987337679, 21.260995058376935, 20.795950885195612, 19.93076617958932, 18.935805331011586, 16.395353666388424, 16.790261003067176, 17.898469078700472, 16.26566047879046, 16.282907821193877, 34.582211645133896, 5.404692813790521, 2.7466958806535606, 2.746687949016834, 2.746686560221631, 2.7466574066006526, 2.74664223110689, 6.194778015779176, 5.695403029900435, 6.173975077052318, 1.860664295612159, 1.860664295612159, 1.860633331865647, 1.860633331865647, 1.8606333318623307, 1.8606211399119104, 1.8606211398848096, 1.8606169982773226, 3.632734149491782, 6.614370457121768, 1.708851760373042, 8.976928357464802, 2.746695700820525, 5.521388525932095, 2.74668405577137, 2.746701832208133, 5.4047919195402825, 2.7467056769470157, 4.3920504310465, 2.746701139704787, 7.540619441539024, 24.621139115365096, 8.526024563699519, 14.42999293087244, 6.728502386798126, 9.834991784813393, 32.55958686018133, 7.141308379711748, 32.42442403075816, 8.734930538647221, 7.001448055960265, 7.851727636979648, 4.025976407528877, 3.63273426076472, 14.337065454802985, 29.475230297191285, 17.47896422420577, 9.317726073454454, 8.062911541559638, 10.684900970201333, 36.80220446317527, 9.459603371448308, 15.6409841111926, 30.551135313207688, 14.451164880317629, 12.844171150890308, 24.494072436104467, 20.103251210853607, 22.104584406821857, 10.578349252103164, 19.16359616228588, 18.96950194694144, 13.982690857886562, 10.278086038193582, 13.550333550013237, 14.397554795757264, 16.815861344006972, 14.977565414342925, 11.233565287918594, 14.025381522947304, 13.865786049355425, 12.653021103113444, 12.766325203824954, 13.86693617635078, 11.57628794891677, 12.305695228994725, 11.9614259716008, 3.4917703316492927, 6.898363101942833, 4.185752160227512, 1.7884695974071776, 1.7884695974024065, 1.7884680079505337, 1.7884680079426265, 1.7884680078236588, 1.7884614924309103, 1.7884614924309103, 1.7884538078292787, 1.7884538078243861, 1.7884512698880173, 1.7882999765310303, 3.4506715641316936, 18.39421541711867, 2.640117509868392, 2.640113134662009, 6.898389684140566, 2.640126460703398, 2.520260395043357, 6.046733071002556, 2.542570225907331, 6.04673901602452, 6.832162392541822, 6.898405806647223, 4.343431568493756, 1.788467559297729, 1.7883007287316588, 1.7884673725246838, 4.027274482846499, 3.491774843453956, 3.491774575009741, 18.767064285470642, 22.087375291854812, 12.165159717751523, 20.45966265745545, 17.96987875949076, 6.898393332010017, 8.601687759359557, 8.139562719677064, 6.942214834550419, 10.238046519773208, 5.4639821816299285, 14.042361936247298, 12.658261976163322, 9.210971758352864, 34.00060850901087, 6.939663310512252, 5.195025018889699, 5.126837642609921, 8.657075849948178, 20.655919125683514, 17.329082700308085, 10.788184297655928, 11.466529647124919, 9.165333351266153, 9.327515004389229, 13.148134554553911, 13.981317835092728, 13.40031738384075, 14.905622350735875, 11.245520943586842, 13.241236150635514, 11.569965169244965, 13.04533901544672, 9.93029479441334, 9.50159171245256, 9.59342914424989, 9.550632437731734, 9.584356614207294, 9.292679324777911, 11.145439472273436, 7.7422920661946675, 4.339085800237677, 4.339012215976323, 4.339008395404185, 4.337069978634334, 3.488284013549142, 3.48828365363235, 3.4882760331814935, 10.019982715631077, 2.637483150077331, 2.6374827263987957, 2.6374827263987957, 2.6374116192444848, 9.971778871553711, 10.234695210864647, 8.365444090743294, 1.786681244174647, 1.786681244174647, 1.786681244174647, 1.7866810042932983, 1.7866803220300624, 1.7866759064081166, 1.7866703678787255, 1.7866661289712553, 1.7866661289712553, 1.786661676242974, 1.7866244764397603, 1.7866138391960456, 2.9760077659951767, 13.178389353833307, 2.9760058369876115, 3.488294856860955, 3.4882847700528927, 6.189764652671959, 8.568427119367811, 31.73516822846962, 5.987801824888612, 21.0349828187026, 5.959732541730494, 5.1898706758332525, 6.595067703764917, 16.622328810048845, 12.408370400209925, 9.132283623133807, 15.689277555728292, 16.667111267905792, 11.710407317111272, 10.839098450805627, 10.158698762989983, 10.101274008855896, 16.98774623434326, 10.571641577201143, 9.678260379304492, 16.37931663067666, 7.961991325685688, 19.41461128074168, 19.26595971501194, 14.160204900310557, 16.020054397264186, 13.283008570487945, 13.628003618905996, 10.796381123360323, 12.279532275909146, 10.873484742929003, 10.38793657692411, 10.833348358314685, 10.224980504902872, 10.161695137955416, 10.089318170268546], \"Total\": [128.0, 99.0, 112.0, 138.0, 108.0, 92.0, 59.0, 332.0, 80.0, 138.0, 36.0, 111.0, 114.0, 123.0, 125.0, 82.0, 127.0, 218.0, 63.0, 47.0, 77.0, 97.0, 158.0, 45.0, 64.0, 67.0, 55.0, 70.0, 40.0, 285.0, 31.988045117455126, 15.47758737865133, 12.563977375271948, 9.65035648245613, 8.679163598384521, 8.679162764712855, 8.679147650938194, 17.33482356889179, 7.707958672710346, 7.707958104353163, 7.707956496266048, 7.707950004622135, 6.736757015890222, 6.7367568176063, 6.736755826263169, 6.73674770213939, 6.73674730707456, 6.73667110572315, 6.735830240342201, 56.457364341691544, 12.50892925153056, 12.514875806909869, 5.765553705664755, 5.7655537056637565, 5.76555362155299, 5.76555362155276, 5.765553608841017, 5.765553537421558, 5.7655533691110765, 5.765553369110862, 18.222181616111108, 19.261597797550163, 11.4729443317731, 12.50916859030141, 11.525221005557787, 19.16705241757704, 38.29666237459911, 23.06925624553615, 21.987384507814532, 14.41861410915904, 22.869447834432297, 47.79186796542718, 16.322818073678135, 64.80044942929919, 30.575860482835886, 39.91349551776722, 51.42215747032154, 296.41748071857006, 18.996076759110373, 26.690419045976093, 58.83819077962827, 159.80479536989452, 74.20037086959051, 45.229334459204026, 59.41063265949943, 183.3444606020803, 21.065533667917425, 58.41832832967101, 118.20177106784709, 314.5934741584301, 134.40807402003486, 238.15143622977934, 301.9398077201909, 272.02743432713254, 117.7917103855111, 419.6624469042985, 239.76300560848364, 266.8827695195627, 146.06818005080686, 302.6516485274157, 50.04240393874871, 265.5559598359562, 104.85938423175524, 149.91455650722276, 285.3180855652458, 159.12847388873178, 201.24985934493822, 209.3170032779414, 332.48900755471084, 175.4474039198716, 124.7323375194955, 240.90630129606197, 241.31983192999513, 191.82882572634708, 176.50359908615746, 26.28442055855966, 13.12752726618742, 10.30799121628171, 10.307990848532901, 9.368145550080866, 6.548609280196983, 13.111619532296382, 5.608763882556571, 5.608763904446458, 5.608763926342295, 5.608763780316992, 5.60876183880319, 18.831422857939017, 56.0266864960869, 23.378305938076675, 4.668918462680115, 4.668917879633904, 4.668918458538818, 4.668918481555271, 4.66891846672232, 4.668918618011379, 4.668916634835233, 5.590992794780802, 30.785154052042472, 9.3995022463631, 8.41055967192795, 7.412504380687199, 23.486949471739656, 3.7290730395139455, 3.729072953490979, 108.22445960619704, 12.149985854337812, 127.43525335386246, 37.197741950124595, 12.204395497452339, 34.240690232042304, 19.816561191818092, 18.692269511957793, 38.87607848596416, 125.55774472839701, 18.7306620654426, 102.05786577865287, 92.9847181102114, 25.434787485187012, 43.02152502308327, 28.058956655728284, 239.76300560848364, 310.67618024616417, 77.84154145241371, 79.44973490445457, 43.04701869669204, 139.84795304952044, 63.01183516308085, 165.6297277637634, 265.5559598359562, 302.6516485274157, 419.6624469042985, 175.4474039198716, 47.39914957847694, 86.82463181956592, 226.5100159862435, 332.48900755471084, 301.9398077201909, 285.3180855652458, 241.31983192999513, 240.90630129606197, 191.82882572634708, 266.8827695195627, 314.5934741584301, 209.3170032779414, 201.24985934493822, 140.37191881456957, 7.407647255267188, 7.407648835962766, 6.479344286774452, 42.488398326116, 5.5510408515802006, 4.622742755577889, 4.6227422995738285, 4.622742106191107, 4.622745781372305, 4.622737669714577, 80.7532441014843, 4.62322377836747, 10.250165082872538, 12.964059325250265, 112.77214187603194, 3.6944412604196364, 3.694441289579154, 3.6944414092690487, 3.694441318757934, 3.6944412978506005, 3.694441891366405, 3.6944413429873912, 3.694440086472114, 3.69444060134143, 3.694441297103716, 3.6944410601035096, 3.694442720605142, 3.6944399657883107, 3.694441870007352, 3.6944410342314256, 7.331001505552813, 7.365195827949223, 7.404963797863137, 40.49185081761946, 7.401374856316931, 9.282348109396692, 11.093347007540878, 74.45311946832027, 18.373751403191378, 9.23996066941217, 58.281130699706374, 6.473150457547597, 21.33554548426107, 158.9095483761488, 67.61529041548243, 27.591312524544318, 138.65428906418867, 40.50161559063554, 17.65610888350211, 24.922097729393833, 16.636163952068074, 15.593013355611731, 67.61828670822378, 79.24776112064713, 25.848322658615395, 218.87330606885197, 25.898671966815204, 51.612881907460306, 121.39939073700228, 272.02743432713254, 41.70484037703411, 97.59638926652983, 239.76300560848364, 419.6624469042985, 171.6105018912045, 301.9398077201909, 285.3180855652458, 143.1679002209295, 209.3170032779414, 64.83809616521944, 75.13044460011454, 332.48900755471084, 97.960477829473, 314.5934741584301, 143.6968151550683, 266.8827695195627, 296.41748071857006, 302.6516485274157, 201.24985934493822, 310.67618024616417, 240.90630129606197, 28.678079025540725, 21.263672414099194, 14.75422272384236, 6.463244626526691, 6.463246369953517, 6.463246369953517, 6.4632465595805355, 17.50656397053975, 5.537626797913227, 5.537627198872966, 5.537628671202814, 10.179945403202973, 27.009149138285846, 4.6120090772544104, 4.6120090685059925, 4.612009033555693, 4.6120109892189, 4.612000336586391, 4.612010847415951, 9.252772975992064, 12.89863651195875, 8.274898111417654, 8.305019861004597, 9.136382531011545, 3.6863909247653317, 3.6863909247649977, 3.686390804438712, 3.6863909634394854, 3.686389687393154, 3.6863908582707, 7.3148991709046856, 7.434434332950158, 16.679475731560004, 21.41553872717377, 7.405409951409958, 34.342268283489226, 26.652667571875885, 22.267117171842187, 12.048259801541247, 64.27805776752945, 11.177743312961638, 10.216367372933426, 123.0722001879883, 77.84154145241371, 27.306092473463735, 21.377756437312026, 69.51865780098913, 86.82463181956592, 48.9563019047082, 125.55774472839701, 37.36387023011361, 69.42750556710602, 18.5565940148859, 24.39146718088903, 41.0241907634501, 36.90805398355879, 266.8827695195627, 137.565318123527, 226.5100159862435, 419.6624469042985, 265.5559598359562, 314.5934741584301, 310.67618024616417, 183.3444606020803, 238.15143622977934, 240.90630129606197, 64.81583145803432, 178.25937000747945, 302.6516485274157, 165.6297277637634, 296.41748071857006, 79.09917071141352, 175.4474039198716, 239.76300560848364, 301.9398077201909, 241.31983192999513, 272.02743432713254, 171.6105018912045, 27.65060956932739, 22.11797276373957, 27.664060946953004, 7.364278249250403, 7.364229004791234, 8.282729854356493, 6.44217213131253, 6.442172178723203, 5.520065983498265, 5.52006626860387, 11.071801357331173, 4.597960345857031, 4.597960436792284, 4.597960259422669, 4.597960401830069, 4.597960470873673, 4.597960799793389, 4.5979605270776185, 4.597962716436888, 13.67799314056976, 14.732559688685779, 3.6758544531300323, 3.6758544775049184, 3.675854477504618, 3.675854333770799, 3.675854734492928, 3.6758546067570594, 3.675854547096498, 3.675854539285228, 3.675854650951231, 10.910092635620167, 11.089522557601443, 35.179488926027396, 27.357646967962648, 35.25839365460279, 13.537207135549211, 6.370866738410845, 18.61623529251934, 77.93527646548637, 62.81067526530582, 16.781535024659433, 37.590446200307014, 11.006210413834996, 105.91449057938168, 36.9674548598484, 138.65428906418867, 64.8392921156694, 11.98144753632775, 84.26548293513176, 332.48900755471084, 82.96834297949178, 63.01183516308085, 114.28955436137849, 70.5382921493934, 60.28095664180465, 32.20911085283323, 226.5100159862435, 68.36006272961063, 58.15148241448259, 419.6624469042985, 96.16241611412707, 285.3180855652458, 67.61529041548243, 310.67618024616417, 192.050278902888, 201.24985934493822, 241.31983192999513, 272.02743432713254, 238.15143622977934, 240.90630129606197, 176.50359908615746, 149.91455650722276, 171.6105018912045, 302.6516485274157, 10.070773857202312, 9.154947619168729, 7.322633018330881, 6.406477492916965, 28.131210389770523, 5.4834538084995765, 21.121784913214206, 14.60925862444125, 4.5741637176067105, 4.574163944985263, 4.5741640864142425, 4.57416464376527, 4.574160956835028, 40.444074255426166, 9.16089732619271, 3.658006981941364, 3.6580069016814445, 3.6580071470749935, 3.6580059709455015, 3.6580059709455015, 3.658006895251355, 3.6577665541490965, 10.064547174523812, 6.393971244947083, 10.181196358474182, 6.461509878753473, 10.085127198567811, 2.7418502462769894, 2.741850246276094, 2.741850264579955, 5.424965656007375, 5.425816030918113, 5.425816237051627, 8.273016120544776, 70.15480404268366, 10.077560683758701, 15.181023675635954, 138.5864073463542, 10.09473210553561, 8.188338067587765, 68.36006272961063, 26.703668337547786, 21.05052576244556, 33.70954383769149, 52.326842934234314, 19.445869606625056, 114.28955436137849, 83.58523449475304, 52.57029263739803, 18.280433398536257, 12.896477847188741, 218.87330606885197, 111.27012326646101, 41.70484037703411, 71.06372282026773, 48.38705054596716, 104.85938423175524, 143.1679002209295, 201.24985934493822, 97.59638926652983, 310.67618024616417, 419.6624469042985, 137.565318123527, 314.5934741584301, 48.72291457559246, 192.050278902888, 241.31983192999513, 332.48900755471084, 296.41748071857006, 272.02743432713254, 265.5559598359562, 238.15143622977934, 191.82882572634708, 266.8827695195627, 302.6516485274157, 301.9398077201909, 285.3180855652458, 128.31561621304192, 9.94769789297678, 8.138741791680184, 8.138741210729158, 6.331440355643387, 15.462485372706134, 4.524139234435239, 4.524139290247193, 4.5241398824857075, 9.078586681853526, 3.620488524489383, 3.620488619670307, 3.62048860781243, 3.6204888868100893, 3.6204888868100893, 3.620489420618832, 3.6204887716856664, 3.620490251795157, 3.62061054211207, 9.007158731799088, 2.7168379413097394, 2.7168379413096533, 2.716837941309574, 2.7168379873598414, 2.716838534415919, 2.7168382234437622, 2.716837286303215, 2.716838940506852, 2.7168381859834168, 2.7168393437510554, 6.411826032795146, 9.188560128742361, 8.199587114078792, 8.219633968341892, 4.569140760363462, 9.098738286568897, 47.191935984956885, 34.133297922819, 40.067819506098445, 23.55616174073392, 99.99187267266416, 35.933594965957695, 25.720747369303496, 25.969863144672964, 24.69071125861894, 82.96834297949178, 48.149219535914945, 10.975776768772421, 37.49365272753577, 63.57704426827674, 138.5864073463542, 44.306067838930545, 218.87330606885197, 192.050278902888, 92.30233258079078, 178.25937000747945, 240.90630129606197, 31.078568834447317, 114.28955436137849, 332.48900755471084, 314.5934741584301, 76.83059823887278, 143.1679002209295, 241.31983192999513, 165.6297277637634, 89.35485686028342, 301.9398077201909, 302.6516485274157, 310.67618024616417, 238.15143622977934, 132.0797014981423, 176.50359908615746, 419.6624469042985, 209.3170032779414, 285.3180855652458, 36.44193146584172, 6.225734048073856, 3.567634751571051, 3.5676353693727907, 3.567635131672218, 3.567636646207217, 3.5676329360475014, 8.09220012861289, 8.020429270576756, 8.882696898312004, 2.681602121649448, 2.681602121649448, 2.6816036464524844, 2.6816036464524844, 2.681603646452522, 2.6816006613166032, 2.681600661316908, 2.6816008299268854, 5.369823894076745, 9.903461800469797, 2.696194682211246, 14.410228549119648, 4.419286990779126, 8.969152871478625, 4.471285424501787, 4.49593599877838, 8.855644095540086, 4.507479681117997, 7.22708609509168, 4.538837324674488, 12.597170789199811, 47.46532042399875, 15.331453006049426, 28.02987641922532, 12.569019333588953, 20.525664301685403, 92.30233258079078, 14.453986196580844, 99.99187267266416, 19.102235307394448, 14.610967445634454, 17.135070100380965, 7.221051840771582, 6.333357938603339, 38.72891064682903, 111.1814560775502, 55.20886738343548, 23.42250842569127, 20.203486792594642, 33.18257539624717, 285.3180855652458, 29.31297468948162, 88.89295766550639, 419.6624469042985, 79.09917071141352, 63.479564365402936, 310.67618024616417, 209.3170032779414, 301.9398077201909, 42.191677191306916, 272.02743432713254, 302.6516485274157, 116.06045948910369, 43.604883786921356, 114.28955436137849, 143.6968151550683, 266.8827695195627, 176.50359908615746, 62.81067526530582, 241.31983192999513, 226.5100159862435, 139.84795304952044, 159.12847388873178, 296.41748071857006, 83.27271942107014, 240.90630129606197, 265.5559598359562, 4.316145754485063, 9.54256209918204, 6.018442581056476, 2.6128411534665057, 2.6128411534667926, 2.6128413765885066, 2.6128413765889804, 2.6128413765961063, 2.612841648322743, 2.612841648322743, 2.612841757100834, 2.6128417571011266, 2.6128420199151674, 2.6128564995467776, 5.166907103564528, 28.32268855268536, 4.3681446327193925, 4.368146342996101, 11.43264528766502, 4.392806340318046, 4.389727716888195, 10.559524514125316, 4.449386386757641, 10.66627544839329, 12.330912052406998, 12.513755442030883, 7.9378740811794115, 3.5164920165493045, 3.5165070368002143, 3.5289982631500556, 8.062403258524562, 7.101052928782892, 7.229755458085565, 45.41455827939723, 63.27901986166546, 33.26087347436127, 64.81583145803432, 55.94734490145302, 16.22001134723691, 25.434787485187012, 24.286914945034912, 19.581177884637068, 36.9789252375664, 14.402201357857976, 62.378389529198834, 53.91588533514907, 34.636945862219214, 332.48900755471084, 23.522701073551264, 15.181023675635954, 15.23483186126559, 43.80973223343071, 302.6516485274157, 241.31983192999513, 83.58523449475304, 111.1814560775502, 62.980513973203784, 69.42750556710602, 191.82882572634708, 240.90630129606197, 265.5559598359562, 419.6624469042985, 139.84795304952044, 314.5934741584301, 178.25937000747945, 310.67618024616417, 97.960477829473, 105.91449057938168, 121.39939073700228, 132.0797014981423, 296.41748071857006, 149.91455650722276, 11.96995665684295, 8.566749498515088, 5.163543924533783, 5.163545547721549, 5.1635458411985855, 5.163749971520128, 4.312742549493014, 4.312742701679702, 4.312743150243377, 13.10043395109128, 3.4619410970789164, 3.461941226927428, 3.461941226927428, 3.4619430439096543, 13.913534289541666, 14.901542124425422, 12.214819695778242, 2.6111398307206355, 2.6111398307206355, 2.6111398307206355, 2.6111397521332567, 2.611139961213214, 2.6111405860950585, 2.6111407749845688, 2.6111415315360316, 2.6111415315360316, 2.611141445259988, 2.611144312888786, 2.611145335812903, 4.357790419524867, 19.345883839776715, 4.357790745695553, 5.228898487361542, 5.238360600005714, 9.697238465488578, 14.144386925289169, 59.21535895899635, 10.481599946145824, 45.313101820725535, 10.483952232662855, 8.927956825214803, 12.329161345199433, 45.665082053196414, 33.82315688871956, 21.769648767479655, 50.97475611026434, 56.76645215343335, 34.54839519769865, 33.65297937215688, 30.803268028089477, 31.57689812811461, 97.960477829473, 39.17174679386206, 37.590446200307014, 132.0797014981423, 24.39146718088903, 266.8827695195627, 265.5559598359562, 123.0722001879883, 178.25937000747945, 191.82882572634708, 218.87330606885197, 111.1814560775502, 226.5100159862435, 175.4474039198716, 137.565318123527, 302.6516485274157, 310.67618024616417, 332.48900755471084, 171.6105018912045], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.1538, -6.908, -7.1294, -7.4144, -7.5308, -7.5308, -7.5308, -6.8438, -7.6626, -7.6626, -7.6626, -7.6626, -7.8144, -7.8144, -7.8144, -7.8144, -7.8144, -7.8147, -7.8179, -5.6936, -7.2157, -7.2157, -7.9934, -7.9934, -7.9934, -7.9934, -7.9934, -7.9934, -7.9934, -7.9934, -6.8491, -6.796, -7.311, -7.2262, -7.3101, -6.8439, -6.1864, -6.6784, -6.7394, -7.1332, -6.7267, -6.0827, -7.0286, -5.8172, -6.4828, -6.252, -6.0839, -4.6603, -6.9267, -6.6555, -6.0321, -5.2688, -5.8689, -6.2531, -6.0725, -5.299, -6.8538, -6.1503, -5.6647, -4.9938, -5.581, -5.1934, -5.0605, -5.1841, -5.723, -4.9822, -5.324, -5.2832, -5.6568, -5.2725, -6.2725, -5.4099, -5.9019, -5.7308, -5.4383, -5.7096, -5.621, -5.6376, -5.4933, -5.7876, -5.8766, -5.7787, -5.7832, -5.8377, -5.8653, -5.831, -6.5573, -6.8173, -6.8173, -6.9216, -7.3216, -6.6465, -7.5006, -7.5006, -7.5006, -7.5006, -7.5006, -6.2947, -5.2195, -6.0983, -7.7189, -7.7189, -7.7189, -7.7189, -7.7189, -7.7189, -7.7189, -7.5409, -5.8456, -7.038, -7.1698, -7.2978, -6.155, -7.9985, -7.9985, -4.6559, -6.8298, -4.6512, -5.8109, -6.8395, -5.9164, -6.4151, -6.4997, -5.9104, -4.9271, -6.5331, -5.2234, -5.3326, -6.3511, -5.9711, -6.3086, -4.8161, -4.6815, -5.6351, -5.6439, -6.0506, -5.4586, -5.8962, -5.4053, -5.1808, -5.1306, -4.9811, -5.4078, -6.0472, -5.7693, -5.3559, -5.2718, -5.3251, -5.3777, -5.4381, -5.4463, -5.6903, -5.6485, -5.6508, -5.7033, -5.7485, -5.7793, -7.0033, -7.0033, -7.1551, -5.289, -7.3342, -7.5524, -7.5524, -7.5524, -7.5524, -7.5525, -4.6926, -7.5552, -6.7936, -6.5712, -4.4101, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.832, -7.1551, -7.156, -7.1551, -5.5074, -7.1572, -6.9442, -6.7847, -5.0186, -6.34, -7.0033, -5.3764, -7.3342, -6.2875, -4.544, -5.3112, -6.1261, -4.8262, -5.8225, -6.5017, -6.2728, -6.5945, -6.6645, -5.6074, -5.5131, -6.3012, -4.8864, -6.3323, -5.9429, -5.4267, -4.996, -6.0841, -5.6046, -5.1504, -4.8584, -5.3352, -5.0884, -5.1391, -5.5183, -5.3835, -5.9022, -5.8418, -5.2556, -5.7461, -5.3147, -5.6199, -5.417, -5.4205, -5.438, -5.5675, -5.5697, -5.6038, -5.4017, -5.757, -6.1601, -6.9979, -6.998, -6.998, -6.998, -6.0122, -7.177, -7.177, -7.177, -6.598, -5.624, -7.3952, -7.3952, -7.3953, -7.3953, -7.3953, -7.3953, -6.701, -6.3993, -6.8461, -6.8461, -6.7632, -7.6748, -7.6748, -7.6748, -7.6748, -7.6748, -7.6748, -6.998, -6.9979, -6.2338, -6.0118, -7.025, -5.5996, -5.8697, -6.0644, -6.6047, -5.3237, -6.7144, -6.7904, -4.9357, -5.3956, -6.1237, -6.3046, -5.5669, -5.4486, -5.8174, -5.2388, -5.9866, -5.706, -6.4194, -6.2747, -6.0045, -6.0738, -5.0667, -5.4302, -5.1912, -4.9063, -5.1312, -5.073, -5.1698, -5.4431, -5.3809, -5.3851, -5.8543, -5.5541, -5.4087, -5.5954, -5.4466, -5.8043, -5.6457, -5.6357, -5.6068, -5.6957, -5.7005, -5.7403, -5.4137, -5.6446, -5.4912, -6.8243, -6.8244, -6.7173, -6.9761, -6.9761, -7.1551, -7.1551, -6.4795, -7.3734, -7.3734, -7.3734, -7.3734, -7.3734, -7.3734, -7.3734, -7.3734, -6.2917, -6.2597, -7.653, -7.653, -7.653, -7.653, -7.653, -7.653, -7.653, -7.653, -7.653, -6.5761, -6.5845, -5.4856, -5.7767, -5.5496, -6.4719, -7.1552, -6.2185, -5.0228, -5.2702, -6.3629, -5.7637, -6.7171, -5.05, -5.8598, -4.9349, -5.4765, -6.6925, -5.3543, -4.4596, -5.3781, -5.5749, -5.2712, -5.562, -5.6851, -6.0339, -5.0203, -5.6518, -5.7598, -4.8265, -5.5762, -5.1326, -5.7458, -5.1679, -5.3616, -5.4514, -5.4057, -5.3745, -5.4611, -5.4742, -5.5331, -5.6111, -5.6064, -5.5935, -6.4587, -6.5624, -6.8106, -6.9624, -5.5117, -7.1623, -5.8222, -6.1981, -7.3597, -7.3597, -7.3597, -7.3597, -7.3597, -5.1854, -6.6788, -7.6393, -7.6393, -7.6393, -7.6393, -7.6393, -7.6393, -7.6405, -6.6788, -7.1414, -6.6788, -7.1414, -6.724, -8.0287, -8.0287, -8.0287, -7.3597, -7.3597, -7.3597, -6.9624, -5.0182, -6.8106, -6.4581, -4.6959, -6.8185, -7.0003, -5.354, -6.0964, -6.2815, -5.9554, -5.6434, -6.3637, -5.1366, -5.43, -5.7661, -6.4599, -6.6896, -4.9605, -5.4038, -6.0279, -5.7389, -5.9608, -5.5566, -5.4108, -5.3016, -5.6455, -5.1873, -5.0943, -5.5678, -5.271, -6.0013, -5.4822, -5.4045, -5.3179, -5.3987, -5.4308, -5.4407, -5.4893, -5.5752, -5.5363, -5.5369, -5.7093, -5.7314, -3.5938, -6.198, -6.4162, -6.4162, -6.6998, -5.8672, -7.0971, -7.0971, -7.0971, -6.4162, -7.3767, -7.3767, -7.3767, -7.3767, -7.3767, -7.3767, -7.3767, -7.3767, -7.3778, -6.548, -7.7661, -7.7661, -7.7661, -7.7661, -7.7661, -7.7661, -7.7661, -7.7662, -7.7662, -7.7662, -6.9168, -6.5713, -6.6998, -6.7103, -7.275, -6.6545, -5.2852, -5.5776, -5.4518, -5.9235, -4.729, -5.6213, -5.8817, -5.8746, -5.937, -5.0944, -5.4979, -6.5923, -5.7825, -5.4855, -5.008, -5.7975, -4.9577, -5.0594, -5.4766, -5.1922, -5.0462, -6.0587, -5.4621, -5.0315, -5.0813, -5.6594, -5.4171, -5.2209, -5.4133, -5.6225, -5.3499, -5.372, -5.4145, -5.4657, -5.6098, -5.586, -5.522, -5.6177, -5.6166, -4.5984, -6.4544, -7.1313, -7.1313, -7.1313, -7.1313, -7.1313, -6.318, -6.402, -6.3214, -7.5208, -7.5208, -7.5208, -7.5208, -7.5208, -7.5208, -7.5208, -7.5208, -6.8517, -6.2525, -7.6059, -5.947, -7.1313, -6.4331, -7.1313, -7.1313, -6.4544, -7.1313, -6.6619, -7.1313, -6.1214, -4.9381, -5.9986, -5.4724, -6.2353, -5.8557, -4.6586, -6.1758, -4.6628, -5.9744, -6.1956, -6.081, -6.7489, -6.8517, -5.4788, -4.7581, -5.2807, -5.9098, -6.0544, -5.7729, -4.5361, -5.8947, -5.3918, -4.7223, -5.4709, -5.5888, -4.9433, -5.1408, -5.0459, -5.7829, -5.1887, -5.1989, -5.5039, -5.8117, -5.5353, -5.4746, -5.3194, -5.4351, -5.7228, -5.5008, -5.5123, -5.6038, -5.5949, -5.5122, -5.6927, -5.6316, -5.66, -6.7427, -6.0619, -6.5615, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4118, -7.4119, -6.7546, -5.0811, -7.0223, -7.0223, -6.0619, -7.0223, -7.0688, -6.1936, -7.06, -6.1936, -6.0715, -6.0619, -6.5245, -7.4118, -7.4119, -7.4118, -6.6001, -6.7427, -6.7427, -5.061, -4.8981, -5.4946, -4.9747, -5.1044, -6.0619, -5.8412, -5.8964, -6.0555, -5.667, -6.295, -5.3511, -5.4548, -5.7727, -4.4668, -6.0559, -6.3454, -6.3587, -5.8348, -4.9651, -5.1408, -5.6147, -5.5537, -5.7777, -5.7602, -5.4169, -5.3554, -5.3979, -5.2914, -5.5732, -5.4098, -5.5447, -5.4247, -5.6976, -5.7417, -5.7321, -5.7365, -5.733, -5.7639, -5.5132, -5.8775, -6.4565, -6.4565, -6.4565, -6.457, -6.6748, -6.6748, -6.6748, -5.6196, -6.9544, -6.9544, -6.9544, -6.9544, -5.6244, -5.5984, -5.8001, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3438, -7.3439, -7.3439, -6.8336, -5.3456, -6.8336, -6.6748, -6.6748, -6.1013, -5.7761, -4.4668, -6.1345, -4.878, -6.1392, -6.2775, -6.0379, -5.1134, -5.4058, -5.7124, -5.1712, -5.1108, -5.4637, -5.541, -5.6059, -5.6115, -5.0917, -5.566, -5.6543, -5.1282, -5.8495, -4.9582, -4.9658, -5.2738, -5.1503, -5.3377, -5.3121, -5.545, -5.4163, -5.5379, -5.5835, -5.5416, -5.5994, -5.6056, -5.6127], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4211, 1.3929, 1.3799, 1.3588, 1.3485, 1.3485, 1.3485, 1.3437, 1.3354, 1.3354, 1.3354, 1.3354, 1.3183, 1.3183, 1.3183, 1.3183, 1.3183, 1.318, 1.3149, 1.3132, 1.2981, 1.2976, 1.2949, 1.2949, 1.2949, 1.2949, 1.2949, 1.2949, 1.2949, 1.2949, 1.2885, 1.2861, 1.2892, 1.2875, 1.2856, 1.2432, 1.2085, 1.2233, 1.2104, 1.2385, 1.1837, 1.0907, 1.2191, 1.0518, 1.1372, 1.1015, 1.0163, 0.6882, 1.1693, 1.1005, 0.9333, 0.6975, 0.8646, 0.9754, 0.8833, 0.5299, 1.1388, 0.8223, 0.6031, 0.2951, 0.5584, 0.3739, 0.2695, 0.2502, 0.5483, 0.0185, 0.2366, 0.1702, 0.3994, 0.0551, 0.8548, 0.0485, 0.4857, 0.2994, -0.0517, 0.2609, 0.1147, 0.0588, -0.2597, 0.0852, 0.3375, -0.2228, -0.2291, -0.0541, 0.0016, 1.9403, 1.9083, 1.89, 1.89, 1.8813, 1.8394, 1.8203, 1.8153, 1.8153, 1.8153, 1.8153, 1.8153, 1.81, 1.7949, 1.7902, 1.7804, 1.7804, 1.7804, 1.7804, 1.7804, 1.7804, 1.7804, 1.7781, 1.7676, 1.7616, 1.741, 1.7393, 1.7288, 1.7256, 1.7256, 1.7001, 1.7131, 1.5414, 1.6131, 1.6989, 1.5904, 1.6386, 1.6124, 1.4694, 1.2804, 1.577, 1.1913, 1.1752, 1.4531, 1.3074, 1.3973, 0.7445, 0.62, 1.0505, 1.0212, 1.2273, 0.6411, 1.0007, 0.5252, 0.2776, 0.197, 0.0197, 0.4651, 1.1344, 0.807, 0.2615, -0.0382, 0.0049, 0.009, 0.116, 0.1095, 0.0934, -0.1951, -0.3618, -0.0069, -0.0129, 0.3166, 2.0344, 2.0344, 2.0165, 2.002, 1.9921, 1.9568, 1.9568, 1.9568, 1.9568, 1.9568, 1.9562, 1.9539, 1.9194, 1.9068, 1.9048, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.9014, 1.893, 1.8874, 1.883, 1.8317, 1.8814, 1.8679, 1.8492, 1.7115, 1.7893, 1.8134, 1.5986, 1.8384, 1.6923, 1.4279, 1.5152, 1.5966, 1.282, 1.5164, 1.6675, 1.5517, 1.6342, 1.6289, 1.2189, 1.1545, 1.4868, 0.7653, 1.4538, 1.1536, 0.8145, 0.4383, 1.2255, 0.8548, 0.4102, 0.1423, 0.5598, 0.2416, 0.2476, 0.5579, 0.3128, 0.9661, 0.8792, -0.022, 0.7096, -0.0257, 0.4526, 0.0364, -0.072, -0.1104, 0.1681, -0.2682, -0.048, 2.2824, 2.2263, 2.1886, 2.1762, 2.1761, 2.1761, 2.1761, 2.1654, 2.1517, 2.1517, 2.1517, 2.1218, 2.12, 2.1163, 2.1163, 2.1163, 2.1163, 2.1163, 2.1163, 2.1143, 2.0838, 2.0809, 2.0772, 2.0647, 2.0608, 2.0608, 2.0607, 2.0607, 2.0607, 2.0607, 2.0524, 2.0362, 1.9923, 1.9643, 2.013, 1.9043, 1.8876, 1.8727, 1.9466, 1.5533, 1.9119, 1.9258, 1.2918, 1.29, 1.6095, 1.6733, 1.2318, 1.1277, 1.3319, 0.9687, 1.433, 1.094, 1.7, 1.5713, 1.3215, 1.358, 0.3867, 0.6859, 0.4263, 0.0945, 0.3272, 0.2159, 0.1317, 0.3858, 0.1865, 0.1707, 1.0144, 0.3029, -0.0811, 0.3351, -0.0981, 0.8652, 0.2272, -0.0751, -0.2768, -0.1416, -0.2662, 0.1547, 2.3069, 2.2993, 2.2289, 2.2193, 2.2192, 2.2088, 2.2013, 2.2013, 2.1767, 2.1767, 2.1564, 2.1412, 2.1412, 2.1412, 2.1412, 2.1412, 2.1412, 2.1412, 2.1412, 2.1327, 2.0904, 2.0855, 2.0855, 2.0855, 2.0855, 2.0855, 2.0855, 2.0855, 2.0855, 2.0855, 2.0744, 2.0498, 1.9941, 1.9546, 1.9279, 1.9629, 2.0333, 1.8977, 1.6615, 1.6299, 1.857, 1.6498, 1.9247, 1.3276, 1.5704, 1.1733, 1.3919, 1.8643, 1.2519, 0.774, 1.2436, 1.322, 1.0303, 1.2221, 1.2562, 1.534, 0.5972, 1.1636, 1.2174, 0.1743, 0.898, 0.254, 1.0806, 0.1336, 0.4209, 0.2843, 0.1484, 0.0598, 0.1062, 0.0816, 0.3338, 0.4191, 0.2886, -0.2658, 2.2719, 2.2635, 2.2387, 2.2205, 2.1916, 2.1762, 2.1677, 2.1605, 2.1601, 2.1601, 2.1601, 2.1601, 2.1601, 2.1549, 2.1465, 2.104, 2.104, 2.104, 2.104, 2.104, 2.104, 2.1029, 2.0524, 2.0434, 2.0409, 2.0329, 2.0052, 2.0029, 2.0029, 2.0029, 1.9895, 1.9894, 1.9894, 1.9648, 1.7713, 1.9193, 1.862, 1.4129, 1.9097, 1.9372, 1.4615, 1.659, 1.7118, 1.567, 1.4393, 1.7089, 1.1649, 1.1844, 1.312, 1.6745, 1.7936, 0.6912, 0.9244, 1.2817, 1.0377, 1.2002, 0.8311, 0.6654, 0.4341, 0.8139, 0.1142, -0.0936, 0.5484, 0.0179, 1.1528, 0.3002, 0.1497, -0.0843, -0.0502, 0.0035, 0.0177, 0.078, 0.2085, -0.0829, -0.2092, -0.3793, -0.3448, 2.5919, 2.5449, 2.5274, 2.5274, 2.4949, 2.4346, 2.4337, 2.4337, 2.4337, 2.4181, 2.3769, 2.3769, 2.3769, 2.3769, 2.3769, 2.3769, 2.3769, 2.3769, 2.3758, 2.2942, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2746, 2.2653, 2.2509, 2.2364, 2.2234, 2.2459, 2.1776, 1.9008, 1.9324, 1.8979, 1.9573, 1.7061, 1.8373, 1.9113, 1.9086, 1.8968, 1.5274, 1.6681, 2.0522, 1.6336, 1.4025, 1.1008, 1.4516, 0.694, 0.723, 1.0386, 0.6648, 0.5096, 1.545, 0.8394, 0.2022, 0.2076, 1.0392, 0.6591, 0.3332, 0.5172, 0.9251, -0.0199, -0.0444, -0.113, 0.1016, 0.5471, 0.2809, -0.5213, 0.0787, -0.23, 2.8462, 2.7571, 2.637, 2.637, 2.637, 2.637, 2.637, 2.6313, 2.5562, 2.5348, 2.5331, 2.5331, 2.533, 2.533, 2.533, 2.533, 2.533, 2.533, 2.5077, 2.4949, 2.4425, 2.4253, 2.423, 2.4134, 2.4113, 2.4058, 2.4048, 2.4032, 2.4005, 2.3963, 2.3854, 2.2421, 2.3118, 2.2346, 2.2737, 2.1628, 1.8565, 2.1935, 1.7724, 2.1161, 2.1629, 2.1181, 2.3143, 2.3427, 1.9048, 1.5709, 1.7484, 1.9768, 1.98, 1.7653, 0.8505, 1.7675, 1.161, 0.2785, 1.1986, 1.3007, 0.3582, 0.5556, 0.2841, 1.5151, 0.2456, 0.1288, 0.7822, 1.4534, 0.7662, 0.5979, 0.134, 0.4317, 1.1773, 0.0533, 0.1052, 0.4959, 0.3756, -0.1637, 0.9254, -0.0758, -0.2016, 2.8351, 2.7226, 2.6839, 2.668, 2.668, 2.668, 2.668, 2.668, 2.668, 2.668, 2.668, 2.668, 2.668, 2.6679, 2.6434, 2.6155, 2.5436, 2.5436, 2.5419, 2.5379, 2.4922, 2.4896, 2.4875, 2.4795, 2.4566, 2.4516, 2.4441, 2.371, 2.3709, 2.3674, 2.353, 2.3373, 2.3193, 2.1634, 1.9945, 2.0413, 1.894, 1.9114, 2.1921, 1.9629, 1.9539, 2.0101, 1.7629, 2.0779, 1.5559, 1.598, 1.7226, 0.7669, 1.8264, 1.9747, 1.958, 1.4256, 0.3625, 0.4134, 0.9997, 0.7754, 1.1197, 1.0398, 0.3668, 0.2004, 0.0605, -0.2906, 0.5265, -0.1209, 0.3123, -0.1232, 0.7581, 0.6359, 0.5091, 0.4203, -0.3845, 0.2663, 3.0447, 3.0149, 2.9421, 2.9421, 2.9421, 2.9416, 2.9039, 2.9039, 2.9039, 2.848, 2.844, 2.844, 2.844, 2.844, 2.7829, 2.7404, 2.7375, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7366, 2.7347, 2.7321, 2.7347, 2.7113, 2.7094, 2.6671, 2.6148, 2.4923, 2.5561, 2.3486, 2.5512, 2.5736, 2.4904, 2.1055, 2.1133, 2.2473, 1.9377, 1.8905, 2.0342, 1.9831, 2.0068, 1.9763, 1.364, 1.8063, 1.7592, 1.0287, 1.9965, 0.4953, 0.4926, 0.9537, 0.7066, 0.4459, 0.3397, 0.7841, 0.2012, 0.335, 0.5326, -0.2139, -0.2979, -0.3719, 0.2823]}, \"token.table\": {\"Topic\": [7, 7, 3, 1, 1, 2, 3, 4, 7, 10, 6, 10, 10, 1, 3, 4, 5, 7, 9, 10, 3, 6, 10, 10, 10, 2, 4, 2, 9, 1, 6, 8, 6, 10, 2, 2, 2, 9, 1, 4, 9, 1, 7, 5, 2, 10, 1, 2, 4, 2, 2, 3, 5, 6, 5, 5, 1, 4, 5, 6, 1, 1, 2, 3, 4, 7, 9, 10, 1, 2, 3, 6, 7, 9, 5, 10, 1, 5, 8, 4, 5, 6, 7, 8, 1, 2, 3, 5, 6, 7, 8, 10, 5, 6, 1, 2, 3, 4, 5, 6, 10, 1, 2, 4, 10, 1, 2, 3, 4, 8, 9, 7, 7, 3, 4, 4, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 5, 1, 2, 3, 4, 5, 6, 7, 9, 2, 6, 1, 2, 3, 4, 5, 10, 3, 6, 1, 3, 5, 7, 10, 4, 3, 5, 2, 2, 1, 10, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 4, 4, 6, 3, 8, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 9, 1, 1, 3, 5, 7, 1, 5, 8, 9, 1, 9, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 2, 3, 7, 5, 3, 1, 3, 4, 7, 8, 9, 1, 3, 4, 2, 1, 2, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 9, 6, 9, 1, 2, 4, 6, 7, 8, 10, 2, 4, 6, 2, 3, 4, 2, 3, 5, 6, 7, 1, 2, 4, 9, 1, 2, 9, 5, 6, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 10, 9, 3, 7, 8, 3, 6, 2, 3, 5, 6, 7, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 10, 1, 1, 2, 3, 5, 6, 7, 8, 3, 4, 6, 7, 9, 1, 2, 3, 4, 6, 7, 8, 9, 5, 6, 9, 10, 6, 2, 4, 5, 7, 9, 10, 4, 9, 10, 6, 9, 8, 9, 2, 3, 4, 5, 6, 7, 3, 6, 7, 8, 6, 8, 1, 3, 1, 2, 5, 6, 7, 8, 9, 8, 6, 1, 4, 5, 1, 6, 2, 5, 6, 10, 7, 8, 1, 4, 7, 4, 8, 1, 2, 3, 4, 6, 7, 8, 1, 7, 8, 1, 6, 10, 1, 2, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 7, 8, 9, 8, 5, 2, 3, 5, 6, 1, 2, 10, 2, 1, 5, 6, 1, 3, 5, 6, 7, 8, 9, 3, 9, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 4, 7, 6, 1, 2, 3, 4, 6, 10, 2, 1, 2, 3, 7, 9, 2, 3, 5, 6, 7, 8, 9, 1, 3, 5, 7, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 7, 3, 1, 9, 4, 10, 10, 3, 1, 2, 3, 4, 5, 6, 8, 9, 6, 1, 3, 4, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 1, 2, 4, 6, 8, 9, 10, 1, 5, 6, 9, 1, 2, 4, 6, 9, 10, 1, 8, 1, 8, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 4, 7, 5, 6, 7, 8, 10, 7, 1, 2, 3, 6, 8, 5, 9, 6, 5, 1, 5, 7, 8, 10, 5, 1, 2, 3, 4, 6, 8, 9, 10, 3, 10, 1, 3, 4, 6, 7, 8, 9, 1, 3, 5, 6, 7, 8, 1, 3, 5, 8, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 4, 5, 1, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 5, 6, 7, 10, 5, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 6, 7, 9, 1, 2, 3, 5, 6, 7, 8, 9, 2, 7, 2, 4, 1, 1, 2, 3, 4, 5, 6, 8, 3, 6, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 7, 1, 4, 9, 4, 1, 3, 4, 5, 6, 7, 10, 1, 5, 3, 5, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 7, 3, 4, 5, 7, 2, 4, 6, 1, 3, 4, 5, 6, 7, 8, 1, 2, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 7, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 1, 2, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 8, 9, 4, 1, 3, 4, 5, 6, 7, 8, 4, 1, 2, 3, 4, 5, 6, 8, 9, 3, 5, 6, 7, 8, 9, 3, 5, 6, 7, 8, 10, 7, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 1, 2, 3, 4, 6, 10, 1, 6, 6, 7, 8, 1, 2, 4, 5, 1, 2, 3, 6, 9, 1, 2, 5, 7, 9, 10, 5, 1, 2, 10, 4, 10, 1, 2, 3, 4, 5, 7, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 7, 1, 3, 5, 6, 1, 7, 1, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 10, 2, 8, 1, 1, 2, 4, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 2, 4, 5, 8, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 7, 1, 4, 5, 6, 7, 8, 5, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 7, 2, 4, 8, 3, 4, 10, 4, 5, 9, 5, 5, 1, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 6, 9, 8, 5, 7, 8, 7, 9, 9, 1, 2, 3, 8, 1, 4, 5, 6, 7, 2, 9, 3, 5, 8, 6, 7, 8, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 3, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 9, 1, 2, 5, 6, 7, 8, 10, 7, 7, 6, 8, 10, 2, 4, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 5, 1, 2, 3, 4, 4, 10, 3, 5, 7, 1, 2, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 6, 1, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 9, 6, 8, 1, 2, 3, 4, 5, 2, 8, 3, 5, 6, 7, 4, 5, 5, 10, 7, 1, 2, 5, 8, 9, 10, 6, 1, 2, 4, 5, 6, 9, 10, 1, 2, 3, 5, 6, 8, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 7, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 9, 8, 5, 2, 3, 4, 5, 6, 7, 8, 3, 5, 6, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 8, 8, 3, 2, 3, 4, 6, 10, 1, 2, 3, 5, 6, 7, 2, 4, 8, 10, 10, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 4, 7, 9, 1, 7, 1, 2, 6, 7, 10, 1, 2, 5, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 10, 2, 4, 10, 1, 6, 3, 7, 9, 2, 3, 4, 5, 6, 7, 3, 2, 3, 6, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 8, 9, 1, 8, 3, 5, 7, 10, 5, 6, 7, 10, 9, 10, 5, 10, 1, 2, 3, 4, 8, 9, 1, 2, 4, 9, 10, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 4, 10, 7, 8, 1, 4, 9, 10, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 6, 9, 1, 2, 5, 7, 9, 1, 2, 5, 8, 10, 4, 6, 1, 5, 6, 7, 6, 1, 2, 3, 4, 5, 6, 7, 9, 6, 9, 2, 3, 7, 3, 5, 6, 7, 8, 1, 2, 3, 4, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 7, 4, 1, 10, 3, 1, 2, 3, 5, 6, 7, 2, 5, 10, 1, 2, 5, 6, 7, 9, 10, 1, 3, 4, 7, 1, 2, 3, 4, 5, 6, 10, 6, 7, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 1, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 4, 8, 10, 1, 3, 10, 7, 9, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 6, 1, 2, 3, 4, 9, 1, 2, 4, 5, 6, 7, 8, 9, 10, 3, 9, 5, 1, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 8, 1, 3, 5, 6, 7, 1, 3, 5, 9, 1, 4, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 4, 5, 9, 10, 1, 2, 4, 10, 1, 2, 3, 5, 6, 7, 9, 2, 2, 4, 10, 1, 2, 3, 5, 6, 1, 7, 5, 9, 1, 3, 6, 7, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 7, 2, 5, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 4, 10, 10, 5, 1, 2, 4, 6, 8, 10, 3, 4, 9, 7, 9, 1, 4, 9, 1, 2, 3, 4, 5, 10, 10, 1, 2, 4, 4, 2, 3, 4, 9, 10, 3, 8, 1, 6, 7, 8, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 6, 8, 1, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 6, 7, 6, 9, 1, 2, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 3, 5, 2, 3, 5, 6, 9, 2, 6, 3, 5, 8, 1, 4, 9, 3, 7, 9, 2, 8, 1, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 3, 1, 2, 3, 6, 9, 10, 6, 2, 4, 4, 1, 2, 3, 4, 7, 8, 9, 10, 7, 9, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 6, 8, 10, 3, 6, 7, 8, 9, 10, 6, 7, 9, 3, 4, 5, 6, 7, 8, 10, 3, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 9, 10], \"Freq\": [0.8841460867415879, 0.947651665809666, 0.8120307622069893, 0.8672194082644256, 0.15317162217897717, 0.12764301848248097, 0.05105720739299239, 0.3318718480544505, 0.05105720739299239, 0.2808146406614581, 0.8744766119103573, 0.7659488005550039, 0.8665658703815935, 0.11034336205413055, 0.06620601723247833, 0.198618051697435, 0.02206867241082611, 0.08827468964330444, 0.04413734482165222, 0.4634421206273483, 0.12212490394825552, 0.6106245197412776, 0.12212490394825552, 0.7659489838571273, 0.8665658378789365, 0.6954746715846173, 0.26749025830177586, 0.8567294606898725, 0.765450282864139, 0.8906362753542165, 0.18622584645709947, 0.7449033858283979, 0.19124486780859107, 0.5737346034257732, 0.8914620165268223, 0.9141096991592932, 0.8914620200069665, 0.7654501378925342, 0.8672194462352324, 0.820243705322043, 0.1367072842203405, 0.21885953014948326, 0.6565785904484498, 0.8699509726737503, 0.8094430292186554, 0.13490717153644258, 0.08193769205600962, 0.7374392285040865, 0.08193769205600962, 0.8567297968345843, 0.8567294621915152, 0.7724214094498723, 0.15448428188997446, 0.8201189456299811, 0.8161367699290896, 0.8161367699290228, 0.2148662142021454, 0.05371655355053635, 0.6445986426064362, 0.05371655355053635, 0.9691432930102478, 0.21045749160483904, 0.15032677971774216, 0.060130711887096865, 0.030065355943548432, 0.0901960678306453, 0.3607842713225812, 0.0901960678306453, 0.06943382995089085, 0.2777353198035634, 0.06943382995089085, 0.06943382995089085, 0.1388676599017817, 0.34716914975445423, 0.8699509457297478, 0.7659477073434294, 0.06787686736935292, 0.8145224084322351, 0.06787686736935292, 0.07956070187016415, 0.07956070187016415, 0.07956070187016415, 0.1591214037403283, 0.5569249130911491, 0.043748530020427084, 0.017499412008170834, 0.061247942028597915, 0.27124088612664793, 0.30623971014298956, 0.16624441407762292, 0.12249588405719583, 0.017499412008170834, 0.10915961225117259, 0.7641172857582081, 0.17655798059203953, 0.19617553399115506, 0.03923510679823101, 0.058852660197346515, 0.19617553399115506, 0.019617553399115504, 0.31388085438584806, 0.0819958875441089, 0.12299383131616336, 0.49197532526465343, 0.3279835501764356, 0.17077590185056996, 0.08538795092528498, 0.12808192638792745, 0.04269397546264249, 0.3842457791637824, 0.21346987731321243, 0.7361495277960243, 0.8286170632589059, 0.8652871707328997, 0.9283260542090632, 0.9283260542090632, 0.8673006220823095, 0.8744773167684279, 0.1320960823241317, 0.15611355183761016, 0.012008734756739244, 0.09606987805391395, 0.14410481708087092, 0.1681222865943494, 0.04803493902695698, 0.14410481708087092, 0.07205240854043546, 0.04803493902695698, 0.8699509220324535, 0.8699509563200518, 0.1996527516885895, 0.06655091722952984, 0.2795138523640253, 0.07986110067543581, 0.07986110067543581, 0.11979165101315371, 0.11979165101315371, 0.05324073378362387, 0.46282322066655074, 0.5142480229628341, 0.20800535734412623, 0.11886020419664356, 0.11886020419664356, 0.17829030629496534, 0.08914515314748267, 0.3268655615407698, 0.8651971420281239, 0.8201187384021754, 0.6790076363283003, 0.20061589255154327, 0.030863983469468195, 0.07715995867367048, 0.015431991734734097, 0.8673009816323373, 0.0722959656514307, 0.9036995706428839, 0.8044894718369543, 0.9162250705877385, 0.3360231303457072, 0.5600385505761787, 0.7746308442626749, 0.04628498828935614, 0.04628498828935614, 0.16971162372763918, 0.27770992973613684, 0.0771416471489269, 0.01542832942978538, 0.01542832942978538, 0.3085665885957076, 0.03085665885957076, 0.8673009832775013, 0.8428637278602801, 0.12040910398004001, 0.11292233396141382, 0.5646116698070691, 0.22584466792282765, 0.8184420635373413, 0.1364070105895569, 0.20224631003967655, 0.10890185925213353, 0.031114816929181006, 0.4667222539377151, 0.09334445078754303, 0.031114816929181006, 0.015557408464590503, 0.015557408464590503, 0.031114816929181006, 0.765450217496751, 0.9217494991152985, 0.08346236938132892, 0.16692473876265784, 0.5842365856693025, 0.16692473876265784, 0.24660895201416755, 0.24660895201416755, 0.06165223800354189, 0.4315656660247932, 0.41495179434387097, 0.41495179434387097, 0.08789071617935934, 0.11718762157247911, 0.058593810786239556, 0.49804739168303624, 0.08789071617935934, 0.14648452696559888, 0.11770683233428328, 0.6513111389163675, 0.04708273293371331, 0.03138848862247554, 0.09416546586742662, 0.023541366466856655, 0.023541366466856655, 0.01569424431123777, 0.9691120506480725, 0.2743541596365292, 0.4605230536756026, 0.02939508853248527, 0.02939508853248527, 0.019596725688323515, 0.02939508853248527, 0.03919345137664703, 0.0685885399091323, 0.02939508853248527, 0.009798362844161758, 0.7654457871478656, 0.2154627230555341, 0.7541195306943694, 0.7361499077990038, 0.9057862273208896, 0.8120308247453477, 0.7015171093580614, 0.02505418247707362, 0.1252709123853681, 0.02505418247707362, 0.05010836495414724, 0.05010836495414724, 0.8322575185903144, 0.13870958643171907, 0.9029135568445334, 0.9511337693103004, 0.10724369513099401, 0.08936974594249501, 0.19661344107348902, 0.232361339450487, 0.035747898376998, 0.321731085392982, 0.34011972080591407, 0.08817918687560736, 0.0839801779767689, 0.11757224916747647, 0.10917423136979958, 0.10077621357212269, 0.07978116907793047, 0.029393062291869117, 0.03359207119070756, 0.012597026696515336, 0.1094065964628685, 0.16410989469430276, 0.1094065964628685, 0.4923296840829082, 0.1094065964628685, 0.8531449471056968, 0.14219082451761614, 0.09500616519799766, 0.2533497738613271, 0.09500616519799766, 0.09500616519799766, 0.03166872173266589, 0.09500616519799766, 0.3166872173266589, 0.2620677214943004, 0.6697286215965454, 0.05823727144317786, 0.16599907645950285, 0.08299953822975142, 0.6639963058380114, 0.06011001111080587, 0.6011001111080587, 0.06011001111080587, 0.1803300333324176, 0.06011001111080587, 0.16016518740866573, 0.6940491454375515, 0.10677679160577715, 0.05338839580288857, 0.039316231778322934, 0.589743476674844, 0.35384608600490636, 0.19081056425316228, 0.09540528212658114, 0.09540528212658114, 0.5724316927594868, 0.8201186819437607, 0.17974276843484355, 0.1258199379043905, 0.21569132212181227, 0.03594855368696871, 0.04493569210871089, 0.2426527373870388, 0.09885852263916395, 0.026961415265226534, 0.03594855368696871, 0.2708432633567162, 0.15799190362475113, 0.06771081583917905, 0.02257027194639302, 0.04514054389278604, 0.31598380724950226, 0.11285135973196508, 0.765450106025237, 0.2334393718010566, 0.2334393718010566, 0.4668787436021132, 0.7211529572239896, 0.1802882393059974, 0.04938074619577457, 0.5184978350556331, 0.04938074619577457, 0.09876149239154915, 0.27159410407676016, 0.8672194082644602, 0.7654501060251513, 0.1696533465752907, 0.1542303150684461, 0.3084606301368922, 0.04626909452053383, 0.06169212602737843, 0.1542303150684461, 0.015423031506844608, 0.015423031506844608, 0.04626909452053383, 0.030846063013689216, 0.7661675697399885, 0.1094525099628555, 0.9551115575564508, 0.8672193956128472, 0.08001354314024373, 0.05334236209349582, 0.2667118104674791, 0.21336944837398328, 0.3733965346544707, 0.02667118104674791, 0.28502851034267684, 0.04750475172377948, 0.5225522689615743, 0.04750475172377948, 0.04750475172377948, 0.22124236485655846, 0.015803026061182748, 0.015803026061182748, 0.26865144304010674, 0.015803026061182748, 0.031606052122365495, 0.07901513030591374, 0.34766657334602047, 0.19076775204764676, 0.09538387602382338, 0.09538387602382338, 0.5723032561429402, 0.8201726259968736, 0.053205008244452956, 0.026602504122226478, 0.5054475783223031, 0.026602504122226478, 0.10641001648890591, 0.2660250412222648, 0.07061476513007003, 0.6355328861706303, 0.24715167795524515, 0.22780456203531535, 0.6834136861059461, 0.6788425386854309, 0.22628084622847694, 0.11606168955802951, 0.5029339880847945, 0.077374459705353, 0.077374459705353, 0.0386872298526765, 0.154748919410706, 0.3924384052030727, 0.035676218654824794, 0.035676218654824794, 0.49946706116754713, 0.12468160571760634, 0.748089634305638, 0.09755940435251574, 0.7804752348201259, 0.16590949841269323, 0.18961085532879227, 0.11850678458049517, 0.0711040707482971, 0.18961085532879227, 0.26071492607708935, 0.023701356916099033, 0.7458231581970873, 0.8744767184632551, 0.7964311028475868, 0.12252786197655181, 0.06126393098827591, 0.8793718294196978, 0.0799428935836089, 0.6717154310889647, 0.1752301124579908, 0.029205018742998468, 0.11682007497199387, 0.8286172534892328, 0.7458227989355125, 0.0646726561672399, 0.0646726561672399, 0.8407445301741187, 0.8459318659575328, 0.12084740942250469, 0.19480377642792404, 0.055658221836549726, 0.11131644367309945, 0.055658221836549726, 0.027829110918274863, 0.4452657746923978, 0.08348733275482459, 0.8672193956129973, 0.6661368116915113, 0.2220456038971704, 0.7369942845322367, 0.10528489779031953, 0.10528489779031953, 0.16166759899976432, 0.1077783993331762, 0.5388919966658811, 0.1077783993331762, 0.4314656162869701, 0.07614099110946532, 0.1776623125887524, 0.11844154172583493, 0.033840440493095696, 0.016920220246547848, 0.04230055061636962, 0.025380330369821773, 0.033840440493095696, 0.033840440493095696, 0.3464748472311111, 0.14848922024190478, 0.04949640674730159, 0.3959712539784127, 0.04949640674730159, 0.7458232050920091, 0.9505407824017609, 0.804489490395071, 0.7713633322028133, 0.15427266644056267, 0.72943444038043, 0.1703073447155385, 0.7663830512199231, 0.04257683617888462, 0.8567294579680803, 0.908153269780213, 0.198311826972695, 0.6940913944044325, 0.07864473816840944, 0.06291579053472755, 0.20447631923786455, 0.1258315810694551, 0.29885000503995585, 0.17301842397050077, 0.06291579053472755, 0.22764490909189464, 0.6829347272756839, 0.7407416031506573, 0.16460924514459052, 0.1864745207225825, 0.15746737305462521, 0.07873368652731261, 0.08702144300387182, 0.11188471243354949, 0.11188471243354949, 0.09945307771871066, 0.058014295335914554, 0.07044593005075338, 0.04143878238279611, 0.10208198471028014, 0.08166558776822411, 0.23478856483364433, 0.18374757247850426, 0.06124919082616808, 0.05104099235514007, 0.10208198471028014, 0.17353937400747624, 0.816136741231635, 0.28495727876199906, 0.19234616316434935, 0.1780982992262494, 0.12823077544289957, 0.09261111559764969, 0.02137179590714993, 0.04274359181429986, 0.04274359181429986, 0.014247863938099953, 0.014247863938099953, 0.7654500290319639, 0.1957642992849561, 0.6851750474973464, 0.09788214964247804, 0.9365521078679557, 0.2531690991656309, 0.44304592353985406, 0.021097424930469243, 0.08438969972187697, 0.021097424930469243, 0.16877939944375395, 0.8914623483223012, 0.08065005677017853, 0.6989671586748806, 0.053766704513452356, 0.026883352256726178, 0.10753340902690471, 0.8731090094240954, 0.030002438396377208, 0.19001544317705565, 0.030002438396377208, 0.4000325119516961, 0.3200260095613569, 0.030002438396377208, 0.036243292127202474, 0.5798926740352396, 0.10872987638160742, 0.2537030448904173, 0.036243292127202474, 0.6956130502973488, 0.1561877725159837, 0.1822190679353143, 0.14317212480631838, 0.1301564770966531, 0.01301564770966531, 0.09110953396765716, 0.20825036335464495, 0.05206259083866124, 0.02603129541933062, 0.4092016078721707, 0.1636806431488683, 0.11904046774463148, 0.07440029234039468, 0.05952023387231574, 0.07440029234039468, 0.022320087702118404, 0.03720014617019734, 0.022320087702118404, 0.022320087702118404, 0.8120307669665429, 0.18045709372851082, 0.13534282029638312, 0.11428949269472352, 0.042106655203319195, 0.21053327601659597, 0.0872209286354469, 0.0872209286354469, 0.009022854686425542, 0.10225901977948947, 0.03007618228808514, 0.894295554211321, 0.736149637058309, 0.9007319768826171, 0.8716158390402072, 0.08716158390402072, 0.22947407490544583, 0.6884222247163375, 0.6956130257507774, 0.8120307423128291, 0.2159184584406758, 0.07711373515738422, 0.015422747031476843, 0.07711373515738422, 0.38556867578692106, 0.09253648218886106, 0.06169098812590737, 0.06169098812590737, 0.8201189456299811, 0.7118659308897018, 0.03746662794156326, 0.11239988382468977, 0.07493325588312652, 0.03746662794156326, 0.22256891313953645, 0.11128445656976822, 0.037094818856589405, 0.16692668485465234, 0.018547409428294703, 0.12983186599806293, 0.05564222828488411, 0.037094818856589405, 0.24111632256783114, 0.8914620397362168, 0.8120307668023792, 0.11009685416819964, 0.17615496666911942, 0.044038741667279856, 0.044038741667279856, 0.1541355958354795, 0.41836804583915865, 0.044038741667279856, 0.8793550043388578, 0.07994136403080525, 0.737216296536164, 0.184304074134041, 0.16058293648510394, 0.10705529099006929, 0.42822116396027715, 0.026763822747517322, 0.16058293648510394, 0.10705529099006929, 0.22032074041598682, 0.6609622212479604, 0.922997568242506, 0.057687348015156625, 0.8286174395153417, 0.03455240681278594, 0.3109716613150734, 0.06910481362557187, 0.3109716613150734, 0.09213975150076249, 0.046069875750381246, 0.057587344687976556, 0.06910481362557187, 0.36621863819285294, 0.07324372763857058, 0.5127060934699941, 0.03662186381928529, 0.2966608533386446, 0.10595030476380166, 0.4873714019134876, 0.04238012190552066, 0.04238012190552066, 0.7361498313524443, 0.09040889575856398, 0.4520444787928199, 0.060272597172375986, 0.060272597172375986, 0.3314992844480679, 0.8042115452868105, 0.14622028096123826, 0.8738444317528932, 0.8161368018417944, 0.7870762831842033, 0.08745292035380037, 0.043726460176900184, 0.043726460176900184, 0.043726460176900184, 0.931362881602724, 0.08128307174733168, 0.10837742899644225, 0.1354717862455528, 0.3793210014875479, 0.18966050074377394, 0.02709435724911056, 0.02709435724911056, 0.02709435724911056, 0.8652872922854411, 0.6956129534008312, 0.6190672565667631, 0.110547724386922, 0.0442190897547688, 0.0442190897547688, 0.0442190897547688, 0.0442190897547688, 0.0884381795095376, 0.064941433812533, 0.129882867625066, 0.1515300122292437, 0.38964860287519804, 0.2164714460417767, 0.04329428920835534, 0.15704968302001307, 0.2617494717000218, 0.05234989434000436, 0.47114904906003924, 0.05234989434000436, 0.5595254783177813, 0.03996610559412724, 0.09991526398531811, 0.13988136957944533, 0.05994915839119086, 0.01998305279706362, 0.01998305279706362, 0.03996610559412724, 0.01998305279706362, 0.02957910833053293, 0.5324239499495927, 0.14789554165266466, 0.2810015291400629, 0.14374492910139536, 0.07187246455069768, 0.7187246455069768, 0.5948517372175584, 0.03399152784100334, 0.10197458352301, 0.050987291761505, 0.11897034744351166, 0.01699576392050167, 0.06798305568200667, 0.01699576392050167, 0.1417669707514459, 0.05670678830057836, 0.21265045612716885, 0.32606403272832557, 0.21265045612716885, 0.02835339415028918, 0.01417669707514459, 0.9505344261961473, 0.08025006649585291, 0.5617504654709704, 0.08025006649585291, 0.08025006649585291, 0.040125033247926455, 0.12037509974377937, 0.040125033247926455, 0.11831119049970006, 0.28098907743678764, 0.3993002679364877, 0.029577797624925014, 0.029577797624925014, 0.05915559524985003, 0.10352229168723756, 0.170048498575598, 0.0425121246438995, 0.1275363739316985, 0.1275363739316985, 0.085024249287799, 0.0425121246438995, 0.085024249287799, 0.29758487250729654, 0.11014930352527463, 0.7710451246769224, 0.8389505181190572, 0.07626822891991429, 0.8906375327005559, 0.034114586137818016, 0.13645834455127206, 0.34114586137818015, 0.06822917227563603, 0.034114586137818016, 0.034114586137818016, 0.3070312752403621, 0.7575789822539718, 0.2164511377868491, 0.027440916542454247, 0.9604320789858987, 0.23795548769234082, 0.14164017124544095, 0.11897774384617041, 0.0736528890476293, 0.13597456439562333, 0.09064970959708221, 0.09631531644689985, 0.08498410274726458, 0.016996820549452916, 0.005665606849817638, 0.3146918424319995, 0.09536116437333318, 0.11443339724799982, 0.12396951368533314, 0.06039540410311101, 0.09536116437333318, 0.08900375341511096, 0.03178705479111106, 0.04132317122844438, 0.03178705479111106, 0.3981576523243578, 0.054542144154021616, 0.0981758594772389, 0.14726378921585837, 0.08726743064643458, 0.04363371532321729, 0.06545057298482594, 0.04908792973861945, 0.038179500907815134, 0.016362643246206485, 0.9494541034261338, 0.13503641345468043, 0.6751820672734021, 0.4755802216328082, 0.09386451742752794, 0.11263742091303353, 0.09386451742752794, 0.037545806971011175, 0.08760688293235941, 0.012515268990337058, 0.012515268990337058, 0.018772903485505588, 0.05631871045651676, 0.1347278130728858, 0.6287297943401338, 0.08981854204859054, 0.08981854204859054, 0.04490927102429527, 0.31964824776460804, 0.07991206194115201, 0.5593844335880641, 0.8673026253420945, 0.12010748446298189, 0.5833792102487692, 0.01715821206614027, 0.03431642413228054, 0.01715821206614027, 0.20589854479368325, 0.01715821206614027, 0.87895398801533, 0.07990490800139363, 0.8120310330610214, 0.8161367544778546, 0.04586642197633538, 0.1834656879053415, 0.02293321098816769, 0.11466605494083845, 0.11466605494083845, 0.2522653208698446, 0.2293321098816769, 0.04586642197633538, 0.3021305189209816, 0.08497420844652606, 0.04720789358140337, 0.05664947229768404, 0.3682215699349463, 0.01888315743256135, 0.009441578716280674, 0.09441578716280674, 0.01888315743256135, 0.921747893993399, 0.12195736030208006, 0.12195736030208006, 0.7317441618124804, 0.27942404374718494, 0.0931413479157283, 0.4346596236067321, 0.1862826958314566, 0.09468896725431383, 0.047344483627156916, 0.8048562216616676, 0.01911065036460971, 0.1719958532814874, 0.24843845473992623, 0.01911065036460971, 0.4013236576568039, 0.05733195109382913, 0.07644260145843884, 0.059953922778751015, 0.11990784555750203, 0.7194470733450121, 0.059953922778751015, 0.07192313773251328, 0.17261553055803186, 0.07192313773251328, 0.3452310611160637, 0.10069239282551859, 0.08630776527901593, 0.11507702037202125, 0.014384627546502656, 0.7361500852785294, 0.15427431905905653, 0.1424070637468214, 0.10680529781011606, 0.04746902124894047, 0.3322831487425833, 0.04746902124894047, 0.0712035318734107, 0.03560176593670535, 0.03560176593670535, 0.7458223748486427, 0.32154859615635467, 0.18374205494648838, 0.045935513736622094, 0.4134196236295989, 0.5722881322416551, 0.03366400777892089, 0.05049601166838133, 0.03366400777892089, 0.08416001944730221, 0.05049601166838133, 0.11782402722622311, 0.05049601166838133, 0.03366400777892089, 0.08112729022617293, 0.027042430075390974, 0.24338187067851877, 0.1081697203015639, 0.1352121503769549, 0.1081697203015639, 0.2704243007539098, 0.9283263046202173, 0.2681344785915994, 0.0731275850704362, 0.365637925352181, 0.12187930845072699, 0.048751723380290796, 0.024375861690145398, 0.0731275850704362, 0.9283260269726427, 0.33343646590332426, 0.06351170779110939, 0.19053512337332815, 0.07938963473888673, 0.06351170779110939, 0.06351170779110939, 0.06351170779110939, 0.14290134252999612, 0.1425419133651316, 0.0712709566825658, 0.5559134621240133, 0.11403353069210528, 0.0712709566825658, 0.042762574009539485, 0.07960430259475026, 0.4935466760874516, 0.09552516311370031, 0.11144602363265037, 0.1751294657084506, 0.047762581556850155, 0.8286174020098472, 0.09031953949732414, 0.8128758554759173, 0.22431105225316902, 0.14019440765823066, 0.15070898823259796, 0.0630874834462038, 0.1261749668924076, 0.06659234363765956, 0.056077763063292256, 0.12967982708386336, 0.02453402134019036, 0.0210291611487346, 0.8138041133315974, 0.8600838040046607, 0.1298563514868748, 0.1623204393585935, 0.0324640878717187, 0.2921767908454683, 0.0324640878717187, 0.324640878717187, 0.8906362491399931, 0.9559402994082554, 0.015586567395502413, 0.9585738948233984, 0.015586567395502413, 0.7802592250230417, 0.04334773472350232, 0.04334773472350232, 0.08669546944700464, 0.06413128605704121, 0.06413128605704121, 0.5771815745133709, 0.12826257211408243, 0.12826257211408243, 0.064353027665264, 0.09652954149789601, 0.41829467982421603, 0.32176513832632003, 0.032176513832632, 0.064353027665264, 0.9057862741037961, 0.1063886122679448, 0.8511088981435584, 0.765948745146389, 0.22947409208105762, 0.6884222762431729, 0.051386443862308284, 0.3982449399328892, 0.051386443862308284, 0.359705107036158, 0.051386443862308284, 0.012846610965577071, 0.051386443862308284, 0.025693221931154142, 0.8120306410434954, 0.32018238334105603, 0.026681865278421334, 0.12673886007250135, 0.06670466319605334, 0.14675025903131733, 0.060034196876448, 0.08671606215486934, 0.04669326423723734, 0.060034196876448, 0.060034196876448, 0.8286174585854008, 0.8286174368014418, 0.0494510020768165, 0.02472550103840825, 0.098902004153633, 0.8159415342674722, 0.15596181101689435, 0.6238472440675774, 0.23174814584077547, 0.21068013258252316, 0.021068013258252315, 0.5267003314563079, 0.24780965299518637, 0.16851056403672673, 0.10573211861127953, 0.09251560378486959, 0.07269083154525467, 0.07599496025185716, 0.06938670283865218, 0.06277844542544722, 0.06938670283865218, 0.036345415772627336, 0.81380407823074, 0.7373318567594195, 0.18433296418985487, 0.22185346817846743, 0.6655604045354023, 0.890636406415246, 0.08946349652173588, 0.17892699304347176, 0.6262444756521511, 0.02052422373970513, 0.20524223739705133, 0.16419378991764105, 0.20524223739705133, 0.30786335609557697, 0.02052422373970513, 0.02052422373970513, 0.04104844747941026, 0.02052422373970513, 0.04050106088583916, 0.08100212177167831, 0.2025053044291958, 0.16200424354335663, 0.48601273063006983, 0.07503939313415729, 0.637834841640337, 0.07503939313415729, 0.18759848283539324, 0.12403249601074932, 0.24806499202149865, 0.4961299840429973, 0.03606091116074525, 0.06490964008934146, 0.41830656946464495, 0.08654618678578861, 0.31012383598240917, 0.014424364464298102, 0.021636546696447153, 0.03606091116074525, 0.2406696161284892, 0.1429720491852411, 0.13344057923955835, 0.10961190437535151, 0.11437763934819288, 0.08578322951114466, 0.04289161475557233, 0.07386889207904124, 0.03574301229631027, 0.02382867486420685, 0.9217478054554777, 0.21263533586885694, 0.06714800080069167, 0.07833933426747361, 0.11191333466781944, 0.05595666733390972, 0.1790613354685111, 0.1790613354685111, 0.02238266693356389, 0.04476533386712778, 0.04476533386712778, 0.0990615689000438, 0.0990615689000438, 0.5943694134002628, 0.0990615689000438, 0.2699939921207483, 0.08307507449869178, 0.10384384312336473, 0.10384384312336473, 0.373837835244113, 0.04153753724934589, 0.8161367562121628, 0.024957684554004666, 0.024957684554004666, 0.24957684554004667, 0.17470379187803264, 0.4741960065260886, 0.04991536910800933, 0.277275300062321, 0.11615586894502636, 0.11990283245938203, 0.14613157705987187, 0.04496356217226827, 0.08618016083018085, 0.04121659865791258, 0.06369837974404671, 0.0337226716292012, 0.07119230677275809, 0.940571299750588, 0.04702856498752939, 0.32870385351025555, 0.18439484465209457, 0.09620600590544065, 0.10422317306422736, 0.08818883874665392, 0.09620600590544065, 0.056120170111507044, 0.024051501476360163, 0.01603433431757344, 0.00801716715878672, 0.10317879701215844, 0.1375717293495446, 0.1375717293495446, 0.03439293233738615, 0.32673285720516837, 0.03439293233738615, 0.17196466168693073, 0.05158939850607922, 0.7361498953213447, 0.8127192812997777, 0.08554939803155555, 0.042774699015777774, 0.8120307750296298, 0.1908994199442683, 0.5726982598328049, 0.9029137969088303, 0.6648343273381453, 0.2216114424460484, 0.8161367314193627, 0.8451319942927015, 0.9081525049331814, 0.019022150150418013, 0.11413290090250808, 0.22826580180501616, 0.019022150150418013, 0.22826580180501616, 0.34239870270752426, 0.019022150150418013, 0.24842702305914366, 0.13854583978298396, 0.15765561078753348, 0.07166164126706068, 0.062106755764785915, 0.07166164126706068, 0.07643908401819804, 0.09554885502274756, 0.02866465650682427, 0.04777442751137378, 0.2535824526659366, 0.14490425866624948, 0.03622606466656237, 0.07245212933312474, 0.09056516166640594, 0.03622606466656237, 0.30792154966578017, 0.05433909699984356, 0.041333372822549334, 0.16533349129019734, 0.16533349129019734, 0.31000029616912, 0.22733355052402132, 0.062000059233824, 0.041333372822549334, 0.03248318973195634, 0.8120797432989084, 0.03248318973195634, 0.03248318973195634, 0.06496637946391268, 0.7417861971153094, 0.6945041736319675, 0.036552851243787765, 0.25586995870651436, 0.8286173756614443, 0.6950645716453369, 0.7654501378925342, 0.06844173760025628, 0.34220868800128135, 0.06844173760025628, 0.47909216320179393, 0.037448038500160256, 0.29958430800128205, 0.11234411550048078, 0.48682450050208337, 0.037448038500160256, 0.856729568418463, 0.7654502174989775, 0.9449694024000224, 0.4842954673400672, 0.4842954673400672, 0.8213969174263254, 0.06844974311886044, 0.06844974311886044, 0.812030454083913, 0.8731090405732109, 0.2520496188017348, 0.14177791057597583, 0.09451860705065056, 0.03150620235021685, 0.09451860705065056, 0.015753101175108426, 0.04725930352532528, 0.20479031527640953, 0.0630124047004337, 0.04725930352532528, 0.8408932553084123, 0.8906374804706698, 0.27696796036106236, 0.5539359207221247, 0.8907587908116849, 0.8408928087419788, 0.09439332093811044, 0.09439332093811044, 0.4845523808156336, 0.025171552250162786, 0.1258577612508139, 0.025171552250162786, 0.044050216437784874, 0.025171552250162786, 0.044050216437784874, 0.025171552250162786, 0.8780507371221473, 0.05487817107013421, 0.05487817107013421, 0.01205278982427287, 0.01205278982427287, 0.3374781150796404, 0.2048974270126388, 0.3254253252553675, 0.0843695287699101, 0.01205278982427287, 0.860083865398254, 0.7361499077990271, 0.11257842200942733, 0.675470532056564, 0.11257842200942733, 0.13127811440341394, 0.39383434321024186, 0.32819528600853487, 0.06563905720170697, 0.1062199231812766, 0.2575028440758221, 0.09012599542653772, 0.11265749428317215, 0.10943870873222437, 0.1062199231812766, 0.06437571101895552, 0.07725085322274662, 0.041844212162321086, 0.03218785550947776, 0.15476258935828102, 0.7738129467914051, 0.24853518648487746, 0.18451854754180297, 0.07907937751791556, 0.1393303318172798, 0.045188215724523174, 0.09790780073646688, 0.04142253108081291, 0.045188215724523174, 0.04895390036823344, 0.07154800823049504, 0.10213890154019555, 0.1532083523102933, 0.10213890154019555, 0.10213890154019555, 0.3574861553906844, 0.1532083523102933, 0.40749896442546446, 0.11036430286522997, 0.05093737055318306, 0.16130167341841303, 0.07640605582977458, 0.02546868527659153, 0.02546868527659153, 0.059426932312046904, 0.016979123517727686, 0.06791649407091074, 0.8138040867684824, 0.11356779149149557, 0.1362813497897947, 0.052998302696031264, 0.1362813497897947, 0.06814067489489735, 0.1362813497897947, 0.1211389775909286, 0.037855930497165194, 0.07571186099433039, 0.1211389775909286, 0.8699508704336368, 0.9764703353936506, 0.10620546387214883, 0.8496437109771906, 0.8102672968815094, 0.13504454948025157, 0.2456032978560361, 0.6549421276160963, 0.10990534824769606, 0.21981069649539212, 0.6594320894861764, 0.1539739196962264, 0.08981811982279872, 0.19246739962028298, 0.5132463989874213, 0.05132463989874213, 0.34915201916160443, 0.11638400638720148, 0.08899953429609524, 0.12323012440997803, 0.06846118022776557, 0.08215341627331868, 0.02053835406832967, 0.03423059011388278, 0.0479228261594359, 0.06846118022776557, 0.8138043599295827, 0.8699509391147688, 0.8676623203301452, 0.08676623203301453, 0.08580747689421507, 0.26457305375716317, 0.042903738447107535, 0.14301246149035846, 0.1573137076393943, 0.10725934611776884, 0.028602492298071693, 0.092958099968733, 0.07865685381969716, 0.007150623074517923, 0.7659474072810885, 0.8744767455012781, 0.12165991865958056, 0.12165991865958056, 0.6082995932979028, 0.10113886059801193, 0.06321178787375746, 0.012642357574751491, 0.24020479392027833, 0.012642357574751491, 0.11378121817276342, 0.1390659333222664, 0.17699300604652088, 0.06321178787375746, 0.06321178787375746, 0.04321053990770722, 0.14403513302569074, 0.1584386463282598, 0.30247377935395053, 0.11522810642055259, 0.10082459311798352, 0.12963161972312165, 0.3344797488667879, 0.6689594977335758, 0.08508612245323792, 0.056724081635491946, 0.08508612245323792, 0.11344816327098389, 0.6523269388081574, 0.3157882468334087, 0.6315764936668175, 0.812030819058715, 0.6360045589534504, 0.0908577941362072, 0.1817155882724144, 0.8138040867684085, 0.8161367128709018, 0.7332660012327432, 0.1833165003081858, 0.7361498415026355, 0.08115246265596718, 0.08115246265596718, 0.4598639550504807, 0.05410164177064479, 0.13525410442661198, 0.16230492531193436, 0.8744767889709195, 0.1429846562680585, 0.1429846562680585, 0.367674830403579, 0.040852758933731005, 0.1838374152017895, 0.08170551786746201, 0.020426379466865503, 0.08443755282244, 0.033775021128976, 0.033775021128976, 0.033775021128976, 0.20265012677385602, 0.050662531693464005, 0.540400338063616, 0.13450922494101533, 0.8070553496460919, 0.38146323567563484, 0.038146323567563485, 0.12397555159458133, 0.08582922802701784, 0.047682904459454355, 0.21934136051349004, 0.047682904459454355, 0.038146323567563485, 0.019073161783781743, 0.009536580891890871, 0.09374027964156678, 0.6093118176701842, 0.14061041946235017, 0.09374027964156678, 0.8347658080867608, 0.15651858901626764, 0.16474547259736816, 0.14827092533763137, 0.2635927561557891, 0.04942364177921045, 0.032949094519473636, 0.04118636814934204, 0.10708455718828931, 0.08237273629868408, 0.08237273629868408, 0.04118636814934204, 0.8841460758343374, 0.1636254672924166, 0.390183806620378, 0.1636254672924166, 0.012586574407108968, 0.0755194464426538, 0.08810602084976278, 0.0755194464426538, 0.012586574407108968, 0.7458223748486532, 0.869950507797882, 0.01658898689916449, 0.06635594759665796, 0.01658898689916449, 0.3317797379832898, 0.24883480348746737, 0.1990678427899739, 0.09953392139498694, 0.8120307686204266, 0.09935866787243842, 0.695510675107069, 0.09935866787243842, 0.8201187189664277, 0.12335903580451463, 0.10051476991478969, 0.25128692478697423, 0.0319819722456149, 0.054826238135339836, 0.18732298029574443, 0.1416344485162946, 0.004568853177944986, 0.03655082542355989, 0.0639639444912298, 0.8408931657183909, 0.7458227989355125, 0.8031181482201157, 0.9260196301417594, 0.8914620234861651, 0.81203063634882, 0.08110851760321612, 0.3244340704128645, 0.5677596232225128, 0.04388527277785147, 0.16091266685212205, 0.029256848518567642, 0.30719690944496025, 0.409595879259947, 0.04388527277785147, 0.2019495849325258, 0.1009747924662629, 0.7068235472638402, 0.9189674044234364, 0.7746615117523323, 0.6646237703738034, 0.16615594259345084, 0.09297671335113744, 0.5113719234312559, 0.09297671335113744, 0.13946507002670616, 0.06973253501335308, 0.02324417833778436, 0.04648835667556872, 0.7659490221356816, 0.773170633094538, 0.0909612509522986, 0.0454806254761493, 0.0454806254761493, 0.8672194209189626, 0.7361499077990487, 0.15551647635145363, 0.19439559543931703, 0.11663735726359022, 0.46654942905436086, 0.03887911908786341, 0.035697274371913584, 0.8388859477399693, 0.053545911557870376, 0.017848637185956792, 0.017848637185956792, 0.035697274371913584, 0.8120310595870969, 0.30792729171940986, 0.1319688393083185, 0.06912653487578588, 0.1005476870920522, 0.09426345664879893, 0.08797922620554567, 0.0816949957622924, 0.0816949957622924, 0.043989613102772836, 0.006284230443253262, 0.29612575059197443, 0.2919549653723692, 0.1751729792234215, 0.09175727483131603, 0.037537066976447465, 0.016683140878421095, 0.012512355658815822, 0.03336628175684219, 0.04170785219605274, 0.8665653830665073, 0.10807570904362127, 0.8646056723489701, 0.7746614677234088, 0.19644056843431046, 0.6875419895200866, 0.870825953852384, 0.09414334636241989, 0.023535836590604972, 0.11583605537164206, 0.5019562399437822, 0.03861201845721402, 0.23167211074328412, 0.03861201845721402, 0.07722403691442804, 0.9449692007558854, 0.12087490045095552, 0.12087490045095552, 0.7252494027057331, 0.8673006487487225, 0.08833057112292637, 0.1261865301756091, 0.3659409375092664, 0.08833057112292637, 0.08833057112292637, 0.10094922414048728, 0.02523730603512182, 0.02523730603512182, 0.07571191810536546, 0.01261865301756091, 0.8699509326664453, 0.09130391344751579, 0.20543380525691052, 0.06847793508563685, 0.022825978361878947, 0.20543380525691052, 0.1369558701712737, 0.06847793508563685, 0.20543380525691052, 0.9081522485042844, 0.8408931096922654, 0.2971621640674771, 0.0424517377239253, 0.5094208526871036, 0.1273552131717759, 0.11551851402864906, 0.15402468537153208, 0.5005802274574793, 0.23103702805729812, 0.7654502828640549, 0.7659485485286924, 0.7848225689378028, 0.15696451378756054, 0.115483623062825, 0.2598381518913563, 0.08661271729711875, 0.23096724612565, 0.02887090576570625, 0.2598381518913563, 0.44348314527088645, 0.11826217207223638, 0.029565543018059096, 0.029565543018059096, 0.35478651621670915, 0.2247500920522932, 0.6742502761568796, 0.2241581776731592, 0.15117644540747946, 0.12511154102687955, 0.07819471314179972, 0.07819471314179972, 0.11468557927463958, 0.03649086613283987, 0.05212980876119981, 0.06776875138955976, 0.06776875138955976, 0.8652866043636436, 0.06710714848504704, 0.06710714848504704, 0.13421429697009407, 0.6710714848504703, 0.22364933236428874, 0.6709479970928662, 0.08109700205061472, 0.24329100615184415, 0.567679014354303, 0.08109700205061472, 0.12597831481995742, 0.12597831481995742, 0.12597831481995742, 0.5039132592798297, 0.2134857613028098, 0.0520696978787341, 0.12496727490896184, 0.08331151660597456, 0.14579515406045548, 0.13017424469683525, 0.14579515406045548, 0.02082787915149364, 0.03124181872724046, 0.0520696978787341, 0.13271131287586221, 0.06286325346751369, 0.20255937228421075, 0.12572650693502738, 0.1816049544617062, 0.13969611881669708, 0.06984805940834854, 0.03492402970417427, 0.04889364158584397, 0.737216268528399, 0.18430406713209976, 0.17819621952975367, 0.5702279024952117, 0.14255697562380293, 0.07127848781190146, 0.03563924390595073, 0.7833580823977495, 0.10444774431969993, 0.026111936079924982, 0.052223872159849964, 0.026111936079924982, 0.902913862285587, 0.7294344403806682, 0.09110972472081758, 0.18221944944163515, 0.09110972472081758, 0.5466583483249055, 0.8201187369605606, 0.14336033224343983, 0.5017611628520393, 0.015928925804826646, 0.26282727577963966, 0.015928925804826646, 0.015928925804826646, 0.04778677741447994, 0.007964462902413323, 0.5928453964829864, 0.3293585536016591, 0.28318810407156053, 0.6230138289574332, 0.056637620814312103, 0.08667169914690646, 0.20584528547390285, 0.1516754735070863, 0.20584528547390285, 0.3575207589809891, 0.16261288730171497, 0.46460824943347134, 0.09292164988669427, 0.16261288730171497, 0.0696912374150207, 0.023230412471673567, 0.10658626247362366, 0.13463527891405092, 0.10658626247362366, 0.13463527891405092, 0.10658626247362366, 0.07292744274511091, 0.14024508220213638, 0.044878426304683645, 0.06731763945702546, 0.08975685260936729, 0.3742207477879702, 0.514553528208459, 0.046777593473496276, 0.9763554935134663, 0.15266669848241157, 0.7633334924120578, 0.8120307568814035, 0.05685130913088133, 0.05685130913088133, 0.085276963696322, 0.7106413641360166, 0.05685130913088133, 0.028425654565440665, 0.18035041541342794, 0.7214016616537118, 0.7746617552713393, 0.10949285044917631, 0.15328999062884682, 0.08759428035934105, 0.15328999062884682, 0.021898570089835263, 0.08759428035934105, 0.37227569152719947, 0.8672194462352001, 0.8652872560879633, 0.775275742573928, 0.15505514851478558, 0.6417467026552913, 0.058340609332299204, 0.058340609332299204, 0.07778747910973227, 0.058340609332299204, 0.058340609332299204, 0.038893739554866136, 0.7819866259097292, 0.15639732518194585, 0.8286173756614443, 0.7458232050919243, 0.1290534643098755, 0.4516871250845642, 0.08603564287325033, 0.053772276795781455, 0.13980791966903178, 0.03226336607746887, 0.021508910718312582, 0.06452673215493775, 0.021508910718312582, 0.7458223748486532, 0.23835721786607994, 0.5958930446651999, 0.11917860893303997, 0.3699368060424026, 0.10569623029782932, 0.0352320767659431, 0.0352320767659431, 0.0352320767659431, 0.0352320767659431, 0.05284811514891466, 0.05284811514891466, 0.2994726525105164, 0.21209827020754154, 0.07069942340251385, 0.07069942340251385, 0.6362948106226247, 0.10312214178901465, 0.10312214178901465, 0.6187328507340879, 0.2843730979449235, 0.568746195889847, 0.8811036842349146, 0.0677772064796088, 0.20492561405505702, 0.10246280702752851, 0.27664957897432696, 0.0204925614055057, 0.051231403513764255, 0.21517189475780987, 0.06147768421651711, 0.06147768421651711, 0.01024628070275285, 0.9118340692958509, 0.12012071991215197, 0.7576845409843432, 0.02772016613357353, 0.07392044302286276, 0.009240055377857844, 0.08094875096547044, 0.20686903024509112, 0.035977222651320195, 0.12592027927962068, 0.053965833976980286, 0.026982916988490143, 0.2608348642220714, 0.09893736229113054, 0.09893736229113054, 0.4224725586595782, 0.4224725586595782, 0.8161367753408912, 0.8679115748911329, 0.017712481120227202, 0.08856240560113601, 0.19700628456249594, 0.08443126481249826, 0.04221563240624913, 0.05628750987499884, 0.08443126481249826, 0.2673656719062445, 0.09850314228124797, 0.1547906521562468, 0.01407187746874971, 0.8408936832284184, 0.07754055113722283, 0.15508110227444566, 0.07754055113722283, 0.5427838579605598, 0.07754055113722283, 0.18750687713594244, 0.09375343856797122, 0.09375343856797122, 0.5625206314078274, 0.7120636123662433, 0.18988362996433153, 0.04747090749108288, 0.7659485232206004, 0.15310012247842114, 0.12526373657325365, 0.18093650838358863, 0.04871367533404309, 0.12526373657325365, 0.11134554362066992, 0.0695909647629187, 0.09742735066808618, 0.04871367533404309, 0.02783638590516748, 0.7659485232206004, 0.5306553762555187, 0.13694332290465, 0.18829706899389373, 0.0342358307261625, 0.01711791536308125, 0.08558957681540624, 0.15507174677807975, 0.10338116451871984, 0.05169058225935992, 0.671977569371679, 0.00886743821093049, 0.01773487642186098, 0.780334562561883, 0.07980694389837441, 0.02660231463279147, 0.05320462926558294, 0.02660231463279147, 0.8567294614316023, 0.05712143180596784, 0.8568214770895176, 0.05712143180596784, 0.07193409620749995, 0.047956064138333296, 0.407626545175833, 0.11989016034583325, 0.3356924489683331, 0.21766195921642623, 0.6529858776492786, 0.3788049352647707, 0.5682074028971561, 0.932608035398646, 0.7075310705326078, 0.05442546696404675, 0.16327640089214027, 0.05442546696404675, 0.867219410176483, 0.15265475547509014, 0.1744625776858173, 0.03634637035121194, 0.19627039989654446, 0.10903911105363583, 0.1671933036155749, 0.03634637035121194, 0.043615644421454325, 0.014538548140484777, 0.07269274070242387, 0.9047319386683558, 0.9081523154681738, 0.2214316698641833, 0.08740723810428289, 0.20395022224332673, 0.11654298413904385, 0.12819728255294824, 0.06409864127647412, 0.08740723810428289, 0.023308596827808767, 0.005827149206952192, 0.058271492069521924, 0.18679461582325826, 0.15358668412134568, 0.11207676949395495, 0.11622776095669403, 0.10377478656847682, 0.058113880478347014, 0.1203787524194331, 0.04981189755286887, 0.058113880478347014, 0.03320793170191258, 0.05812502400813177, 0.3681251520515012, 0.21312508802981647, 0.23250009603252708, 0.11625004801626354, 0.8322870621040837, 0.11889815172915481, 0.8652881227082421, 0.3080084096966226, 0.13910057212105534, 0.1490363272725593, 0.07617412282819698, 0.06955028606052767, 0.06623836767669303, 0.06955028606052767, 0.07286220444436232, 0.02980726545451186, 0.019871510303007905, 0.03206238594960544, 0.17634312272282993, 0.01603119297480272, 0.288561473546449, 0.11221835082361906, 0.03206238594960544, 0.12824954379842177, 0.2244367016472381, 0.17366942706501381, 0.2894490451083564, 0.20261433157584946, 0.34733885413002763, 0.8665658378789365, 0.9313628747484302, 0.18074352939676946, 0.05164100839907699, 0.025820504199538494, 0.15492302519723097, 0.36148705879353893, 0.20656403359630796, 0.26240646189178796, 0.08746882063059598, 0.6122817444141718, 0.22893005899479604, 0.6867901769843882, 0.23347533133292578, 0.7004259939987774, 0.04669506626658516, 0.5525578850819869, 0.13477021587365534, 0.0404310647620966, 0.17520128063575194, 0.06738510793682767, 0.013477021587365533, 0.7659490221356816, 0.20185136872544412, 0.7064797905390544, 0.05046284218136103, 0.8138041014476992, 0.617346217383139, 0.02572275905763079, 0.2315048315186771, 0.07716827717289237, 0.02572275905763079, 0.22242309505111205, 0.6672692851533362, 0.06939515196385225, 0.1387903039277045, 0.06939515196385225, 0.6245563676746702, 0.8285895334796106, 0.11201058532101109, 0.11201058532101109, 0.15509157967524612, 0.14647538080439912, 0.17232397741694014, 0.10339438645016408, 0.017232397741694012, 0.1206267841918581, 0.02584859661254102, 0.02584859661254102, 0.19567616968960944, 0.06522538989653648, 0.13045077979307296, 0.5870285090688283, 0.8306683676073333, 0.10383354595091666, 0.2289301486286751, 0.6867904458860253, 0.20798146311406823, 0.23917868258117847, 0.1559860973355512, 0.031197219467110234, 0.23917868258117847, 0.06239443893422047, 0.010399073155703412, 0.010399073155703412, 0.031197219467110234, 0.8906476070804007, 0.1984607250466137, 0.694612537663148, 0.09923036252330684, 0.2833665322088823, 0.5667330644177646, 0.0634801381303624, 0.3808808287821745, 0.3650107942495839, 0.07935017266295302, 0.0634801381303624, 0.04761010359777181, 0.10595557947184807, 0.18100744826440712, 0.09271113203786706, 0.1501037375851181, 0.17659263245308013, 0.08388150041521306, 0.061807421358578044, 0.061807421358578044, 0.030903710679289022, 0.052977789735924034, 0.18112690520622482, 0.23546497676809228, 0.024150254027496643, 0.1388639606581057, 0.09056345260311241, 0.10263857961686074, 0.12075127013748321, 0.030187817534370803, 0.030187817534370803, 0.04226294454811912, 0.10124536562126696, 0.0899958805522373, 0.10124536562126696, 0.13499382082835595, 0.1799917611044746, 0.10124536562126696, 0.056247425345148316, 0.1799917611044746, 0.06749691041417798, 0.07178301330693333, 0.1316021910627111, 0.04785534220462222, 0.11963835551155555, 0.3110597243300444, 0.07178301330693333, 0.07178301330693333, 0.1316021910627111, 0.04785534220462222, 0.8106601971225813, 0.13511003285376355, 0.04117443496891871, 0.45291878465810587, 0.04117443496891871, 0.12352330490675614, 0.3293954797513497, 0.9607024092321358, 0.8936751164920135, 0.07938290404519643, 0.31753161618078574, 0.6350632323615715, 0.14809796412023749, 0.8145388026613061, 0.7654502174988387, 0.716193491145163, 0.22226694552780918, 0.024696327280867687, 0.27673670600912204, 0.5534734120182441, 0.7195218598132326, 0.06541107816483932, 0.16352769541209833, 0.03270553908241966, 0.7659490451884217, 0.2564871237453699, 0.22228884057932058, 0.04559771088806576, 0.12539370494218083, 0.10259484949814796, 0.07979599405411508, 0.02279885544403288, 0.04559771088806576, 0.03989799702705754, 0.06269685247109041, 0.2633542213272262, 0.13913053202193082, 0.13913053202193082, 0.07453421358317723, 0.12919263687750718, 0.14409947959414263, 0.034782633005482705, 0.034782633005482705, 0.02981368543327089, 0.01987579028884726, 0.884145960094026, 0.8120309198937228, 0.14832594662433177, 0.08899556797459907, 0.08899556797459907, 0.44497783987299533, 0.17799113594919813, 0.029665189324866354, 0.7294344355111584, 0.09823235394615813, 0.7858588315692651, 0.8673009898500013, 0.2275087303000273, 0.14625561233573184, 0.08125311796429546, 0.3575137190429, 0.008125311796429547, 0.04062655898214773, 0.016250623592859094, 0.11375436515001365, 0.28437431260864604, 0.5687486252172921, 0.6904940401968035, 0.02092406182414556, 0.1046203091207278, 0.08369624729658225, 0.04184812364829112, 0.02092406182414556, 0.049533613720497915, 0.024766806860248958, 0.8173046263882155, 0.024766806860248958, 0.03715021029037344, 0.012383403430124479, 0.012383403430124479, 0.14615848509973264, 0.048719495033244214, 0.19487798013297686, 0.4871949503324421, 0.09743899006648843, 0.048719495033244214, 0.10479365914587205, 0.10479365914587205, 0.7335556140211044, 0.6447009922858096, 0.026862541345242064, 0.0805876240357262, 0.06715635336310516, 0.09401889470834722, 0.013431270672621032, 0.05372508269048413, 0.8146422906002555, 0.13577371510004257, 0.1235757870673687, 0.7414547224042122, 0.30144018452708377, 0.0845502956600357, 0.1801288907539891, 0.07719809603742389, 0.1029307947165652, 0.0955785950939534, 0.0441131977356708, 0.06984589641481209, 0.0147043992452236, 0.0294087984904472, 0.8567294329288846, 0.9338431106671993, 0.46893320752554346, 0.057351543366433375, 0.10795584633681576, 0.0910877453466883, 0.02361534138617845, 0.0910877453466883, 0.05060430297038239, 0.0472306827723569, 0.033736201980254926, 0.030362581782229434, 0.736149747091971, 0.7659490221356816, 0.5806181415435882, 0.19353938051452937], \"Term\": [\"036\", \"043\", \"1011\", \"124\", \"14\", \"14\", \"14\", \"14\", \"14\", \"14\", \"145\", \"14x7\", \"15mm\", \"16\", \"16\", \"16\", \"16\", \"16\", \"16\", \"16\", \"185\", \"185\", \"185\", \"2055516rear\", \"2255016\", \"265\", \"265\", \"2655\", \"3033\", \"30k\", \"3100\", \"3100\", \"3200cs\", \"3200cs\", \"345\", \"364\", \"39\", \"4310316\", \"525i\", \"528i\", \"528i\", \"530\", \"530\", \"5series\", \"5spd\", \"5spd\", \"5speed\", \"5speed\", \"5speed\", \"5th\", \"633\", \"65\", \"65\", \"6505740561\", \"87k\", \"89\", \"91\", \"91\", \"91\", \"91\", \"911\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"adjustment\", \"adjustment\", \"adjustment\", \"adjustment\", \"adjustment\", \"adjustment\", \"advanced\", \"af\", \"afm\", \"afm\", \"afm\", \"aftermarket\", \"aftermarket\", \"aftermarket\", \"aftermarket\", \"aftermarket\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"airdam\", \"airdam\", \"alpina\", \"alpina\", \"alpina\", \"alpina\", \"alpina\", \"alpina\", \"alpina\", \"alpinas\", \"alpinas\", \"alpinas\", \"alpinas\", \"aluminum\", \"aluminum\", \"aluminum\", \"aluminum\", \"aluminum\", \"aluminum\", \"analog\", \"armrest\", \"audio\", \"autocross\", \"autocrossed\", \"autocrossing\", \"availability\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"b34\", \"b35\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"badge\", \"badge\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"barrel\", \"bath\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"bbq\", \"beam\", \"beam\", \"betweeen\", \"bidding\", \"bilsteins\", \"bilsteins\", \"bilstien\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blaster\", \"blasting\", \"blasting\", \"bleed\", \"bleed\", \"bleed\", \"blow\", \"blow\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"bmwcca\", \"bodyshop\", \"boost\", \"boost\", \"boost\", \"boost\", \"bore\", \"bore\", \"bore\", \"bore\", \"bored\", \"bored\", \"bosch\", \"bosch\", \"bosch\", \"bosch\", \"bosch\", \"bosch\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"brace\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"brake\", \"branch\", \"breaker\", \"breaker\", \"brilliant\", \"btdc\", \"btw\", \"bucket\", \"bucket\", \"bucket\", \"bucket\", \"bucket\", \"bucket\", \"bushing\", \"bushing\", \"cabin\", \"caliper\", \"cam\", \"cam\", \"cam\", \"cam\", \"cam\", \"cam\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"carb\", \"carb\", \"carb\", \"carb\", \"carb\", \"carbs\", \"carbs\", \"carl\", \"carl\", \"carl\", \"carl\", \"carl\", \"carl\", \"carl\", \"carpet\", \"carpet\", \"carpet\", \"carrier\", \"carrier\", \"carrier\", \"causing\", \"causing\", \"causing\", \"causing\", \"causing\", \"centre\", \"centre\", \"centre\", \"centre\", \"chain\", \"chain\", \"chain\", \"chamber\", \"chamber\", \"chamber\", \"chamber\", \"champ\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"checked\", \"checked\", \"checked\", \"checked\", \"checked\", \"checked\", \"checked\", \"cheeky\", \"chip\", \"chip\", \"chip\", \"choke\", \"choke\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"civic\", \"climate\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"cloth\", \"cloth\", \"clunk\", \"clunking\", \"coil\", \"coil\", \"coil\", \"coil\", \"coil\", \"coil\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"combustion\", \"combustion\", \"combustion\", \"combustion\", \"complaining\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compressor\", \"compressor\", \"compressor\", \"condenser\", \"condenser\", \"confused\", \"confused\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connector\", \"connector\", \"connector\", \"connector\", \"constant\", \"constant\", \"continuity\", \"continuity\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"converting\", \"core\", \"country\", \"country\", \"country\", \"coupeking\", \"coupeking\", \"cr\", \"cr\", \"cr\", \"cr\", \"creating\", \"crimp\", \"csr\", \"csr\", \"csr\", \"cup\", \"cup\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"cvs\", \"dagmars\", \"dagmars\", \"daily\", \"daily\", \"daily\", \"dark\", \"dark\", \"dark\", \"dark\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dealer\", \"dealer\", \"dealer\", \"dealer\", \"dealer\", \"debating\", \"density\", \"derived\", \"diagram\", \"diagram\", \"diaphram\", \"diff\", \"diff\", \"diff\", \"diffs\", \"distribution\", \"dizzy\", \"dizzy\", \"djet\", \"djet\", \"djet\", \"djet\", \"djet\", \"djet\", \"djet\", \"doable\", \"doable\", \"dogleg\", \"dogleg\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"drilling\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"dryer\", \"dust\", \"dust\", \"dust\", \"dynamat\", \"e12\", \"e12\", \"e12\", \"e12\", \"e12\", \"e12\", \"e12s\", \"e3\", \"e3\", \"e3\", \"e3\", \"e3\", \"ebrake\", \"ecu\", \"ecu\", \"ecu\", \"ecu\", \"ecu\", \"ecu\", \"electrical\", \"electrical\", \"electrical\", \"electrical\", \"electrical\", \"electrode\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"engages\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"enginetrans\", \"ensure\", \"entry\", \"estate\", \"estate\", \"et11\", \"et11\", \"et24\", \"etype\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"euro\", \"evan\", \"event\", \"event\", \"event\", \"event\", \"event\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"exhaust\", \"experiencing\", \"extinguisher\", \"factory\", \"factory\", \"factory\", \"factory\", \"factory\", \"factory\", \"factory\", \"family\", \"family\", \"faq\", \"faq\", \"fender\", \"fender\", \"fender\", \"fender\", \"fender\", \"fender\", \"ferrari\", \"ferrari\", \"fiat\", \"fiat\", \"finest\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"floor\", \"floor\", \"floor\", \"floor\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flowed\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"flywheel\", \"flywheel\", \"foam\", \"fooling\", \"ford\", \"ford\", \"ford\", \"ford\", \"ford\", \"forged\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"freed\", \"freon\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fuel\", \"fuel\", \"fuel\", \"fuel\", \"fuel\", \"fuel\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fuse\", \"fuse\", \"fuse\", \"fuse\", \"gap\", \"gap\", \"gap\", \"garage\", \"garage\", \"garage\", \"garage\", \"garage\", \"garage\", \"garage\", \"garage\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gasoline\", \"gauge\", \"gauge\", \"gauge\", \"gauge\", \"gauge\", \"gauge\", \"gauge\", \"gear\", \"gear\", \"gear\", \"gear\", \"gear\", \"gear\", \"gear\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"german\", \"gerrit\", \"gerrit\", \"getrag\", \"getrag\", \"giving\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glue\", \"glue\", \"gm\", \"gm\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"gps\", \"gray\", \"gray\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"grey\", \"grey\", \"grey\", \"grey\", \"grey\", \"grill\", \"grill\", \"grill\", \"grit\", \"ground\", \"ground\", \"ground\", \"ground\", \"ground\", \"ground\", \"ground\", \"gt\", \"gt\", \"hans\", \"harmonic\", \"harness\", \"harness\", \"harness\", \"harness\", \"harness\", \"harness\", \"harness\", \"harness\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"heading\", \"headlamp\", \"headlamp\", \"headlamp\", \"headlight\", \"headlight\", \"headlight\", \"headlight\", \"headliner\", \"headliner\", \"headliner\", \"heat\", \"heat\", \"heat\", \"heat\", \"heat\", \"heat\", \"heat\", \"heater\", \"heater\", \"heater\", \"heater\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"hibernation\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hindsight\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hood\", \"hood\", \"hood\", \"hood\", \"hood\", \"hood\", \"hood\", \"horizontal\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"ice\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idle\", \"idle\", \"idle\", \"idle\", \"idle\", \"idle\", \"ignition\", \"ignition\", \"ignition\", \"ignition\", \"ignition\", \"ignition\", \"iii\", \"illuminate\", \"illuminate\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"imo\", \"impedance\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"industry\", \"injected\", \"injector\", \"injector\", \"injector\", \"insert\", \"insert\", \"insert\", \"insert\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"intake\", \"intake\", \"intake\", \"intake\", \"intake\", \"intake\", \"integrated\", \"interchange\", \"interchange\", \"interfere\", \"interference\", \"interference\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"iron\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"jag\", \"january\", \"jet\", \"jet\", \"jet\", \"jet\", \"john\", \"john\", \"key\", \"key\", \"key\", \"key\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"knowjoseph\", \"korman\", \"korman\", \"lajolla\", \"lajolla\", \"lap\", \"layer\", \"layer\", \"layer\", \"leak\", \"leak\", \"leak\", \"leak\", \"leak\", \"leak\", \"leak\", \"leak\", \"leak\", \"lean\", \"lean\", \"lean\", \"lean\", \"lean\", \"leather\", \"leather\", \"leather\", \"leather\", \"lhd\", \"lhd\", \"lhd\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"linda\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linkage\", \"linkage\", \"linkage\", \"linkage\", \"list\", \"list\", \"list\", \"list\", \"list\", \"list\", \"lites\", \"ljet\", \"ljet\", \"ljet\", \"ljet\", \"ljet\", \"ljet\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"loop\", \"loop\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"lowimpedence\", \"lsd\", \"lsd\", \"lsd\", \"lube\", \"lug\", \"lug\", \"m1\", \"m30b35\", \"m30b35\", \"m53535\", \"maf\", \"magazine\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"malc\", \"malc\", \"malc\", \"malc\", \"malc\", \"malc\", \"malc\", \"malc\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manual\", \"manual\", \"manual\", \"manual\", \"manual\", \"manually\", \"map\", \"map\", \"map\", \"march\", \"mario\", \"marios\", \"master\", \"master\", \"master\", \"master\", \"material\", \"material\", \"material\", \"material\", \"material\", \"mechanically\", \"mechanics\", \"mechanism\", \"megasquirt\", \"megasquirt\", \"mesa\", \"mesa\", \"mesa\", \"messing\", \"metric\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"mile\", \"module\", \"molex\", \"monica\", \"monica\", \"morris\", \"motivation\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motorcycle\", \"motorcycle\", \"motorcycle\", \"motronic\", \"motronic\", \"motronic\", \"motronic\", \"motronic\", \"motronic\", \"motronic\", \"motronics\", \"ms\", \"msd\", \"msd\", \"msd\", \"muffler\", \"muffler\", \"muffler\", \"muffler\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"needle\", \"needle\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nozzle\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"observation\", \"octane\", \"od\", \"od\", \"offending\", \"offending\", \"offset\", \"offset\", \"ohm\", \"ohm\", \"ohm\", \"oil\", \"oil\", \"oil\", \"oil\", \"oil\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"omg\", \"opposed\", \"optima\", \"optima\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"oversized\", \"oxygen\", \"pack\", \"pack\", \"pack\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"panel\", \"panel\", \"panel\", \"panel\", \"panel\", \"panel\", \"panel\", \"parameter\", \"parameter\", \"parking\", \"parking\", \"parking\", \"parking\", \"parking\", \"paste\", \"paste\", \"penetrating\", \"pertronix\", \"pertronix\", \"pertronix\", \"pinched\", \"ping\", \"pinging\", \"pinging\", \"pinking\", \"piston\", \"piston\", \"piston\", \"piston\", \"piston\", \"piston\", \"pitted\", \"plate\", \"plate\", \"plate\", \"plate\", \"plate\", \"plate\", \"plate\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"pn\", \"pn\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pop\", \"pop\", \"pop\", \"pop\", \"porsche\", \"porsche\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"pot\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"premarked\", \"premium\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"prevents\", \"primary\", \"primary\", \"primary\", \"priming\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"programmable\", \"programmed\", \"programming\", \"propane\", \"propshaft\", \"protectedoremail\", \"psi\", \"psi\", \"psi\", \"pump\", \"pump\", \"pump\", \"pump\", \"pump\", \"pump\", \"quoted\", \"quoted\", \"quoted\", \"r12\", \"r134\", \"r134a\", \"r134a\", \"rail\", \"rail\", \"rail\", \"rail\", \"rail\", \"rail\", \"rail\", \"rake\", \"rally\", \"rally\", \"rally\", \"rally\", \"ramp\", \"rapidly\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"rattling\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"rear\", \"rear\", \"rear\", \"rear\", \"rear\", \"rear\", \"rear\", \"rear\", \"rear\", \"recharge\", \"redone\", \"redone\", \"refrigerant\", \"register\", \"register\", \"regulator\", \"regulator\", \"regulator\", \"relay\", \"relay\", \"relay\", \"relay\", \"relay\", \"relay\", \"release\", \"remaining\", \"remaining\", \"remaining\", \"removable\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"remove\", \"repeater\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replica\", \"reservior\", \"resistance\", \"resistance\", \"resistance\", \"resistance\", \"resistor\", \"resistor\", \"resistor\", \"resistor\", \"resonator\", \"restos\", \"retard\", \"retard\", \"rhd\", \"rhd\", \"rhd\", \"rhd\", \"rhd\", \"rhd\", \"ride\", \"ride\", \"ride\", \"ride\", \"ride\", \"rig\", \"rig\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"riley\", \"rim\", \"rim\", \"rim\", \"rim\", \"ripped\", \"ripped\", \"roundel\", \"roundel\", \"roundel\", \"roundel\", \"row\", \"row\", \"row\", \"row\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"running\", \"running\", \"running\", \"running\", \"running\", \"running\", \"running\", \"running\", \"running\", \"runs\", \"runs\", \"s38\", \"s38\", \"s38\", \"s38\", \"s38\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sanding\", \"satisfaction\", \"scope\", \"scope\", \"scope\", \"scope\", \"screwy\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"secondary\", \"secondary\", \"sender\", \"sender\", \"sender\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shift\", \"shift\", \"shift\", \"shifter\", \"shim\", \"shim\", \"shit\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"significantly\", \"significantly\", \"silbers\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"ski\", \"soaking\", \"soda\", \"soda\", \"sold\", \"sold\", \"sold\", \"sold\", \"sold\", \"sold\", \"sold\", \"solenoid\", \"solenoid\", \"sorn\", \"specialize\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"splice\", \"spoiler\", \"spoiler\", \"spoiler\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"st\", \"st\", \"st\", \"st\", \"staggered\", \"staggered\", \"staggered\", \"stahl\", \"stahl\", \"stamped\", \"stamped\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"steady\", \"steering\", \"steering\", \"steering\", \"steering\", \"steering\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stroke\", \"stroke\", \"stronger\", \"strut\", \"strut\", \"strut\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuffi\", \"stumble\", \"stumble\", \"stumble\", \"stumble\", \"stumble\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"subframe\", \"subframe\", \"subframe\", \"subwoofer\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"survey\", \"suspension\", \"suspension\", \"suspension\", \"suspension\", \"suspension\", \"suspension\", \"sway\", \"sway\", \"sway\", \"sway\", \"switch\", \"switch\", \"switch\", \"switch\", \"switch\", \"switch\", \"switch\", \"taller\", \"tan\", \"tan\", \"tan\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"taxed\", \"taxed\", \"tdc\", \"tdc\", \"technology\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"tep\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thermostat\", \"theyve\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"throttle\", \"throttle\", \"throttle\", \"throttle\", \"throttle\", \"tighten\", \"tighten\", \"tilt\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timing\", \"timing\", \"timing\", \"timing\", \"timing\", \"timing\", \"timing\", \"timing\", \"tire\", \"tire\", \"tire\", \"tire\", \"tires\", \"toluene\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tooth\", \"tooth\", \"tooth\", \"touching\", \"touching\", \"tow\", \"tow\", \"tow\", \"track\", \"track\", \"track\", \"track\", \"track\", \"track\", \"traction\", \"tranny\", \"tranny\", \"tranny\", \"transformation\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"triangular\", \"triangular\", \"trigger\", \"trigger\", \"trigger\", \"trigger\", \"troubleshooting\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tuning\", \"tuning\", \"tuning\", \"tuning\", \"turbo\", \"turbo\", \"turbos\", \"turbos\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"twist\", \"underhood\", \"underhood\", \"underhood\", \"undertaking\", \"undertaking\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"valve\", \"valve\", \"valve\", \"valve\", \"valve\", \"valve\", \"valve\", \"valve\", \"valve\", \"veloce\", \"veloce\", \"vent\", \"vent\", \"vent\", \"vent\", \"vent\", \"vented\", \"venturi\", \"verona\", \"verona\", \"verona\", \"vin\", \"vin\", \"visually\", \"voltage\", \"voltage\", \"voltage\", \"vr\", \"vr\", \"vw\", \"vw\", \"vw\", \"vw\", \"w7dc\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wbo2\", \"wd40\", \"weber\", \"weber\", \"weber\", \"weber\", \"weber\", \"weber\", \"wegweiser\", \"wet\", \"wet\", \"wharf\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"widely\", \"widely\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"windshield\", \"windshield\", \"windshield\", \"windshield\", \"windshield\", \"windshield\", \"wing\", \"wing\", \"wing\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wired\", \"wired\", \"wolf\", \"wolf\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worthwhile\", \"wr9ls\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"yoko\", \"york\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 9, 4, 6, 7, 3, 2, 10, 8, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1068831350300487398882439700399\", ldavis_el1068831350300487398882439700399_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1068831350300487398882439700399\", ldavis_el1068831350300487398882439700399_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1068831350300487398882439700399\", ldavis_el1068831350300487398882439700399_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Visualization\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/cleaned_corpus.csv')\n",
        "df['THREAD_ALL_POSTS'].fillna('', inplace=True)\n",
        "\n",
        "# Create a document-term matrix\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['THREAD_ALL_POSTS'])  # Document-term matrix\n",
        "\n",
        "# Fit LDA model\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
        "lda.fit(dtm)\n",
        "\n",
        "# Prepare the visualization data for pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "prepared_data = pyLDAvis.prepare(\n",
        "    topic_term_dists=lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis],  # Normalized topic-term distributions\n",
        "    doc_topic_dists=lda.transform(dtm),  # Document-topic distributions\n",
        "    doc_lengths=list(dtm.sum(axis=1).A1),  # Length of each document\n",
        "    vocab=vectorizer.get_feature_names_out(),  # List of all terms in the vocabulary\n",
        "    term_frequency=np.asarray(dtm.sum(axis=0)).ravel().tolist(),  # Frequency of each term in the corpus\n",
        "    mds='tsne'  # Using t-SNE for multi-dimensional scaling\n",
        ")\n",
        "\n",
        "# Display the visualization\n",
        "pyLDAvis.display(prepared_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fnf6I4FXNYi"
      },
      "source": [
        "## Coverting Corpus into Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2n-Pg-XSTa-",
        "outputId": "509778c3-562b-437c-8333-da82e0cbae87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   THREAD_ID          400 non-null    int64 \n",
            " 1   THREAD_TITLE       398 non-null    object\n",
            " 2   THREAD_FIRST_POST  399 non-null    object\n",
            " 3   THREAD_ALL_POSTS   399 non-null    object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 12.6+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/cleaned_corpus.csv')\n",
        "\n",
        "df.info()\n",
        "\n",
        "# Fill missing values with an empty string to ensure concatenation works without errors\n",
        "df['THREAD_FIRST_POST'].fillna('', inplace=True)\n",
        "df['THREAD_ALL_POSTS'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "# Change THREAD_FIRST_POST to 'QUESTION'\n",
        "df.rename(columns={'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "\n",
        "# Change THREAD_ALL_POSTS to 'ANSWER'\n",
        "df.rename(columns={'THREAD_ALL_POSTS': 'ANSWER'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rtgtaGw-q-C"
      },
      "source": [
        "### 2.4.2 Tokenization Strategies\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "### Word-level Tokenization\n",
        "- **Tools:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers\n",
        "    - **Pros:**\n",
        "        - Preserves word integrity and semantic meaning, crucial for comprehension tasks.\n",
        "        - Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "    - **Cons:**\n",
        "        - Can result in a large vocabulary size, increasing memory and processing requirements.\n",
        "        - May overlook nuances in character-level variations.\n",
        "\n",
        "### Character-level Tokenization\n",
        "- **Tools:** Supported by deep learning frameworks like TensorFlow and Keras\n",
        "    - **Pros:**\n",
        "        - Captures morphological nuances at the character level, aiding languages with rich morphology.\n",
        "        - Simplifies the vocabulary to a set of unique characters, reducing model complexity.\n",
        "    - **Cons:**\n",
        "        - Leads to longer input sequences, which can increase computational costs.\n",
        "        - Loses direct access to semantic information encoded in words or phrases.\n",
        "\n",
        "### Subword Tokenization (BPE and Hugging Face Tokenizers)\n",
        "- **Tools** A blend of word-level and character-level tokenization, aiming to balance vocabulary size and semantic richness.\n",
        "    - **Pros:**\n",
        "        - Offers a middle ground, effectively managing vocabulary size while preserving semantic information.\n",
        "        - Facilitates handling of rare or unknown words by breaking them down into recognizable subwords.\n",
        "    - **Cons:**\n",
        "        - Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "        - Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "\n",
        "\n",
        "### Model-Specific Tokenization\n",
        "- **Tools:** Hugging Face's transformers library provides access to pre-built tokenizers corresponding to each pre-trained model, ensuring that tokenization is consistent with the model's original training data.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **For BERT:** AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "- **For GPT-2:** AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "- **For T5:** AutoTokenizer.from_pretrained('t5-small')\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Hugging Face for tokenization, sequencing and padding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDHfJH1avzcn"
      },
      "source": [
        "### 2.4.3 Embedding Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfvFT8OJbt5t"
      },
      "source": [
        "# BERT (Bidirectional Encoder Representations from Transformers)\n",
        "## Characteristics:\n",
        "- **Bidirectional Context**: BERT is designed to pre-train deep bidirectional representations by conditioning on both left and right context in all layers. This results in a rich understanding of language context and nuances.\n",
        "- **Architecture**: Primarily used for classification tasks, answering questions, and filling masked words.\n",
        "\n",
        "## Pros:\n",
        "- **Understanding Context**: Excellent at understanding the context of a given word in a sentence or a paragraph, making it well-suited for tasks that require a deep understanding of sentence relationships.\n",
        "- **Fine-Tuning Flexibility**: BERT can be fine-tuned with just one additional output layer, which makes it versatile for a wide range of tasks beyond just understanding language, including classification, entity recognition, and question answering.\n",
        "\n",
        "## Cons:\n",
        "- **Output Limitations**: BERT is not inherently generative; it's more about understanding context and relationships between sentences or parts of text. It doesn’t generate coherent text in the same way GPT models do.\n",
        "\n",
        "# GPT-2 (Generative Pre-trained Transformer 2)\n",
        "## Characteristics:\n",
        "- **Unidirectional Nature**: GPT-2 operates in a left-to-right manner, where every token can only attend to previous tokens in the self-attention layers of the transformer.\n",
        "- **Generative Capabilities**: Designed to generate text, GPT-2 excels in tasks that require new content creation based on the input given.\n",
        "\n",
        "## Pros:\n",
        "- **Text Generation**: GPT-2 is highly effective at generating coherent and contextually relevant text passages, making it ideal for applications that need creative text outputs.\n",
        "- **Adaptability**: Can be adapted to a wide variety of tasks without needing task-specific data handling.\n",
        "\n",
        "## Cons:\n",
        "- **Context Limitation**: As it is unidirectional, it may sometimes miss the context that could have been captured from the right side (future context) in a given text scenario.\n",
        "\n",
        "# Application to RAG Systems\n",
        "- **BERT’s Strengths**: If your RAG system relies heavily on understanding the context and nuances of user queries or needs to provide answers where understanding the question's context is crucial, BERT might be more appropriate.\n",
        "- **GPT-2’s Strengths**: If your RAG system is focused on generating explanatory or expansive text based on the input queries, or if you need to generate text that flows naturally and is engaging, GPT-2 would be better.\n",
        "\n",
        "## Suggested Approach:\n",
        "- **Hybrid Model**: Considering that RAG systems often benefit from both the understanding of context and the ability to generate text, a hybrid approach might also be viable. For instance, use BERT for understanding and encoding the query and context retrieval parts of your RAG, and use GPT-2 or another generative model for the response generation part.\n",
        "\n",
        "## Implementation Consideration:\n",
        "- Evaluate both models in a small-scale pilot study to see which model aligns best with your specific requirements.\n",
        "- Consider computational resources and training time, as both models are resource-intensive.\n",
        "\n",
        "# Simplified Approach for a Retrieval-Augmented Generation (RAG) System\n",
        "## Model Selection:\n",
        "- **Choose DistilBERT**: Instead of full BERT or GPT-2, consider using DistilBERT. It’s a lighter version of BERT that retains most of its capabilities but is faster and requires less computational resources. DistilBERT provides a good balance between performance and efficiency, making it ideal for initial prototyping.\n",
        "\n",
        "## Data Preparation:\n",
        "- **Minimize Preprocessing Steps**: Focus on essential preprocessing steps such as tokenization and basic cleaning (e.g., removing special characters and converting to lowercase). Skip more complex steps like semantic clustering or detailed entity recognition initially.\n",
        "- **Use Existing Splits**: If possible, use predefined data splits (train, test, validation) to speed up the process.\n",
        "\n",
        "## Tokenization and Embedding:\n",
        "- **Use Hugging Face’s Tokenizers**: Leverage the AutoTokenizer from Hugging Face for DistilBERT. This tool will handle both tokenization and embedding preparation, simplifying the data preparation pipeline.\n",
        "\n",
        "## Model Training:\n",
        "- **Simplified Training Loop**: For the initial runs, set up a simple training loop without extensive hyperparameter tuning. Focus on getting the model to run with basic parameters.\n",
        "- **Monitor Simple Metrics**: Track basic performance metrics like accuracy or loss during the initial runs. This provides quick feedback on how the model is performing.\n",
        "\n",
        "## Evaluation:\n",
        "- **Basic Evaluation**: Use straightforward metrics such as accuracy or F1 score for initial evaluation. This helps in understanding the baseline performance without getting into more complex evaluation strategies.\n",
        "\n",
        "## Iterative Refinement:\n",
        "- **Feedback Loop**: Once the basic system is up and running, gradually introduce more complexity by adding one component at a time (e.g., more sophisticated preprocessing, advanced metrics, or fine-tuning strategies).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smTeHp0ndLWz",
        "outputId": "4d0dd6e8-b0a4-4dbe-c0c9-5674ad3c0299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the most similar answer: 0\n",
            "Most similar answer text: New owner Goin NC drive home NJ cant wait post experience read laugh cry critiqueI look forward coming lurker statusRegards Congrats NJ New OwnerCongrats pecsokfrom one new owner another javascriptemoticonBack left coast last week wrap thing Coupeking due arrive 2nd week MayWill look forward meeting ristate coupstersChuckL Hi ChuckHow thing go imagine look great Agave CoupeHi LenCar way look great cant wait start active ownership CoupeKing experience immersive Take look latest Bimmer Magazine get senseWill look hook soon touch eastChuck Sounds like plan Chuck know youve waiting day quite must feel great close Looking forward meeting seeing Hi LenIm Belleville work Fair Lawn rt 4208 ChuckCongrats completion must excited arrivalYesterday beautiful dayLooking foward tristate gatheringJohn C Thanks Chuck kudosId love little mini coupefest tristate area Theres great place near I9580 interchange called Overpeck Park large lot parking Len would know speak Id late May get together Maybe Memorial Day weekend\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, DistilBertModel\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preparing and saving tokenized data\n",
        "def prepare_and_save_data(df, question_col, answer_col, model='bert-base-uncased', max_length=512, save_path='tokenized_data.pkl'):\n",
        "    # Load the tokenizer for the specified model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    # Limit the DataFrame to the first 10 rows for troubleshooting\n",
        "    df = df.head(100)\n",
        "\n",
        "    # Tokenize the question and answer texts\n",
        "    tokenized_questions = tokenizer(list(df[question_col].fillna('')), max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    tokenized_answers = tokenizer(list(df[answer_col].fillna('')), max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "    # Save tokenized data to disk\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump((tokenized_questions, tokenized_answers), f)\n",
        "\n",
        "    print(f\"Tokenized data saved to {save_path}\")\n",
        "\n",
        "# Assuming df is your DataFrame loaded with the data\n",
        "# Example usage, make sure 'df' is defined and loaded with your full dataset\n",
        "# prepare_and_save_data(df, 'QUESTION', 'ANSWER')\n",
        "\n",
        "# Step 2: Loading data and generating embeddings\n",
        "def load_data_and_generate_embeddings(model_path='distilbert-base-uncased', data_path='tokenized_data.pkl'):\n",
        "    # Load the model\n",
        "    model = DistilBertModel.from_pretrained(model_path)\n",
        "\n",
        "    # Load the tokenized data\n",
        "    with open(data_path, 'rb') as f:\n",
        "        tokenized_questions, tokenized_answers = pickle.load(f)\n",
        "\n",
        "    # Ensure only input_ids and attention_mask are passed to the model\n",
        "    with torch.no_grad():\n",
        "        question_embeddings = model(input_ids=tokenized_questions['input_ids'],\n",
        "                                    attention_mask=tokenized_questions['attention_mask']).last_hidden_state.mean(dim=1)\n",
        "        answer_embeddings = model(input_ids=tokenized_answers['input_ids'],\n",
        "                                  attention_mask=tokenized_answers['attention_mask']).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    return question_embeddings, answer_embeddings\n",
        "\n",
        "# Load data and generate embeddings\n",
        "question_embeddings, answer_embeddings = load_data_and_generate_embeddings()\n",
        "\n",
        "# Step 3: Finding the most similar answer\n",
        "def find_most_similar(query_embedding, answer_embeddings):\n",
        "    # Convert PyTorch tensors to numpy arrays for cosine similarity computation\n",
        "    query_embedding_np = query_embedding.detach().cpu().numpy().reshape(1, -1)\n",
        "    answer_embeddings_np = answer_embeddings.detach().cpu().numpy()\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = cosine_similarity(query_embedding_np, answer_embeddings_np)\n",
        "\n",
        "    # Find the index of the most similar answer\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "    return most_similar_index\n",
        "\n",
        "# Example usage: Assuming you're using the first question's embedding as a query\n",
        "index_of_most_similar = find_most_similar(question_embeddings[0], answer_embeddings)\n",
        "print(\"Index of the most similar answer:\", index_of_most_similar)\n",
        "print(\"Most similar answer text:\", df.iloc[index_of_most_similar]['ANSWER'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHbxeF7Z9Pa8"
      },
      "source": [
        "Evaluate and Optimize the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH5r_c3jdLZF",
        "outputId": "d30f1d94-9e38-48d5-edcb-0b8df357234f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5000\n",
            "Precision: 0.3333\n",
            "Recall: 0.5000\n",
            "F1-Score: 0.3833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Step 4: Generate Predictions\n",
        "def generate_predictions(question_embeddings, answer_embeddings):\n",
        "    predictions = []\n",
        "    for i in range(len(question_embeddings)):\n",
        "        index_of_most_similar = find_most_similar(question_embeddings[i], answer_embeddings)\n",
        "        predictions.append(index_of_most_similar)\n",
        "    return predictions\n",
        "\n",
        "# Generate predictions\n",
        "predictions = generate_predictions(question_embeddings, answer_embeddings)\n",
        "\n",
        "# Define ground truth\n",
        "ground_truth = list(range(len(question_embeddings)))\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "def evaluate_model(predictions, ground_truth):\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, precision, recall, f1 = evaluate_model(predictions, ground_truth)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "i0WNzGEEdLbe",
        "outputId": "cac63a7c-586b-4811-8ec4-d56e4d7ed17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Execution stopped.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Execution stopped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Stop the execution of code\n",
        "sys.exit(\"Execution stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi-AfZ-GKcO0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWZX-eZellc5"
      },
      "source": [
        "# 4. Training and Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh6QKCnFK2ul"
      },
      "source": [
        "\n",
        "1. **PyTorch and Hugging Face Transformers**\n",
        "   - **Pros:** PyTorch offers dynamic computation graphs that are intuitive for RAG model development. Hugging Face's Transformers library provides easy access to pre-trained models and tokenizers, facilitating both training and evaluation with extensive support for RAG architectures.\n",
        "   - **Cons:** While highly flexible, this combination might require a steep learning curve for those not familiar with PyTorch or the Transformers library.\n",
        "\n",
        "2. **TensorFlow and T5**\n",
        "   - **Pros:** TensorFlow provides robust tools for model development and deployment, with T5 being a versatile model for text-to-text tasks, adaptable for RAG purposes. TensorFlow's extensive ecosystem includes TensorBoard for monitoring training processes.\n",
        "   - **Cons:** TensorFlow's static computation graph can be less intuitive than PyTorch's dynamic graphs. T5's text-to-text format might require additional preprocessing steps.\n",
        "\n",
        "3. **JAX and Flax/Haiku**\n",
        "   - **Pros:** JAX offers accelerated NumPy operations and automatic differentiation, making it efficient for large-scale model training. Flax and Haiku provide neural network libraries for JAX, supporting complex RAG model architectures.\n",
        "   - **Cons:** JAX's ecosystem is less mature, with fewer pre-trained models and community resources available compared to PyTorch and TensorFlow. This can make development and troubleshooting more challenging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ1Ha13mpeGE"
      },
      "source": [
        "## 4.1 Hyperparameter Tuning\n",
        "\n",
        "4.1 Hyperparameter Tuning\n",
        "Tuning Strategy: Outline your strategy for hyperparameter tuning, including the tools or techniques (like grid search or random search) you plan to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNXeUAspgU4"
      },
      "source": [
        "## 4.2 Model Evaluation\n",
        "\n",
        "Evaluation Metrics: Detail the metrics you will use to evaluate your model, such as F1 score, precision, recall, and explain why each is important for your project’s success.\n",
        "\n",
        "\n",
        "The project's success will be assessed based on the accuracy and speed of responses generated by the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRp4JbC7KyX5"
      },
      "source": [
        "\n",
        "1. **PyTorch and Hugging Face Transformers**\n",
        "   - **Pros:** PyTorch offers dynamic computation graphs that are intuitive for RAG model development. Hugging Face's Transformers library provides easy access to pre-trained models and tokenizers, facilitating both training and evaluation with extensive support for RAG architectures.\n",
        "   - **Cons:** While highly flexible, this combination might require a steep learning curve for those not familiar with PyTorch or the Transformers library.\n",
        "\n",
        "2. **TensorFlow and T5**\n",
        "   - **Pros:** TensorFlow provides robust tools for model development and deployment, with T5 being a versatile model for text-to-text tasks, adaptable for RAG purposes. TensorFlow's extensive ecosystem includes TensorBoard for monitoring training processes.\n",
        "   - **Cons:** TensorFlow's static computation graph can be less intuitive than PyTorch's dynamic graphs. T5's text-to-text format might require additional preprocessing steps.\n",
        "\n",
        "3. **JAX and Flax/Haiku**\n",
        "   - **Pros:** JAX offers accelerated NumPy operations and automatic differentiation, making it efficient for large-scale model training. Flax and Haiku provide neural network libraries for JAX, supporting complex RAG model architectures.\n",
        "   - **Cons:** JAX's ecosystem is less mature, with fewer pre-trained models and community resources available compared to PyTorch and TensorFlow. This can make development and troubleshooting more challenging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxRaOdwlrWY"
      },
      "source": [
        "# 5. Results and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGiqPFHIpsn6"
      },
      "source": [
        "## 5.1 Experimental Results\n",
        "5.1 Experimental Results\n",
        "Visualizations: Include charts or graphs that visualize the results, such as learning curves or performance benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56b36gepvMG"
      },
      "source": [
        "## 5.2 Performance Analysis\n",
        "\n",
        "5.2 Performance Analysis\n",
        "Comparative Analysis: If applicable, compare the performance of your model against baseline models or previous benchmarks in similar tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 6. Deployment and Integration\n",
        "6.1 Deployment Tools\n",
        "Deployment Plan: Provide a step-by-step plan for deploying the model, including any cloud resources or services used.\n",
        "6.2 Integration Strategies\n",
        "API Specifications: If your model will be accessed via an API, provide the API specifications including endpoint descriptions, request format, and response format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "## 6.1 Deployment Tools\n",
        "# Deployment and Serving Infrastructure Selection\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.\n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n",
        "## Decision\n",
        "For deploying a RAG model, especially within an academic or portfolio context where ease of use, accessibility, and cost-effectiveness are key considerations, Hugging Face Spaces is highly recommended. It allows you to quickly deploy your models with minimal setup and offers a user-friendly platform for showcasing your work to a wide audience. For projects that might evolve into more scalable or commercial applications, starting with Docker for containerization and then moving to a Kubernetes-based deployment as needs grow could be a strategic approach. This path provides a balance between initial simplicity and long-term scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRe4qbx5p1nj"
      },
      "source": [
        "## 6.2 Integration Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KGSPx5olwxJ"
      },
      "source": [
        "# 7. Reflection and Learning\n",
        "\n",
        "7.1 Challenges Faced\n",
        "Problem-Solving Approaches: Discuss specific problems you anticipate and outline the approaches you plan to take to solve them.\n",
        "7.2 Lessons Learned\n",
        "Documentation of Insights: Plan to document insights and lessons learned throughout the project to guide future projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWoXGpWRp53s"
      },
      "source": [
        "## 7.1 Challenges Faced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmm_IZYBp7Wr"
      },
      "source": [
        "## 7.2 Lessons Learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cXEIxsnly0H"
      },
      "source": [
        "#8. Future Work and Improvements\n",
        "\n",
        "8.1 Potential Enhancements\n",
        "Technology Upgrades: Identify areas for technological upgrades or enhancements, like implementing more advanced NLP techniques or exploring newer models.\n",
        "8.2 Areas for Further Research\n",
        "Extended Applications: Suggest areas for extending the application of your project to other types of vehicles or different domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9bwbXcuqADQ"
      },
      "source": [
        "## 8.1 Potential Enhancements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lOHxB-hqB1n"
      },
      "source": [
        "## 8.2 Areas for Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 9. Appendix/Parking lot of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t9bG7hojt7Y"
      },
      "outputs": [],
      "source": [
        "# Keep. This code corresponds to the following step in the workflow:\n",
        "\n",
        "# 17. Build FAISS Index\n",
        "\n",
        "# 'embeddings' column with embeddings was stored as lists\n",
        "# Need to convert the embeddings from the 'embeddings' column into an array\n",
        "embeddings_np = np.array(df['embeddings'].tolist())\n",
        "\n",
        "# Dimension of embeddings\n",
        "d = embeddings_np.shape[1]\n",
        "\n",
        "# Create a FAISS index - using IndexFlatL2 for L2 distance\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jQzspuiSB5g"
      },
      "outputs": [],
      "source": [
        "#Keep. This code corresponds to the following steps in the workflow:\n",
        "\n",
        "#18. Query Processing and Search\n",
        "#19. Retrieve and Rank\n",
        "#20. Answer Generation (the commented-out prompt generation part)\n",
        "\n",
        "\n",
        "\n",
        "# Query Expansion\n",
        "# Matching the query text to the supplemental corpus\n",
        "# Need to visual inspect the accuracy here\n",
        "\n",
        "def generate_query_embedding(query_text):\n",
        "    inputs = tokenizer(query_text, return_tensors=\"pt\", padding=True,\n",
        "                       truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    query_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    return query_embedding.reshape(1, -1)  # Reshape for FAISS\n",
        "\n",
        "# Example query text\n",
        "query_text = \"e9 prices\"\n",
        "\n",
        "# Generate the query embedding\n",
        "query_embedding = generate_query_embedding(query_text)\n",
        "\n",
        "# Use the FAISS index to retrieve the indices of the 5 most similar documents\n",
        "D, I = index.search(query_embedding, 5)  # Assuming 'index' is your FAISS index\n",
        "\n",
        "# Retrieve the details of the most similar documents from `e9_forum_corpus`\n",
        "# For simplicity, assuming the DataFrame index aligns with the FAISS index\n",
        "retrieved_documents = e9_forum_corpus.iloc[I[0]]\n",
        "\n",
        "# Display the retrieved documents or their details as needed\n",
        "print(retrieved_documents)\n",
        "\n",
        "# Look at the generated prompt for insite into possible improvements\n",
        "#concatenated_prompt = \"[QUESTION] \" + query_text + \" [CONTEXT] \"\n",
        "#concatenated_prompt += \" [CONTEXT] \".join(retrieved_documents['THREAD_ALL_POSTS'].tolist())\n",
        "\n",
        "# Print or inspect the concatenated prompt\n",
        "#print(\"Generated Prompt:\", concatenated_prompt)\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/retrieved_documents.csv'\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "retrieved_documents.to_csv(file_path, mode='a', header=['thread_id'], index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BG2TAxMSB7r"
      },
      "outputs": [],
      "source": [
        "# Keep. This code corresponds to the following step in the workflow:\n",
        "\n",
        "# 19. Retrieve and Rank (combining retrieved documents to form a comprehensive input for the generative model)\n",
        "\n",
        "# Create a single string that includes the query and the texts of the retrieved documents\n",
        "\n",
        "combined_input = query_text + \" \" + \" \".join(retrieved_documents['concatenated'].tolist())\n",
        "\n",
        "# Now, 'combined_input' is ready to be used as input for GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVF9AYOXSCAO"
      },
      "outputs": [],
      "source": [
        "#Keep. This code corresponds to the following steps in the workflow:\n",
        "\n",
        "# 20. Answer Generation\n",
        "\n",
        "\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Generate a response using the model\n",
        "output = model.generate(**input_tokens, max_length=1000, num_return_sequences=1)\n",
        "\n",
        "# Decode the output tokens to text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v44M2gQHRibp"
      },
      "outputs": [],
      "source": [
        "#Keep. This code corresponds to the following steps in the workflow:\n",
        "\n",
        "#19. Retrieve and Rank (combining retrieved documents to form a comprehensive input for the generative model)\n",
        "#20. Answer Generation (generating a response using the model)\n",
        "\n",
        "# New prompt\n",
        "new_prompt = \"Your new prompt here\"\n",
        "\n",
        "# Concatenate the new prompt with relevant information or documents\n",
        "# For demonstration, let's use the same process (this would typically involve retrieving relevant documents for the new prompt)\n",
        "combined_input_new = new_prompt + \" \" + \" \".join(retrieved_documents['concatenated'].tolist())\n",
        "\n",
        "# Tokenize the new combined input\n",
        "input_tokens_new = tokenizer(combined_input_new, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Generate a response using the model\n",
        "output_new = model.generate(**input_tokens_new, max_length=1000, num_return_sequences=1)\n",
        "\n",
        "# Decode the output tokens to text for the new prompt\n",
        "generated_text_new = tokenizer.decode(output_new[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response to the new prompt:\", generated_text_new)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FCKCpDO6aoQEsn3hrEsi5TBmbfFx4KUQ",
      "authorship_tag": "ABX9TyNxwtHy5YRAazpbyrsr/QfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}