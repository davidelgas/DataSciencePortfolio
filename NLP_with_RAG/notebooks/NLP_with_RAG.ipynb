{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/NLP_with_RAG/notebooks/NLP_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2vTigd3lYAM"
      },
      "source": [
        "# 1. Project Scope and Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEVpPV9NmPK2"
      },
      "source": [
        "## 1.1 Project Overview\n",
        "The primary objective of this project is the development of a Natural Language Processing (NLP) model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparision and selection of architectures, libraries and methods. This is a complement to my pursuit of a Masters Degree in Data Science.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFqXbkd_mWjv"
      },
      "source": [
        "## 1.2 Objectives\n",
        "The project aims to build a generative language model capable of processing written, unstructured questions in English from users and providing targeted written answers. The  use case for this project is a virtual mechanic to help owners maintain a specific make and model of classic car: the BMW 3.0 CS. The results of this effort will be provided to several intenational BMW clubs to solicity feedback on usability and potential extension to other makes and models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlctA2pymZYv"
      },
      "source": [
        "## 1.3 Limitations and Challenges\n",
        "Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs. Data storage will be done in a Snowflake database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "# 2. Data Collection and Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jOHmOOCkEvX",
        "outputId": "8b2a9d61-dfda-4ab5-ba23-8fb9b757a184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svb8UxyCkC7O",
        "outputId": "ea62e426-b340-439b-b3c4-86fa1cbee259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.5)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.10.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.13.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<4.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.11.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (0.12.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "\n",
        "# 2.1 Data Collection\n",
        "import os\n",
        "\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "\n",
        "!pip3 install requests\n",
        "import requests\n",
        "\n",
        "!pip3 install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "\n",
        "# 2.2 Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip3 install numpy\n",
        "import numpy as np\n",
        "\n",
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otWnw_6_mgNg"
      },
      "source": [
        "## 2.1 Data Collection\n",
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available.\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hCSAv30XkVQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e8eb6f-d904-4345-fc02-9de20ad586ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 17000\n",
            "Processing additional 500 threads\n",
            "Ending with thread_id 17500\n"
          ]
        }
      ],
      "source": [
        "# Generate the list of thread_ids to scrape and parse\n",
        "# There are currently approximately 15k threads\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_thread_ids.csv'\n",
        "\n",
        "# Set the number of incremental thread_ids to process\n",
        "threads = 500\n",
        "\n",
        "# Check if the file exists and has content. If it does, update last_thread_id\n",
        "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "    e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "    last_thread_id = e9_forum_thread_ids['thread_id'].iloc[-1]\n",
        "    last_thread_id = int(last_thread_id)  # Convert to integer\n",
        "\n",
        "else:\n",
        "    last_thread_id = 0\n",
        "\n",
        "# Function to create URLs from the thread_ids\n",
        "def create_urls(threads, last_thread_id):\n",
        "    urls = []\n",
        "    for thread_id in range(last_thread_id + 1, last_thread_id + threads + 1):\n",
        "        urls.append({'thread_id': thread_id})\n",
        "    return urls\n",
        "\n",
        "urls = create_urls(threads, last_thread_id)\n",
        "\n",
        "last_thread_id_processed = urls[-1]['thread_id']\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "e9_forum_thread_ids = pd.DataFrame(urls)\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "e9_forum_thread_ids.to_csv(file_path, mode='a', header=['thread_id'], index=False)\n",
        "\n",
        "print(\"Starting with thread_id \" + str(last_thread_id))\n",
        "print(\"Processing additional \" + str(threads) + \" threads\")\n",
        "print(\"Ending with thread_id \" + str(last_thread_id_processed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3_uNK-00kqpE"
      },
      "outputs": [],
      "source": [
        "# Generate the URL and title for each thread\n",
        "\n",
        "pages = 1\n",
        "\n",
        "def fetch_thread_data(df, pages=1):\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fetch thread URLs and title\n",
        "e9_forum_threads = fetch_thread_data(e9_forum_thread_ids)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_threads.csv'\n",
        "\n",
        "header = ['thread_id', 'thread_title', 'thread_url']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WdKddVmMkyvC"
      },
      "outputs": [],
      "source": [
        "# Find the first post in the thread creation\n",
        "# I may use this as part of the question portion of the RAG\n",
        "\n",
        "def fetch_first_post_content(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        if first_post:\n",
        "            post_content = first_post.get_text(strip=True)\n",
        "        else:\n",
        "            post_content = \"No content found\"  # Handle case where no post content is found\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_first_post': post_content})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Fetch first post content\n",
        "data = fetch_first_post_content(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_threads_decorated = pd.DataFrame(data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_threads_decorated.csv'\n",
        "\n",
        "header = not os.path.exists(file_path)\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads_decorated.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9gZIxW8gk2ur"
      },
      "outputs": [],
      "source": [
        "# Find all posts associated with each thread\n",
        "\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            # Extracting post timestamp instead of post ID\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "            post_data.append({\n",
        "                'thread_id': row['thread_id'],\n",
        "                'post_timestamp': post_timestamp,\n",
        "                'post_raw': content\n",
        "            })\n",
        "\n",
        "    return post_data\n",
        "\n",
        "# Fetch all thread post content\n",
        "post_data = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_posts = pd.DataFrame(post_data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = ('/content/drive/MyDrive/Data_sets/e9/e9_forum_posts.csv')\n",
        "\n",
        "header = ['thread_id', 'post_timestamp','post_raw']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_posts.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the corpus by aggregating all posts into one column\n",
        "# and merging with the threads df\n",
        "\n",
        "# Group by THREAD_ID and concatenate the POST_RAW values\n",
        "aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Rename the column to indicate that it contains concatenated post content\n",
        "aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "# Cast 'thread_id' column to int64 in both DataFrames\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype('int64')\n",
        "aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "# Merge the two DataFrames\n",
        "e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_corpus.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_corpus.csv', index=False)"
      ],
      "metadata": {
        "id": "gZ_Sg3dgj0V2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sAFI2Hl3opeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580fd849-2839-408d-ca75-b1441a7fedc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Create the db and schema in Snowfake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create a database for the corpus and load the tables\n",
        "try:\n",
        "    # Create a new database\n",
        "    cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "\n",
        "    # Use the new database\n",
        "    cur.execute(\"USE DATABASE e9_corpus\")\n",
        "\n",
        "    # Create a new schema\n",
        "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "\n",
        "    print(\"Database and schema created successfully.\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "cur.close()\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A9OH_E9702TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4852fdf7-49d9-4afb-ae62-ad91ab9bf34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inserted into e9_forum_corpus table.\n"
          ]
        }
      ],
      "source": [
        "# Save the data to Snowflake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Check if the table exists\n",
        "try:\n",
        "    cur.execute(\"SELECT 1 FROM e9_corpus.e9_corpus_schema.e9_forum_corpus LIMIT 1\")\n",
        "    table_exists = True\n",
        "except snowflake.connector.errors.ProgrammingError:\n",
        "    table_exists = False\n",
        "\n",
        "# If the table does not exist, create it\n",
        "if not table_exists:\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            thread_id NUMBER(38,0),\n",
        "            thread_title VARCHAR(16777216),\n",
        "            thread_first_post VARCHAR(16777216),\n",
        "            thread_all_posts VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# Insert data into e9_forum_corpus table\n",
        "for index, row in e9_forum_corpus.iterrows():\n",
        "\n",
        "    row = row.where(pd.notnull(row), None)\n",
        "\n",
        "    # Prepare the INSERT command with placeholders for the values\n",
        "    insert_command = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (thread_id, thread_title, thread_first_post, thread_all_posts)\n",
        "    VALUES\n",
        "    (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the row values as parameters to safely insert the data\n",
        "    cur.execute(insert_command, (row['thread_id'], row['thread_title'], row['thread_first_post'], row['thread_all_posts']))\n",
        "    conn.commit()\n",
        "\n",
        "print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mSOwR1x9041o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5019c1e-4409-4760-9e27-d1c34e40b559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16437 entries, 0 to 16436\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   THREAD_ID          16437 non-null  int16 \n",
            " 1   THREAD_TITLE       16437 non-null  object\n",
            " 2   THREAD_FIRST_POST  16437 non-null  object\n",
            " 3   THREAD_ALL_POSTS   16137 non-null  object\n",
            "dtypes: int16(1), object(3)\n",
            "memory usage: 417.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# Confirm dataset in Snowflake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Select source data\n",
        "query = \"\"\"\n",
        "SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "# Load data into a df.\n",
        "e9_forum_corpus = cur.fetch_pandas_all()\n",
        "e9_forum_corpus.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHJWehqmrr6a"
      },
      "source": [
        "### 2.2 Preprocessing\n",
        "\n",
        "Preprocessing Workflow\n",
        "\n",
        "\n",
        "**Convert Text to Lowercase:**\n",
        "\n",
        "This standardizes your text, helping to reduce the complexity of your vocabulary by ensuring that words are treated the same regardless of their place in a sentence. It's particularly useful for matching and retrieval processes, as \"Car\" and \"car\" will be considered the same.\n",
        "\n",
        "**Remove Stop Words:**\n",
        "\n",
        "Stop words (e.g., \"the\", \"is\", \"in\") are very common in the English language but usually don't add much informational value to the text for the purposes of retrieval and generation. Eliminating them can help focus on the more meaningful parts of the texts. However, be mindful that for some NLP tasks, the context provided by stop words could be significant. Given your project's focus on generating coherent answers, you might want to carefully evaluate the impact of removing stop words on the quality of generated text.\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        " This process reduces words to their base or dictionary form (lemma) while considering the context. Lemmatization helps in consolidating variations of a word into a single, base form, thereby simplifying the vocabulary your model needs to understand. It's more sophisticated than stemming and beneficial for maintaining the semantic meaning of the text.\n",
        "\n",
        "**Remove Unnecessary Characters**\n",
        "\n",
        "Cleaning out HTML tags, extra whitespace, non-printable characters, and other irrelevant elements is crucial for reducing noise in your data. This step ensures that the model focuses on the textual content that carries meaning and context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "e4-4iikhgeDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092b6489-979c-401b-da25-00dbb79d241b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16437 entries, 0 to 16436\n",
            "Data columns (total 1 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   concatenated  16437 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 128.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# Load the data from Snowflake for pre-processing\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Select source data\n",
        "query = \"\"\"\n",
        "SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "# Load data into a df.\n",
        "e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "# Format the corpus as required for an LLM RAG\n",
        "e9_forum_corpus['concatenated'] = e9_forum_corpus['THREAD_TITLE'] + \"\\n\" + e9_forum_corpus['THREAD_ALL_POSTS']\n",
        "\n",
        "#Drop the columns I will not need\n",
        "e9_forum_corpus.drop(['THREAD_ID', 'THREAD_TITLE','THREAD_FIRST_POST','THREAD_ALL_POSTS'], inplace=True, axis=1)\n",
        "\n",
        "# Populate empty rows\n",
        "e9_forum_corpus['concatenated'].fillna('No answer provided', inplace=True)\n",
        "\n",
        "#Confirm there are no null values\n",
        "null_counts = e9_forum_corpus.isnull().sum()\n",
        "e9_forum_corpus.info()\n",
        "\n",
        "# Creating a small sample of my corpus for a dry-run to troubleshoot\n",
        "sample_corpus = e9_forum_corpus.sample(n=500, random_state=1).copy()\n",
        "\n",
        "df = sample_corpus.copy()\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/sample_corpus.csv'\n",
        "\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "sample_corpus.to_csv(file_path, mode='a', header=['thread_id'], index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "_tCJMNVboZwq"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'column_name' is the name of the column to clean\n",
        "def clean_and_update_column(df, column_name='concatenated'):\n",
        "    # Set up stopwords and lemmatizer\n",
        "    additional_stopwords = {'car', 'csi', 'cs', 'csl', 'e9', 'coupe', 'http', 'https', 'www', 'ebay', 'bmw', 'html'}\n",
        "    all_stopwords = set(stopwords.words('english')).union(additional_stopwords)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[column_name]\n",
        "\n",
        "        # Detect and remove non-English text\n",
        "        try:\n",
        "            if detect(text) != 'en':\n",
        "                text = \"\"\n",
        "        except Exception as e:\n",
        "            pass  # If language detection fails, keep the text as-is\n",
        "\n",
        "        # Normalize to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs, emails, and special characters\n",
        "        text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "\n",
        "        # Tokenize and remove stopwords\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_tokens = [word for word in tokens if word not in all_stopwords]\n",
        "\n",
        "        # Lemmatize\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "        # Update the DataFrame with the cleaned text\n",
        "        df.at[index, column_name] = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "df = clean_and_update_column(df, 'concatenated')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rtgtaGw-q-C"
      },
      "source": [
        "### 2.2.1 Tokenization Strategies\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "### Word-level Tokenization\n",
        "- **Tools:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers\n",
        "    - **Pros:**\n",
        "        - Preserves word integrity and semantic meaning, crucial for comprehension tasks.\n",
        "        - Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "    - **Cons:**\n",
        "        - Can result in a large vocabulary size, increasing memory and processing requirements.\n",
        "        - May overlook nuances in character-level variations.\n",
        "\n",
        "### Character-level Tokenization\n",
        "- **Tools:** Supported by deep learning frameworks like TensorFlow and Keras\n",
        "    - **Pros:**\n",
        "        - Captures morphological nuances at the character level, aiding languages with rich morphology.\n",
        "        - Simplifies the vocabulary to a set of unique characters, reducing model complexity.\n",
        "    - **Cons:**\n",
        "        - Leads to longer input sequences, which can increase computational costs.\n",
        "        - Loses direct access to semantic information encoded in words or phrases.\n",
        "\n",
        "### Subword Tokenization (BPE and Hugging Face Tokenizers)\n",
        "- **Tools** A blend of word-level and character-level tokenization, aiming to balance vocabulary size and semantic richness.**\n",
        "    - **Pros:**\n",
        "        - Offers a middle ground, effectively managing vocabulary size while preserving semantic information.\n",
        "        - Facilitates handling of rare or unknown words by breaking them down into recognizable subwords.\n",
        "    - **Cons:**\n",
        "        - Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "        - Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "\n",
        "\n",
        "### Model-Specific Tokenization\n",
        "- **Tools:** Hugging Face's transformers library provides access to pre-built tokenizers corresponding to each pre-trained model, ensuring that tokenization is consistent with the model's original training data.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **For BERT:** AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "- **For GPT-2:** AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "- **For T5:** AutoTokenizer.from_pretrained('t5-small')\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Hugging Face for tokenization, sequencing and padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ngzfL6rxY8hi"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()  # Put the model in evaluation mode\n",
        "\n",
        "def get_embeddings_for_dataframe(df, text_column):\n",
        "    embeddings_list = []\n",
        "    for text in df[text_column]:\n",
        "        # Tokenize and encode the text\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Extract embeddings and convert to list\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :].squeeze().tolist()\n",
        "        embeddings_list.append(embeddings)\n",
        "    # Add embeddings as a new column to the df\n",
        "    df['embeddings'] = embeddings_list\n",
        "    return df\n",
        "\n",
        "# Process the 'concatenated' column to get embeddings for the sample\n",
        "df = get_embeddings_for_dataframe(df, 'concatenated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "6t9bG7hojt7Y"
      },
      "outputs": [],
      "source": [
        "# 'embeddings' column with embeddings was stored as lists\n",
        "# Need to convert the embeddings from the 'embeddings' column into an array\n",
        "embeddings_np = np.array(df['embeddings'].tolist())\n",
        "\n",
        "# Dimension of embeddings\n",
        "d = embeddings_np.shape[1]\n",
        "\n",
        "# Create a FAISS index - using IndexFlatL2 for L2 distance\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "r_a33aeNc2a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe9e304-7639-4ca6-dd5e-845e8b7e06b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          concatenated\n",
            "134  WTB rear bumper, 73 or older\\nGot tapped just ...\n",
            "33   450 sel 6.9\\nAvailable below on SF Craigslist....\n",
            "470  Brake Pressure Regulator - 34344750020\\nNLA.Ma...\n",
            "66   Windshield wiper motor\\nHi GroupThis is my fir...\n",
            "108  CS front bumper with spoiler...how to make it ...\n"
          ]
        }
      ],
      "source": [
        "# Query Expansion\n",
        "# Matching the query text to the supplemental corpus\n",
        "# Need to visual inspect the accuracy here\n",
        "\n",
        "\n",
        "def generate_query_embedding(query_text):\n",
        "    inputs = tokenizer(query_text, return_tensors=\"pt\", padding=True,\n",
        "                       truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    query_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    return query_embedding.reshape(1, -1)  # Reshape for FAISS\n",
        "\n",
        "# Example query text\n",
        "query_text = \"e9 prices\"\n",
        "\n",
        "# Generate the query embedding\n",
        "query_embedding = generate_query_embedding(query_text)\n",
        "\n",
        "# Use the FAISS index to retrieve the indices of the 5 most similar documents\n",
        "D, I = index.search(query_embedding, 5)  # Assuming 'index' is your FAISS index\n",
        "\n",
        "# Retrieve the details of the most similar documents from `e9_forum_corpus`\n",
        "# For simplicity, assuming the DataFrame index aligns with the FAISS index\n",
        "retrieved_documents = e9_forum_corpus.iloc[I[0]]\n",
        "\n",
        "# Display the retrieved documents or their details as needed\n",
        "print(retrieved_documents)\n",
        "\n",
        "# Look at the generated prompt for insite into possible improvements\n",
        "#concatenated_prompt = \"[QUESTION] \" + query_text + \" [CONTEXT] \"\n",
        "#concatenated_prompt += \" [CONTEXT] \".join(retrieved_documents['THREAD_ALL_POSTS'].tolist())\n",
        "\n",
        "# Print or inspect the concatenated prompt\n",
        "#print(\"Generated Prompt:\", concatenated_prompt)\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/retrieved_documents.csv'\n",
        "\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "retrieved_documents.to_csv(file_path, mode='a', header=['thread_id'], index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The problem here is that retrieved_documents is very dirty and undstructured. This will need to cleaned to improve the results.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"Windshield wiper motor\n",
        "Hi GroupThis is my first post. I am posting to help my friend Gary with his 1971 3.0CS with an automatic transmission.Today's problem is a bad windshield motor. This is a Bosch DHP 12V 0 390 341 051. Bosch's web site tells me that this is a windshield wiper motor and they do not make them any more. From looking on the web I get the feeling that this wiper motor fits all the E9 coupes. Is this true? Does it fit other BMWs? Maybe it fits some thing unrelated?The problem with this wiper motor is rust. In particular the magnets. Has anyone seen this before? and fixed it?Anyone out there with a working used wiper motor for sale? Any ideas were to look for a used or rebuilt wiper motor.Thank YouTomC in Ohio Have you searched the forum for cleaning/rebuild info?  There's been a couple threads on this. Part Cross-referenceRealOEM.com HomeLanguage: ƒçesk√ΩdeutschŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨englishUS englishespa√±olFran√ßaisitalianoÊó•Êú¨Ë™ûÌïúÍµ≠Ïñ¥Nederlandspolskiportugu√™s—Ä—É—Å—Å–∫–∏–πsvensk‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢T√ºrk√ßeÊ±âËØ≠ÁπÅÈ´î‰∏≠ÊñáPart 61611354583 (WIPER MOTOR) was found on the following vehicles:E12:Details on E12E12 528i SedanE12 530i SedanE24:Details on E24E24 630CSi CoupeE24 633CSi CoupeE3:Details on E3E3 2500 SedanE3 2800 SedanE3 3.0S SedanE3 3.0SBav SedanE3 3.0Si SedanE9:Details on E9E9 2800CS CoupeE9 3.0CS Coupe hi here is helphere you can find a little work that may help youif the engine is not broken, this will do goodenjoyhttp://www.e9coupe.com/forum/showthread.php?t=7794 Hi deQuinceyThank you! My search did not pick this thread up.It looks like Bill had the same problem that we have. The magnets are coming apart. Has anyone found a good fix for this?TomC in OhiodeQuincey said:here you can find a little work that may help youif the engine is not broken, this will do goodenjoyhttp://www.e9coupe.com/forum/showthread.php?t=7794Click to expand... Hi61porscheThank you for the interchange information. It give us more places to look.TomC in Ohio magnetsOn my wife's 240 Volvo, the magnets came loose from the housing and made the armature made an awful grinding sound.  Not really knowing what was going on, I figured I'd take it apart since there was nothing to lose by doing so.  Aha!  Magnets not attached to housing.Solution:  clean the cylindrical housing and the magnets (they were intact) and epoxy glue them back together.  Worked like a champ. I agree with using epoxy...get the high temp, metal to metal stuff. I did this on a lawn tractor starter motor and it works well. On my coupe, the wiper motor had magnets that were crumbling into pieces. I happened to have a spare wiper motor housing with good magnets so I didn't go the epoxy route, but  I would definitely use epoxy the next time the wiper motor self-destructs.\""
      ],
      "metadata": {
        "id": "IUXqMcjZt_gK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize Text: Ensure consistent casing (usually lower case), remove extra whitespace, and standardize the format of dates, numbers, and other entities.\n",
        "\n",
        "Remove Noise: Strip out non-relevant text such as HTML tags, URLs, email addresses, and any forum-specific jargon or codes that do not contribute to the general understanding of the text.\n",
        "\n",
        "Handle Special Characters: Determine if special characters like emoticons or language-specific diacritics are relevant to your use case. Remove or replace them as necessary.\n",
        "\n",
        "Filter Irrelevant Sections: If certain sections of your posts are consistently off-topic (like signatures or offhand comments), find patterns to identify and remove them.\n",
        "\n",
        "Consolidate Threads: If your corpus contains multiple posts from the same thread, consider concatenating them to form a single coherent document that captures the full conversation.\n",
        "\n",
        "Deduplication: Check for and remove duplicate entries to prevent the model from overfitting on repeated information.\n",
        "\n",
        "Entity Recognition and Anonymization: If personal information (PII) like names and locations are not relevant, use Named Entity Recognition (NER) to identify and anonymize or remove them.\n",
        "\n",
        "Keyword and Phrase Filtering: Develop a list of relevant keywords and phrases that are important for your domain. Use this list to either prioritize the inclusion of content or to filter out content that does not contain any of these terms.\n",
        "\n",
        "Semantic Clustering: Group similar content together using clustering techniques. This can help in identifying and separating out off-topic content.\n",
        "\n",
        "Quality Scoring: Implement or use existing text quality scoring systems to rank the corpus content. You can then choose to exclude low-quality entries."
      ],
      "metadata": {
        "id": "7AnolddsvAZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3LEhYbrisCl"
      },
      "outputs": [],
      "source": [
        "# Create a single string that includes the query and the texts of the retrieved documents\n",
        "\n",
        "combined_input = query_text + \" \" + \" \".join(retrieved_documents['concatenated'].tolist())\n",
        "\n",
        "# Now, 'combined_input' is ready to be used as input for GPT-2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x4xIdIhjE79"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer for GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Now tokenize your input with padding and truncation as needed\n",
        "input_tokens = tokenizer(combined_input, return_tensors='pt', padding=True, truncation=True, max_length=512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up7gmt4NjbDJ"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Generate a response using the model\n",
        "output = model.generate(**input_tokens, max_length=1000, num_return_sequences=1)\n",
        "\n",
        "# Decode the output tokens to text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, looks like I have more work to do here. The generated response is actually worse than a prompt into native GPT. I need to structure my corpus\n",
        "to where each row is a unique question and answer pair. Im going to try Named Entity Recognition (NER) using spaCy"
      ],
      "metadata": {
        "id": "hjpcCldhfgDk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtf-mdf6XLQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAR8igtCXLTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code cell will stop execution of subsequent cells\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass  # This will prevent the traceback from being shown\n",
        "\n",
        "raise StopExecution(\"Execution stopped by user\")"
      ],
      "metadata": {
        "id": "bb_OaCPRSXJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v44M2gQHRibp"
      },
      "outputs": [],
      "source": [
        "# New prompt\n",
        "new_prompt = \"Your new prompt here\"\n",
        "\n",
        "# Concatenate the new prompt with relevant information or documents\n",
        "# For demonstration, let's use the same process (this would typically involve retrieving relevant documents for the new prompt)\n",
        "combined_input_new = new_prompt + \" \" + \" \".join(retrieved_documents['concatenated'].tolist())\n",
        "\n",
        "# Tokenize the new combined input\n",
        "input_tokens_new = tokenizer(combined_input_new, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Generate a response using the model\n",
        "output_new = model.generate(**input_tokens_new, max_length=1000, num_return_sequences=1)\n",
        "\n",
        "# Decode the output tokens to text for the new prompt\n",
        "generated_text_new = tokenizer.decode(output_new[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated response to the new prompt:\", generated_text_new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "## 2.2.2 Summarization Strategies\n",
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through extractive or abstractive methods, or a combination of both.\n",
        "\n",
        "### Extractive Summarization\n",
        "- **Tools:** OpenNMT, Sumy, Gensim\n",
        "    - **Pros:**\n",
        "        - **Good with Raw Text:** Works directly with raw text, selecting key sentences without deep processing.\n",
        "        - **Straightforward Implementation:** Simplifies implementation without needing complex preprocessing.\n",
        "    - **Cons:**\n",
        "        - **Limited Depth in Understanding:** May not fully grasp nuanced meanings in complex texts.\n",
        "        - **Less Effective with Poorly Structured Text:** Struggles with identifying main points in unstructured or informal text.\n",
        "\n",
        "### Abstractive Summarization\n",
        "- **Tools:** sshleifer/distilbart-cnn-12-6, T5, BERTSum\n",
        "    - **Pros:**\n",
        "        - **Advanced Processing Capabilities:** Designed to interpret and rephrase raw text, capturing contextual nuances.\n",
        "        - **Higher Tolerance for Unstructured Text:** Manages and refines unstructured or informal text into coherent summaries.\n",
        "    - **Cons:**\n",
        "        - **Dependence on Preprocessing:** The output's quality can be improved with proper preprocessing for complex texts.\n",
        "        - **Potential Overhead:** More computational resources required for understanding raw text.\n",
        "\n",
        "### Hybrid Summarization\n",
        "- **Combines extractive and abstractive methods for a balanced approach to summarization.**\n",
        "    - **Pros:**\n",
        "        - **Flexibility in Text Processing:** Handles both raw and preprocessed text, adapting to text complexity.\n",
        "        - **Balanced Approach:** Leverages strengths of both methods for identifying key points and generating summaries.\n",
        "    - **Cons:**\n",
        "        - **Complex Preprocessing Requirements:** Integrating both methods may require sophisticated preprocessing.\n",
        "        - **Potential for Processing Inefficiencies:** Could lead to redundancies or inefficiencies if not carefully managed.\n",
        "\n",
        "*Note: After attempting sshleifer/distilbart-cnn-12-6, which has a character limit too restrictive for my needs, T5 was chosen for its lack of character limits and broader applicability to the project's goals.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 2.3 Data Storage and Database\n",
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "### Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHg-WlWRLIL"
      },
      "source": [
        "## 2.4 Data Ethics\n",
        "The data collected here is a collection of posts from widely avaialble public sources. However, should this project move into a public forum additional steps will be necessary to endure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the projects goals and data gathering process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh5XkJIblh7_"
      },
      "source": [
        "# 3. Model Selection and Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXBTyOK2ogL8"
      },
      "source": [
        "## 3.1 Architectural Approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlHaUZfTouay"
      },
      "source": [
        "### 3.1.1 Pre-trained Models\n",
        "- **Tools:**\n",
        "  - **Libraries/Frameworks:** GPT, PyTorch, Hugging Face Transformers. These tools offer extensive collections of pre-trained models for various NLP tasks.\n",
        "- **Pros:**\n",
        "  - **Time and Resource Efficiency:** Saves significant time and computational resources.\n",
        "  - **Immediate Advanced Capabilities:** Suitable for a wide range of applications with out-of-the-box capabilities.\n",
        "  - **Accessibility:** Easily accessible through libraries and frameworks.\n",
        "- **Cons:**\n",
        "  - **Generic Features:** May not excel without fine-tuning on niche or domain-specific tasks.\n",
        "  - **Black Box Nature:** Complex and less interpretable.\n",
        "  - **Size and Complexity:** Often large and resource-intensive, posing challenges for limited-resource deployment.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using GPT2-Small for my pre-trained model.**\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHlFsQMx1pKl"
      },
      "source": [
        "### 3.1.2 Scratch Models\n",
        "- **Tools:**\n",
        "  - **Development Environments:** Custom development often utilizes lower-level APIs or libraries for greater control over the model architecture. Examples include TensorFlow’s and PyTorch’s core libraries, along with specialized libraries for specific model components or functions.\n",
        "- **Pros:**\n",
        "  - **Customization:** Tailored specifically to project needs, potentially leading to better task-specific performance.\n",
        "  - **Interpretability:** Easier to interpret and diagnose.\n",
        "  - **Flexibility in Architecture:** Allows for experimentation with innovative architectures.\n",
        "- **Cons:**\n",
        "  - **Time and Resource Intensive:** Requires significant training time and resources.\n",
        "  - **Data Requirements:** Needs large, often domain-specific datasets.\n",
        "  - **Risk of Overfitting:** Increased risk without the vast, diverse datasets of pre-trained models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVLI760wo3vg"
      },
      "source": [
        "## 3.2 Specific Model Architectures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kOWfk55o8Oo"
      },
      "source": [
        "### 3.2.1 Retriever-Generator Models\n",
        "Retriever-Generator Models combine the capabilities of a document retriever and a text generator to enhance natural language understanding and generation. By retrieving relevant documents or passages to a query and then generating responses based on this retrieved information, RAG models can provide more accurate and contextually rich outputs for tasks like question answering and information retrieval.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Enhanced Contextual Understanding:** Leverages external knowledge sources for better context understanding.\n",
        "  - **Improved Accuracy:** Can generate more accurate and informative responses by using retrieved documents.\n",
        "  - **Flexibility:** Suitable for a wide range of applications beyond simple text generation, including complex question answering and chatbots.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Increased Complexity:** Integrating retrieval and generation components adds to the model complexity.\n",
        "  - **Resource Intensive:** Requires significant computational resources for both retrieving documents and generating responses.\n",
        "  - **Dependency on External Sources:** The performance heavily relies on the quality and relevance of the retrieved documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_OmJkdpHUn"
      },
      "source": [
        "### 3.2.2 Sequence-to-Sequence Models\n",
        "Sequence-to-Sequence (Seq2Seq) models are designed for tasks where both the input and output are sequences, such as translation, text summarization, and chatbot development. They typically employ an encoder-decoder architecture where the encoder processes the input sequence, and the decoder generates the output sequence.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Versatility:** Effective for a wide range of NLP tasks involving sequence generation.\n",
        "  - **Contextual Understanding:** Can capture long-range dependencies and contextual information within sequences.\n",
        "  - **End-to-End Learning:** Directly maps input sequences to output sequences, simplifying the training process.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Complexity:** Can be computationally intensive and require significant training data.\n",
        "  - **Fixed-length Bottleneck:** The fixed-length internal representation can limit the model's ability to handle very long sequences.\n",
        "  - **Difficulty in Parallelization:** The sequential nature of the decoder can make it challenging to parallelize, affecting training and inference speed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt-7G1cbpKBt"
      },
      "source": [
        "### 3.2.3 Transformer Models\n",
        "Transformer Models revolutionized NLP by introducing an architecture based entirely on attention mechanisms, eliminating the need for recurrent layers. This design allows for significantly improved parallelization during training and has led to state-of-the-art performances in various NLP tasks, including language understanding, translation, and text generation.\n",
        "\n",
        "- **Pros:**\n",
        "  - **High Efficiency and Scalability:** Enables faster training and better handling of long sequences compared to RNNs and CNNs.\n",
        "  - **State-of-the-Art Performance:** Achieves superior results on a broad spectrum of NLP tasks.\n",
        "  - **Versatile:** Foundation models like BERT and GPT are adaptable to numerous tasks with minimal adjustments.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Resource Intensive:** Requires substantial computational resources for training and fine-tuning large models.\n",
        "  - **Complexity:** The architecture and the mechanisms involved are complex, which can make customization and understanding challenging.\n",
        "  - **Overfitting Risk:** There's a tendency for smaller datasets to lead to overfitting, necessitating careful regularization and training strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umsgM3VYpMci"
      },
      "source": [
        "### 3.2.4 Convolutional Neural Networks (CNNs)\n",
        "Convolutional Neural Networks (CNNs), primarily known for their application in image processing, are also utilized in NLP for tasks like text classification and sentiment analysis. They work by applying convolutional layers to extract and learn features from sequences of text data, making them efficient for identifying local and position-invariant patterns in text.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Efficient Pattern Recognition:** Excellently identifies local and position-invariant features in text data.\n",
        "  - **Highly Efficient:** CNNs are computationally efficient, making them faster to train compared to models like RNNs.\n",
        "  - **Versatile Applications:** Effective for a range of NLP tasks, including classification, topic categorization, and sentiment analysis.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Limited Contextual Understanding:** Struggles with capturing long-range dependencies in text due to the fixed size of convolutional filters.\n",
        "  - **Overemphasis on Local Features:** May overlook the global context or the overall structure of the text, affecting tasks requiring a deeper understanding.\n",
        "  - **Less Effective for Sequential Data:** Not as naturally suited for sequence-to-sequence tasks compared to RNNs or Transformer models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z-c-n4jpOuM"
      },
      "source": [
        "### 3.2.5 Recurrent Neural Networks (RNNs)\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data, making them ideal for NLP tasks such as language modeling, text generation, and machine translation. RNNs process sequences by maintaining a hidden state that captures information about previous elements, allowing them to exhibit temporal dynamic behavior.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Temporal Sequence Modeling:** Naturally suited for tasks involving sequential data, capturing temporal dependencies effectively.\n",
        "  - **Flexible Input/Output Length:** Can handle variable-length input and output sequences, which is beneficial for many NLP tasks.\n",
        "  - **Foundation for Advanced Models:** Serve as the basis for more complex architectures like LSTM and GRU, which address some of the RNN's limitations.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Vanishing Gradient Problem:** Struggles with long-range dependencies due to gradients diminishing over long sequences, making training difficult.\n",
        "  - **Computational Intensity:** Sequential nature of processing makes it less efficient in terms of parallelization and speed compared to models like Transformers.\n",
        "  - **Risk of Overfitting:** Particularly for smaller datasets, RNNs can easily overfit, necessitating careful regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zORHOO-7KI"
      },
      "source": [
        "#### 3.2.5.1 RNN Language Model Architectures\n",
        "\n",
        "**Gated Recurrent Unit (GRU):**\n",
        "\n",
        "**Pros:**\n",
        "- **Efficient Training:** GRUs simplify the RNN architecture by merging the forget and input gates into a single update gate, reducing the model complexity and making it faster to train compared to LSTMs.\n",
        "- **Flexibility in Capturing Dependencies:** GRUs provide a mechanism to adaptively capture dependencies of different time scales, making them versatile for a range of sequence modeling tasks.\n",
        "- **Lesser Parameters:** With a simpler gating mechanism than LSTM, GRUs have fewer parameters, making them more efficient in terms of memory and computationally less demanding.\n",
        "- **Good Performance on Smaller Datasets:** GRUs can achieve competitive performance with LSTMs on many tasks, often excelling when the dataset is relatively small.\n",
        "\n",
        "**Cons:**\n",
        "- **Limited Memory Control:** The simplified gating mechanism in GRUs might limit their ability to precisely control the flow of information across time steps compared to LSTMs.\n",
        "- **Potential for Overfitting:** Due to their efficiency and capacity to model complex patterns, GRUs can be prone to overfitting, especially on smaller datasets without proper regularization.\n",
        "- **Challenges with Extremely Long Sequences:** While GRUs handle long sequences better than vanilla RNNs, they can still struggle with very long dependencies, a domain where LSTMs might perform slightly better.\n",
        "\n",
        "**Long Short-Term Memory (LSTM):**\n",
        "\n",
        "**Pros:**\n",
        "- **Advanced Memory Management:** LSTMs include three gates (input, output, and forget gates), allowing for more nuanced control over the memory cell, which helps in learning long-term dependencies more effectively.\n",
        "- **Robustness to Vanishing Gradient Problem:** The architecture of LSTMs specifically addresses the vanishing gradient problem, making them capable of learning from data with long-range temporal dependencies.\n",
        "- **Widespread Adoption and Support:** LSTMs have been extensively studied and applied across various domains, benefiting from a wealth of research, optimizations, and implementations.\n",
        "\n",
        "**Cons:**\n",
        "- **Computational Complexity:** The more complex architecture of LSTMs, with multiple gates and a memory cell, makes them computationally more expensive to train and infer compared to GRUs.\n",
        "- **Parameter Heavy:** LSTMs have a larger number of parameters due to their intricate gating mechanism, leading to increased memory requirements and potentially longer training times.\n",
        "- **Efficiency Trade-offs:** While LSTMs can capture longer dependencies better than GRUs in some cases, this advantage comes at the cost of increased computational resources and training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK1EVhic4QtV"
      },
      "source": [
        "### 3.2.6 BERT (Bidirectional Encoder Representations from Transformers)\n",
        "BERT revolutionized NLP by introducing a method for pre-training language representations on a large corpus and then fine-tuning on specific tasks. It captures deep bidirectional contexts by training on a masked language model, enabling it to excel at tasks like question answering, named entity recognition, and sentiment analysis.\n",
        "\n",
        "- **Pros:**\n",
        "  - Bidirectional context understanding enhances word meaning comprehension.\n",
        "  - Versatile across a broad range of NLP tasks.\n",
        "  - Achieved state-of-the-art results on benchmark tasks.\n",
        "- **Cons:**\n",
        "  - Large model size, requiring significant resources.\n",
        "  - Computationally expensive to train and fine-tune.\n",
        "  - Fixed architecture limits customization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24jU7VWd4n9K"
      },
      "source": [
        "### 3.2.7 T5 (Text-to-Text Transfer Transformer)\n",
        "T5 simplifies the NLP landscape by framing every task as a text-to-text problem, whether it's translation, summarization, or question answering. This approach, combined with a colossal scale and multi-task learning, allows T5 to achieve state-of-the-art results across a wide range of benchmarks.\n",
        "- **Pros:**\n",
        "  - Unified text-to-text framework simplifies various NLP tasks.\n",
        "  - Trained on a diverse set of tasks, enabling good generalization.\n",
        "  - Efficient fine-tuning process for task adaptation.\n",
        "- **Cons:**\n",
        "  - Complex architecture requires more resources and expertise.\n",
        "  - Large model size challenges deployment on limited-resource devices.\n",
        "  - Task-specific fine-tuning may still be necessary for optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naj4cWDVpVn9"
      },
      "source": [
        "## 3.3 Incremental Architecture Options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxl8-iXrHRz1"
      },
      "source": [
        "### 3.3.1 Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS3jddTGvFtz"
      },
      "source": [
        "**Rule-based Systems:**\n",
        "\n",
        "Rule-based systems make decisions based on a set of explicit rules. These systems are particularly useful for tasks that can be clearly defined by rules and where the logic behind decisions needs to be transparent.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Interpretability:** Offers transparent decision-making processes, allowing for easy understanding and validation of outputs.\n",
        "  - **Domain Expertise Incorporation:** Effectively leverages subject matter expertise by encoding domain-specific rules and heuristics.\n",
        "  - **Flexibility:** Enables easy modifications or extensions of rules by domain experts, accommodating new knowledge or changing requirements.\n",
        "  - **Low Data Dependency:** Suitable for domains with limited or noisy data, as they require less data for \"training\" compared to data-driven models.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Limited Complexity:** May struggle with capturing complex patterns or relationships, especially in highly variable or uncertain domains.\n",
        "  - **Maintenance Overhead:** Requires constant monitoring and refinement, making the maintenance and updating of rules labor-intensive.\n",
        "  - **Difficulty Handling Exceptions:** Can perform suboptimally in scenarios involving edge cases or exceptions not covered by the rules.\n",
        "  - **Scalability Limitations:** Managing a large rule base and ensuring efficient inference can become challenging, potentially affecting scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIpGtRs-vAu8"
      },
      "source": [
        "**Knowledge Graph Integration:**\n",
        "\n",
        "Knowledge Graph Integration involves using structured representations of domain-specific knowledge to enhance model performance. By mapping out entities and their interrelations, knowledge graphs contribute to a model's ability to understand context and make informed decisions.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Structured Representation:** Provides an organized way to store and retrieve domain-specific knowledge.\n",
        "  - **Contextual Understanding:** Enhances the model's grasp of the domain by leveraging entity relationships, improving decision accuracy.\n",
        "  - **Scalability:** Capable of incorporating vast amounts of information, supporting the model's ability to access extensive domain expertise.\n",
        "  - **Interpretability:** Makes the decision-making process more transparent, with interpretable relationships and entities.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Knowledge Acquisition:** Requires a significant investment in developing and maintaining a comprehensive knowledge base.\n",
        "  - **Complexity:** Adds to the system's architectural and operational complexity, necessitating specialized integration techniques.\n",
        "  - **Knowledge Representation Bias:** Depends heavily on the quality and breadth of the underlying data, which can introduce biases.\n",
        "  - **Scalability Challenges:** While knowledge graphs can scale, managing and querying large, complex graphs may require substantial computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOm7aD_duzxS"
      },
      "source": [
        "**Retrieval-augmented Generation (RAG):**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) models enhance natural language generation by integrating retrieval mechanisms with generative models. This approach utilizes retrieved documents or data to inform the generation process, aiming to produce more accurate, diverse, and contextually relevant outputs.\n",
        "\n",
        "- **Pros:**\n",
        "  - **Improved Relevance:** Combines retrieval and generation for contextually accurate responses.\n",
        "  - **Increased Diversity:** Generates diverse responses using varied retrieved content.\n",
        "  - **Enhanced Coherence:** Produces coherent responses by incorporating relevant context.\n",
        "  - **Adaptability:** Can be tailored to various domains with domain-specific retrieval.\n",
        "  - **Interpretability:** Offers insights into decision-making by showing retrieved context.\n",
        "  - **Efficiency:** Can leverage pre-computed embeddings and efficient retrieval for computational efficiency.\n",
        "\n",
        "- **Cons:**\n",
        "  - **Complexity:** Requires integrating complex retrieval and generation components.\n",
        "  - **Training Data Requirements:** Needs extensive datasets for effective retrieval and generation.\n",
        "  - **Fine-tuning Challenges:** Demands meticulous tuning for task-specific optimization.\n",
        "  - **Evaluation Difficulty:** Presents challenges in assessing retrieval and generation quality.\n",
        "  - **Resource Intensive:** Demands substantial computational resources for training and deployment.\n",
        "  - **Domain Specificity:** Performance may vary significantly across different domains and tasks.\n",
        "\n",
        "  **Conclusion: I will be using RAG architecture.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7KVeJI-ICQm"
      },
      "source": [
        "#### 3.3.1.1 Specific Components for RAG Architecture\n",
        "\n",
        "**Embedding Model Options for Retriever**\n",
        "- **Dense Passage Retrieval (DPR):** Optimized for retrieving relevant passages with dense vector embeddings.\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers):** Provides deep contextual embeddings for effective retrieval.\n",
        "- **Sentence-BERT (SBERT):** Specialized for generating sentence embeddings quickly.\n",
        "\n",
        "\n",
        "1. Dense Passage Retrieval (DPR)\n",
        "Pros: Highly efficient at finding relevant documents or passages using dense vector embeddings. It's particularly effective for tasks where the retrieval of specific information is critical.\n",
        "Cons: Requires a separate index of pre-computed embeddings for your corpus, which can be resource-intensive to create and maintain.\n",
        "2. BERT\n",
        "Pros: Provides contextually rich embeddings that take into account the full context of words in a sentence, making it highly effective for complex retrieval tasks.\n",
        "Cons: Can be computationally expensive due to its deep architecture, especially when processing large documents or a vast number of retrieval candidates.\n",
        "3. Sentence-BERT (SBERT)\n",
        "Pros: Designed for efficient computation of sentence-level embeddings, making it faster than BERT for retrieval tasks while still leveraging the depth of BERT’s understanding.\n",
        "Cons: May not capture the granularity of information at the same level as DPR when it comes to specific passage retrieval, but it is generally more efficient.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Search Engine/Indexer Options**\n",
        "- **Elasticsearch:** A scalable full-text search engine with a robust ecosystem.\n",
        "- **FAISS (Facebook AI Similarity Search):** Optimized for efficient similarity search in large-scale datasets.\n",
        "- **Anserini:** Built on Lucene for reproducible information retrieval research.\n",
        "\n",
        "1.Elasticsearch\n",
        "\n",
        "Use Case: If your project benefits from a full-text search engine that can handle complex queries, including full-text search capabilities, filtering, and ranking, Elasticsearch is a strong choice. It's particularly well-suited for applications where you need to perform text search alongside other types of data queries.\n",
        "Pros: Highly scalable, with a rich ecosystem and community support. Offers out-of-the-box features for text analysis and advanced search capabilities.\n",
        "Cons: Can be resource-intensive and may require more infrastructure setup and maintenance compared to the other options.\n",
        "\n",
        "2.FAISS (Facebook AI Similarity Search)\n",
        "Use Case: FAISS is ideal for projects that require efficient similarity search in large-scale datasets, especially where the primary concern is finding the nearest neighbors in embedding space (e.g., for document retrieval based on semantic similarity).\n",
        "Pros: Highly efficient at similarity search, especially on large datasets. Supports GPU acceleration for faster processing.\n",
        "Cons: Focuses primarily on vector similarity and does not offer the full-text search capabilities or analysis features of Elasticsearch.\n",
        "\n",
        "3. Anserini\n",
        "Use Case: Anserini is built on Lucene and is designed for information retrieval research, making it suitable for academic projects or scenarios where reproducibility and experimentation with different retrieval models are key.\n",
        "Pros: Supports reproducible research with a focus on information retrieval. Offers robust support for traditional IR models and integration with newer neural models.\n",
        "Cons: Might have a steeper learning curve for those not familiar with Lucene. Focused more on research use cases than production-level full-text search applications.\n",
        "For a Retrieval-Augmented Generation (RAG) system:\n",
        "\n",
        "If your focus is on leveraging semantic embeddings for retrieval, FAISS is a highly efficient choice.\n",
        "If you need rich text search capabilities alongside semantic search or have a complex application that requires the advanced features of a full-text search engine, Elasticsearch could be more appropriate.\n",
        "If you're conducting research or developing a system with a strong emphasis on information retrieval techniques and reproducibility, Anserini might be the best fit.\n",
        "Given the context of your project focusing on creating a RAG system with GPT-2, FAISS appears to be a suitable choice for efficiently handling the embedding-based retrieval part of your workflow. It offers the speed and scalability necessary for dealing with large-scale datasets and embedding vectors, aligning well with the retrieval needs of a RAG system.\n",
        "\n",
        "\n",
        "\n",
        "**Generator Component Selection**\n",
        "- **GPT-3 (Generative Pre-trained Transformer 3):** Advanced in generating coherent, contextually relevant text.\n",
        "- **T5 (Text-to-Text Transfer Transformer):** Adapts to various NLP tasks with a text-to-text approach.\n",
        "- **BART (Bidirectional and Auto-Regressive Transformers):** Excels in generation tasks requiring full input context understanding.\n",
        "\n",
        "**Integration Layer Selection**\n",
        "- **Hugging Face Transformers Library:** Provides support for integrating retriever and generator components with pre-trained models.\n",
        "- **Custom Python Scripts:** Offers complete control over the integration process for custom logic.\n",
        "- **Apache NiFi or Apache Airflow:** Manages robust data processing pipelines for complex workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AdHHJxUP6b5"
      },
      "source": [
        "#### 3.3.2 Frameworks and Libraries for Model Implementation\n",
        "\n",
        "The choice of frameworks and libraries is crucial for the effective implementation of NLP models, including but not limited to Retriever-Generator Models, Transformer Models, CNNs, and RNNs. Below are the technology stacks that offer comprehensive tools and pre-trained models for developing and deploying these architectures.\n",
        "\n",
        "- **PyTorch and Hugging Face Transformers:**\n",
        "  - **Pros:** Intuitive dynamic computation graphs; extensive support for pre-trained models and RAG architectures.\n",
        "  - **Cons:** May present a steep learning curve for newcomers.\n",
        "\n",
        "- **TensorFlow and T5:**\n",
        "  - **Pros:** Robust tools for model development and deployment; versatile T5 model for text-to-text tasks.\n",
        "  - **Cons:** Less intuitive static computation graph; potential need for additional preprocessing steps.\n",
        "\n",
        "- **JAX and Flax/Haiku:**\n",
        "  - **Pros:** Accelerated operations and auto-differentiation; support for complex model architectures.\n",
        "  - **Cons:** Less mature ecosystem with fewer pre-trained models and resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWZX-eZellc5"
      },
      "source": [
        "# 4. Training and Evaluation\n",
        "\n",
        "1. **PyTorch and Hugging Face Transformers**\n",
        "   - **Pros:** PyTorch offers dynamic computation graphs that are intuitive for RAG model development. Hugging Face's Transformers library provides easy access to pre-trained models and tokenizers, facilitating both training and evaluation with extensive support for RAG architectures.\n",
        "   - **Cons:** While highly flexible, this combination might require a steep learning curve for those not familiar with PyTorch or the Transformers library.\n",
        "\n",
        "2. **TensorFlow and T5**\n",
        "   - **Pros:** TensorFlow provides robust tools for model development and deployment, with T5 being a versatile model for text-to-text tasks, adaptable for RAG purposes. TensorFlow's extensive ecosystem includes TensorBoard for monitoring training processes.\n",
        "   - **Cons:** TensorFlow's static computation graph can be less intuitive than PyTorch's dynamic graphs. T5's text-to-text format might require additional preprocessing steps.\n",
        "\n",
        "3. **JAX and Flax/Haiku**\n",
        "   - **Pros:** JAX offers accelerated NumPy operations and automatic differentiation, making it efficient for large-scale model training. Flax and Haiku provide neural network libraries for JAX, supporting complex RAG model architectures.\n",
        "   - **Cons:** JAX's ecosystem is less mature, with fewer pre-trained models and community resources available compared to PyTorch and TensorFlow. This can make development and troubleshooting more challenging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ1Ha13mpeGE"
      },
      "source": [
        "## 4.1 Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNXeUAspgU4"
      },
      "source": [
        "## 4.2 Model Evaluation\n",
        "\n",
        "The project's success will be assessed based on the accuracy and speed of responses generated by the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxRaOdwlrWY"
      },
      "source": [
        "# 5. Results and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGiqPFHIpsn6"
      },
      "source": [
        "## 5.1 Experimental Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56b36gepvMG"
      },
      "source": [
        "## 5.2 Performance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 6. Deployment and Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "## 6.1 Deployment Tools\n",
        "# Deployment and Serving Infrastructure Selection\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.\n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n",
        "## Decision\n",
        "For deploying a RAG model, especially within an academic or portfolio context where ease of use, accessibility, and cost-effectiveness are key considerations, Hugging Face Spaces is highly recommended. It allows you to quickly deploy your models with minimal setup and offers a user-friendly platform for showcasing your work to a wide audience. For projects that might evolve into more scalable or commercial applications, starting with Docker for containerization and then moving to a Kubernetes-based deployment as needs grow could be a strategic approach. This path provides a balance between initial simplicity and long-term scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRe4qbx5p1nj"
      },
      "source": [
        "## 6.2 Integration Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KGSPx5olwxJ"
      },
      "source": [
        "# 7. Reflection and Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWoXGpWRp53s"
      },
      "source": [
        "## 7.1 Challenges Faced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmm_IZYBp7Wr"
      },
      "source": [
        "## 7.2 Lessons Learned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cXEIxsnly0H"
      },
      "source": [
        "#8. Future Work and Improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9bwbXcuqADQ"
      },
      "source": [
        "## 8.1 Potential Enhancements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lOHxB-hqB1n"
      },
      "source": [
        "## 8.2 Areas for Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 9. Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyI1lIB40-z1"
      },
      "source": [
        "\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "**Pros:**\n",
        "- **Bidirectional Context:** BERT captures bidirectional context during pre-training, enabling it to understand the meaning of words in their full context.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- **Versatility:** BERT is versatile and can be fine-tuned for a wide range of NLP tasks, including classification, regression, and question answering.\n",
        "- **State-of-the-Art Performance:** BERT achieved state-of-the-art results on various benchmark NLP tasks, making it a reliable choice for many applications.\n",
        "\n",
        "**Cons:**\n",
        "- **Large Model Size:** BERT's model size can be large, making it resource-intensive for fine-tuning and inference, especially on devices with limited memory.\n",
        "- **Computationally Expensive:** Training and fine-tuning BERT models require significant computational resources, including powerful GPUs or TPUs, which may not be accessible for all users.\n",
        "- **Fixed Architecture:** BERT's architecture is fixed, limiting flexibility for customizing model structure or adding domain-specific features.\n",
        "\n",
        "**GPT (Generative Pre-trained Transformer)**\n",
        "\n",
        "**Pros:**\n",
        "- **Generative Capability:** GPT models are capable of generating coherent and contextually relevant text, making them suitable for tasks like text completion, summarization, and dialogue generation.\n",
        "- **Large Context Window:** GPT models have a large context window, allowing them to capture long-range dependencies in text and generate more contextually relevant responses.\n",
        "- **Fine-tuning Flexibility:** GPT models can be fine-tuned for various downstream tasks, offering flexibility for adapting to specific applications and domains.\n",
        "\n",
        "**Cons:**\n",
        "- **Unidirectional Context:** GPT models rely on unidirectional context during pre-training, which may limit their understanding of context compared to bidirectional models like BERT.\n",
        "- **Lack of Discriminative Ability:** GPT models prioritize generative capabilities over discriminative tasks like classification, which may result in suboptimal performance for tasks requiring precise classification or labeling.\n",
        "- **Limited Control:** GPT models generate text based on learned patterns in the data, which may lead to generation of biased or inappropriate content without proper control mechanisms.\n",
        "\n",
        "**T5 (Text-to-Text Transfer Transformer)**\n",
        "\n",
        "**Pros:**\n",
        "- **Text-to-Text Format:** T5 frames all NLP tasks as text-to-text problems, providing a unified framework for various tasks like translation, summarization, and question answering.\n",
        "- **Multi-task Learning:** T5 is trained on a diverse set of NLP tasks simultaneously, enabling it to generalize well across different tasks and domains.\n",
        "- **Fine-tuning Efficiency:** T5 fine-tuning is efficient due to its text-to-text format, allowing for easy adaptation to specific tasks with minimal additional training data.\n",
        "\n",
        "**Cons:**\n",
        "- **Complexity:** T5's architecture is complex, which may require more computational resources and expertise to train and fine-tune compared to simpler models like BERT.\n",
        "- **Large Model Size:** Like other transformer-based models, T5 can have a large model size, which may pose challenges for deployment and inference on resource-constrained devices.\n",
        "- **Task-specific Fine-tuning:** While T5 is capable of handling multiple tasks, fine-tuning for specific tasks may still be necessary to achieve optimal performance, requiring additional effort and resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data Manipulation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from transformers import BertModel, BertTokenizer\n",
        "# import torch\n",
        "# import pandas as pd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hFZfFT6ljH7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HBaTl_s-luFh",
        "9KGSPx5olwxJ",
        "2cXEIxsnly0H"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNT8dq5WxuwUYCk+I88OrPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}