# -*- coding: utf-8 -*-
"""LLM RAG Frontend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rdGpMEfO2-LX7S3gp5DI1HsLDpySkAIC
"""

import streamlit as st

# Set page configuration - MUST BE FIRST STREAMLIT COMMAND
st.set_page_config(
    page_title="BMW E9 Knowledge Base",
    page_icon="ðŸš—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Then import other libraries
import pandas as pd
import snowflake.connector
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI
import os
import json

# Debug information can go here, after st.set_page_config
st.write("Version: 2.0 - Using environment variables")

# Get credentials from environment variables
sf_user = os.environ.get('SNOWFLAKE_USER')
sf_pass = os.environ.get('SNOWFLAKE_PASSWORD')
sf_acct = os.environ.get('SNOWFLAKE_ACCOUNT')
openai_key = os.environ.get('OPENAI_API_KEY')

# Check if all required environment variables are set
missing_vars = []
if not sf_user: missing_vars.append('SNOWFLAKE_USER')
if not sf_pass: missing_vars.append('SNOWFLAKE_PASSWORD')
if not sf_acct: missing_vars.append('SNOWFLAKE_ACCOUNT')
if not openai_key: missing_vars.append('OPENAI_API_KEY')

if missing_vars:
    st.error(f"Missing environment variables: {', '.join(missing_vars)}")
    st.info("Please set the required environment variables before running the app.")
    st.stop()

# ======== SIDEBAR: Configuration ========
with st.sidebar:
    st.title("BMW E9 Knowledge Base")
    try:
        st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/BMW_E9_front_20071007.jpg/1280px-BMW_E9_front_20071007.jpg", width=300)
    except Exception as e:
        st.write("Image unavailable")
    st.markdown("---")

    # Configuration options
    st.subheader("Configuration")

    # Model selection
    embedding_model = st.selectbox(
        "Embedding Model",
        ["all-MiniLM-L6-v2", "all-mpnet-base-v2"],
        index=0
    )

    llm_model = st.selectbox(
        "Language Model",
        ["gpt-3.5-turbo", "gpt-4"],
        index=0
    )

    # Search parameters
    top_k = st.slider("Number of documents to retrieve", 1, 10, 5)
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)

    # Add a citation toggle
    show_citations = st.toggle("Show Citations", value=True)

    st.markdown("---")
    st.caption("Created by David Elgas")

# ======== FUNCTIONS ========
@st.cache_resource
def load_data_from_snowflake():
    """Load data from Snowflake database"""
    try:
        # Connect to Snowflake using environment variables
        conn = snowflake.connector.connect(
            user=sf_user,
            password=sf_pass,
            account=sf_acct,
            database='E9_CORPUS',
            schema='E9_CORPUS_SCHEMA',
            warehouse='COMPUTE_WH'
        )

        cur = conn.cursor()
        cur.execute("SELECT THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS FROM E9_CORPUS.E9_CORPUS_SCHEMA.E9_FORUM_CORPUS")
        rows = cur.fetchall()

        # Convert to DataFrame
        df = pd.DataFrame(rows, columns=['thread_id', 'thread_title', 'thread_first_post', 'thread_all_posts'])

        # Create a combined text field
        df["full_text"] = (
            df["thread_title"].fillna("") + "\n\n" +
            df["thread_first_post"].fillna("") + "\n\n" +
            df["thread_all_posts"].fillna("")
        )

        return df
    except Exception as e:
        st.error(f"Error connecting to Snowflake: {str(e)}")
        raise e

@st.cache_resource
def load_embedding_model(model_name):
    """Load the embedding model"""
    try:
        return SentenceTransformer(model_name)
    except Exception as e:
        st.error(f"Error loading embedding model: {str(e)}")
        raise e

@st.cache_resource
def create_faiss_index(df, _model):  # Added underscore to model parameter
    """Create FAISS index from corpus embeddings"""
    try:
        corpus_embeddings = _model.encode(df["full_text"].tolist(), show_progress_bar=True)

        dimension = corpus_embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(corpus_embeddings)

        return index, corpus_embeddings
    except Exception as e:
        st.error(f"Error creating FAISS index: {str(e)}")
        raise e

def search_similar_documents(question, index, model, df, top_k=5):
    """Search for similar documents using the FAISS index"""
    try:
        question_embedding = model.encode([question])
        distances, indices = index.search(question_embedding, top_k)

        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            results.append({
                'index': idx,
                'distance': float(dist),
                'thread_id': int(df.iloc[idx]["thread_id"]),
                'title': df.iloc[idx]["thread_title"],
                'first_post': df.iloc[idx]["thread_first_post"],
                'full_text': df.iloc[idx]["full_text"]
            })

        return results
    except Exception as e:
        st.error(f"Error searching documents: {str(e)}")
        return []

def generate_answer(question, retrieved_docs, with_context=True):
    """Generate answer using OpenAI API"""
    try:
        client = OpenAI(api_key=openai_key)

        if with_context:
            context = "\n\n".join([f"Thread {i+1}:\n{doc['full_text']}" for i, doc in enumerate(retrieved_docs)])
            prompt = f"""You are an expert on BMW E9 maintenance. Use the following forum threads to answer the question.

{context}

Question: {question}
Answer:"""
        else:
            prompt = f"""You are an expert on BMW E9 maintenance.

Question: {question}
Answer:"""

        response = client.chat.completions.create(
            model=llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )

        return response.choices[0].message.content
    except Exception as e:
        st.error(f"Error generating answer: {str(e)}")
        return "Sorry, I couldn't generate an answer. Please try again."

# ======== MAIN APP ========
# Load data and models
try:
    with st.spinner("Loading data and models..."):
        df = load_data_from_snowflake()
        model = load_embedding_model(embedding_model)
        index, embeddings = create_faiss_index(df, model)
        st.success(f"Loaded {len(df)} forum threads")
except Exception as e:
    st.error(f"Error loading data: {str(e)}")
    st.stop()

# App title and description
st.title("BMW E9 Knowledge Base")
st.markdown("""
This application provides answers to your BMW E9 maintenance and repair questions using data from an E9 forum.
The system uses RAG (Retrieval-Augmented Generation) to find relevant forum posts and generate accurate answers.
""")

# User input
question = st.text_input("Ask a question about BMW E9 maintenance:", "How do I remove the steering wheel in an E9?")

if st.button("Ask Question", type="primary"):
    if not question:
        st.warning("Please enter a question")
    else:
        # Search for relevant documents
        with st.spinner("Searching for relevant information..."):
            results = search_similar_documents(question, index, model, df, top_k)

        # Generate answer with context
        with st.spinner("Generating answer..."):
            answer = generate_answer(question, results)

        # Display answer
        st.markdown("### Answer")
        st.markdown(answer)

        # Display source documents
        if show_citations:
            st.markdown("### Sources")
            for i, result in enumerate(results):
                with st.expander(f"Thread {i+1}: {result['title']} (Distance: {result['distance']:.4f})"):
                    st.markdown(f"**Thread ID:** {result['thread_id']}")
                    st.markdown(f"**First Post:**\n{result['first_post'][:500]}...")

# Add information about the database
st.markdown("---")
with st.expander("About the Database"):
    st.markdown(f"""
    - **Total Threads:** {len(df)}
    - **Embedding Model:** {embedding_model}
    - **Language Model:** {llm_model}
    - **Index Type:** FAISS (Facebook AI Similarity Search)
    """)

    # Display statistics
    if st.checkbox("Show Database Statistics"):
        # Calculate statistics
        avg_title_len = df['thread_title'].str.len().mean()
        avg_posts_len = df['thread_all_posts'].str.len().mean()

        col1, col2 = st.columns(2)
        with col1:
            st.metric("Avg. Title Length", f"{avg_title_len:.1f} chars")
        with col2:
            st.metric("Avg. Post Length", f"{avg_posts_len:.1f} chars")