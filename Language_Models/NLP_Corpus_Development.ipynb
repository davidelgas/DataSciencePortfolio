{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/NLP_Corpus_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "#1 Project Description\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egcgaDGAzB6r"
      },
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ff_KyF7fz7"
      },
      "source": [
        "#2 Create Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWO8xyGhyy0O",
        "outputId": "5b2b8d6f-b5aa-4c29-d4ab-ce4f447bc05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "phiTC3nry3T8",
        "outputId": "40ee9320-d971-4afb-eb90-a0ee4600dac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.0)\n",
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.13.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.7)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.3.0)\n",
            "Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: asn1crypto, tomlkit, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.14.0 tomlkit-0.13.2\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "\n",
        "import os\n",
        "\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "\n",
        "!pip3 install requests\n",
        "import requests\n",
        "\n",
        "!pip3 install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ACGDp-y8w-"
      },
      "source": [
        "#3 Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m6LMQ0gazNjP"
      },
      "outputs": [],
      "source": [
        "# Set the base paths\n",
        "\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "CREDENTIALS_PATH = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpG0Jg0UrrFy",
        "outputId": "62598d13-1817-4588-c2bd-4ead27325cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 5800\n",
            "Processing additional 200 threads\n",
            "Ending with thread_id 6000\n"
          ]
        }
      ],
      "source": [
        "# Create URLs from the thread_ids and save to a CSV\n",
        "def create_urls():\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_thread_ids.csv'\n",
        "\n",
        "    # Set the number of incremental thread_ids to process\n",
        "    threads = 200\n",
        "\n",
        "    # Check if the file exists and has content. If it does, update last_thread_id\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = e9_forum_thread_ids['thread_id'].iloc[-1]\n",
        "        last_thread_id = int(last_thread_id)  # Convert to integer\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "\n",
        "    urls = []\n",
        "    for thread_id in range(last_thread_id + 1, last_thread_id + threads + 1):\n",
        "        urls.append({'thread_id': thread_id})\n",
        "\n",
        "    last_thread_id_processed = urls[-1]['thread_id']\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    e9_forum_thread_ids = pd.DataFrame(urls)\n",
        "\n",
        "    # Save DataFrame to CSV file\n",
        "    e9_forum_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    print(\"Starting with thread_id \" + str(last_thread_id))\n",
        "    print(\"Processing additional \" + str(threads) + \" threads\")\n",
        "    print(\"Ending with thread_id \" + str(last_thread_id_processed))\n",
        "\n",
        "    return last_thread_id, last_thread_id_processed, e9_forum_thread_ids\n",
        "\n",
        "# Ingest thread_ids and return title, id and URL\n",
        "def fetch_thread_data(df):\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_threads.csv'\n",
        "\n",
        "    # Set the number of pages to process\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "            df.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Find the first post in the thread creation\n",
        "def fetch_first_post_content(df):\n",
        "    # Define the file path inside the function using the correct base path\n",
        "    file_path = BASE_PATH + 'e9_forum_threads_decorated.csv'\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        if first_post:\n",
        "            post_content = first_post.get_text(strip=True)\n",
        "        else:\n",
        "            post_content = \"No content found\"  # Handle case where no post content is found\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_first_post': post_content})\n",
        "\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    e9_forum_threads_decorated = pd.DataFrame(data)\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_threads_decorated.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    return e9_forum_threads_decorated\n",
        "\n",
        "\n",
        "# Original UDF to fetch and parse thread posts\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "            post_data.append({\n",
        "                'thread_id': row['thread_id'],\n",
        "                'post_timestamp': post_timestamp,\n",
        "                'post_raw': content\n",
        "            })\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "\n",
        "    e9_forum_posts['post_raw'] = e9_forum_posts['post_raw'].astype(str)\n",
        "\n",
        "    # Define the output path\n",
        "    output_path = os.path.join(BASE_PATH, 'e9_forum_posts.csv')\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_posts.to_csv(output_path, index=False)\n",
        "\n",
        "    return e9_forum_posts\n",
        "\n",
        "\n",
        "# Define the UDF to process the data\n",
        "def create_forum_corpus(e9_forum_posts, e9_forum_threads_decorated):\n",
        "\n",
        "    # Group by THREAD_ID and concatenate the POST_RAW values\n",
        "    aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # Rename the column to indicate that it contains concatenated post content\n",
        "    aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    # Ensure thread_id columns are of type int64\n",
        "    e9_forum_threads_decorated['thread_id'] = e9_forum_threads_decorated['thread_id'].astype('int64')\n",
        "    aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "    # Merge the two DataFrames\n",
        "    e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "    # Define the output path\n",
        "    output_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "\n",
        "    # Export and save result\n",
        "    e9_forum_corpus.to_csv(output_path, index=False)\n",
        "\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    # Execute the function and print results\n",
        "    last_thread_id, last_thread_id_processed, e9_forum_thread_ids = create_urls()\n",
        "\n",
        "    # Fetch thread URLs and title\n",
        "    e9_forum_threads = fetch_thread_data(e9_forum_thread_ids)\n",
        "\n",
        "    # Fetch first post content\n",
        "    e9_forum_threads_decorated = fetch_first_post_content(e9_forum_threads)\n",
        "\n",
        "    # Fetch all thread post content\n",
        "    e9_forum_posts = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "    e9_forum_corpus = create_forum_corpus(e9_forum_posts, e9_forum_threads_decorated)\n",
        "\n",
        "# Ensure the main function is called\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5CZPw7f7aQe"
      },
      "source": [
        "# 4 Data Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AlUH2V2brvbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bcf4fe-6524-4d52-dfe1-6c99fd599258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n",
            "e9_forum_corpus table created successfully.\n",
            "Data inserted into e9_forum_corpus table.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6000 entries, 0 to 5999\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   THREAD_ID          6000 non-null   int16 \n",
            " 1   THREAD_TITLE       6000 non-null   object\n",
            " 2   THREAD_FIRST_POST  5982 non-null   object\n",
            " 3   THREAD_ALL_POSTS   5923 non-null   object\n",
            "dtypes: int16(1), object(3)\n",
            "memory usage: 152.5+ KB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the e9_forum_corpus DataFrame from the CSV file\n",
        "e9_forum_corpus = pd.read_csv(BASE_PATH + 'e9_forum_corpus.csv')\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "def create_db_and_schema(cur):\n",
        "    \"\"\"Create the database and schema in Snowflake.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        print(\"Database and schema created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database and schema: {e}\")\n",
        "\n",
        "def create_table_if_not_exists(cur):\n",
        "    \"\"\"Create the e9_forum_corpus table if it does not exist.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            thread_id NUMBER(38,0),\n",
        "            thread_title VARCHAR(16777216),\n",
        "            thread_first_post VARCHAR(16777216),\n",
        "            thread_all_posts VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table: {e}\")\n",
        "\n",
        "def insert_data_into_table(cur, df):\n",
        "    \"\"\"Insert data from the DataFrame into the e9_forum_corpus table.\"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        row = row.where(pd.notnull(row), None)\n",
        "        insert_command = f\"\"\"\n",
        "        INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "        (thread_id, thread_title, thread_first_post, thread_all_posts)\n",
        "        VALUES (%s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cur.execute(insert_command, (\n",
        "                row['thread_id'], row['thread_title'],\n",
        "                row['thread_first_post'], row['thread_all_posts']\n",
        "            ))\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting data: {e}\")\n",
        "\n",
        "def fetch_data_from_table(cur):\n",
        "    \"\"\"Fetch all data from the e9_forum_corpus table.\"\"\"\n",
        "    query = \"SELECT * FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    cur.execute(query)\n",
        "    return cur.fetch_pandas_all()\n",
        "\n",
        "def main():\n",
        "    # Load Snowflake credentials\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "\n",
        "    # Connect to Snowflake\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create the database, schema, and table if they don't exist\n",
        "    create_db_and_schema(cur)\n",
        "    create_table_if_not_exists(cur)\n",
        "\n",
        "    # Insert data into the table\n",
        "    insert_data_into_table(cur, e9_forum_corpus)\n",
        "    conn.commit()\n",
        "    print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "    # Fetch data from the table\n",
        "    e9_forum_corpus_df = fetch_data_from_table(cur)\n",
        "    e9_forum_corpus_df.info()\n",
        "\n",
        "    # Close cursor and connection\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptt6rzYdbG32"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}