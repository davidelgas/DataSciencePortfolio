{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/NLP_Corpus_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "#1 Project Description\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egcgaDGAzB6r"
      },
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the websiteâ€™s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ff_KyF7fz7"
      },
      "source": [
        "#2 Create Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWO8xyGhyy0O"
      },
      "outputs": [],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "phiTC3nry3T8"
      },
      "outputs": [],
      "source": [
        "#Packages and libraries\n",
        "\n",
        "!pip install snowflake\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import snowflake.connector\n",
        "import concurrent.futures\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Settings ---\n",
        "base_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9'\n",
        "credentials_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ACGDp-y8w-"
      },
      "source": [
        "#3 Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Utility Functions ---\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    if not os.path.exists(path_to_credentials):\n",
        "        raise FileNotFoundError(f\"Credentials file not found: {path_to_credentials}\")\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Skipping invalid line {line_num}: {line}\")\n",
        "    for var in ['USER', 'PASSWORD', 'ACCOUNT']:\n",
        "        if not os.environ.get(var):\n",
        "            raise EnvironmentError(f\"Missing environment variable: {var}\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    try:\n",
        "        conn = snowflake.connector.connect(\n",
        "            user=os.environ.get('USER'),\n",
        "            password=os.environ.get('PASSWORD'),\n",
        "            account=os.environ.get('ACCOUNT')\n",
        "        )\n",
        "        print(f\"Connected to Snowflake account: {os.environ.get('ACCOUNT')}\")\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        raise ConnectionError(f\"Failed to connect to Snowflake: {e}\")\n",
        "\n",
        "def create_db_schema_table(cur):\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "                THREAD_ID NUMBER(38,0) PRIMARY KEY,\n",
        "                THREAD_TITLE STRING,\n",
        "                THREAD_FIRST_POST STRING,\n",
        "                THREAD_ALL_POSTS STRING\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"Database, schema, and table checked/created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database/schema/table: {e}\")\n",
        "\n",
        "def fetch_existing_thread_ids(cur):\n",
        "    query = \"SELECT THREAD_ID FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    try:\n",
        "        cur.execute(query)\n",
        "        result = cur.fetchall()\n",
        "        return set(row[0] for row in result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching existing thread IDs: {e}\")\n",
        "        return set()\n",
        "\n",
        "def insert_missing_data(cur, df, existing_thread_ids):\n",
        "    \"\"\"Insert only new data into Snowflake, skipping already loaded threads.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No data to insert.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Original DataFrame has {len(df)} rows.\")\n",
        "    df.columns = [col.upper() for col in df.columns]\n",
        "\n",
        "    # Filter only missing threads\n",
        "    new_df = df[~df['THREAD_ID'].isin(existing_thread_ids)]\n",
        "    print(f\"{len(new_df)} new threads will be inserted into Snowflake.\")\n",
        "\n",
        "    if new_df.empty:\n",
        "        print(\"No new threads to insert.\")\n",
        "        return\n",
        "\n",
        "    # Replace NaN values with None\n",
        "    new_df = new_df.where(pd.notnull(new_df), None)\n",
        "\n",
        "    insert_query = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS)\n",
        "    VALUES (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list of tuples\n",
        "    rows_to_insert = [\n",
        "        (\n",
        "            row['THREAD_ID'],\n",
        "            row['THREAD_TITLE'],\n",
        "            row['THREAD_FIRST_POST'],\n",
        "            row['THREAD_ALL_POSTS']\n",
        "        )\n",
        "        for _, row in new_df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Batch insert all rows at once\n",
        "    cur.executemany(insert_query, rows_to_insert)\n",
        "\n",
        "def upload_corpus_to_snowflake(base_path: str, credentials_path: str, filename: str):\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Corpus file not found: {file_path}\")\n",
        "\n",
        "    forum_corpus_df = pd.read_csv(file_path)\n",
        "    print(f\"Loaded {len(forum_corpus_df)} rows from {file_path} to upload.\")\n",
        "\n",
        "    load_credentials(credentials_path)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        create_db_schema_table(cur)\n",
        "        existing_thread_ids = fetch_existing_thread_ids(cur)\n",
        "        print(f\"Snowflake already has {len(existing_thread_ids)} threads.\")\n",
        "        insert_missing_data(cur, forum_corpus_df, existing_thread_ids)\n",
        "        conn.commit()\n",
        "        print(f\"Data from {filename} committed successfully.\")\n",
        "\n",
        "        cur.execute(\"SELECT COUNT(*) FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\")\n",
        "        final_count = cur.fetchone()[0]\n",
        "        print(f\"Total threads now in Snowflake: {final_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upload: {e}\")\n",
        "        conn.rollback()\n",
        "        raise e\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n"
      ],
      "metadata": {
        "id": "Ao-68xqOSFXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_e9_corpus = pd.read_csv('/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads_decorated.csv')\n",
        "df_e9_corpus.info()"
      ],
      "metadata": {
        "id": "ZwgVu9XagTnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Data Storage"
      ],
      "metadata": {
        "id": "-xhKA3MiT23t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_urls(base_path: str, filename: str = 'e9_forum_thread_ids.csv', threads: int = 1):\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(existing_ids['thread_id'].iloc[-1])\n",
        "        print(f\"Existing thread_ids found. Last thread_id: {last_thread_id}\")\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "        print(f\"No existing thread_ids. Starting from {last_thread_id}\")\n",
        "\n",
        "    new_ids = [{'thread_id': tid} for tid in range(last_thread_id + 1, last_thread_id + threads + 1)]\n",
        "    new_thread_ids = pd.DataFrame(new_ids)\n",
        "    new_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    print(f\"Added {threads} new thread_ids. Ending at {new_ids[-1]['thread_id']}\")\n",
        "    return new_thread_ids\n",
        "\n",
        "def fetch_full_thread_data(df, base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv'):\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "\n",
        "    existing_posts = pd.read_csv(posts_file) if os.path.exists(posts_file) else pd.DataFrame(columns=['thread_id', 'post_timestamp', 'post_raw'])\n",
        "    existing_decorated = pd.read_csv(decorated_file) if os.path.exists(decorated_file) else pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_first_post'])\n",
        "\n",
        "    existing_thread_ids = set(existing_posts['thread_id'].tolist()) | set(existing_decorated['thread_id'].tolist())\n",
        "    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n",
        "\n",
        "    post_data = []\n",
        "    decorated_data = []\n",
        "\n",
        "    for thread_id in new_threads['thread_id']:\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}/?page=1\"\n",
        "        try:\n",
        "            print(f\"Fetching thread {thread_id}...\")\n",
        "            response = requests.get(thread_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error {response.status_code} fetching {thread_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('article', class_='message--post')\n",
        "            if not articles:\n",
        "                print(f\"No posts found for thread {thread_id}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            title_element = soup.find('title')\n",
        "            thread_title = title_element.get_text().split('|')[0].strip() if title_element else \"No Title\"\n",
        "\n",
        "            first_post_element = soup.find('article', class_='message-body')\n",
        "            first_post = first_post_element.get_text(strip=True) if first_post_element else \"No content\"\n",
        "\n",
        "            decorated_data.append({\n",
        "                'thread_id': thread_id,\n",
        "                'thread_title': thread_title,\n",
        "                'thread_first_post': first_post\n",
        "            })\n",
        "\n",
        "            for article in articles:\n",
        "                timestamp_element = article.find('time')\n",
        "                content_element = article.find('div', class_='bbWrapper')\n",
        "                post_data.append({\n",
        "                    'thread_id': thread_id,\n",
        "                    'post_timestamp': timestamp_element['datetime'] if timestamp_element else \"N/A\",\n",
        "                    'post_raw': content_element.get_text(strip=True) if content_element else \"No content\"\n",
        "                })\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching thread {thread_id}: {e}\")\n",
        "\n",
        "    if post_data:\n",
        "        new_posts_df = pd.DataFrame(post_data)\n",
        "        combined_posts = pd.concat([existing_posts, new_posts_df], ignore_index=True)\n",
        "        combined_posts.to_csv(posts_file, index=False)\n",
        "        print(f\"Saved {len(new_posts_df)} new posts. Total posts: {len(combined_posts)}\")\n",
        "\n",
        "    if decorated_data:\n",
        "        new_decorated_df = pd.DataFrame(decorated_data)\n",
        "        combined_decorated = pd.concat([existing_decorated, new_decorated_df], ignore_index=True)\n",
        "        combined_decorated.to_csv(decorated_file, index=False)\n",
        "        print(f\"Saved {len(new_decorated_df)} new decorated threads. Total threads: {len(combined_decorated)}\")\n",
        "\n",
        "def create_forum_corpus(base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv', corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "    corpus_file = os.path.join(base_path, corpus_filename)\n",
        "\n",
        "    posts_df = pd.read_csv(posts_file)\n",
        "    decorated_df = pd.read_csv(decorated_file)\n",
        "\n",
        "    aggregated = posts_df.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(str(i) for i in x)).reset_index()\n",
        "    aggregated.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    decorated_df['thread_id'] = decorated_df['thread_id'].astype('int64')\n",
        "    aggregated['thread_id'] = aggregated['thread_id'].astype('int64')\n",
        "\n",
        "    decorated_df = decorated_df[decorated_df['thread_id'].isin(aggregated['thread_id'])]\n",
        "\n",
        "    forum_corpus = pd.merge(decorated_df, aggregated, on='thread_id', how='inner')\n",
        "    forum_corpus.to_csv(corpus_file, index=False)\n",
        "    print(f\"Saved corpus with {len(forum_corpus)} threads to {corpus_file}\")\n",
        "\n",
        "    return forum_corpus\n",
        "\n",
        "def update_local_corpus(base_path: str, threads_to_add: int = 5, corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    print(\"\\n=== Starting Local Forum Corpus Update ===\\n\")\n",
        "    new_thread_ids = create_urls(base_path, threads=threads_to_add)\n",
        "    fetch_full_thread_data(new_thread_ids, base_path)\n",
        "    forum_corpus_df = create_forum_corpus(base_path, corpus_filename=corpus_filename)\n",
        "    print(\"\\n=== Local Forum Corpus Update Complete ===\\n\")\n",
        "    return forum_corpus_df\n"
      ],
      "metadata": {
        "id": "yGEWd5gEHxlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Orchestration"
      ],
      "metadata": {
        "id": "CCuARH_Thu_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== START BATCH SCRAPE + BACKGROUND UPLOAD ======\n",
        "\n",
        "num_batches = 10\n",
        "threads_per_batch = 10\n",
        "max_workers_upload = 3\n",
        "\n",
        "executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_upload)\n",
        "\n",
        "for batch_num in range(num_batches):\n",
        "    print(f\"\\n=== Starting batch {batch_num + 1} ===\\n\")\n",
        "\n",
        "    batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n",
        "    forum_corpus_df = update_local_corpus(base_path, threads_to_add=threads_per_batch, corpus_filename=batch_filename)\n",
        "\n",
        "    future = executor.submit(upload_corpus_to_snowflake, base_path, credentials_path, batch_filename)\n",
        "\n",
        "    def handle_upload_result(fut):\n",
        "        try:\n",
        "            fut.result()\n",
        "        except Exception as e:\n",
        "            print(f\"UPLOAD FAILED for {batch_filename}: {e}\")\n",
        "\n",
        "    future.add_done_callback(handle_upload_result)\n",
        "\n",
        "executor.shutdown(wait=True)\n",
        "print(\"\\n=== All scraping and uploads complete ===\\n\")\n"
      ],
      "metadata": {
        "id": "I5QtR4GTSFd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CExjvLzMSFic"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}