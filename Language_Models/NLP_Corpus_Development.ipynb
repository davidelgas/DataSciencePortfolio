{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/NLP_Corpus_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "#1 Project Description\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egcgaDGAzB6r"
      },
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ff_KyF7fz7"
      },
      "source": [
        "#2 Create Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWO8xyGhyy0O",
        "outputId": "0efd5c3d-9a0d-4a4b-a13d-c77f21778118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "phiTC3nry3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5b8a80-e13d-49ce-9760-e61b863f7202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake\n",
            "  Downloading snowflake-1.4.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting snowflake-core==1.4.0 (from snowflake)\n",
            "  Downloading snowflake_core-1.4.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting snowflake-legacy (from snowflake)\n",
            "  Downloading snowflake_legacy-1.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.32.3)\n",
            "Collecting snowflake-connector-python (from snowflake-core==1.4.0->snowflake)\n",
            "  Downloading snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m596.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->snowflake-core==1.4.0->snowflake) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (2025.1.31)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting boto3>=1.24 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading boto3-1.38.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore>=1.24 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading botocore-1.38.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (4.3.7)\n",
            "Collecting tomlkit (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n",
            "  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.22)\n",
            "Downloading snowflake-1.4.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading snowflake_core-1.4.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowflake_legacy-1.0.0-py3-none-any.whl (3.1 kB)\n",
            "Downloading snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.5-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, tomlkit, snowflake-legacy, jmespath, botocore, s3transfer, boto3, snowflake-connector-python, snowflake-core, snowflake\n",
            "Successfully installed asn1crypto-1.5.1 boto3-1.38.5 botocore-1.38.5 jmespath-1.0.1 s3transfer-0.12.0 snowflake-1.4.0 snowflake-connector-python-3.15.0 snowflake-core-1.4.0 snowflake-legacy-1.0.0 tomlkit-0.13.2\n"
          ]
        }
      ],
      "source": [
        "#Packages and libraries\n",
        "\n",
        "!pip install snowflake\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import snowflake.connector\n",
        "import concurrent.futures\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Settings ---\n",
        "base_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9'\n",
        "credentials_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ACGDp-y8w-"
      },
      "source": [
        "#3 Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Utility Functions ---\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    if not os.path.exists(path_to_credentials):\n",
        "        raise FileNotFoundError(f\"Credentials file not found: {path_to_credentials}\")\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Skipping invalid line {line_num}: {line}\")\n",
        "    for var in ['USER', 'PASSWORD', 'ACCOUNT']:\n",
        "        if not os.environ.get(var):\n",
        "            raise EnvironmentError(f\"Missing environment variable: {var}\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    try:\n",
        "        conn = snowflake.connector.connect(\n",
        "            user=os.environ.get('USER'),\n",
        "            password=os.environ.get('PASSWORD'),\n",
        "            account=os.environ.get('ACCOUNT')\n",
        "        )\n",
        "        print(f\"Connected to Snowflake account: {os.environ.get('ACCOUNT')}\")\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        raise ConnectionError(f\"Failed to connect to Snowflake: {e}\")\n",
        "\n",
        "def create_db_schema_table(cur):\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "                THREAD_ID NUMBER(38,0) PRIMARY KEY,\n",
        "                THREAD_TITLE STRING,\n",
        "                THREAD_FIRST_POST STRING,\n",
        "                THREAD_ALL_POSTS STRING\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"Database, schema, and table checked/created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database/schema/table: {e}\")\n",
        "\n",
        "def fetch_existing_thread_ids(cur):\n",
        "    query = \"SELECT THREAD_ID FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    try:\n",
        "        cur.execute(query)\n",
        "        result = cur.fetchall()\n",
        "        return set(row[0] for row in result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching existing thread IDs: {e}\")\n",
        "        return set()\n",
        "\n",
        "def insert_missing_data(cur, df, existing_thread_ids):\n",
        "    \"\"\"Insert only new data into Snowflake, skipping already loaded threads.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No data to insert.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Original DataFrame has {len(df)} rows.\")\n",
        "    df.columns = [col.upper() for col in df.columns]\n",
        "\n",
        "    # Filter only missing threads\n",
        "    new_df = df[~df['THREAD_ID'].isin(existing_thread_ids)]\n",
        "    print(f\"{len(new_df)} new threads will be inserted into Snowflake.\")\n",
        "\n",
        "    if new_df.empty:\n",
        "        print(\"No new threads to insert.\")\n",
        "        return\n",
        "\n",
        "    # Replace NaN values with None\n",
        "    new_df = new_df.where(pd.notnull(new_df), None)\n",
        "\n",
        "    insert_query = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS)\n",
        "    VALUES (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list of tuples\n",
        "    rows_to_insert = [\n",
        "        (\n",
        "            row['THREAD_ID'],\n",
        "            row['THREAD_TITLE'],\n",
        "            row['THREAD_FIRST_POST'],\n",
        "            row['THREAD_ALL_POSTS']\n",
        "        )\n",
        "        for _, row in new_df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Batch insert all rows at once\n",
        "    cur.executemany(insert_query, rows_to_insert)\n",
        "\n",
        "def upload_corpus_to_snowflake(base_path: str, credentials_path: str, filename: str):\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Corpus file not found: {file_path}\")\n",
        "\n",
        "    forum_corpus_df = pd.read_csv(file_path)\n",
        "    print(f\"Loaded {len(forum_corpus_df)} rows from {file_path} to upload.\")\n",
        "\n",
        "    load_credentials(credentials_path)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        create_db_schema_table(cur)\n",
        "        existing_thread_ids = fetch_existing_thread_ids(cur)\n",
        "        print(f\"Snowflake already has {len(existing_thread_ids)} threads.\")\n",
        "        insert_missing_data(cur, forum_corpus_df, existing_thread_ids)\n",
        "        conn.commit()\n",
        "        print(f\"Data from {filename} committed successfully.\")\n",
        "\n",
        "        cur.execute(\"SELECT COUNT(*) FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\")\n",
        "        final_count = cur.fetchone()[0]\n",
        "        print(f\"Total threads now in Snowflake: {final_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upload: {e}\")\n",
        "        conn.rollback()\n",
        "        raise e\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n"
      ],
      "metadata": {
        "id": "Ao-68xqOSFXH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_e9_corpus = pd.read_csv('/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads_decorated.csv')\n",
        "df_e9_corpus.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwgVu9XagTnQ",
        "outputId": "af70aca8-188a-40e8-bebd-684873a3fc0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3407 entries, 0 to 3406\n",
            "Data columns (total 3 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   thread_id          3407 non-null   int64 \n",
            " 1   thread_title       3407 non-null   object\n",
            " 2   thread_first_post  3399 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 80.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Data Storage"
      ],
      "metadata": {
        "id": "-xhKA3MiT23t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_urls(base_path: str, filename: str = 'e9_forum_thread_ids.csv', threads: int = 1):\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(existing_ids['thread_id'].iloc[-1])\n",
        "        print(f\"Existing thread_ids found. Last thread_id: {last_thread_id}\")\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "        print(f\"No existing thread_ids. Starting from {last_thread_id}\")\n",
        "\n",
        "    new_ids = [{'thread_id': tid} for tid in range(last_thread_id + 1, last_thread_id + threads + 1)]\n",
        "    new_thread_ids = pd.DataFrame(new_ids)\n",
        "    new_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    print(f\"Added {threads} new thread_ids. Ending at {new_ids[-1]['thread_id']}\")\n",
        "    return new_thread_ids\n",
        "\n",
        "def fetch_full_thread_data(df, base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv'):\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "\n",
        "    existing_posts = pd.read_csv(posts_file) if os.path.exists(posts_file) else pd.DataFrame(columns=['thread_id', 'post_timestamp', 'post_raw'])\n",
        "    existing_decorated = pd.read_csv(decorated_file) if os.path.exists(decorated_file) else pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_first_post'])\n",
        "\n",
        "    existing_thread_ids = set(existing_posts['thread_id'].tolist()) | set(existing_decorated['thread_id'].tolist())\n",
        "    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n",
        "\n",
        "    post_data = []\n",
        "    decorated_data = []\n",
        "\n",
        "    for thread_id in new_threads['thread_id']:\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}/?page=1\"\n",
        "        try:\n",
        "            print(f\"Fetching thread {thread_id}...\")\n",
        "            response = requests.get(thread_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error {response.status_code} fetching {thread_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('article', class_='message--post')\n",
        "            if not articles:\n",
        "                print(f\"No posts found for thread {thread_id}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            title_element = soup.find('title')\n",
        "            thread_title = title_element.get_text().split('|')[0].strip() if title_element else \"No Title\"\n",
        "\n",
        "            first_post_element = soup.find('article', class_='message-body')\n",
        "            first_post = first_post_element.get_text(strip=True) if first_post_element else \"No content\"\n",
        "\n",
        "            decorated_data.append({\n",
        "                'thread_id': thread_id,\n",
        "                'thread_title': thread_title,\n",
        "                'thread_first_post': first_post\n",
        "            })\n",
        "\n",
        "            for article in articles:\n",
        "                timestamp_element = article.find('time')\n",
        "                content_element = article.find('div', class_='bbWrapper')\n",
        "                post_data.append({\n",
        "                    'thread_id': thread_id,\n",
        "                    'post_timestamp': timestamp_element['datetime'] if timestamp_element else \"N/A\",\n",
        "                    'post_raw': content_element.get_text(strip=True) if content_element else \"No content\"\n",
        "                })\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching thread {thread_id}: {e}\")\n",
        "\n",
        "    if post_data:\n",
        "        new_posts_df = pd.DataFrame(post_data)\n",
        "        combined_posts = pd.concat([existing_posts, new_posts_df], ignore_index=True)\n",
        "        combined_posts.to_csv(posts_file, index=False)\n",
        "        print(f\"Saved {len(new_posts_df)} new posts. Total posts: {len(combined_posts)}\")\n",
        "\n",
        "    if decorated_data:\n",
        "        new_decorated_df = pd.DataFrame(decorated_data)\n",
        "        combined_decorated = pd.concat([existing_decorated, new_decorated_df], ignore_index=True)\n",
        "        combined_decorated.to_csv(decorated_file, index=False)\n",
        "        print(f\"Saved {len(new_decorated_df)} new decorated threads. Total threads: {len(combined_decorated)}\")\n",
        "\n",
        "def create_forum_corpus(base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv', corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "    corpus_file = os.path.join(base_path, corpus_filename)\n",
        "\n",
        "    posts_df = pd.read_csv(posts_file)\n",
        "    decorated_df = pd.read_csv(decorated_file)\n",
        "\n",
        "    aggregated = posts_df.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(str(i) for i in x)).reset_index()\n",
        "    aggregated.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    decorated_df['thread_id'] = decorated_df['thread_id'].astype('int64')\n",
        "    aggregated['thread_id'] = aggregated['thread_id'].astype('int64')\n",
        "\n",
        "    decorated_df = decorated_df[decorated_df['thread_id'].isin(aggregated['thread_id'])]\n",
        "\n",
        "    forum_corpus = pd.merge(decorated_df, aggregated, on='thread_id', how='inner')\n",
        "    forum_corpus.to_csv(corpus_file, index=False)\n",
        "    print(f\"Saved corpus with {len(forum_corpus)} threads to {corpus_file}\")\n",
        "\n",
        "    return forum_corpus\n",
        "\n",
        "def update_local_corpus(base_path: str, threads_to_add: int = 5, corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    print(\"\\n=== Starting Local Forum Corpus Update ===\\n\")\n",
        "    new_thread_ids = create_urls(base_path, threads=threads_to_add)\n",
        "    fetch_full_thread_data(new_thread_ids, base_path)\n",
        "    forum_corpus_df = create_forum_corpus(base_path, corpus_filename=corpus_filename)\n",
        "    print(\"\\n=== Local Forum Corpus Update Complete ===\\n\")\n",
        "    return forum_corpus_df\n"
      ],
      "metadata": {
        "id": "yGEWd5gEHxlZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestration"
      ],
      "metadata": {
        "id": "CCuARH_Thu_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== START BATCH SCRAPE + BACKGROUND UPLOAD ======\n",
        "\n",
        "num_batches = 10\n",
        "threads_per_batch = 10\n",
        "max_workers_upload = 3\n",
        "\n",
        "executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_upload)\n",
        "\n",
        "for batch_num in range(num_batches):\n",
        "    print(f\"\\n=== Starting batch {batch_num + 1} ===\\n\")\n",
        "\n",
        "    batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n",
        "    forum_corpus_df = update_local_corpus(base_path, threads_to_add=threads_per_batch, corpus_filename=batch_filename)\n",
        "\n",
        "    future = executor.submit(upload_corpus_to_snowflake, base_path, credentials_path, batch_filename)\n",
        "\n",
        "    def handle_upload_result(fut):\n",
        "        try:\n",
        "            fut.result()\n",
        "        except Exception as e:\n",
        "            print(f\"UPLOAD FAILED for {batch_filename}: {e}\")\n",
        "\n",
        "    future.add_done_callback(handle_upload_result)\n",
        "\n",
        "executor.shutdown(wait=True)\n",
        "print(\"\\n=== All scraping and uploads complete ===\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5QtR4GTSFd_",
        "outputId": "57f22485-31d0-4c59-eab3-f5265f640e63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting batch 1 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4310\n",
            "Added 10 new thread_ids. Ending at 4320\n",
            "Fetching thread 4311...\n",
            "Fetching thread 4312...\n",
            "Fetching thread 4313...\n",
            "Fetching thread 4314...\n",
            "Fetching thread 4315...\n",
            "Fetching thread 4316...\n",
            "Fetching thread 4317...\n",
            "Fetching thread 4318...\n",
            "Fetching thread 4319...\n",
            "Fetching thread 4320...\n",
            "Saved 56 new posts. Total posts: 23331\n",
            "Saved 10 new decorated threads. Total threads: 4317\n",
            "Saved corpus with 4317 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_1.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 2 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4320\n",
            "Added 10 new thread_ids. Ending at 4330\n",
            "Loaded 4317 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_1.csv to upload.\n",
            "Fetching thread 4321...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Fetching thread 4322...\n",
            "Snowflake already has 4307 threads.\n",
            "Original DataFrame has 4317 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Data from e9_forum_corpus_batch_1.csv committed successfully.\n",
            "Total threads now in Snowflake: 4912\n",
            "Fetching thread 4323...\n",
            "Fetching thread 4324...\n",
            "Fetching thread 4325...\n",
            "Fetching thread 4326...\n",
            "Fetching thread 4327...\n",
            "Fetching thread 4328...\n",
            "Fetching thread 4329...\n",
            "Fetching thread 4330...\n",
            "Saved 70 new posts. Total posts: 23401\n",
            "Saved 10 new decorated threads. Total threads: 4327\n",
            "Saved corpus with 4327 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_2.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 3 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4330\n",
            "Added 10 new thread_ids. Ending at 4340\n",
            "Loaded 4327 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_2.csv to upload.\n",
            "Fetching thread 4331...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4317 threads.\n",
            "Original DataFrame has 4327 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4332...\n",
            "Data from e9_forum_corpus_batch_2.csv committed successfully.\n",
            "Total threads now in Snowflake: 4922\n",
            "Fetching thread 4333...\n",
            "Fetching thread 4334...\n",
            "Fetching thread 4335...\n",
            "Fetching thread 4336...\n",
            "Fetching thread 4337...\n",
            "Fetching thread 4338...\n",
            "Fetching thread 4339...\n",
            "Fetching thread 4340...\n",
            "Saved 73 new posts. Total posts: 23474\n",
            "Saved 10 new decorated threads. Total threads: 4337\n",
            "Saved corpus with 4337 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_3.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 4 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4340\n",
            "Added 10 new thread_ids. Ending at 4350\n",
            "Loaded 4337 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_3.csv to upload.\n",
            "Fetching thread 4341...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Fetching thread 4342...\n",
            "Snowflake already has 4327 threads.\n",
            "Original DataFrame has 4337 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4343...\n",
            "Data from e9_forum_corpus_batch_3.csv committed successfully.\n",
            "Total threads now in Snowflake: 4932\n",
            "Fetching thread 4344...\n",
            "Fetching thread 4345...\n",
            "Fetching thread 4346...\n",
            "Fetching thread 4347...\n",
            "Fetching thread 4348...\n",
            "Fetching thread 4349...\n",
            "Fetching thread 4350...\n",
            "Saved 64 new posts. Total posts: 23538\n",
            "Saved 10 new decorated threads. Total threads: 4347\n",
            "Saved corpus with 4347 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_4.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 5 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4350\n",
            "Added 10 new thread_ids. Ending at 4360\n",
            "Loaded 4347 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_4.csv to upload.\n",
            "Fetching thread 4351...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4337 threads.\n",
            "Original DataFrame has 4347 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4352...\n",
            "Data from e9_forum_corpus_batch_4.csv committed successfully.\n",
            "Total threads now in Snowflake: 4942\n",
            "Fetching thread 4353...\n",
            "Fetching thread 4354...\n",
            "Fetching thread 4355...\n",
            "Fetching thread 4356...\n",
            "Fetching thread 4357...\n",
            "Fetching thread 4358...\n",
            "Fetching thread 4359...\n",
            "Fetching thread 4360...\n",
            "Saved 98 new posts. Total posts: 23636\n",
            "Saved 10 new decorated threads. Total threads: 4357\n",
            "Saved corpus with 4357 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_5.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 6 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4360\n",
            "Added 10 new thread_ids. Ending at 4370\n",
            "Loaded 4357 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_5.csv to upload.\n",
            "Fetching thread 4361...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4347 threads.\n",
            "Original DataFrame has 4357 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4362...\n",
            "Data from e9_forum_corpus_batch_5.csv committed successfully.\n",
            "Total threads now in Snowflake: 4952\n",
            "Fetching thread 4363...\n",
            "Fetching thread 4364...\n",
            "Fetching thread 4365...\n",
            "Fetching thread 4366...\n",
            "Fetching thread 4367...\n",
            "Fetching thread 4368...\n",
            "Fetching thread 4369...\n",
            "Fetching thread 4370...\n",
            "Saved 112 new posts. Total posts: 23748\n",
            "Saved 10 new decorated threads. Total threads: 4367\n",
            "Saved corpus with 4367 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_6.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 7 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4370\n",
            "Added 10 new thread_ids. Ending at 4380\n",
            "Loaded 4367 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_6.csv to upload.\n",
            "Fetching thread 4371...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4357 threads.\n",
            "Original DataFrame has 4367 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4372...\n",
            "Data from e9_forum_corpus_batch_6.csv committed successfully.\n",
            "Total threads now in Snowflake: 4962\n",
            "Fetching thread 4373...\n",
            "Fetching thread 4374...\n",
            "Fetching thread 4375...\n",
            "Fetching thread 4376...\n",
            "Fetching thread 4377...\n",
            "Fetching thread 4378...\n",
            "Fetching thread 4379...\n",
            "Fetching thread 4380...\n",
            "Saved 96 new posts. Total posts: 23844\n",
            "Saved 10 new decorated threads. Total threads: 4377\n",
            "Saved corpus with 4377 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_7.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 8 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4380\n",
            "Added 10 new thread_ids. Ending at 4390\n",
            "Loaded 4377 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_7.csv to upload.\n",
            "Fetching thread 4381...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4367 threads.\n",
            "Original DataFrame has 4377 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4382...\n",
            "Data from e9_forum_corpus_batch_7.csv committed successfully.\n",
            "Total threads now in Snowflake: 4972\n",
            "Fetching thread 4383...\n",
            "Fetching thread 4384...\n",
            "Fetching thread 4385...\n",
            "Fetching thread 4386...\n",
            "Fetching thread 4387...\n",
            "Fetching thread 4388...\n",
            "Fetching thread 4389...\n",
            "Fetching thread 4390...\n",
            "Saved 64 new posts. Total posts: 23908\n",
            "Saved 10 new decorated threads. Total threads: 4387\n",
            "Saved corpus with 4387 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_8.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 9 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4390\n",
            "Added 10 new thread_ids. Ending at 4400\n",
            "Loaded 4387 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_8.csv to upload.\n",
            "Fetching thread 4391...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4377 threads.\n",
            "Original DataFrame has 4387 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4392...\n",
            "Data from e9_forum_corpus_batch_8.csv committed successfully.\n",
            "Total threads now in Snowflake: 4982\n",
            "Fetching thread 4393...\n",
            "Fetching thread 4394...\n",
            "Fetching thread 4395...\n",
            "Fetching thread 4396...\n",
            "Fetching thread 4397...\n",
            "Fetching thread 4398...\n",
            "Fetching thread 4399...\n",
            "Fetching thread 4400...\n",
            "Saved 62 new posts. Total posts: 23970\n",
            "Saved 10 new decorated threads. Total threads: 4397\n",
            "Saved corpus with 4397 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_9.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 10 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 4400\n",
            "Added 10 new thread_ids. Ending at 4410\n",
            "Loaded 4397 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_9.csv to upload.\n",
            "Fetching thread 4401...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4387 threads.\n",
            "Original DataFrame has 4397 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Fetching thread 4402...\n",
            "Data from e9_forum_corpus_batch_9.csv committed successfully.\n",
            "Total threads now in Snowflake: 4992\n",
            "Fetching thread 4403...\n",
            "Fetching thread 4404...\n",
            "Fetching thread 4405...\n",
            "Fetching thread 4406...\n",
            "Fetching thread 4407...\n",
            "Fetching thread 4408...\n",
            "Fetching thread 4409...\n",
            "Fetching thread 4410...\n",
            "Saved 94 new posts. Total posts: 24064\n",
            "Saved 10 new decorated threads. Total threads: 4407\n",
            "Saved corpus with 4407 threads to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_10.csv\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "Loaded 4407 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_10.csv to upload.\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 4397 threads.\n",
            "Original DataFrame has 4407 rows.\n",
            "10 new threads will be inserted into Snowflake.\n",
            "Data from e9_forum_corpus_batch_10.csv committed successfully.\n",
            "Total threads now in Snowflake: 5002\n",
            "\n",
            "=== All scraping and uploads complete ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WesUPIvGVzM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8Esmk8kVzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxGH633dVzRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mv-ONhekVzTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cio58OL0VzWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_XZcMfOVzYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-M_x7oDVza8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B1EeBDYYVzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1-c841nVzfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nY0lsV4sVzh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOXFNw2rVzkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CExjvLzMSFic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLDER CODE"
      ],
      "metadata": {
        "id": "hi4L8mm5QA9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PLmvGxWEP9-L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOwqhvpgPpTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6LMQ0gazNjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a3bea-6656-40a5-9f7e-52f3d94adfe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base path set to: /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9\n",
            "Credentials path set to: /content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set base paths\n",
        "base_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9'\n",
        "credentials_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n",
        "\n",
        "# Make sure base path exists\n",
        "if not os.path.exists(base_path):\n",
        "    raise FileNotFoundError(f\"Base path does not exist: {base_path}\")\n",
        "\n",
        "if not os.path.exists(credentials_path):\n",
        "    raise FileNotFoundError(f\"Credentials file does not exist: {credentials_path}\")\n",
        "\n",
        "print(f\"Base path set to: {base_path}\")\n",
        "print(f\"Credentials path set to: {credentials_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def create_urls(base_path: str, filename: str = 'e9_forum_thread_ids.csv', threads: int = 1):\n",
        "    \"\"\"\n",
        "    Create thread_id entries for URLs and append them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        base_path (str): Directory where the CSV file is located.\n",
        "        filename (str): Name of the CSV file. Defaults to 'e9_forum_thread_ids.csv'.\n",
        "        threads (int): Number of new thread_ids to add. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        last_thread_id (int): The last existing thread_id before adding new ones.\n",
        "        last_thread_id_processed (int): The last thread_id after adding new ones.\n",
        "        new_thread_ids (DataFrame): DataFrame containing the newly added thread_ids.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "\n",
        "    # Check for existing file and get the last thread_id\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(existing_ids['thread_id'].iloc[-1])\n",
        "        print(f\"Existing file found. Last thread_id: {last_thread_id}\")\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "        print(f\"No existing file found. Starting from thread_id: {last_thread_id}\")\n",
        "\n",
        "    # Generate new thread_ids\n",
        "    new_ids = [{'thread_id': tid} for tid in range(last_thread_id + 1, last_thread_id + threads + 1)]\n",
        "\n",
        "    new_thread_ids = pd.DataFrame(new_ids)\n",
        "\n",
        "    # Append new thread_ids to the CSV\n",
        "    new_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    # Info messages\n",
        "    print(f\"Adding {threads} new threads.\")\n",
        "    print(f\"Ending at thread_id: {new_ids[-1]['thread_id']}\")\n",
        "\n",
        "    return last_thread_id, new_ids[-1]['thread_id'], new_thread_ids\n",
        "\n",
        "\n",
        "last_id, last_processed_id, new_threads_df = create_urls(base_path, threads=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8dxSDATEMbl",
        "outputId": "7e5306fa-188e-4609-d257-438fd8ea6811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing file found. Last thread_id: 5\n",
            "Adding 5 new threads.\n",
            "Ending at thread_id: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_thread_data(df, base_path: str, filename: str = 'e9_forum_threads.csv'):\n",
        "    \"\"\"\n",
        "    Ingest a DataFrame of thread_ids and fetch thread titles and URLs.\n",
        "    Saves results to a CSV file, ensuring only unique thread_ids are added.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame containing thread_ids to process.\n",
        "        base_path (str): Directory where the output CSV is located.\n",
        "        filename (str): Name of the output CSV file. Defaults to 'e9_forum_threads.csv'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Updated DataFrame with all thread data (old + new).\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "\n",
        "    # Load existing data if it exists\n",
        "    existing_thread_ids = set()\n",
        "    last_thread_id = 0\n",
        "\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        try:\n",
        "            existing_df = pd.read_csv(file_path)\n",
        "            existing_thread_ids = set(existing_df['thread_id'].tolist())\n",
        "            if not existing_df.empty:\n",
        "                last_thread_id = max(existing_df['thread_id'])\n",
        "            print(f\"Loaded {len(existing_thread_ids)} existing thread IDs from {file_path}\")\n",
        "            print(f\"Last thread ID in existing file: {last_thread_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load existing data: {e}\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_url'])\n",
        "\n",
        "    # Identify new thread IDs to process\n",
        "    new_thread_ids = [thread_id for thread_id in df['thread_id'] if thread_id not in existing_thread_ids]\n",
        "\n",
        "    print(f\"Found {len(new_thread_ids)} new thread IDs to process.\")\n",
        "    if new_thread_ids:\n",
        "        print(f\"New thread IDs: {new_thread_ids}\")\n",
        "\n",
        "    # Process new threads\n",
        "    new_data = []\n",
        "    for thread_id in new_thread_ids:\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        page_url = f\"{thread_url}/?page=1\"  # Only processing page 1\n",
        "\n",
        "        try:\n",
        "            print(f\"Fetching data for thread {thread_id}...\")\n",
        "            response = requests.get(page_url)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Got status code {response.status_code} for {page_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title_element = soup.find('title')\n",
        "\n",
        "            if title_element:\n",
        "                title = title_element.get_text()\n",
        "                thread_title = title.split('|')[0].strip()\n",
        "\n",
        "                new_data.append({\n",
        "                    'thread_id': thread_id,\n",
        "                    'thread_title': thread_title,\n",
        "                    'thread_url': page_url\n",
        "                })\n",
        "\n",
        "                print(f\"Found thread {thread_id}: '{thread_title}'\")\n",
        "            else:\n",
        "                print(f\"Warning: No title found for thread {thread_id}\")\n",
        "\n",
        "            # Be nice to server\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing thread {thread_id}: {e}\")\n",
        "\n",
        "    # Save new data\n",
        "    if new_data:\n",
        "        new_df = pd.DataFrame(new_data)\n",
        "\n",
        "        # Append or create the file\n",
        "        if os.path.exists(file_path):\n",
        "            new_df.to_csv(file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            new_df.to_csv(file_path, index=False)\n",
        "\n",
        "        print(f\"Added {len(new_data)} new threads to {file_path}\")\n",
        "        new_ids = [item['thread_id'] for item in new_data]\n",
        "        print(f\"New thread IDs added: {new_ids}\")\n",
        "    else:\n",
        "        print(\"No new threads to add.\")\n",
        "\n",
        "    # Return updated dataset\n",
        "    if os.path.exists(file_path):\n",
        "        return pd.read_csv(file_path)\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_url'])\n",
        "\n",
        "all_threads_df = fetch_thread_data(new_threads_df, base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmYmRhxJHxZf",
        "outputId": "2770ff8b-56bb-4275-a922-edbf81d9cdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 existing thread IDs from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads.csv\n",
            "Last thread ID in existing file: 10\n",
            "Found 0 new thread IDs to process.\n",
            "No new threads to add.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_first_post_content(df, base_path: str, filename: str = 'e9_forum_threads_decorated.csv'):\n",
        "    \"\"\"\n",
        "    Fetch the first post content for each thread and save to a CSV,\n",
        "    ensuring no duplicate processing of already existing threads.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame containing thread_id, thread_url, and thread_title.\n",
        "        base_path (str): Directory where the output CSV will be saved.\n",
        "        filename (str): Name of the output CSV file. Defaults to 'e9_forum_threads_decorated.csv'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Updated DataFrame with thread_id, thread_title, and first post content.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "\n",
        "    # Step 1: Load existing data if available\n",
        "    existing_thread_ids = set()\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_df = pd.read_csv(file_path)\n",
        "        existing_thread_ids = set(existing_df['thread_id'].tolist())\n",
        "        print(f\"Loaded {len(existing_thread_ids)} existing thread IDs from {file_path}\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_first_post'])\n",
        "\n",
        "    # Step 2: Identify new threads to process\n",
        "    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n",
        "\n",
        "    print(f\"Found {len(new_threads)} new threads to fetch first posts.\")\n",
        "\n",
        "    if new_threads.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return existing_df\n",
        "\n",
        "    # Step 3: Fetch first posts\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(new_threads['thread_id'], new_threads['thread_url'], new_threads['thread_title']):\n",
        "        try:\n",
        "            print(f\"Fetching first post for thread {thread_id}...\")\n",
        "            response = requests.get(thread_url)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Got status code {response.status_code} for {thread_url}\")\n",
        "                post_content = \"Failed to fetch content\"\n",
        "            else:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                first_post = soup.find('article', class_='message-body')\n",
        "\n",
        "                if first_post:\n",
        "                    post_content = first_post.get_text(strip=True)\n",
        "                else:\n",
        "                    post_content = \"No content found\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching thread {thread_id}: {e}\")\n",
        "            post_content = \"Error fetching content\"\n",
        "\n",
        "        data.append({\n",
        "            'thread_id': thread_id,\n",
        "            'thread_title': thread_title,\n",
        "            'thread_first_post': post_content\n",
        "        })\n",
        "\n",
        "        # Be kind to the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Step 4: Save combined results\n",
        "    if data:\n",
        "        new_df = pd.DataFrame(data)\n",
        "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "        combined_df.to_csv(file_path, index=False)\n",
        "        print(f\"Saved updated decorated thread data to {file_path}\")\n",
        "    else:\n",
        "        print(\"No new data fetched.\")\n",
        "        combined_df = existing_df\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "decorated_threads_df = fetch_first_post_content(all_threads_df, base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EoH1t3CHxbx",
        "outputId": "71be54e2-15c0-4081-ce62-77ae49da25ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 existing thread IDs from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads_decorated.csv\n",
            "Found 0 new threads to fetch first posts.\n",
            "No new threads to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_and_parse_thread(df, base_path: str, filename: str = 'e9_forum_posts.csv'):\n",
        "    \"\"\"\n",
        "    Fetch all posts from threads that haven't been processed yet and save to a CSV.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame containing thread_id and thread_url.\n",
        "        base_path (str): Directory where the output CSV will be saved.\n",
        "        filename (str): Name of the output CSV file. Defaults to 'e9_forum_posts.csv'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Updated DataFrame with thread_id, post_timestamp, and post_raw content.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "\n",
        "    # Step 1: Load existing data if available\n",
        "    existing_thread_ids = set()\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_posts = pd.read_csv(file_path)\n",
        "        existing_thread_ids = set(existing_posts['thread_id'].tolist())\n",
        "        print(f\"Loaded {len(existing_thread_ids)} existing thread IDs from {file_path}\")\n",
        "    else:\n",
        "        existing_posts = pd.DataFrame(columns=['thread_id', 'post_timestamp', 'post_raw'])\n",
        "\n",
        "    # Step 2: Identify new threads to process\n",
        "    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n",
        "\n",
        "    print(f\"Found {len(new_threads)} new threads to fetch.\")\n",
        "\n",
        "    if new_threads.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return existing_posts  # Just return what already exists\n",
        "\n",
        "    # Step 3: Fetch new thread posts\n",
        "    post_data = []\n",
        "\n",
        "    for index, row in new_threads.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = row['thread_url']\n",
        "\n",
        "        try:\n",
        "            print(f\"Fetching posts for thread {thread_id}...\")\n",
        "            response = requests.get(thread_url)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Status code {response.status_code} for {thread_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('article', class_='message--post')\n",
        "\n",
        "            for article in articles:\n",
        "                timestamp_element = article.find('time')\n",
        "                post_timestamp = timestamp_element['datetime'] if timestamp_element else 'N/A'\n",
        "\n",
        "                content_element = article.find('div', class_='bbWrapper')\n",
        "                post_content = content_element.get_text(strip=True) if content_element else 'No content found'\n",
        "\n",
        "                post_data.append({\n",
        "                    'thread_id': thread_id,\n",
        "                    'post_timestamp': post_timestamp,\n",
        "                    'post_raw': post_content\n",
        "                })\n",
        "\n",
        "            # Be kind to the server\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing thread {thread_id}: {e}\")\n",
        "\n",
        "    # Step 4: Save new posts\n",
        "    if post_data:\n",
        "        new_posts_df = pd.DataFrame(post_data)\n",
        "        new_posts_df['post_raw'] = new_posts_df['post_raw'].astype(str)\n",
        "\n",
        "        # Append to existing posts\n",
        "        combined_posts = pd.concat([existing_posts, new_posts_df], ignore_index=True)\n",
        "        combined_posts.to_csv(file_path, index=False)\n",
        "\n",
        "        print(f\"Added {len(new_posts_df)} new posts. Total posts now: {len(combined_posts)}\")\n",
        "    else:\n",
        "        print(\"No new posts fetched.\")\n",
        "        combined_posts = existing_posts\n",
        "\n",
        "    return combined_posts\n",
        "\n",
        "all_posts_df = fetch_and_parse_thread(all_threads_df, base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsRElx0CHxeR",
        "outputId": "a185aeb1-1c17-4b11-bd9a-f16a7dcf08ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 existing thread IDs from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_posts.csv\n",
            "Found 0 new threads to fetch.\n",
            "No new threads to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def create_forum_corpus(e9_forum_posts, e9_forum_threads_decorated, base_path: str, filename: str = 'e9_forum_corpus.csv'):\n",
        "    \"\"\"\n",
        "    Create a final forum corpus combining thread metadata and all posts.\n",
        "\n",
        "    Args:\n",
        "        e9_forum_posts (DataFrame): DataFrame with all posts (thread_id, post_timestamp, post_raw).\n",
        "        e9_forum_threads_decorated (DataFrame): DataFrame with thread_id, thread_title, and first_post.\n",
        "        base_path (str): Directory where the output CSV will be saved.\n",
        "        filename (str): Name of the output CSV file. Defaults to 'e9_forum_corpus.csv'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Final corpus DataFrame with thread_id, thread_title, first post, and all posts.\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(base_path, filename)\n",
        "\n",
        "    # Group by thread_id and concatenate all posts\n",
        "    aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # Rename column\n",
        "    aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    e9_forum_threads_decorated['thread_id'] = e9_forum_threads_decorated['thread_id'].astype('int64')\n",
        "    aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "    # Merge decorated thread info with all posts\n",
        "    e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "    # Save to CSV\n",
        "    e9_forum_corpus.to_csv(output_path, index=False)\n",
        "\n",
        "    # PRINTS\n",
        "    print(f\"Saved forum corpus to {output_path}\")\n",
        "    print(f\"Total threads in corpus: {len(e9_forum_corpus)}\")\n",
        "\n",
        "    return e9_forum_corpus\n",
        "\n",
        "forum_corpus_df = create_forum_corpus(all_posts_df, decorated_threads_df, base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TBv_8jbHxgv",
        "outputId": "e9bab880-eabd-4897-9765-77ca4654c0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved forum corpus to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus.csv\n",
            "Total threads in corpus: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_local_corpus_and_upload(base_path: str, credentials_path: str, threads_to_add: int = 5):\n",
        "    \"\"\"\n",
        "    Master function to update the local forum corpus and upload to Snowflake.\n",
        "\n",
        "    Args:\n",
        "        base_path (str): Path to the local data folder.\n",
        "        credentials_path (str): Path to Snowflake credentials file.\n",
        "        threads_to_add (int): Number of new thread IDs to add.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting corpus update process ===\\n\")\n",
        "\n",
        "    # Step 1: Create new thread IDs\n",
        "    print(\"Step 1: Creating new thread IDs...\")\n",
        "    last_id, last_processed_id, new_threads_df = create_urls(base_path, threads=threads_to_add)\n",
        "\n",
        "    # Step 2: Fetch basic thread data (titles, URLs)\n",
        "    print(\"\\nStep 2: Fetching thread titles and URLs...\")\n",
        "    all_threads_df = fetch_thread_data(new_threads_df, base_path)\n",
        "\n",
        "    # Step 3: Fetch first post content\n",
        "    print(\"\\nStep 3: Fetching first post content...\")\n",
        "    decorated_threads_df = fetch_first_post_content(all_threads_df, base_path)\n",
        "\n",
        "    # Step 4: Fetch all posts in threads\n",
        "    print(\"\\nStep 4: Fetching all posts...\")\n",
        "    all_posts_df = fetch_and_parse_thread(all_threads_df, base_path)\n",
        "\n",
        "    # Step 5: Build final corpus\n",
        "    print(\"\\nStep 5: Building the final forum corpus...\")\n",
        "    forum_corpus_df = create_forum_corpus(all_posts_df, decorated_threads_df, base_path)\n",
        "\n",
        "    # Step 6: Upload to Snowflake\n",
        "    print(\"\\nStep 6: Uploading forum corpus to Snowflake...\")\n",
        "    upload_corpus_to_snowflake(base_path, credentials_path)\n",
        "\n",
        "    print(\"\\n=== Corpus update and upload complete! ===\\n\")\n"
      ],
      "metadata": {
        "id": "fNjwqUQGHxi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjqjlRTOHxn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAArexisHxqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQC4T1giHxsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92Tcib3zHxur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMlq5b5hHxxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIfEE9vNEMd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bvKrHee9EMgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hepxTHjEMil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVPmdL-gEMlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5CZPw7f7aQe"
      },
      "source": [
        "# 4 Data Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlUH2V2brvbJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b7ca60a4-9e09-4dc8-9dd7-54a5aebb6068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n",
            "e9_forum_corpus table created successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c1fed88e299a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-c1fed88e299a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Insert data into the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0minsert_data_into_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me9_forum_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data inserted into e9_forum_corpus table.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-c1fed88e299a>\u001b[0m in \u001b[0;36minsert_data_into_table\u001b[0;34m(cur, df)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             cur.execute(insert_command, (\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'THREAD_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'THREAD_TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'THREAD_FIRST_POST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'THREAD_ALL_POSTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/cursor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m         self._sfqid = (\n\u001b[1;32m   1018\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"queryId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/cursor.py\u001b[0m in \u001b[0;36m_execute_helper\u001b[0;34m(self, query, timeout, statement_params, binding_params, binding_stage, is_internal, describe_only, _no_results, _is_put_get, _no_retry, dataframe_ast)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             ret = self._connection.cmd_query(\n\u001b[0m\u001b[1;32m    723\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sequence_counter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/connection.py\u001b[0m in \u001b[0;36mcmd_query\u001b[0;34m(self, sql, sequence_counter, request_id, binding_params, binding_stage, is_file_transfer, statement_params, is_internal, describe_only, _no_results, _update_current_object, _no_retry, timeout, dataframe_ast)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0murl_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mREQUEST_ID\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m         ret = self.rest.request(\n\u001b[0m\u001b[1;32m   1589\u001b[0m             \u001b[0;34m\"/queries/v1/query-request?\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/network.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, url, body, method, client, timeout, _no_results, _include_retry_params, _no_retry)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHTTP_HEADER_SERVICE_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             return self._post_request(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/network.py\u001b[0m in \u001b[0;36m_post_request\u001b[0;34m(self, url, headers, body, token, timeout, socket_timeout, _no_results, no_retry, _include_retry_params)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         ret = self.fetch(\n\u001b[0m\u001b[1;32m    760\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/network.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, method, full_url, headers, data, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mretry_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 ret = self._request_exec_wrapper(\n\u001b[0m\u001b[1;32m    875\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/network.py\u001b[0m in \u001b[0;36m_request_exec_wrapper\u001b[0;34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mraise_raw_http_failure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise_raw_http_failure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             return_object = self._request_exec(\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/network.py\u001b[0m in \u001b[0;36m_request_exec\u001b[0;34m(self, session, method, full_url, headers, data, token, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;31m# the response within the time. If not, ConnectReadTimeout or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;31m# ReadTimeout is raised.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             raw_ret = session.request(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    486\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    716\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The read operation timed out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/util/wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[0;34m(sock, timeout)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwait_for_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/util/wait.py\u001b[0m in \u001b[0;36mpoll_wait_for_socket\u001b[0;34m(sock, read, write, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpoll_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_retry_on_intr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_poll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/util/wait.py\u001b[0m in \u001b[0;36m_retry_on_intr\u001b[0;34m(fn, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Modern Python, that retries syscalls by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/vendored/urllib3/util/wait.py\u001b[0m in \u001b[0;36mdo_poll\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpoll_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_retry_on_intr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_poll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snowflake/connector/cursor.py\u001b[0m in \u001b[0;36minterrupt_handler\u001b[0;34m(*_)\u001b[0m\n\u001b[1;32m    709\u001b[0m                         \u001b[0;31m# ignore failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the e9_forum_corpus DataFrame from the CSV file\n",
        "e9_forum_corpus = pd.read_csv(BASE_PATH + 'e9_forum_corpus_dirty.csv')\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "def create_db_and_schema(cur):\n",
        "    \"\"\"Create the database and schema in Snowflake.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        print(\"Database and schema created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database and schema: {e}\")\n",
        "\n",
        "def create_table_if_not_exists(cur):\n",
        "    \"\"\"Create the e9_forum_corpus table if it does not exist.\"\"\"\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            THREAD_ID NUMBER(38,0),\n",
        "            THREAD_TITLE VARCHAR(16777216),\n",
        "            THREAD_FIRST_POST VARCHAR(16777216),\n",
        "            THREAD_ALL_POSTS VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table: {e}\")\n",
        "\n",
        "def insert_data_into_table(cur, df):\n",
        "    \"\"\"Insert data from the DataFrame into the e9_forum_corpus table.\"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        row = row.where(pd.notnull(row), None)\n",
        "        insert_command = f\"\"\"\n",
        "        INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "        (THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS)\n",
        "        VALUES (%s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cur.execute(insert_command, (\n",
        "                row['THREAD_ID'], row['THREAD_TITLE'],\n",
        "                row['THREAD_FIRST_POST'], row['THREAD_ALL_POSTS']\n",
        "            ))\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting data: {e}\")\n",
        "\n",
        "def fetch_data_from_table(cur):\n",
        "    \"\"\"Fetch all data from the e9_forum_corpus table.\"\"\"\n",
        "    query = \"SELECT * FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    cur.execute(query)\n",
        "    return cur.fetch_pandas_all()\n",
        "\n",
        "def main():\n",
        "    # Load Snowflake credentials\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "\n",
        "    # Connect to Snowflake\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create the database, schema, and table if they don't exist\n",
        "    create_db_and_schema(cur)\n",
        "    create_table_if_not_exists(cur)\n",
        "\n",
        "    # Insert data into the table\n",
        "    insert_data_into_table(cur, e9_forum_corpus)\n",
        "    conn.commit()\n",
        "    print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "    # Fetch data from the table\n",
        "    e9_forum_corpus_df = fetch_data_from_table(cur)\n",
        "    e9_forum_corpus_df.info()\n",
        "\n",
        "    # Close cursor and connection\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptt6rzYdbG32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjCi7tgGB-_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1OEZHrNDD9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9-YugRPDEAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6WXZWeqDECn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZcZkQv3DEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2kCKMDRDEHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}