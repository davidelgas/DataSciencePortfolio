{"cells":[{"cell_type":"markdown","metadata":{"id":"b_Ff_KyF7fz7"},"source":["#Create Enviornment"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fWO8xyGhyy0O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8dc17f3-3cbc-41d9-8d6b-8050549792dc","executionInfo":{"status":"ok","timestamp":1747186468421,"user_tz":420,"elapsed":33126,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting snowflake\n","  Downloading snowflake-1.4.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting snowflake-core==1.4.0 (from snowflake)\n","  Downloading snowflake_core-1.4.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting snowflake-legacy (from snowflake)\n","  Downloading snowflake_legacy-1.0.0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.11.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.9.0.post0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.32.3)\n","Collecting snowflake-connector-python (from snowflake-core==1.4.0->snowflake)\n","  Downloading snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.4.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (4.13.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->snowflake-core==1.4.0->snowflake) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (2025.4.26)\n","Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting boto3>=1.24 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading boto3-1.38.15-py3-none-any.whl.metadata (6.6 kB)\n","Collecting botocore>=1.24 (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading botocore-1.38.15-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.17.1)\n","Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (43.0.3)\n","Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2.1)\n","Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.10.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2025.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2)\n","Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (3.18.0)\n","Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.4.0)\n","Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (4.3.8)\n","Collecting tomlkit (from snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.13.0,>=0.12.0 (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake)\n","  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.22)\n","Downloading snowflake-1.4.0-py3-none-any.whl (5.6 kB)\n","Downloading snowflake_core-1.4.0-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading snowflake_legacy-1.0.0-py3-none-any.whl (3.1 kB)\n","Downloading snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.38.15-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.38.15-py3-none-any.whl (13.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: asn1crypto, tomlkit, snowflake-legacy, jmespath, botocore, s3transfer, boto3, snowflake-connector-python, snowflake-core, snowflake\n","Successfully installed asn1crypto-1.5.1 boto3-1.38.15 botocore-1.38.15 jmespath-1.0.1 s3transfer-0.12.0 snowflake-1.4.0 snowflake-connector-python-3.15.0 snowflake-core-1.4.0 snowflake-legacy-1.0.0 tomlkit-0.13.2\n"]}],"source":["# Access to Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install snowflake\n","import os\n","import time\n","import requests\n","import pandas as pd\n","import concurrent.futures\n","import snowflake.connector\n","import concurrent.futures\n","import json\n","\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","\n","BASE_PATH = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/'"]},{"cell_type":"markdown","metadata":{"id":"t_ACGDp-y8w-"},"source":["#Fetch Data\n"]},{"cell_type":"code","source":["# Fetch data from forum\n","def read_csv_safely(file_path):\n","    \"\"\"\n","    Read a CSV file with error handling for malformed data.\n","    \"\"\"\n","    try:\n","        # First attempt with normal settings\n","        return pd.read_csv(file_path)\n","    except pd.errors.ParserError:\n","        print(f\"CSV parsing error detected in {file_path}. Attempting to read with error handling...\")\n","        try:\n","            # For newer pandas versions, use integer for quoting\n","            return pd.read_csv(file_path,\n","                              on_bad_lines='skip',\n","                              quoting=3,  # 3 is QUOTE_NONE in pandas\n","                              escapechar='\\\\')\n","        except (TypeError, AttributeError):\n","            try:\n","                # Fallback for older pandas versions\n","                return pd.read_csv(file_path,\n","                                  error_bad_lines=False,\n","                                  quoting=3,  # 3 is QUOTE_NONE in pandas\n","                                  escapechar='\\\\')\n","            except Exception as e:\n","                # Last resort if all else fails\n","                print(f\"Advanced CSV handling failed: {e}. Trying simple approach...\")\n","                return pd.read_csv(file_path, dtype=str, engine='python')\n","\n","# --- Data Fetching Functions ---\n","\n","def create_urls(base_path: str, filename: str = 'e9_forum_thread_ids.csv', threads: int = 1):\n","    \"\"\"\n","    Generates and appends new thread IDs to a CSV file for later fetching.\n","\n","    Args:\n","        base_path: Directory to store data files\n","        filename: File to store thread IDs\n","        threads: Number of new thread IDs to create\n","\n","    Returns:\n","        DataFrame with newly created thread IDs\n","    \"\"\"\n","    file_path = os.path.join(base_path, filename)\n","\n","    # Check if thread ID file exists and is non-empty\n","    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n","        existing_ids = pd.read_csv(file_path)\n","        last_thread_id = int(existing_ids['thread_id'].iloc[-1])\n","        print(f\"Existing thread_ids found. Last thread_id: {last_thread_id}\")\n","    else:\n","        last_thread_id = 0\n","        print(f\"No existing thread_ids. Starting from {last_thread_id}\")\n","\n","    # Generate the next sequence of thread IDs\n","    new_ids = [{'thread_id': tid} for tid in range(last_thread_id + 1, last_thread_id + threads + 1)]\n","    new_thread_ids = pd.DataFrame(new_ids)\n","\n","    # Append new IDs to the CSV file\n","    new_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n","    print(f\"Added {threads} new thread_ids. Ending at {new_ids[-1]['thread_id']}\")\n","    return new_thread_ids\n","\n","\n","def fetch_full_thread_data(df, base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv'):\n","    \"\"\"\n","    Downloads forum thread HTML data, parses content, and extracts post metadata.\n","\n","    Args:\n","        df: DataFrame with thread_id column\n","        base_path: Directory to store output files\n","        posts_filename: File to store individual post content\n","        decorated_filename: File to store thread metadata (title + first post)\n","    \"\"\"\n","    os.makedirs(base_path, exist_ok=True)  # Ensure directory exists\n","\n","    posts_file = os.path.join(base_path, posts_filename)\n","    decorated_file = os.path.join(base_path, decorated_filename)\n","\n","    # Load existing datasets if they exist\n","    existing_posts = pd.read_csv(posts_file) if os.path.exists(posts_file) else pd.DataFrame(columns=['thread_id', 'post_timestamp', 'post_raw'])\n","    existing_decorated = pd.read_csv(decorated_file) if os.path.exists(decorated_file) else pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_first_post'])\n","\n","    # Identify which threads are new and need to be fetched\n","    existing_thread_ids = set(existing_posts['thread_id'].tolist()) | set(existing_decorated['thread_id'].tolist())\n","    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n","\n","    if new_threads.empty:\n","        print(\"No new threads to fetch.\")\n","        return\n","\n","    print(f\"Fetching data for {len(new_threads)} new threads...\")\n","\n","    post_data = []         # List to store individual posts\n","    decorated_data = []    # List to store thread metadata\n","\n","    # Iterate over each thread ID to fetch\n","    for thread_id in new_threads['thread_id']:\n","        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}/?page=1\"\n","        try:\n","            print(f\"Fetching thread {thread_id}...\")\n","            response = requests.get(thread_url)\n","\n","            if response.status_code != 200:\n","                print(f\"Error {response.status_code} fetching {thread_url}\")\n","                continue\n","\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            articles = soup.find_all('article', class_='message--post')\n","\n","            if not articles:\n","                print(f\"No posts found for thread {thread_id}. Skipping.\")\n","                continue\n","\n","            post_count = len(articles)\n","            print(f\"Found {post_count} posts in thread {thread_id}\")\n","\n","            # Extract thread title\n","            title_element = soup.find('title')\n","            thread_title = title_element.get_text().split('|')[0].strip() if title_element else \"No Title\"\n","\n","            # Extract first post (from message body class)\n","            first_post_element = soup.find('article', class_='message-body')\n","            first_post = first_post_element.get_text(strip=True) if first_post_element else \"No content\"\n","\n","            # Save thread-level metadata\n","            decorated_data.append({\n","                'thread_id': thread_id,\n","                'thread_title': thread_title,\n","                'thread_first_post': first_post\n","            })\n","\n","            # Loop through all articles/posts in the thread\n","            for article in articles:\n","                timestamp_element = article.find('time')\n","                content_element = article.find('div', class_='bbWrapper')\n","\n","                post_data.append({\n","                    'thread_id': thread_id,\n","                    'post_timestamp': timestamp_element['datetime'] if timestamp_element else \"N/A\",\n","                    'post_raw': content_element.get_text(strip=True) if content_element else \"No content\"\n","                })\n","\n","            time.sleep(1)  # Throttle requests to avoid hammering the site\n","\n","        except Exception as e:\n","            print(f\"Error fetching thread {thread_id}: {e}\")\n","\n","    # Save post-level data to file\n","    if post_data:\n","        new_posts_df = pd.DataFrame(post_data)\n","        combined_posts = pd.concat([existing_posts, new_posts_df], ignore_index=True)\n","        combined_posts.to_csv(posts_file, index=False)\n","        print(f\"Saved {len(new_posts_df)} new posts. Total posts: {len(combined_posts)}\")\n","\n","    # Save thread-level metadata to file\n","    if decorated_data:\n","        new_decorated_df = pd.DataFrame(decorated_data)\n","        combined_decorated = pd.concat([existing_decorated, new_decorated_df], ignore_index=True)\n","        combined_decorated.to_csv(decorated_file, index=False)\n","        print(f\"Saved {len(new_decorated_df)} new decorated threads. Total threads: {len(combined_decorated)}\")\n","\n","\n","def create_forum_corpus(base_path: str, posts_filename: str = 'e9_forum_posts.csv',\n","                       decorated_filename: str = 'e9_forum_threads_decorated.csv',\n","                       corpus_filename: str = 'e9_forum_corpus.csv',\n","                       append_to_main_corpus: bool = True):\n","    \"\"\"\n","    Merges post data with thread metadata to build a text corpus.\n","\n","    Args:\n","        base_path: Directory containing data files\n","        posts_filename: File containing individual posts\n","        decorated_filename: File containing thread metadata\n","        corpus_filename: Output file for the batch corpus\n","        append_to_main_corpus: Whether to append to the full persistent corpus\n","\n","    Returns:\n","        DataFrame containing the complete corpus\n","    \"\"\"\n","    posts_file = os.path.join(base_path, posts_filename)\n","    decorated_file = os.path.join(base_path, decorated_filename)\n","    corpus_file = os.path.join(base_path, corpus_filename)\n","    main_corpus_file = os.path.join(base_path, 'e9_forum_corpus.csv')\n","\n","    # Check for required inputs\n","    if not os.path.exists(posts_file) or not os.path.exists(decorated_file):\n","        print(f\"ERROR: Required input files not found. Cannot create corpus.\")\n","        if not os.path.exists(posts_file):\n","            print(f\"Missing: {posts_file}\")\n","        if not os.path.exists(decorated_file):\n","            print(f\"Missing: {decorated_file}\")\n","        return pd.DataFrame()\n","\n","    # Load input files safely\n","    print(f\"Reading posts from {posts_file}\")\n","    posts_df = read_csv_safely(posts_file)\n","    print(f\"Reading thread metadata from {decorated_file}\")\n","    decorated_df = read_csv_safely(decorated_file)\n","\n","    print(f\"Found {len(posts_df)} posts across {posts_df['thread_id'].nunique()} threads\")\n","    print(f\"Found {len(decorated_df)} threads with metadata\")\n","\n","    # Aggregate all post text per thread\n","    print(\"Aggregating posts by thread ID...\")\n","    aggregated = posts_df.groupby('thread_id')['post_raw'].agg(\n","        lambda x: ' '.join(str(i) for i in x if pd.notna(i))).reset_index()\n","    aggregated.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n","\n","    # Ensure correct dtype for merging\n","    decorated_df['thread_id'] = decorated_df['thread_id'].astype('int64')\n","    aggregated['thread_id'] = aggregated['thread_id'].astype('int64')\n","\n","    # Keep only threads that have both metadata and posts\n","    common_thread_ids = set(decorated_df['thread_id']) & set(aggregated['thread_id'])\n","    print(f\"Found {len(common_thread_ids)} threads with both metadata and posts\")\n","\n","    filtered_decorated = decorated_df[decorated_df['thread_id'].isin(common_thread_ids)]\n","    filtered_aggregated = aggregated[aggregated['thread_id'].isin(common_thread_ids)]\n","\n","    # Merge into a single corpus\n","    batch_corpus = pd.merge(filtered_decorated, filtered_aggregated, on='thread_id', how='inner')\n","    print(f\"Created corpus with {len(batch_corpus)} threads\")\n","    batch_corpus.to_csv(corpus_file, index=False)\n","    print(f\"Saved batch corpus to {corpus_file}\")\n","\n","    # Optionally update the main corpus\n","    if append_to_main_corpus:\n","        if os.path.exists(main_corpus_file):\n","            try:\n","                # Use safe reading for main corpus\n","                main_corpus = read_csv_safely(main_corpus_file)\n","                print(f\"Loaded existing main corpus with {len(main_corpus)} threads\")\n","\n","                # Only add new threads\n","                existing_main_thread_ids = set(main_corpus['thread_id'].tolist())\n","                new_threads = batch_corpus[~batch_corpus['thread_id'].isin(existing_main_thread_ids)]\n","\n","                if new_threads.empty:\n","                    print(\"No new threads to add to main corpus\")\n","                else:\n","                    combined_corpus = pd.concat([main_corpus, new_threads], ignore_index=True)\n","                    # Try saving the combined corpus\n","                    try:\n","                        combined_corpus.to_csv(main_corpus_file, index=False)\n","                        print(f\"Added {len(new_threads)} new threads to main corpus. Total: {len(combined_corpus)}\")\n","                        return combined_corpus\n","                    except Exception as e:\n","                        print(f\"Error saving combined corpus: {e}\")\n","                        print(\"Continuing with batch file only.\")\n","                        return batch_corpus\n","            except Exception as e:\n","                print(f\"Error updating main corpus: {e}\")\n","                print(\"Continuing with batch file only.\")\n","                return batch_corpus\n","        else:\n","            try:\n","                batch_corpus.to_csv(main_corpus_file, index=False)\n","                print(f\"Created new main corpus with {len(batch_corpus)} threads\")\n","            except Exception as e:\n","                print(f\"Error creating main corpus: {e}\")\n","                print(\"Continuing with batch file only.\")\n","\n","    return batch_corpus\n","\n","def update_local_corpus(base_path: str, threads_to_add: int = 5, corpus_filename: str = 'e9_forum_corpus_batch.csv'):\n","    \"\"\"\n","    Orchestrates fetching new thread data and updating the local corpus.\n","\n","    Args:\n","        base_path: Directory to store all data files\n","        threads_to_add: Number of new threads to fetch\n","        corpus_filename: Filename for the current batch\n","\n","    Returns:\n","        DataFrame of updated corpus\n","    \"\"\"\n","    print(\"\\n=== Starting Local Forum Corpus Update ===\\n\")\n","    os.makedirs(base_path, exist_ok=True)\n","\n","    new_thread_ids = create_urls(base_path, threads=threads_to_add)\n","    fetch_full_thread_data(new_thread_ids, base_path)\n","    forum_corpus_df = create_forum_corpus(base_path, corpus_filename=corpus_filename, append_to_main_corpus=True)\n","\n","    print(\"\\n=== Local Forum Corpus Update Complete ===\\n\")\n","    return forum_corpus_df\n","\n","\n","def create_corpus_backup(base_path: str, corpus_filename: str = 'e9_forum_corpus.csv'):\n","    \"\"\"\n","    Creates a timestamped backup of the current corpus CSV.\n","\n","    Args:\n","        base_path: Directory of corpus file\n","        corpus_filename: Filename to back up\n","    \"\"\"\n","    corpus_path = os.path.join(base_path, corpus_filename)\n","    if not os.path.exists(corpus_path):\n","        print(f\"Corpus file not found: {corpus_path}\")\n","        return\n","\n","    backup_dir = os.path.join(base_path, 'backups')\n","    os.makedirs(backup_dir, exist_ok=True)\n","\n","    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    backup_filename = f\"{os.path.splitext(corpus_filename)[0]}_{timestamp}.csv\"\n","    backup_path = os.path.join(backup_dir, backup_filename)\n","\n","    import shutil\n","    shutil.copy2(corpus_path, backup_path)\n","    print(f\"Created backup: {backup_path}\")\n","\n","\n","def save_corpus_to_json(base_path: str, corpus_filename: str = 'e9_forum_corpus.csv'):\n","    \"\"\"\n","    Converts the CSV corpus into a JSON file for use in downstream applications.\n","\n","    Args:\n","        base_path: Directory containing the CSV\n","        corpus_filename: Input CSV to convert\n","    \"\"\"\n","    corpus_path = os.path.join(base_path, corpus_filename)\n","    if not os.path.exists(corpus_path):\n","        print(f\"Corpus file not found: {corpus_path}\")\n","        return\n","\n","    corpus_df = pd.read_csv(corpus_path)\n","    if corpus_df.empty:\n","        print(\"No data to save.\")\n","        return\n","\n","    json_filename = f\"{os.path.splitext(corpus_filename)[0]}.json\"\n","    json_path = os.path.join(base_path, json_filename)\n","\n","    records = corpus_df.to_dict(orient='records')\n","    with open(json_path, 'w', encoding='utf-8') as f:\n","        json.dump(records, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"Saved corpus to JSON: {json_path} ({len(records)} threads)\")\n","\n","\n","def fetch_forum_data_in_batches(base_path: str, num_batches: int = 2, threads_per_batch: int = 10):\n","    \"\"\"\n","    Automates multiple batch fetches of new thread data and compiles a complete corpus.\n","\n","    Args:\n","        base_path: Directory to store all data\n","        num_batches: Number of batches to run\n","        threads_per_batch: Threads to fetch in each batch\n","\n","    Returns:\n","        Final DataFrame of the complete corpus\n","    \"\"\"\n","    os.makedirs(base_path, exist_ok=True)\n","\n","    main_corpus_path = os.path.join(base_path, 'e9_forum_corpus.csv')\n","    if os.path.exists(main_corpus_path):\n","        create_corpus_backup(base_path)\n","\n","    print(f\"\\n=== Starting Forum Data Fetching: {num_batches} batches, {threads_per_batch} threads per batch ===\\n\")\n","\n","    for batch_num in range(num_batches):\n","        print(f\"\\n=== Processing Batch {batch_num + 1}/{num_batches} ===\\n\")\n","        batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n","        update_local_corpus(base_path, threads_to_add=threads_per_batch, corpus_filename=batch_filename)\n","\n","    save_corpus_to_json(base_path)\n","\n","    if os.path.exists(main_corpus_path):\n","        final_corpus = pd.read_csv(main_corpus_path)\n","        print(f\"\\n=== Forum Data Fetching Complete: {len(final_corpus)} total threads in corpus ===\\n\")\n","        return final_corpus\n","    else:\n","        print(\"\\n=== Forum Data Fetching Complete, but no corpus was created ===\\n\")\n","        return pd.DataFrame()\n"],"metadata":{"id":"yGEWd5gEHxlZ","executionInfo":{"status":"ok","timestamp":1747186476462,"user_tz":420,"elapsed":525,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#Orchestration"],"metadata":{"id":"PPByZqwQwLQ6"}},{"cell_type":"code","source":["# Orchestration Fetch and Storage\n","\n","\n","NUM_BATCHES = 1\n","THREADS_PER_BATCH = 10\n","MAX_WORKERS = 3\n","\n","# Create executor for concurrent processing\n","executor = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS)\n","futures = []\n","\n","# Process each batch\n","for batch_num in range(NUM_BATCHES):\n","    print(f\"\\n=== Starting batch {batch_num + 1} ===\\n\")\n","\n","    # Generate batch filename\n","    batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n","\n","    # Define a function to fetch and save data\n","    def process_batch(base_path, threads_to_add, filename):\n","        try:\n","            # Fetch and save data locally\n","            forum_corpus_df = update_local_corpus(base_path, threads_to_add=threads_to_add, corpus_filename=filename)\n","            print(f\"Batch {filename} completed successfully\")\n","            return forum_corpus_df\n","        except Exception as e:\n","            print(f\"ERROR processing batch {filename}: {e}\")\n","            raise\n","\n","    # Submit the batch processing task to the executor\n","    future = executor.submit(process_batch, BASE_PATH, THREADS_PER_BATCH, batch_filename)\n","\n","    # Add callback for result handling\n","    def create_callback(filename):\n","        def handle_batch_result(fut):\n","            try:\n","                fut.result()  # This will raise any exception that occurred during execution\n","                print(f\"Processing completed for {filename}\")\n","            except Exception as e:\n","                print(f\"PROCESSING FAILED for {filename}: {e}\")\n","        return handle_batch_result\n","\n","    future.add_done_callback(create_callback(batch_filename))\n","    futures.append(future)\n","\n","# Wait for all processing to complete\n","executor.shutdown(wait=True)\n","print(\"\\n=== All scraping and local saving complete ===\\n\")"],"metadata":{"id":"I5QtR4GTSFd_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d27c8c2b-e152-43b7-fabd-d36dcdc4a035","executionInfo":{"status":"ok","timestamp":1747186516694,"user_tz":420,"elapsed":19784,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Starting batch 1 ===\n","\n","\n","=== Starting Local Forum Corpus Update ===\n","\n","Existing thread_ids found. Last thread_id: 15400\n","Added 10 new thread_ids. Ending at 15410\n","Fetching data for 10 new threads...\n","Fetching thread 15401...\n","Found 9 posts in thread 15401\n","Fetching thread 15402...\n","Found 6 posts in thread 15402\n","Fetching thread 15403...\n","Found 5 posts in thread 15403\n","Fetching thread 15404...\n","Found 12 posts in thread 15404\n","Fetching thread 15405...\n","Found 4 posts in thread 15405\n","Fetching thread 15406...\n","Found 1 posts in thread 15406\n","Fetching thread 15407...\n","Found 15 posts in thread 15407\n","Fetching thread 15408...\n","Found 1 posts in thread 15408\n","Fetching thread 15409...\n","Found 1 posts in thread 15409\n","Fetching thread 15410...\n","Found 14 posts in thread 15410\n","Saved 68 new posts. Total posts: 950\n","Saved 10 new decorated threads. Total threads: 143\n","Reading posts from /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/e9_forum_posts.csv\n","Reading thread metadata from /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/e9_forum_threads_decorated.csv\n","Found 950 posts across 143 threads\n","Found 143 threads with metadata\n","Aggregating posts by thread ID...\n","Found 143 threads with both metadata and posts\n","Created corpus with 143 threads\n","Saved batch corpus to /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/e9_forum_corpus_batch_1.csv\n","Loaded existing main corpus with 15116 threads\n","Added 10 new threads to main corpus. Total: 15126\n","\n","=== Local Forum Corpus Update Complete ===\n","\n","Batch e9_forum_corpus_batch_1.csv completed successfully\n","Processing completed for e9_forum_corpus_batch_1.csv\n","\n","=== All scraping and local saving complete ===\n","\n"]}]},{"cell_type":"markdown","source":["# Check files are correctly synched"],"metadata":{"id":"BS2fDXEMSLgu"}},{"cell_type":"code","source":["# File synchcornization\n","\n","def initialize_local_corpus(base_path):\n","    \"\"\"\n","    Initialize or reset the local corpus file structure.\n","    Creates empty files if they don't exist.\n","    \"\"\"\n","    print(\"\\n=== Initializing Local Corpus ===\\n\")\n","\n","    # Paths to files\n","    corpus_file = os.path.join(base_path, 'e9_forum_corpus.csv')\n","    thread_id_file = os.path.join(base_path, 'e9_forum_thread_ids.csv')\n","\n","    # Create backup directory\n","    backup_dir = os.path.join(base_path, 'backups')\n","    os.makedirs(backup_dir, exist_ok=True)\n","\n","    # Create or backup existing corpus file\n","    if os.path.exists(corpus_file):\n","        # Create backup\n","        create_corpus_backup(base_path)\n","        print(f\"Backed up existing corpus file\")\n","    else:\n","        # Create empty corpus file with headers\n","        empty_corpus = pd.DataFrame(columns=[\n","            'thread_id', 'title', 'first_post', 'all_posts',\n","            'scrape_date', 'post_date', 'update_date'\n","        ])\n","        empty_corpus.to_csv(corpus_file, index=False)\n","        print(f\"Created new empty corpus file at {corpus_file}\")\n","\n","    # Initialize thread ID file if it doesn't exist\n","    if not os.path.exists(thread_id_file):\n","        empty_thread_ids = pd.DataFrame({'thread_id': []})\n","        empty_thread_ids.to_csv(thread_id_file, index=False)\n","        print(f\"Created new empty thread ID file at {thread_id_file}\")\n","\n","    return True\n","\n","def sync_thread_ids_with_corpus(base_path: str):\n","    \"\"\"\n","    Synchronize the thread_id tracking file with the main corpus file\n","    \"\"\"\n","    print(\"\\n=== Syncing thread ID tracking file with main corpus ===\\n\")\n","\n","    # Paths to files\n","    thread_id_file = os.path.join(base_path, 'e9_forum_thread_ids.csv')\n","    main_corpus_file = os.path.join(base_path, 'e9_forum_corpus.csv')\n","\n","    if not os.path.exists(main_corpus_file):\n","        print(\"Main corpus file not found. Cannot sync thread IDs.\")\n","        return False\n","\n","    try:\n","        # Read the main corpus\n","        corpus_df = read_csv_safely(main_corpus_file)\n","\n","        if len(corpus_df) == 0:\n","            print(\"Corpus file is empty. Creating empty thread ID file.\")\n","            empty_thread_ids = pd.DataFrame({'thread_id': []})\n","            empty_thread_ids.to_csv(thread_id_file, index=False)\n","            return True\n","\n","        # Get the highest thread ID\n","        thread_id_col = 'thread_id' if 'thread_id' in corpus_df.columns else 'THREAD_ID'\n","        highest_id = corpus_df[thread_id_col].max()\n","\n","        # Create a new thread ID tracking file with all IDs from 1 to highest\n","        all_ids = [{'thread_id': tid} for tid in range(1, highest_id + 1)]\n","        all_thread_ids = pd.DataFrame(all_ids)\n","\n","        # Backup the existing file if it exists\n","        if os.path.exists(thread_id_file):\n","            backup_dir = os.path.join(base_path, 'backups')\n","            os.makedirs(backup_dir, exist_ok=True)\n","            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","            backup_file = os.path.join(backup_dir, f\"e9_forum_thread_ids_{timestamp}.csv\")\n","            import shutil\n","            shutil.copy2(thread_id_file, backup_file)\n","            print(f\"Backed up thread ID file to {backup_file}\")\n","\n","        # Save the new thread ID file\n","        all_thread_ids.to_csv(thread_id_file, index=False)\n","        print(f\"Updated thread ID file with {len(all_ids)} IDs (1 to {highest_id})\")\n","\n","        return True\n","\n","    except Exception as e:\n","        print(f\"Error syncing thread IDs: {e}\")\n","        return False\n","\n","# Initialize the local corpus structure if needed\n","initialize_local_corpus(BASE_PATH)\n","\n","# Then sync the thread ID file with the local corpus\n","sync_thread_ids_with_corpus(BASE_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9W2OuKyPb3I","outputId":"df474d06-6907-462a-bd15-06d79b64c86b","executionInfo":{"status":"ok","timestamp":1747186522576,"user_tz":420,"elapsed":2007,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Initializing Local Corpus ===\n","\n","Created backup: /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/backups/e9_forum_corpus_20250514_013520.csv\n","Backed up existing corpus file\n","\n","=== Syncing thread ID tracking file with main corpus ===\n","\n","Backed up thread ID file to /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Datasets/backups/e9_forum_thread_ids_20250514_013522.csv\n","Updated thread ID file with 15410 IDs (1 to 15410)\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# Export to HTML"],"metadata":{"id":"2jn-PyGxcbfq"}},{"cell_type":"code","source":["# Export notebook as HTML\n","\n","from nbconvert import HTMLExporter\n","import nbformat\n","import codecs\n","import os\n","import copy\n","\n","notebook_path = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Notebooks/LLM_RAG_ELGASDAVID_Corpus.ipynb'\n","html_path = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Notebooks/LLM_RAG_ELGASDAVID_Corpus.html'\n","\n","# Verify the file exists\n","if not os.path.exists(notebook_path):\n","    print(f\"Error: File not found at {notebook_path}\")\n","else:\n","    # Create the HTML exporter with embedded resources\n","    html_exporter = HTMLExporter()\n","\n","    # Configure to embed images, data, and other resources\n","    html_exporter.embed_images = True\n","\n","    # Optional: Use the full template which includes more styling\n","    html_exporter.template_name = 'classic'\n","\n","    # Set config to embed all resources\n","    html_exporter.exclude_input_prompt = False\n","    html_exporter.exclude_output_prompt = False\n","\n","    try:\n","        # Read the notebook\n","        with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n","            notebook_content = nbformat.read(notebook_file, as_version=4)\n","\n","        # Make a deep copy to avoid modifying the original\n","        notebook_copy = copy.deepcopy(notebook_content)\n","\n","        # Remove widget metadata if present\n","        if 'widgets' in notebook_copy.get('metadata', {}):\n","            del notebook_copy['metadata']['widgets']\n","\n","        # Sanitize all cell metadata\n","        for cell in notebook_copy.cells:\n","            if 'metadata' in cell and 'widgets' in cell['metadata']:\n","                del cell['metadata']['widgets']\n","\n","            # Also clean outputs\n","            if cell.get('cell_type') == 'code' and 'outputs' in cell:\n","                for output in cell['outputs']:\n","                    if 'metadata' in output and 'widgets' in output['metadata']:\n","                        del output['metadata']['widgets']\n","\n","        # Convert to HTML with embedded resources\n","        html_data, resources = html_exporter.from_notebook_node(notebook_copy)\n","\n","        # Check if there are resources to embed\n","        if resources and 'outputs' in resources:\n","            print(f\"Found {len(resources['outputs'])} resources to embed\")\n","\n","        # Write the HTML file\n","        with codecs.open(html_path, 'w', encoding='utf-8') as f:\n","            f.write(html_data)\n","\n","        print(f\"HTML file with embedded resources saved to {html_path}\")\n","    except Exception as e:\n","        print(f\"Error during conversion: {e}\")\n","\n","        # Fallback to basic template\n","        try:\n","            print(\"Attempting fallback method with basic template...\")\n","            html_exporter = HTMLExporter(template_name='basic')\n","            html_exporter.embed_images = True  # Still try to embed images in fallback\n","\n","            # Need to reload the notebook for the fallback attempt\n","            with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n","                notebook_content = nbformat.read(notebook_file, as_version=4)\n","\n","            notebook_copy = copy.deepcopy(notebook_content)\n","\n","            # Apply the same widget cleanup\n","            if 'widgets' in notebook_copy.get('metadata', {}):\n","                del notebook_copy['metadata']['widgets']\n","\n","            for cell in notebook_copy.cells:\n","                if 'metadata' in cell and 'widgets' in cell['metadata']:\n","                    del cell['metadata']['widgets']\n","\n","                if cell.get('cell_type') == 'code' and 'outputs' in cell:\n","                    for output in cell['outputs']:\n","                        if 'metadata' in output and 'widgets' in output['metadata']:\n","                            del output['metadata']['widgets']\n","\n","            html_data, resources = html_exporter.from_notebook_node(notebook_copy)\n","\n","            with codecs.open(html_path, 'w', encoding='utf-8') as f:\n","                f.write(html_data)\n","\n","            print(f\"Fallback method: HTML file saved to {html_path}\")\n","        except Exception as e2:\n","            print(f\"Fallback method also failed: {e2}\")"],"metadata":{"id":"AxkoFvpFcctT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9475841f-bc57-470a-d96f-bedc8ea9ae9e","executionInfo":{"status":"ok","timestamp":1747186535521,"user_tz":420,"elapsed":2157,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["HTML file with embedded resources saved to /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Notebooks/LLM_RAG_ELGASDAVID_Corpus.html\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
