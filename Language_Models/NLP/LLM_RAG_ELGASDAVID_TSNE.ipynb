{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Create Enviornment"],"metadata":{"id":"C9Dvxa158fSK"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"xfdBxISr8NCL","executionInfo":{"status":"ok","timestamp":1747262492170,"user_tz":420,"elapsed":20937,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"efa02630-ffb5-4abb-87e5-719f858324f5","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# E9 Forum Corpus t-SNE Visualization - Using corrected columns\n","\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","from tensorboard.plugins import projector\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","import shutil\n","\n","# Download NLTK resources\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('wordnet', quiet=True)\n","print(\"NLTK resources downloaded/verified\")\n","\n","# Load ONLY the main CSV file with a more forgiving parser\n","print(\"Loading CSV file...\")\n","data_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus.csv'\n","df = pd.read_csv(data_path, engine='python', on_bad_lines='skip')\n","print(f\"Successfully loaded {len(df)} rows from {data_path}\")\n","\n","# Display the actual column names for verification\n","print(\"\\nActual column names in the CSV file:\")\n","print(df.columns.tolist())\n","\n","# Verify that we have the expected columns\n","expected_columns = ['thread_id', 'thread_title', 'thread_first_post', 'thread_all_posts']\n","for col in expected_columns:\n","    if col in df.columns:\n","        print(f\"Found expected column: {col}\")\n","    else:\n","        print(f\"Warning: Expected column '{col}' not found in data\")\n","\n","# Basic preprocessing\n","print(\"\\nPreprocessing text...\")\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","stop_words.update(['e9', 'bmw', 'car', 'cars', 'coupe', 'csi', 'cs', 'csl', 'http', 'https',\n","                   'www', 'com', 'ebay', 'post', 'thread', 'forum', 'html'])\n","\n","def preprocess_text(text):\n","    \"\"\"Clean and tokenize text\"\"\"\n","    if not isinstance(text, str):\n","        return []\n","\n","    # Clean text\n","    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n","    text = re.sub(r'<.*?>', '', text) # Remove HTML tags\n","    text = re.sub(r'[^\\w\\s]', ' ', text.lower()) # Remove special chars\n","    text = re.sub(r'\\d+', '', text) # Remove numbers\n","    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n","\n","    # Tokenize and lemmatize\n","    tokens = [lemmatizer.lemmatize(word) for word in text.split()\n","              if word not in stop_words and len(word) > 2]\n","    return tokens\n","\n","# Create combined text field for better embeddings\n","print(\"Creating combined text field...\")\n","# Use the known column names\n","df['combined_text'] = df['thread_title'].fillna('') + ' ' + df['thread_first_post'].fillna('')\n","df['processed'] = df['combined_text'].apply(preprocess_text)\n","\n","# Remove empty documents\n","empty_docs = df['processed'].apply(len) == 0\n","if empty_docs.sum() > 0:\n","    print(f\"Removing {empty_docs.sum()} documents with empty processed text\")\n","    df = df[~empty_docs]\n","\n","print(f\"Final dataset size: {len(df)} documents\")\n","\n","# Create TF-IDF embeddings\n","print(\"\\nCreating TF-IDF document embeddings...\")\n","df['processed_text'] = df['processed'].apply(lambda x: ' '.join(x))\n","max_features = min(3000, len(df))\n","vectorizer = TfidfVectorizer(max_features=max_features)\n","tfidf_matrix = vectorizer.fit_transform(df['processed_text']).toarray()\n","print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n","\n","# Cluster for visualization\n","print(\"\\nClustering documents...\")\n","kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n","clusters = kmeans.fit_predict(tfidf_matrix)\n","df['cluster'] = clusters\n","\n","# Set up TensorBoard directory\n","log_dir = './tensorboard_logs_e9/'\n","if os.path.exists(log_dir):\n","    shutil.rmtree(log_dir)\n","os.makedirs(log_dir)\n","\n","# Save embeddings as a TensorFlow variable\n","print(\"\\nSaving document embeddings for TensorBoard...\")\n","embeddings_var = tf.Variable(tfidf_matrix, name='document_embeddings')\n","checkpoint = tf.train.Checkpoint(embedding=embeddings_var)\n","checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n","\n","# Create metadata file\n","metadata_path = os.path.join(log_dir, 'metadata.tsv')\n","with open(metadata_path, 'w', encoding='utf-8') as f:\n","    f.write('Title\\tCluster\\n')\n","    for i in range(len(df)):\n","        # Use the known column name\n","        safe_title = str(df.iloc[i]['thread_title'])[:150].replace('\\t', ' ').replace('\\n', ' ')\n","        f.write(f\"{safe_title}\\t{clusters[i]}\\n\")\n","\n","# Configure TensorBoard Projector\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n","embedding.metadata_path = 'metadata.tsv'\n","projector.visualize_embeddings(log_dir, config)\n","\n","print(f\"\\nDocument embeddings configuration saved to {log_dir}\")\n","\n","# Load TensorBoard\n","try:\n","    get_ipython().run_line_magic('load_ext', 'tensorboard')\n","    print(\"\\nTensorBoard extension loaded\")\n","except:\n","    print(\"\\nCould not load TensorBoard extension automatically\")\n","\n","# Print instructions\n","print(\"\\n\" + \"=\"*80)\n","print(\"VISUALIZATION INSTRUCTIONS:\")\n","print(\"=\"*80)\n","print(\"\\nTo visualize in TensorBoard, run this command in your notebook:\")\n","print(\"%load_ext tensorboard\")\n","print(f\"%tensorboard --logdir {log_dir}\")\n","print(\"\\nIn TensorBoard Projector, select t-SNE visualization from the left panel\")\n","print(\"Adjust t-SNE parameters as needed:\")\n","print(\"- Perplexity: 5-50 (default 30)\")\n","print(\"- Learning rate: 10-1000 (default 100)\")\n","print(\"- Iterations: 1000+ for better results\")"],"metadata":{"id":"9XVSpRv88OVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"SnT4Ui8Y8loL"}},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir ./tensorboard_logs_e9/"],"metadata":{"id":"MoCZqzqq8Rrl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Export to HTML"],"metadata":{"id":"E4MS0ewv8rJu"}},{"cell_type":"code","source":["from nbconvert import HTMLExporter\n","import nbformat\n","import codecs\n","import os\n","import copy\n","\n","notebook_path = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Notebooks/LLM_RAG_ELGASDAVID_TSNE.ipynb'\n","html_path = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/Notebooks/LLM_RAG_ELGASDAVID_TSNE.html'\n","\n","# Verify the file exists\n","if not os.path.exists(notebook_path):\n","    print(f\"Error: File not found at {notebook_path}\")\n","else:\n","    # Create the HTML exporter with embedded resources\n","    html_exporter = HTMLExporter()\n","\n","    # Configure to embed images, data, and other resources\n","    html_exporter.embed_images = True\n","\n","    # Optional: Use the full template which includes more styling\n","    html_exporter.template_name = 'classic'\n","\n","    # Set config to embed all resources\n","    html_exporter.exclude_input_prompt = False\n","    html_exporter.exclude_output_prompt = False\n","\n","    try:\n","        # Read the notebook\n","        with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n","            notebook_content = nbformat.read(notebook_file, as_version=4)\n","\n","        # Make a deep copy to avoid modifying the original\n","        notebook_copy = copy.deepcopy(notebook_content)\n","\n","        # Remove widget metadata if present\n","        if 'widgets' in notebook_copy.get('metadata', {}):\n","            del notebook_copy['metadata']['widgets']\n","\n","        # Sanitize all cell metadata\n","        for cell in notebook_copy.cells:\n","            if 'metadata' in cell and 'widgets' in cell['metadata']:\n","                del cell['metadata']['widgets']\n","\n","            # Also clean outputs\n","            if cell.get('cell_type') == 'code' and 'outputs' in cell:\n","                for output in cell['outputs']:\n","                    if 'metadata' in output and 'widgets' in output['metadata']:\n","                        del output['metadata']['widgets']\n","\n","        # Convert to HTML with embedded resources\n","        html_data, resources = html_exporter.from_notebook_node(notebook_copy)\n","\n","        # Check if there are resources to embed\n","        if resources and 'outputs' in resources:\n","            print(f\"Found {len(resources['outputs'])} resources to embed\")\n","\n","        # Write the HTML file\n","        with codecs.open(html_path, 'w', encoding='utf-8') as f:\n","            f.write(html_data)\n","\n","        print(f\"HTML file with embedded resources saved to {html_path}\")\n","    except Exception as e:\n","        print(f\"Error during conversion: {e}\")\n","\n","        # Fallback to basic template\n","        try:\n","            print(\"Attempting fallback method with basic template...\")\n","            html_exporter = HTMLExporter(template_name='basic')\n","            html_exporter.embed_images = True  # Still try to embed images in fallback\n","\n","            # Need to reload the notebook for the fallback attempt\n","            with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n","                notebook_content = nbformat.read(notebook_file, as_version=4)\n","\n","            notebook_copy = copy.deepcopy(notebook_content)\n","\n","            # Apply the same widget cleanup\n","            if 'widgets' in notebook_copy.get('metadata', {}):\n","                del notebook_copy['metadata']['widgets']\n","\n","            for cell in notebook_copy.cells:\n","                if 'metadata' in cell and 'widgets' in cell['metadata']:\n","                    del cell['metadata']['widgets']\n","\n","                if cell.get('cell_type') == 'code' and 'outputs' in cell:\n","                    for output in cell['outputs']:\n","                        if 'metadata' in output and 'widgets' in output['metadata']:\n","                            del output['metadata']['widgets']\n","\n","            html_data, resources = html_exporter.from_notebook_node(notebook_copy)\n","\n","            with codecs.open(html_path, 'w', encoding='utf-8') as f:\n","                f.write(html_data)\n","\n","            print(f\"Fallback method: HTML file saved to {html_path}\")\n","        except Exception as e2:\n","            print(f\"Fallback method also failed: {e2}\")"],"metadata":{"id":"-jf0x5Nj8qE2"},"execution_count":null,"outputs":[]}]}
