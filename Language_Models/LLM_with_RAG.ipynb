{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/LLM_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLMB-P30ER2F"
      },
      "source": [
        "# Project Objective and Limitations\n",
        "\n",
        "## i. Project Overview\n",
        "The advent of modern automobile manufacturing has led to increased technical complexity, often resulting in mechanics opting to replace parts rather than diagnose and fix issues. This approach, while convenient for contemporary vehicles, poses a significant challenge for classic cars built 30 to 40 years ago, where replacement parts are scarce or non-existent.\n",
        "\n",
        "To address this problem, this project aims to leverage Generative AI to create a \"virtual mechanic.\" By building a corpus gathered from a classic car forum, this tool will be capable of understanding unstructured questions and providing relevant answers. This solution aims to assist classic car enthusiasts and mechanics by offering expert guidance, thereby preserving the heritage and functionality of vintage automobiles.\n",
        "\n",
        "## ii. Objectives\n",
        "The primary objective of this project is the development of a model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparison and selection of architectures, libraries, and methods.\n",
        "\n",
        "## iii. Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are driven from previous similar questions. Users will see these answers in plain English. I will have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model.\n",
        "\n",
        "## iv. Limitations and Challenges\n",
        "To address budget constraints, a combination of open source and free resources will be used. Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-jOHmOOCkEvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef19c63c-262e-4b28-e6e6-dafaeb2b1f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, uninstall all potentially conflicting packages\n",
        "!pip uninstall -y numpy tensorflow tensorflow-text faiss-cpu transformers snowflake-connector-python torch\n",
        "\n",
        "# Install numpy first with a version known to work with TensorFlow\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "# Now install packages in order\n",
        "!pip install tensorflow==2.12.0  # Older, more stable version\n",
        "!pip install tensorflow-text==2.12.0  # Matching version\n",
        "!pip install faiss-cpu==1.7.4  # Version compatible with numpy 1.23.5\n",
        "!pip install transformers\n",
        "!pip install --no-deps snowflake-connector-python  # Install without dependencies to avoid conflicts\n",
        "!pip install nltk regex\n",
        "!pip install torch\n",
        "\n",
        "import faiss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jYGVANRJqtcL",
        "outputId": "24ab0d87-8edb-4967-8b5e-1d7db7f90b56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping snowflake-connector-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-hub 0.18.1 requires tensorflow-text; platform_system != \"Darwin\", which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, which is not installed.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "peft 0.14.0 requires torch>=1.13.0, which is not installed.\n",
            "peft 0.14.0 requires transformers, which is not installed.\n",
            "timm 1.0.15 requires torch, which is not installed.\n",
            "accelerate 1.5.2 requires torch>=2.0.0, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "bigframes 1.41.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "57a974e9a5d848349a83cf1859644ec9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.12.0\n",
            "Collecting tensorflow-text==2.12.0\n",
            "  Downloading tensorflow_text-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text==2.12.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.12.0) (2.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.1.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.8.0->tensorflow-text==2.12.0)\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text==2.12.0) (3.2.2)\n",
            "Downloading tensorflow_text-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-keras, tensorflow-text\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.18.0\n",
            "    Uninstalling tf_keras-2.18.0:\n",
            "      Successfully uninstalled tf_keras-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-text-2.12.0 tf-keras-2.15.0\n",
            "Collecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\n",
            "peft 0.14.0 requires torch>=1.13.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.50.1\n",
            "Collecting snowflake-connector-python\n",
            "  Using cached snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snowflake-connector-python\n",
            "Successfully installed snowflake-connector-python-3.14.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m769.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/notebooks/'\n",
        "path_to_credentials = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n"
      ],
      "metadata": {
        "id": "UhG5a_ve9W7t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Logging\n",
        "\n",
        "#import logging\n",
        "\n",
        "# Configure logging\n",
        "#log_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/model_logs.txt'\n",
        "#logging.basicConfig(\n",
        "#    filename=log_path,\n",
        "#    level=logging.INFO,\n",
        "#    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "#)\n",
        "\n",
        "# Initial Setup\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "#!pip install --upgrade tensorflow tensorflow-text\n",
        "\n",
        "#import tensorflow as tf\n",
        "#import tensorflow_text as tf_text\n"
      ],
      "metadata": {
        "id": "SuB21f58ptoj",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOUiNzICFDj8"
      },
      "source": [
        "# 1 Architectures and Frameworks\n",
        "\n",
        "This document provides an overview of various architectures, models, and tools used in natural language processing tasks. Understanding the strengths and weaknesses of different approaches is crucial for designing effective NLP systems tailored to my specific use case and requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VkM_EQvV6sw"
      },
      "source": [
        "\n",
        "## 1.1 LLM Architectures\n",
        "\n",
        "### 1.1.1 Traditional Models\n",
        "\n",
        "**Solution:** Bag-of-Words (BoW)  \n",
        "**Description:** Represents text data as a collection of unique words and their frequencies.  \n",
        "**Example:** TfidfVectorizer  \n",
        "**Pros:**  \n",
        "- Simple and efficient representation.\n",
        "- Works well for tasks like sentiment analysis and document classification.\n",
        "\n",
        "**Cons:**  \n",
        "- Ignores word order and context.\n",
        "- Doesn't capture semantic meanings well.  \n",
        "\n",
        "**Solution:** N-gram Model  \n",
        "**Description:** Represents text data as a sequence of N consecutive words (N-grams).  \n",
        "**Examples:** Bigram, Trigram  \n",
        "**Pros:**  \n",
        "- Captures some local word order and context.\n",
        "- Simple and easy to implement.\n",
        "\n",
        "**Cons:**  \n",
        "- Limited in capturing long-range dependencies.\n",
        "- Can become computationally expensive with larger N values.\n",
        "\n",
        "**Solution:** Rule-Based Models  \n",
        "**Description:** Uses a set of manually crafted linguistic rules to process text data.  \n",
        "**Examples:** Regular Expressions, SpaCy Rule-Based Matching  \n",
        "**Pros:**  \n",
        "- High precision for well-defined tasks.\n",
        "- Transparent and interpretable.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires extensive domain knowledge and manual effort.\n",
        "- Not scalable for large or diverse datasets.\n",
        "\n",
        "### 1.1.2 Statistical NLP Models\n",
        "\n",
        "**Solution:** Hidden Markov Models (HMM)  \n",
        "**Description:** Sequential text models based on hidden state transitions.  \n",
        "**Example:** hmmlearn  \n",
        "**Pros:**  \n",
        "- Captures sequential dependencies effectively.\n",
        "- Suitable for tasks like part-of-speech tagging and named entity recognition.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- May struggle with capturing complex semantic relationships.\n",
        "\n",
        "**Solution:** Conditional Random Fields (CRF)  \n",
        "**Description:** Sequence labeling models.  \n",
        "**Example:** sklearn-crfsuite  \n",
        "**Pros:**  \n",
        "- Effective for sequential labeling tasks.\n",
        "- Incorporates feature dependencies between adjacent labels.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- Less effective for capturing long-range dependencies.\n",
        "\n",
        "**Solution:** Support Vector Machines (SVM)  \n",
        "**Description:** A supervised learning model used for classification and regression analysis.  \n",
        "**Example:** scikit-learn  \n",
        "**Pros:**  \n",
        "- Effective in high-dimensional spaces.\n",
        "- Versatile with different kernel functions for flexibility in decision boundaries.  \n",
        "\n",
        "**Cons:**  \n",
        "- Memory-intensive for large datasets.\n",
        "- May require careful selection of kernel functions and tuning parameters.\n",
        "\n",
        "### 1.1.3 Deep Learning Models\n",
        "\n",
        "**Solution:** Word Embeddings  \n",
        "**Description:** Represent words as dense vectors in a continuous vector space.  \n",
        "**Examples:** Word2Vec, GloVe  \n",
        "**Pros:**  \n",
        "- Captures semantic meanings and relationships between words.\n",
        "- Provides dense vector representations suitable for downstream tasks.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of data for training.\n",
        "- Struggles with out-of-vocabulary words.\n",
        "\n",
        "**Solution:** Recurrent Neural Networks (RNN)  \n",
        "**Description:** Neural networks that process sequences by iterating through elements.  \n",
        "**Examples:** Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)  \n",
        "**Pros:**  \n",
        "- Effective for capturing sequential dependencies in data.\n",
        "- Suitable for tasks like language modeling and machine translation.\n",
        "\n",
        "**Cons:**  \n",
        "- Vulnerable to vanishing and exploding gradient problems.\n",
        "- Computationally expensive to train.\n",
        "\n",
        "**Solution:** Convolutional Neural Networks (CNN) for Text  \n",
        "**Description:** Application of convolution operations to capture local dependencies in text.  \n",
        "**Examples:** TextCNN, KimCNN  \n",
        "**Pros:**  \n",
        "- Effective for tasks like sentence classification and text categorization.\n",
        "- Captures local patterns and relationships in text.  \n",
        "\n",
        "**Cons:**  \n",
        "- May not capture long-range dependencies as effectively as other solutions.\n",
        "- Requires careful tuning of convolutional filters and pooling strategies.\n",
        "\n",
        "### 1.1.4 Transformers\n",
        "\n",
        "**Solution:** Transformer Models  \n",
        "**Description:** Neural network architecture based entirely on self-attention mechanisms.  \n",
        "**Examples:** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer)  \n",
        "**Pros:**  \n",
        "- Captures long-range dependencies effectively.\n",
        "- Parallelizable training process.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of computational resources.\n",
        "- Limited interpretability compared to traditional models.\n",
        "\n",
        "**Solution:** Pre-trained Models  \n",
        "**Description:** Models pre-trained on large corpora and fine-tuned for specific tasks.  \n",
        "**Examples:** BERT, GPT, T5  \n",
        "**Pros:**  \n",
        "- Leverage large amounts of unlabeled data for pre-training.\n",
        "- Achieve state-of-the-art performance on various NLP tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Resource-intensive pre-training process.\n",
        "- May require substantial computational resources for fine-tuning.\n",
        "\n",
        "**Solution:** Attention Mechanisms  \n",
        "**Description:** Mechanisms that enable models to focus on specific parts of the input.  \n",
        "**Examples:** Self-Attention, Multi-Head Attention  \n",
        "**Pros:**  \n",
        "- Improves the ability to capture dependencies and relationships within the data.\n",
        "- Enhances performance in various machine translation and text summarization.\n",
        "\n",
        "**Cons:**  \n",
        "- Can be computationally intensive.\n",
        "- Complexity increases with the number of attention heads and layers.\n",
        "\n",
        "### 1.1.5 Additional Models and Techniques\n",
        "\n",
        "**Solution:** Retriever-Generator Models  \n",
        "**Description:** Models combine retrieval and generation components for text generation tasks.  \n",
        "**Examples:** RAG  \n",
        "**Pros:**  \n",
        "- Incorporates both structured and unstructured information for generation.\n",
        "- Produces more diverse and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires efficient retrieval mechanisms.\n",
        "- Increased complexity in model architecture.\n",
        "\n",
        "**Solution:** Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)  \n",
        "**Description:** A variant of RAG that incorporates knowledge graphs.  \n",
        "**Examples:** Graph-BERT  \n",
        "**Pros:**  \n",
        "- Integrates structured knowledge for improved understanding and generation.\n",
        "- Enables more coherent and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires high-quality and curated knowledge graphs.\n",
        "- Increased computational complexity compared to standard RAG.\n",
        "\n",
        "**Solution:** Elastic Search  \n",
        "**Description:** Distributed search and analytics engine for indexing and searching big data.  \n",
        "**Examples:** Elasticsearch, Apache Solr  \n",
        "**Pros:**  \n",
        "- Scalable and distributed architecture.\n",
        "- Supports full-text search and complex query structures.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires infrastructure for deployment and maintenance.\n",
        "- Indexing and search performance may degrade with large datasets.\n",
        "\n",
        "## Architecture Options Score Card\n",
        "\n",
        "| Model/Architecture  | Key Strength                   | CPU Compatibility | Ease of Use | Performance & Accuracy | Scalability | Integration | Total |\n",
        "|---------------------|-------------------------------|-------------------|-------------|------------------------|-------------|-------------|-------|\n",
        "| RAG                 | Context Understanding          | 1                 | 1           | 2                      | 2           | 2           | 8     |\n",
        "| BoW                 | Simplicity                     | 2                 | 2           | 0                      | 2           | 2           | 8     |\n",
        "| N-gram Model        | Local Context                  | 2                 | 2           | 1                      | 1           | 2           | 8     |\n",
        "| Pre-trained Models  | Accuracy                       | 1                 | 1           | 1                      | 2           | 2           | 7     |\n",
        "| Word Embeddings     | Semantic Understanding         | 1                 | 1           | 2                      | 1           | 2           | 7     |\n",
        "| Elastic Search      | Scalability                    | 2                 | 1           | 1                      | 2           | 1           | 7     |\n",
        "| CNN                 | Local Pattern Recognition      | 1                 | 1           | 2                      | 1           | 1           | 6     |\n",
        "| Rule Based          | High Precision                 | 2                 | 2           | 0                      | 1           | 1           | 6     |\n",
        "| Transformer Models  | State-of-the-Art               | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| KERAG               | Knowledge Integration          | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| HMM                 | Sequence Modeling              | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| CRF                 | Sequential Labeling            | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| SVM                 | Versatile                      | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Sequence Models     | Order Preservation             | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Attention Mechanisms| Focus on Specific Parts        | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| RNN                 | Sequential Dependencies        | 0                 | 1           | 2                      | 0           | 1           | 4     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The score card was a valuable tool to reduce options down to those most appropriate for this project. A final audit is as follows:\n",
        "\n",
        "### RAG (Retriever-Augmented Generation):\n",
        "- **Strengths:** Excellent at context understanding and contextually relevant responses.\n",
        "- **Weaknesses:** Computationally intensive and may require more resources.\n",
        "- **Suitability:** High, if you need detailed, context-aware answers and have the necessary computational resources.\n",
        "\n",
        "### BoW (Bag-of-Words):\n",
        "- **Strengths:** Simple and efficient, easy to implement, and works well for basic tasks.\n",
        "- **Weaknesses:** Ignores word order and context, may not capture semantic meaning well.\n",
        "- **Suitability:** Moderate, for straightforward tasks where simplicity and efficiency are prioritized over contextual understanding.\n",
        "\n",
        "### N-gram Model:\n",
        "- **Strengths:** Captures some local word order and context, relatively simple to implement.\n",
        "- **Weaknesses:** Limited in capturing long-range dependencies, can become computationally expensive with larger N values.\n",
        "- **Suitability:** Moderate, for tasks where local context is important, but computational efficiency is still needed.\n",
        "\n",
        "## Conclusion\n",
        "The RAG model is most appropriate for this effort given its heavy use of domain specific information (that may be missing from a stand-alone pre-trained model). Its contextual accuracy has a higher weighted value for this use case than Ease of Use. Having said that, it is a computationally heavy architecture, and it is unclear if free cloud CPU resources will be sufficient.\n",
        "\n",
        "## Example of RAG Model Implementation\n",
        "1. **Query:** \"What are the benefits of using a RAG model?\"\n",
        "2. **Retriever:**\n",
        "   - Searches a corpus for relevant documents or passages related to \"benefits of using a RAG model\".\n",
        "   - Retrieves top-k documents or passages that discuss the advantages of RAG models.\n",
        "3. **Generator:**\n",
        "   - Takes the retrieved documents and generates a response: \"A RAG model combines the strengths of information retrieval and generative modeling. It retrieves relevant documents to provide context and generates accurate and contextually appropriate responses. This makes it highly effective for tasks requiring detailed and specific information.\"\n",
        "\n",
        "For this project, the Retriever will be the corpus scraped from the online forum processed with Word Embeddings, and the Generator will be from a pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Options\n",
        "\n",
        "Example 1: LLM as the Primary Responder\n",
        "User asks a question.\n",
        "System retrieves relevant documents from a supplemental corpus using a retrieval system (e.g., Elasticsearch, FAISS).\n",
        "Retrieved documents are used to create a prompt that is fed into the LLM.\n",
        "LLM generates a response based on the retrieved documents.\n",
        "In this approach, the LLM takes the retrieved documents and synthesizes a response. The LLM is responsible for understanding the context, extracting relevant information from the documents, and generating a coherent answer.  \n",
        "<br>  \n",
        "\n",
        "Example 2: LLM as the Editor\n",
        "User asks a question.\n",
        "System retrieves relevant documents from a supplemental corpus using a retrieval system.\n",
        "System extracts the answer from the retrieved documents.\n",
        "LLM ensures the answer is grammatically correct and potentially enhances the response for fluency."
      ],
      "metadata": {
        "id": "8KNUjguEluC3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpRVNUJFDm-"
      },
      "source": [
        "## 1.2 Foundational Model / Generator Options\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** GPT (GPT-2)  \n",
        "**Description:** Generative Pre-trained Transformer for generating text.  \n",
        "**Pros:**  \n",
        "- Highly capable of generating coherent and contextually relevant text.\n",
        "- Free to access and use.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources for fine-tuning.\n",
        "- GPT-2 is less powerful than newer models.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Claude (Claude 3)  \n",
        "**Description:** AI assistant designed for safety and ethical considerations.  \n",
        "**Pros:**  \n",
        "- Enhanced safety features and focus on ethical AI use.\n",
        "- Designed for robust handling of varying text lengths.\n",
        "\n",
        "**Cons:**  \n",
        "- Not freely available - requires paid API access or subscription\n",
        "- Does not offer open-weight models for download and local deployment\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** Llama 3\n",
        "**Description:** A powerful open-weight large language model designed for general-purpose use.  \n",
        "**Pros:**  \n",
        "- Fully open-weight and free to download and use.\n",
        "- Relatively efficient for its capability level\n",
        "\n",
        "**Cons:**  \n",
        "- Requires more computational resources than smaller models\n",
        "- Requires careful prompt engineering for best results.\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** FLAN-T5 or FLAN-UL2\n",
        "**Description:** Instruction-tuned transformer models for various language tasks.  \n",
        "**Pros:**  \n",
        "- Open-source and free to use and fine-tune\n",
        "- Strong performance on instruction-following tasks\n",
        "\n",
        "**Cons:**  \n",
        "- Larger versions require significant computational resources\n",
        "- Less performant than Google's latest proprietary models (PaLM, Gemini)\n",
        "\n",
        "**Vendor:** Amazon  \n",
        "**Package:** Amazon Titan\n",
        "**Description:** Amazon's foundation language models for text generation and understanding.  \n",
        "**Pros:**  \n",
        "- Available for deployment through SageMaker (including some open models)\n",
        "- Integrated with AWS ecosystem for scalable deployment.\n",
        "  \n",
        "**Cons:**  \n",
        "- Full Titan models require paid usage through AWS\n",
        "- Limited customization options compared to fully open models\n",
        "\n",
        "## Generator Option Score Card\n",
        "\n",
        "| Vendor    | Package       | Key Strength                  | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------|-------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | FLAN-T5/UL2   | Instruction Following         | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Meta      | Llama 3       | Open Access Performance       | 1                 | 1           | 2                      | 2                         | 2           | 8     |\n",
        "| OpenAI    | GPT-2         | Accessible Baseline           | 1                 | 2           | 1                      | 1                         | 0           | 5     |\n",
        "| Amazon    | Titan         | Cloud Integration             | 0                 | 1           | 1                      | 2                         | 2           | 6     |\n",
        "| Anthropic | Claude 3      | Safety & Ethics               | 0                 | 0           | 2                      | 1                         | 0           | 3     |\n",
        "### Scoring Explanation\n",
        "\n",
        "**CPU Compatibility:**\n",
        "- 2: Optimized for CPU deployment\n",
        "- 1: Can run on CPU with reasonable performance\n",
        "- 0: Requires specialized hardware or cloud-only\n",
        "\n",
        "**Ease of Use:**\n",
        "- 2: Simple implementation with minimal configuration\n",
        "- 1: Moderate setup complexity\n",
        "- 0: Complex integration requirements\n",
        "\n",
        "**Performance & Accuracy:**\n",
        "- 2: High-quality outputs and strong reasoning capabilities\n",
        "- 1: Adequate for general tasks\n",
        "- 0: Limited capabilities or quality issues\n",
        "\n",
        "**Integration & Flexibility:**\n",
        "- 2: Easy to integrate with various systems and customize\n",
        "- 1: Moderate integration capabilities\n",
        "- 0: Limited integration options\n",
        "\n",
        "**Scalability:**\n",
        "- 2: Scales effectively with demand\n",
        "- 1: Some scaling capabilities\n",
        "- 0: Limited scaling options\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLhXj52bwtE"
      },
      "source": [
        "\n",
        "## 1.3 Frameworks and Tools\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** TensorFlow  \n",
        "**Description:** Open-source ML framework for building and deploying models.  \n",
        "**Pros:**  \n",
        "- Comprehensive ecosystem with deep learning support.\n",
        "- Scalable on both CPUs and GPUs.  \n",
        "\n",
        "**Cons:**  \n",
        "- Steeper learning curve than some other frameworks.\n",
        "- Limited support for dynamic computation graphs.\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** PyTorch  \n",
        "**Description:** Open-source deep learning framework by Meta AI Research.  \n",
        "**Pros:**  \n",
        "- Pythonic and intuitive interface for model development.\n",
        "- Dynamic computation graph for easier debugging and experimentation.  \n",
        "\n",
        "**Cons:**  \n",
        "- Less optimized for production deployment than TensorFlow.\n",
        "- Limited built-in support for distributed training.\n",
        "\n",
        "**Vendor:** AWS  \n",
        "**Package:** Amazon Bedrock  \n",
        "**Description:** Fully managed service for building, deploying, and scaling ML models.  \n",
        "**Pros:**  \n",
        "- Integrated support for various ML frameworks.\n",
        "- Scalable infrastructure with extensive AWS services integration.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires AWS-specific knowledge for optimal use.\n",
        "- Potentially high costs for extensive usage.\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "## Framework Score Card\n",
        "\n",
        "| Vendor    | Package             | Key Strength                  | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------------|-------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | PyTorch             | Pythonic Interface            | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Google    | TensorFlow          | Comprehensive Ecosystem       | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| AWS       | Bedrock             | Fully Managed Service         | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| OpenAI    | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Anthropic | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yafxx82Qbq17"
      },
      "source": [
        "\n",
        "## 1.4 Embedding\n",
        "\n",
        "**Solution:** Universal Sentence Encoder  \n",
        "**Provider:** Google  \n",
        "**Libraries:** TensorFlow Hub  \n",
        "**Pros:**  \n",
        "- Captures sentence-level embeddings, enhancing text understanding.\n",
        "- Efficient and easy to integrate with TensorFlow models.  \n",
        "**Cons:**  \n",
        "- May not capture fine-grained word-level nuances.\n",
        "- Performance can vary depending on the complexity of the sentences.\n",
        "\n",
        "**Solution:** FastText  \n",
        "**Provider:** Meta  \n",
        "**Libraries:** Gensim, TensorFlow, PyTorch  \n",
        "**Pros:**  \n",
        "- Handles out-of-vocabulary words as bags of character n-grams.\n",
        "- Captures subword information, enhancing the representation of rare words.  \n",
        "**Cons:**  \n",
        "- Increases computational complexity due to subword representations.\n",
        "- Larger model size compared to Word2Vec and GloVe.\n",
        "\n",
        "**Solution:** Amazon SageMaker Embeddings  \n",
        "**Provider:** AWS  \n",
        "**Libraries:** Amazon SageMaker  \n",
        "**Pros:**  \n",
        "- Provides pre-built models for embeddings, simplifying deployment.\n",
        "- Integrates seamlessly with other AWS services for scalability.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires familiarity with the AWS ecosystem.\n",
        "- Costs can increase with extensive usage.\n",
        "\n",
        "**Solution:** GPT-3 Embeddings  \n",
        "**Provider:** OpenAI  \n",
        "**Libraries:** OpenAI API  \n",
        "**Pros:**  \n",
        "- Generates high-quality, contextually relevant text embeddings.\n",
        "- Handles long-range dependencies and contextual information.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources.\n",
        "- Access may require API usage and associated costs.\n",
        "\n",
        "**Solution:** Claude Embeddings  \n",
        "**Provider:** Anthropic  \n",
        "**Libraries:** Anthropic API  \n",
        "**Pros:**  \n",
        "- Offers state-of-the-art embeddings with a focus on safety and ethics.\n",
        "- Handles context and nuances effectively for complex tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Primarily available for research access, limiting commercial use.\n",
        "- Access may require API usage and associated costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewprjkrLbicq"
      },
      "source": [
        "## 1.5 Tokenization\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "**Solution:** Word-level Tokenization  \n",
        "**Libraries:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers  \n",
        "**Pros:**  \n",
        "- Preserves word integrity and meaning, crucial for comprehension tasks.\n",
        "- Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "\n",
        "**Cons:**  \n",
        "- Can result in a large vocabulary, increasing memory and processing needs.\n",
        "- May overlook nuances in character-level variations.\n",
        "\n",
        "**Solution:** Character-level Tokenization  \n",
        "**Libraries:** Supported by deep learning frameworks like TensorFlow and Keras  \n",
        "**Pros:**  \n",
        "- Captures morphological nuances at the character level, aiding rich languages.\n",
        "- Simplifies vocabulary to unique characters, reducing model complexity.  \n",
        "\n",
        "**Cons:**  \n",
        "- Leads to longer input sequences, increasing computational costs.\n",
        "- Loses direct access to semantic information in words or phrases.\n",
        "\n",
        "**Solution:** Subword Tokenization  \n",
        "**Libraries:** A blend of word-level and character-level tokenization methods  \n",
        "**Pros:**  \n",
        "- Balances vocabulary size and semantic information preservation.\n",
        "- Handles rare or unknown words by breaking them into recognizable subwords.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "- Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "**Solution:** Model-Specific Tokenization  \n",
        "**Libraries:** Hugging Face's transformers library provides access to pre-built tokenizers  \n",
        "**Pros:**  \n",
        "- Ensures tokenization consistency with the model's original training data.\n",
        "- Reduces the need for extra preprocessing steps and custom tokenization.  \n",
        "\n",
        "**Cons:**  \n",
        "- Limited flexibility to change tokenization beyond the model's method.\n",
        "- May not be efficient for tasks outside the model's specific design.\n",
        "\n",
        "Tokenization and embedding must be considered together because tokenization directly impacts the quality of embedding. The choice of tokenization method determines how text is segmented, which in turn affects how embeddings capture context and meaning. Inconsistent tokenization can lead to poor embeddings and reduced model performance. Properly aligned tokenization and embedding processes ensure that the text's structure and semantics are preserved, enhancing overall model effectiveness.\n",
        "\n",
        "## Tokenization and Embedding Score Card\n",
        "\n",
        "| Vendor    | Embedder                    | Tokenizer                      | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-----------------------------|--------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | Universal Sentence Encoder  | TensorFlow Text                | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Meta      | FastText                    | Gensim Tokenizer or NLTK       | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| OpenAI    | GPT-2 Embeddings            | GPT-2 Tokenizer                | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Amazon    | SageMaker Embeddings        | SageMaker’s preprocessing tools| 1                 | 2           | 2                      | 2                         | 1           | 8     |\n",
        "| Anthropic | Claude Embeddings           | Built-in tokenization          | 1                 | 2           | 2                      | 2                         | 0           | 7     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvnMsI-bcPS"
      },
      "source": [
        "\n",
        "## 1.6 Solution Leader Board\n",
        "\n",
        "| Vendor    | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | 6                 | 4           | 6                      | 6                         | 5           | 28    |\n",
        "| Meta      | 6                 | 6           | 5                      | 6                         | 4           | 27    |\n",
        "| Amazon    | 4                 | 5           | 5                      | 6                         | 4           | 24    |\n",
        "| OpenAI    | 4                 | 6           | 6                      | 6                         | 2           | 24    |\n",
        "| Anthropic | 4                 | 5           | 6                      | 5                         | 2           | 22    |\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The results are interesting and paint a clearer picture of how the strengths of each option compare. While close, Google seems to have an edge on Performance and Accuracy but suffers a bit on Ease of Use and Scalability. Having said that, the use of Google would support my efforts to gain Google Cloud Certification—a highly desirable skill in the job market. With that in mind, I’ll be moving forward with a Google dominant stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBhF-lbTb6Q7"
      },
      "source": [
        "# 2 Develop Corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWbtT5GoFDpf"
      },
      "source": [
        "## 2.1 Data Ethics\n",
        "The data collected here is a collection of posts from widely available public forum. However, should this project move into public distribution, additional steps will be necessary to ensure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the project's goals and data gathering process.\n",
        "\n",
        "### Data Collection\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "#### Web Scraping\n",
        "**Tools:** Beautiful Soup, online SaaS products  \n",
        "**Pros:**  \n",
        "- Direct Access to Targeted Data: Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "- Efficiency in Data Collection: Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.  \n",
        "\n",
        "**Cons:**  \n",
        "- Potential for Incomplete Data: May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "- Ethical and Legal Considerations: Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "- Very Platform Dependent: Forum-specific solutions result in forum-specific data schemas that must be reverse engineered for successful text extraction.\n",
        "\n",
        "#### Forum-specific APIs\n",
        "**Tools:** Python (`requests` library for API calls and `json` library for handling responses)  \n",
        "**Pros:**  \n",
        "- Structured and Reliable Data Retrieval: APIs provide structured data, making it easier to process and integrate into your project.\n",
        "- Efficient and Direct Access: Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "- Compliance and Ethical Data Use: Utilizing APIs respects the forum's data policies and ensures access is in line with user agreements.  \n",
        "\n",
        "**Cons:**  \n",
        "- Rate Limiting: APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "- API Changes: Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "- Access Restrictions: Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lJ9umeIXNXy"
      },
      "source": [
        "## 2.2 Ingest Corpus from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1y75VMgRttCo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6c80a5b7-cd79-446e-a989-a7edf3ac3d60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Remove this line if you want to create a new corpus",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-da8b06d43eca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2.2 Ingest Corpus from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Remove this line if you want to create a new corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Remove this line if you want to create a new corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Remove this line if you want to create a new corpus"
          ]
        }
      ],
      "source": [
        "# 2.2 Ingest Corpus from scratch\n",
        "\n",
        "raise RuntimeError(\"Remove this line if you want to create a new corpus\")\n",
        "\n",
        "# Remove this line if you want to create a new corpus\n",
        "\n",
        "\n",
        "\n",
        "# Step 1 Create Corpus\n",
        "# Fetch and process forum threads\n",
        "# Corpus created in LDA notebook can be used.\n",
        "\n",
        "\n",
        "def forum_thread_ids():\n",
        "    threads = 1  # Set the number of incremental threads to process here\n",
        "\n",
        "    file_path = os.path.join(BASE_PATH, 'e9_forum_thread_ids.csv')\n",
        "\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(e9_forum_thread_ids['thread_id'].iloc[-1])\n",
        "    else:\n",
        "        e9_forum_thread_ids = pd.DataFrame(columns=['thread_id'])\n",
        "        last_thread_id = 0\n",
        "\n",
        "    next_thread_id = last_thread_id + 1\n",
        "    new_urls = [{'thread_id': thread_id} for thread_id in range(next_thread_id, next_thread_id + threads)]\n",
        "\n",
        "    new_df = pd.DataFrame(new_urls)\n",
        "    e9_forum_thread_ids = pd.concat([e9_forum_thread_ids, new_df], ignore_index=True)\n",
        "    e9_forum_thread_ids.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Starting with thread_id {last_thread_id}\")\n",
        "    print(f\"Processing additional {threads} thread(s)\")\n",
        "    print(f\"Ending with thread_id {next_thread_id + threads - 1}\")\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def forum_thread_url(df):\n",
        "    if df.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "\n",
        "    df.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_url.csv'), index=False)\n",
        "    return df\n",
        "\n",
        "def forum_thread_first_post(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        post_content = first_post.get_text(strip=True) if first_post else \"No content found\"\n",
        "        data.append({'thread_id': thread_id, 'thread_first_post': post_content})\n",
        "\n",
        "    forum_first_post = pd.DataFrame(data)\n",
        "    forum_first_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_first_post.csv'), index=False)\n",
        "    return forum_first_post\n",
        "\n",
        "def forum_thread_all_post(df):\n",
        "    post_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "            post_data.append({'thread_id': row['thread_id'], 'post_raw': content})\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "    e9_forum_posts['thread_all_posts'] = e9_forum_posts['post_raw'].astype(str)\n",
        "    e9_forum_thread_all_post = e9_forum_posts.groupby('thread_id')['thread_all_posts'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "    e9_forum_thread_all_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_all_post.csv'), index=False)\n",
        "    return e9_forum_thread_all_post\n",
        "\n",
        "def forum_corpus(e9_forum_thread_url, e9_forum_thread_first_post, e9_forum_thread_all_post):\n",
        "    agg_df_1 = pd.merge(e9_forum_thread_url, e9_forum_thread_first_post, on='thread_id', how='left')\n",
        "    agg_df_2 = pd.merge(agg_df_1, e9_forum_thread_all_post, on='thread_id', how='left')\n",
        "\n",
        "    e9_forum_corpus = agg_df_2.dropna()\n",
        "    corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "    if os.path.exists(corpus_path) and os.path.getsize(corpus_path) > 0:\n",
        "        existing_corpus = pd.read_csv(corpus_path)\n",
        "        e9_forum_corpus = pd.concat([existing_corpus, e9_forum_corpus]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    e9_forum_corpus.columns = e9_forum_corpus.columns.str.upper()\n",
        "    e9_forum_corpus.to_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv'), index=False)\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    e9_forum_thread_ids = forum_thread_ids()\n",
        "    e9_forum_thread_url_df = forum_thread_url(e9_forum_thread_ids)\n",
        "    e9_forum_thread_first_post_df = forum_thread_first_post(e9_forum_thread_url_df)\n",
        "    e9_forum_thread_all_post_df = forum_thread_all_post(e9_forum_thread_url_df)\n",
        "    e9_forum_corpus_df = forum_corpus(e9_forum_thread_url_df, e9_forum_thread_first_post_df, e9_forum_thread_all_post_df)\n",
        "    print(f\"Output saved to {os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65J-_EM5DTlM"
      },
      "source": [
        "## 2.3 Ingest previously compiled corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dJ2e5CwqDYl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "87b92bd1-44e2-4dbd-cb1a-ddbeef4c8ee8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'snowflake'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d0f2c934f059>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnowflake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snowflake'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# 2.3 Ingest previously compiled corpus\n",
        "\n",
        "import os\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "\n",
        "# Data here is from corpus workbook stored in Snowflake\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT'),\n",
        "    )\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\"\n",
        "    order by 1 asc;\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    return e9_forum_corpus\n",
        "\n",
        "# Load credentials\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Fetch data from Snowflake\n",
        "e9_forum_corpus = fetch_data_from_snowflake()\n",
        "\n",
        "if not e9_forum_corpus.empty:\n",
        "    # Save the data to a CSV file\n",
        "    output_path = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_dirty.csv'\n",
        "    e9_forum_corpus.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the count of records at the end\n",
        "print(f\"Total number of records retrieved: {len(e9_forum_corpus)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A97WWzNlTEmj"
      },
      "source": [
        "# 3 Preprocessing Text\n",
        "\n",
        "The collected text is very unstructured and needs a reasonable amount of pre-processing to make it usable for NLP. This will address values that are either not localized, use slang, or do not have value from an NLP perspective.\n",
        "\n",
        "- **Clean the Text:**\n",
        "  - Remove HTML tags, extra whitespace, non-printable characters, and other irrelevant elements.\n",
        "\n",
        "- **Standardize the Text:**\n",
        "  - Convert all characters to lowercase to ensure uniformity.\n",
        "\n",
        "- **Filter Out Common Stop Words:**\n",
        "  - Remove stop words to focus on more meaningful content.\n",
        "\n",
        "- **Remove Duplicate Entries:**\n",
        "  - Ensure the uniqueness of the data by eliminating duplicates.\n",
        "\n",
        "- **Lemmatization or Stemming:**\n",
        "  - Convert words to their base or dictionary form to consolidate similar forms of a word.\n",
        "\n",
        "- **Anonymize Personal Information:**\n",
        "  - Identify and anonymize personal information or specific entity names to maintain privacy.\n",
        "\n",
        "- **Remove Irrelevant Sections:**\n",
        "  - Remove sections of the text that do not contribute to the knowledge base or are off-topic.\n",
        "\n",
        "- **Tokenization:**\n",
        "  - Break down the text into smaller units called tokens. Use a tokenizer compatible with your chosen model, such as the BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Cleaning text\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Base path\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# Load the original corpus\n",
        "print(\"Loading original corpus...\")\n",
        "dirty_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')\n",
        "e9_forum_corpus_dirty = pd.read_csv(dirty_corpus_path)\n",
        "print(f\"Loaded {len(e9_forum_corpus_dirty)} threads\")\n",
        "\n",
        "def improved_cleaning(text):\n",
        "    \"\"\"More robust cleaning for forum text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "    # Fix common forum formatting issues\n",
        "    text = re.sub(r'Click to expand\\.\\.\\.', '', text)\n",
        "    text = re.sub(r':cry:|:roll:', '', text)  # Remove emoticons\n",
        "\n",
        "    # Remove quote attributions common in forums\n",
        "    text = re.sub(r'\\w+ said:', '', text)\n",
        "\n",
        "    # Clean up excessive punctuation\n",
        "    text = re.sub(r'\\.{3,}', '...', text)\n",
        "    text = re.sub(r'!{2,}', '!', text)\n",
        "\n",
        "    # Fix spacing\n",
        "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def main():\n",
        "    print(\"Starting cleaning process...\")\n",
        "\n",
        "    # Apply cleaning to the corpus\n",
        "    e9_forum_corpus_dirty['CLEANED_TEXT'] = e9_forum_corpus_dirty['THREAD_ALL_POSTS'].apply(improved_cleaning)\n",
        "\n",
        "    # Save the cleaned corpus\n",
        "    cleaned_path = os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv')\n",
        "    e9_forum_corpus_dirty.to_csv(cleaned_path, index=False)\n",
        "\n",
        "    print(f\"Cleaning complete. Cleaned corpus saved to {cleaned_path}\")\n",
        "\n",
        "    # Create a small comparison sample\n",
        "    sample_size = min(5, len(e9_forum_corpus_dirty))\n",
        "    sample = e9_forum_corpus_dirty.sample(sample_size)\n",
        "\n",
        "    for i, (_, row) in enumerate(sample.iterrows(), 1):\n",
        "        print(f\"\\n=== Sample {i} (ID: {row['THREAD_ID']}) ===\")\n",
        "        print(\"\\nORIGINAL TEXT (first 200 chars):\")\n",
        "        print(row['THREAD_ALL_POSTS'][:200] + \"...\" if len(row['THREAD_ALL_POSTS']) > 200 else row['THREAD_ALL_POSTS'])\n",
        "        print(\"\\nCLEANED TEXT (first 200 chars):\")\n",
        "        print(row['CLEANED_TEXT'][:200] + \"...\" if len(row['CLEANED_TEXT']) > 200 else row['CLEANED_TEXT'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eTX5jO984VvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "# 4 Clustering and Summarization\n",
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through multiple options. For this effort, the following solutions were scored to reduce the potential solution set. However, it's argubale that with a small corpus this may be not be necessary.\n",
        "\n",
        "| Provider  | Specific Package | Key Strength                       | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|------------------|-----------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | FLAN-T5 (small)  | Instruction-tuned efficiency      | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Meta      | BART (distilled) | Efficient summarization           | 1                 | 2           | 2                      | 2                         | 1           | 8     |\n",
        "| Amazon    | AWS Comprehend   | Managed service integration       | 1                 | 2           | 1                      | 2                         | 2           | 8     |\n",
        "| OpenAI    | GPT-2            | Low-resource summarization        | 1                 | 2           | 1                      | 1                         | 0           | 5     |\n",
        "| Anthropic | Claude (API)     | Safety and ethical considerations | 0                 | 0           | 2                      | 1                         | 0           | 3     |\n",
        "\n",
        "\n",
        "\n",
        "A test of solutions can be found in the Appenix. T5 was chosen based on bettter ROUGE scores than BART. This also allows me to stick with Google centric stack.<br>\n",
        "\n",
        "Update: A visual investigation of summarization using FLAN-t5 showed very poor results. Im using BART now."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Clustering and Summarization\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Base path\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Load e9_forum_corpus_clean DataFrame from the CSV\n",
        "e9_forum_corpus_clean = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv'))\n",
        "\n",
        "# Load the existing summarized corpus if it exists, otherwise create it\n",
        "summarized_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv')\n",
        "e9_forum_corpus_summarized = pd.read_csv(summarized_corpus_path) if os.path.exists(summarized_corpus_path) and os.path.getsize(summarized_corpus_path) > 0 else pd.DataFrame(columns=e9_forum_corpus_clean.columns)\n",
        "\n",
        "# Calculate the starting THREAD_ID of the summarized corpus\n",
        "starting_thread_id = e9_forum_corpus_summarized['THREAD_ID'].max() if not e9_forum_corpus_summarized.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = e9_forum_corpus_clean[~e9_forum_corpus_clean['THREAD_ID'].isin(e9_forum_corpus_summarized['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def BART_summarize(text):\n",
        "    \"\"\"Summarization using BART.\"\"\"\n",
        "    try:\n",
        "        if text.strip() == \"\":\n",
        "            return text\n",
        "\n",
        "        unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer([unformatted_text], max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate summary\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=100,\n",
        "            min_length=30,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary if summary else text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def main():\n",
        "    # Check if 'THREAD_ALL_POSTS' column exists in new entries\n",
        "    if 'THREAD_ALL_POSTS' in new_entries.columns:\n",
        "        unique_texts = new_entries['THREAD_ALL_POSTS'].drop_duplicates()\n",
        "        summaries = unique_texts.apply(BART_summarize)\n",
        "        summary_map = dict(zip(unique_texts, summaries))\n",
        "        new_entries.loc[:, 'SUMMARIZED_THREAD'] = new_entries['THREAD_ALL_POSTS'].map(summary_map)\n",
        "\n",
        "        # Append the new summarized data to the existing summarized corpus\n",
        "        updated_summarized_corpus = pd.concat([e9_forum_corpus_summarized, new_entries], ignore_index=True)\n",
        "\n",
        "        # Save the results with the new summarized column\n",
        "        updated_summarized_corpus.to_csv(summarized_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Summarization completed and saved to {summarized_corpus_path}\")\n",
        "    else:\n",
        "        print(\"Error: Column 'THREAD_ALL_POSTS' does not exist in the dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "G2Dq_Gyz4glB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HjwEoXaR3y"
      },
      "source": [
        "# 5 Format Text for Training\n",
        "\n",
        "- Structure text into a question-answer format suitable for training a RAG model.\n",
        "- Ensure the question string ends with a question mark for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47CINHXsYvfh"
      },
      "outputs": [],
      "source": [
        "# 5 Format Text for Training\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "\n",
        "df_summarized = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv'))\n",
        "\n",
        "# Load the existing QA corpus if it exists\n",
        "qa_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv')\n",
        "df_qa = pd.read_csv(qa_corpus_path) if os.path.exists(qa_corpus_path) and os.path.getsize(qa_corpus_path) > 0 else pd.DataFrame(columns=['THREAD_ID', 'QUESTION', 'ANSWER'])\n",
        "\n",
        "# Calculate the starting THREAD_ID of the QA corpus\n",
        "starting_thread_id = df_qa['THREAD_ID'].max() if not df_qa.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_summarized[~df_summarized['THREAD_ID'].isin(df_qa['THREAD_ID'])]\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def create_qa_schema(df):\n",
        "    \"\"\"Creates a QA schema by renaming and dropping specific columns.\"\"\"\n",
        "    # Make explicit copy to avoid the SettingWithCopyWarning\n",
        "    df_copy = df.copy()\n",
        "    df_copy.rename(columns={'SUMMARIZED_THREAD': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "    return df_copy\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        # Process the new entries to create QA schema\n",
        "        df_qa_new = create_qa_schema(new_entries.dropna())\n",
        "\n",
        "        # Append the new QA data to the existing QA corpus\n",
        "        updated_qa_corpus = pd.concat([df_qa, df_qa_new], ignore_index=True)\n",
        "\n",
        "        # Save the updated QA corpus\n",
        "        updated_qa_corpus.to_csv(qa_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Output saved to {qa_corpus_path}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_mp1BMbFXp"
      },
      "source": [
        "# 6 Embedding and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK6L99nQKA_-"
      },
      "outputs": [],
      "source": [
        "# 6 Embedding and Indexing\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartModel\n",
        "\n",
        "# Base path\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# Load the DataFrame from your CSV file\n",
        "print(\"Loading QA corpus...\")\n",
        "df_tok = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv'))\n",
        "print(f\"Loaded {len(df_tok)} entries from QA corpus\")\n",
        "\n",
        "# Initialize the BART tokenizer and model\n",
        "print(\"Loading BART tokenizer and model...\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartModel.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Function to tokenize text using BART tokenizer\n",
        "def tokenize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Generate embeddings directly from text\"\"\"\n",
        "    try:\n",
        "        tokens = tokenize_text(text)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.encoder(tokens)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding text: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_entries(entries):\n",
        "    \"\"\"Process entries to generate embeddings directly from text\"\"\"\n",
        "    print(f\"Processing {len(entries)} entries...\")\n",
        "\n",
        "    # Create new DataFrame to hold the processed data\n",
        "    processed_df = entries[['THREAD_ID', 'QUESTION', 'ANSWER']].copy()\n",
        "\n",
        "    # Generate embeddings directly from text\n",
        "    print(\"Generating question embeddings...\")\n",
        "    processed_df[\"Question_Embeddings\"] = processed_df[\"QUESTION\"].apply(embed_text)\n",
        "\n",
        "    print(\"Generating answer embeddings...\")\n",
        "    processed_df[\"Answer_Embeddings\"] = processed_df[\"ANSWER\"].apply(embed_text)\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "def filter_valid_embeddings(df):\n",
        "    \"\"\"Filter rows where both question and answer embeddings are valid\"\"\"\n",
        "    valid_rows = []\n",
        "    question_embeddings = []\n",
        "    answer_embeddings = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        q_emb = row[\"Question_Embeddings\"]\n",
        "        a_emb = row[\"Answer_Embeddings\"]\n",
        "\n",
        "        # Check if both embeddings exist and have the expected shape\n",
        "        if isinstance(q_emb, list) and isinstance(a_emb, list):\n",
        "            if len(q_emb) == 1024 and len(a_emb) == 1024:\n",
        "                valid_rows.append(row)\n",
        "                question_embeddings.append(q_emb)\n",
        "                answer_embeddings.append(a_emb)\n",
        "\n",
        "    print(f\"Found {len(valid_rows)} valid entries out of {len(df)}\")\n",
        "    return valid_rows, question_embeddings, answer_embeddings\n",
        "\n",
        "def build_faiss_index(embeddings, index_path):\n",
        "    \"\"\"Build and save the FAISS index\"\"\"\n",
        "    print(f\"Building FAISS index with {len(embeddings)} vectors...\")\n",
        "    embeddings_np = np.array(embeddings).astype('float32')  # Convert to NumPy array of type float32\n",
        "    d = embeddings_np.shape[1]  # Dimension of embeddings\n",
        "    print(f\"Embedding dimension: {d}\")\n",
        "\n",
        "    index = faiss.IndexFlatL2(d)  # Build the index\n",
        "    index.add(embeddings_np)  # Add vectors to the index\n",
        "\n",
        "    # Save the index\n",
        "    faiss.write_index(index, index_path)\n",
        "    print(f\"FAISS index saved to {index_path}\")\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    # Process all entries to regenerate embeddings\n",
        "    print(\"Processing all entries to generate fresh BART embeddings...\")\n",
        "    processed_df = process_entries(df_tok)\n",
        "\n",
        "    # Filter to keep only valid embeddings\n",
        "    valid_rows, question_embeddings, answer_embeddings = filter_valid_embeddings(processed_df)\n",
        "\n",
        "    if valid_rows:\n",
        "        # Create a new DataFrame with valid entries\n",
        "        valid_df = pd.DataFrame(valid_rows)\n",
        "\n",
        "        # Save the processed DataFrame with valid embeddings\n",
        "        faiss_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv')\n",
        "        valid_df.to_csv(faiss_corpus_path, index=False)\n",
        "        print(f\"Saved {len(valid_df)} entries with valid embeddings to {faiss_corpus_path}\")\n",
        "\n",
        "        # Save embeddings as NPY files\n",
        "        np.save(os.path.join(BASE_PATH, 'question_embeddings_bart.npy'), np.array(question_embeddings))\n",
        "        np.save(os.path.join(BASE_PATH, 'answer_embeddings_bart.npy'), np.array(answer_embeddings))\n",
        "        print(\"Embeddings saved as NPY files\")\n",
        "\n",
        "        # Build and save the FAISS index\n",
        "        faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_bart.index')\n",
        "        index = build_faiss_index(answer_embeddings, faiss_index_path)\n",
        "        print(f\"FAISS index has been built and saved to {faiss_index_path}\")\n",
        "    else:\n",
        "        print(\"Error: No valid embeddings found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kRXQtQdm-b"
      },
      "source": [
        "# 7 Query Processing and Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueakMTFdd4k3"
      },
      "outputs": [],
      "source": [
        "# 7 Query Processing and Search\n",
        "import faiss\n",
        "\n",
        "# Path configuration\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# Load the FAISS index - update to BART index\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_bart.index')\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "\n",
        "\n",
        "# Load the corpus DataFrame\n",
        "e9_forum_corpus = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv'))\n",
        "\n",
        "# Initialize the BART tokenizer and model\n",
        "from transformers import BartTokenizer, BartModel\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartModel.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_query_embeddings(query, tokenizer, model):\n",
        "    \"\"\"Generate embeddings for the input query using BART model\"\"\"\n",
        "    tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "    with torch.no_grad():\n",
        "        outputs = model.encoder(tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return np.array(embeddings).astype('float32')  # Convert to float32 for FAISS\n",
        "\n",
        "def search_similar_questions(query_embeddings, index, top_k=5):\n",
        "    \"\"\"Search for similar questions in the FAISS index\"\"\"\n",
        "    query_embeddings = query_embeddings.reshape(1, -1)  # Ensure correct shape\n",
        "    D, I = index.search(query_embeddings, top_k)\n",
        "    return I, D\n",
        "\n",
        "def process_query(query, top_k=5):\n",
        "    \"\"\"Process a query through the embedding and search pipeline\"\"\"\n",
        "    #logging.info(f\"Processing query: {query}\")\n",
        "\n",
        "    # Generate embeddings for the query\n",
        "    query_embeddings = generate_query_embeddings(query, tokenizer, model)\n",
        "\n",
        "    # Search for similar questions\n",
        "    I, D = search_similar_questions(query_embeddings, index, top_k)\n",
        "\n",
        "    return query, I, D\n",
        "\n",
        "# Execute this code to create the necessary variables for step 8\n",
        "query = \"How do I fix the transmission issue in my car?\"\n",
        "query, I, D = process_query(query)\n",
        "print(f\"Step 7 complete: Created query, I, and D variables for step 8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSiN7POcPZL"
      },
      "source": [
        "\n",
        "\n",
        "# 8 Retrieve and Rank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1Ql-pHeI-8"
      },
      "outputs": [],
      "source": [
        "# 8 Retrieve and Rank\n",
        "\n",
        "def retrieve_and_rank(df, I, D, query=None):\n",
        "    \"\"\"Retrieve and rank results based on search results\"\"\"\n",
        "    results = []\n",
        "    for i, distances in zip(I, D):\n",
        "        for idx, distance in zip(i, distances):\n",
        "            if idx < len(df):  # Safety check\n",
        "                result = {\n",
        "                    'Thread ID': df.iloc[idx]['THREAD_ID'],\n",
        "                    'Question': df.iloc[idx]['QUESTION'],\n",
        "                    'Answer': df.iloc[idx]['ANSWER'],\n",
        "                    'Similarity': 1 - distance,  # Convert distance to similarity score\n",
        "                    'Distance': distance\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "    # Log the retrieved context if query is provided\n",
        "    if query is not None:\n",
        "        contexts = [result['Answer'] for result in results]\n",
        "        retrieved_context = \" \".join(contexts)\n",
        "        #logging.info(f\"Query: {query}\\nRetrieved Context: {retrieved_context}\")\n",
        "\n",
        "    # Sort by similarity (higher is better)\n",
        "    results = sorted(results, key=lambda x: x['Similarity'], reverse=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "ranked_results = retrieve_and_rank(e9_forum_corpus, I, D, query)\n",
        "print(f\"Step 8 complete: Created ranked_results with {len(ranked_results)} items for step 9\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okjxc-Mn5qXv"
      },
      "source": [
        "\n",
        "# 9 Answer Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Generate Answer\n",
        "# Note: This assumes steps 7 and 8 have been run and created the variables: query, ranked_results\n",
        "\n",
        "def generate_answer(query, ranked_results, tokenizer, model):\n",
        "    \"\"\"Generate an answer using the T5 model based on the query and retrieved context\"\"\"\n",
        "    # Print the original user question\n",
        "    print(f\"Original Question: {query}\")\n",
        "\n",
        "    # Concatenate the retrieved contexts\n",
        "    concatenated_context = \" \".join([result['Answer'] for result in ranked_results])\n",
        "\n",
        "    # Construct the prompt with the query and context\n",
        "    input_text = f\"answer: {query} context: {concatenated_context}\"\n",
        "\n",
        "    # Print the constructed prompt to see what is being sent to the LLM\n",
        "    print(f\"Engineered Prompt: {input_text}\")\n",
        "\n",
        "    # Generate the answer using the T5 model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_answer\n",
        "\n",
        "generated_answer = generate_answer(query, ranked_results, tokenizer, model)\n",
        "print(f\"\\nGenerated Answer: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "SRa1jfBz27Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAmDDBpchI6"
      },
      "source": [
        "# 10 Evaluation and Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQuFH6wleDuc"
      },
      "outputs": [],
      "source": [
        "# 10 Evaluation and Tuning\n",
        "\n",
        "# Update this path to match where your files are actually located\n",
        "# This should match the path you used in steps 7, 8, and 9\n",
        "\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "\n",
        "def create_representative_sentences():\n",
        "    \"\"\"Creates a CSV file with representative test sentences if it doesn't exist.\"\"\"\n",
        "    if os.path.exists(representative_sentences_path):\n",
        "        print(f\"Representative sentences file already exists at: {representative_sentences_path}\")\n",
        "        return\n",
        "\n",
        "    # Create sample representative sentences for testing\n",
        "    representative_sentences = [\n",
        "        {\"Topic\": \"Transmission\", \"Representative Sentence\": \"How do I fix the transmission issue in my car?\"},\n",
        "        {\"Topic\": \"Engine\", \"Representative Sentence\": \"Why is my engine making a knocking sound?\"},\n",
        "        {\"Topic\": \"Brakes\", \"Representative Sentence\": \"What's the process for replacing brake pads?\"},\n",
        "        {\"Topic\": \"Electrical\", \"Representative Sentence\": \"My headlights keep flickering, what could be causing this?\"},\n",
        "        {\"Topic\": \"Maintenance\", \"Representative Sentence\": \"How often should I change the oil in my car?\"}\n",
        "    ]\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_sentences = pd.DataFrame(representative_sentences)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_sentences.to_csv(representative_sentences_path, index=False)\n",
        "    print(f\"Created representative sentences file at: {representative_sentences_path}\")\n",
        "\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "# Check if the FAISS index file exists\n",
        "if not os.path.exists(faiss_index_path):\n",
        "    print(f\"ERROR: FAISS index file not found at: {faiss_index_path}\")\n",
        "    print(f\"Make sure the file exists and the path is correct.\")\n",
        "    # Optional fallback - try to find the file in the current directory\n",
        "    current_dir_files = [f for f in os.listdir() if f.endswith('.index')]\n",
        "    if current_dir_files:\n",
        "        print(f\"Found index files in current directory: {current_dir_files}\")\n",
        "        print(f\"Consider updating the BASE_PATH to point to one of these files.\")\n",
        "\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "try:\n",
        "    index = faiss.read_index(faiss_index_path)\n",
        "    print(f\"Successfully loaded FAISS index from {faiss_index_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FAISS index: {e}\")\n",
        "    # You might want to exit here or provide alternative paths\n",
        "    raise\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "# Load the model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\n",
        "\n",
        "generator = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        # Use the encoder part only to get embeddings\n",
        "        outputs = generator.encoder(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_locally(indices, csv_path):\n",
        "    \"\"\"Fetch answers from local CSV file instead of Snowflake.\"\"\"\n",
        "    try:\n",
        "        # Load the QA corpus\n",
        "        qa_df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Filter by thread IDs\n",
        "        filtered_answers = qa_df[qa_df['THREAD_ID'].isin(indices)]\n",
        "\n",
        "        return filtered_answers\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching answers locally: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "    qa_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv')  # Path to your QA pairs\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers locally\n",
        "        answer = None\n",
        "        score = None\n",
        "\n",
        "        # Check if we have results and if the similarity score meets the threshold\n",
        "        if similar_indices and similarity_scores[0] >= similarity_threshold:\n",
        "            answers = fetch_answers_locally(similar_indices, qa_corpus_path)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        else:\n",
        "            # If similarity score is below threshold or no results found\n",
        "            answer = \"I don't have enough information to answer this question confidently.\"\n",
        "            score = similarity_scores[0] if similarity_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    create_representative_sentences()\n",
        "\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 11 Deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.  \n",
        "\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.  \n",
        "   \n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 12 Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bvouULWteB2"
      },
      "source": [
        "## Summarization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0r8gc0eJQtF"
      },
      "outputs": [],
      "source": [
        "# Summarization Comparison: T5\n",
        "\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def t5_summarize(text, max_length, min_length, num_beams):\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model_t5.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Define the reference summary (ground truth)\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = t5_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zal-aEkL_BFj"
      },
      "outputs": [],
      "source": [
        "# Summarization Comparison: DistilBART\n",
        "\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def distilbart_summarize(text, max_length, min_length, num_beams):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,  # Adjusted to fit within the model's constraints\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text and reference summary\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = distilbart_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RdIXKt4LPUB"
      },
      "source": [
        "Compare ROGE Summarization Scores:\n",
        "\n",
        "T5\n",
        "\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1778\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.1290\n",
        "*   Best parameters: max_length=50, min_length=10, num_beams=2\n",
        "\n",
        "\n",
        "\n",
        "BART\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1270\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.0816\n",
        "*   Best parameters: max_length=100, min_length=10, num_beams=4\n",
        "\n",
        "\n",
        "\n",
        "Based on these scores, Ill be using T5 for training due to its higher Precision and no discernable differnce in Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVaVmP2DVcjM"
      },
      "source": [
        "# Parking lot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt #3\n",
        "# includes both cleaning and summarization\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Base path\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "print(\"Loading model...\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the original corpus\n",
        "dirty_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')\n",
        "e9_forum_corpus_dirty = pd.read_csv(dirty_corpus_path)\n",
        "\n",
        "def improved_cleaning(text):\n",
        "    \"\"\"More robust cleaning for forum text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "    # Fix common forum formatting issues\n",
        "    text = re.sub(r'Click to expand\\.\\.\\.', '', text)\n",
        "    text = re.sub(r':cry:|:roll:', '', text)  # Remove emoticons\n",
        "\n",
        "    # Remove quote attributions common in forums\n",
        "    text = re.sub(r'\\w+ said:', '', text)\n",
        "\n",
        "    # Clean up excessive punctuation\n",
        "    text = re.sub(r'\\.{3,}', '...', text)\n",
        "    text = re.sub(r'!{2,}', '!', text)\n",
        "\n",
        "    # Fix spacing\n",
        "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def summarize_with_bart(text, max_length=100, min_length=30):\n",
        "    \"\"\"Summarize text using BART model\"\"\"\n",
        "    try:\n",
        "        if not text or len(text.strip()) < 100:  # Require more substantial text\n",
        "            return \"Text too short to summarize properly.\"\n",
        "\n",
        "        # Truncate if needed (BART has max 1024 tokens)\n",
        "        if len(text.split()) > 900:\n",
        "            text = ' '.join(text.split()[:900])\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer([text], max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate summary\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Quality check\n",
        "        if len(summary.split()) < 7 or not re.search(r'[A-Z][^\\.!?]*[\\.!?]', summary):\n",
        "            return \"Failed to generate a meaningful summary.\"\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    # Clean and select test threads\n",
        "    e9_forum_corpus_dirty['CLEANED_TEXT'] = e9_forum_corpus_dirty['THREAD_ALL_POSTS'].apply(improved_cleaning)\n",
        "    test_threads = e9_forum_corpus_dirty.head(10).copy()\n",
        "\n",
        "    print(\"Starting test summarization process...\\n\")\n",
        "\n",
        "    # Loop through each thread and summarize\n",
        "    for i, (_, row) in enumerate(test_threads.iterrows(), 1):\n",
        "        thread_id = row['THREAD_ID']\n",
        "        original_text = row['THREAD_ALL_POSTS']\n",
        "        cleaned_text = row['CLEANED_TEXT']\n",
        "\n",
        "        print(f\"=== Thread {i} (ID: {thread_id}) ===\")\n",
        "        print(\"\\nORIGINAL TEXT (first 300 chars):\")\n",
        "        print(original_text[:300] + \"...\" if len(original_text) > 300 else original_text)\n",
        "\n",
        "        print(\"\\nCLEANED TEXT (first 300 chars):\")\n",
        "        print(cleaned_text[:300] + \"...\" if len(cleaned_text) > 300 else cleaned_text)\n",
        "\n",
        "        print(\"\\nGenerating summary...\")\n",
        "        summary = summarize_with_bart(cleaned_text)\n",
        "\n",
        "        print(\"\\nSUMMARY:\")\n",
        "        print(summary)\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Create a comparison DataFrame\n",
        "    comparison_data = []\n",
        "    for _, row in test_threads.iterrows():\n",
        "        comparison_data.append({\n",
        "            'THREAD_ID': row['THREAD_ID'],\n",
        "            'ORIGINAL_TEXT': row['THREAD_ALL_POSTS'],\n",
        "            'CLEANED_TEXT': row['CLEANED_TEXT'],\n",
        "            'SUMMARY': summarize_with_bart(row['CLEANED_TEXT'])\n",
        "        })\n",
        "\n",
        "    # Save comparison data\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_path = os.path.join(BASE_PATH, 'bart_summary_comparison.csv')\n",
        "    comparison_df.to_csv(comparison_path, index=False)\n",
        "    print(f\"Comparison data saved to {comparison_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xIDcQfeI1LDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ir9PZxznFQ"
      },
      "outputs": [],
      "source": [
        "# Parking Lot\n",
        "# Query Processing and Search of LDA derived topics\n",
        "# This step reuires LDA to have run first to generate topic sentences\n",
        "\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus_faiss WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 3.3 Data Storage and Database\n",
        "\n",
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### 3.3.1 Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "\n",
        "### 3.3.2 Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}