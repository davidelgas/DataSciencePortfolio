{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/LLM_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLMB-P30ER2F"
      },
      "source": [
        "# Project Objective and Limitations\n",
        "\n",
        "## i. Project Overview\n",
        "The advent of modern automobile manufacturing has led to increased technical complexity, often resulting in mechanics opting to replace parts rather than diagnose and fix issues. This approach, while convenient for contemporary vehicles, poses a significant challenge for classic cars built 30 to 40 years ago, where replacement parts are scarce or non-existent.\n",
        "\n",
        "To address this problem, this project aims to leverage Generative AI to create a \"virtual mechanic.\" By building a corpus gathered from a classic car forum, this tool will be capable of understanding unstructured questions and providing relevant answers. This solution aims to assist classic car enthusiasts and mechanics by offering expert guidance, thereby preserving the heritage and functionality of vintage automobiles.\n",
        "\n",
        "## ii. Objectives\n",
        "The primary objective of this project is the development of a model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparison and selection of architectures, libraries, and methods.\n",
        "\n",
        "## iii. Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are driven from previous similar questions. Users will see these answers in plain English. I will have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model.\n",
        "\n",
        "## iv. Limitations and Challenges\n",
        "To address budget constraints, a combination of open source and free resources will be used. Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jOHmOOCkEvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2178ed-cfef-4526-d90f-bef845700e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCSnoYOsUFmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0224cd6-cc29-4db0-f45f-a88d84169465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.4.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.12.1 tensorboard-2.16.2 tensorflow-2.16.2 tensorflow-text-2.16.1\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.6.2)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=415899d7b2cd0cbd2914318850d87682ead5a797b78bc161b7c4c9b7ae6977c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=771ab8f20a5f92feb4a409ebb0e50258774c348f9d90ee05f793f55bee2f8cf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.8)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.12.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.15.4)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.2.2)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.13.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n",
            "Installing collected packages: asn1crypto, tomlkit, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.11.0 tomlkit-0.13.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=347ddb1aeeb3be81b20b63bb1a84dc9b9b2daea29f438d87bbc64e1afcbb3eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.14.1)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Suppress DeprecationWarnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# Install or upgrade TensorFlow and TensorFlow Text\n",
        "!pip install --upgrade tensorflow tensorflow-text\n",
        "\n",
        "# Import TensorFlow and TensorFlow Text directly\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "# transformers\n",
        "try:\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "except ImportError:\n",
        "    !pip install transformers\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "\n",
        "# gensim\n",
        "try:\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "except ImportError:\n",
        "    !pip install gensim\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "# sumy\n",
        "try:\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "except ImportError:\n",
        "    !pip install sumy\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "\n",
        "# pyspellchecker\n",
        "try:\n",
        "    from spellchecker import SpellChecker\n",
        "except ImportError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install pyspellchecker\n",
        "    from spellchecker import SpellChecker\n",
        "\n",
        "# faiss\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    !pip install faiss-cpu\n",
        "    import faiss\n",
        "\n",
        "# snowflake.connector\n",
        "try:\n",
        "    import snowflake.connector\n",
        "except ImportError:\n",
        "    !pip install snowflake-connector-python\n",
        "    import snowflake.connector\n",
        "\n",
        "# pandas\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    !pip install pandas\n",
        "    import pandas as pd\n",
        "\n",
        "# requests\n",
        "try:\n",
        "    import requests\n",
        "except ImportError:\n",
        "    !pip install requests\n",
        "    import requests\n",
        "\n",
        "# BeautifulSoup\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "except ImportError:\n",
        "    !pip install beautifulsoup4\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "# nltk\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "except ImportError:\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# re\n",
        "import re\n",
        "\n",
        "# langdetect\n",
        "try:\n",
        "    from langdetect import detect\n",
        "except ImportError:\n",
        "    !pip install langdetect\n",
        "    from langdetect import detect\n",
        "\n",
        "# torch\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    !pip install torch\n",
        "    import torch\n",
        "\n",
        "# numpy\n",
        "try:\n",
        "    import numpy as np\n",
        "except ImportError:\n",
        "    !pip install numpy\n",
        "    import numpy as np\n",
        "\n",
        "# pyLDAvis\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError:\n",
        "    !pip install pyLDAvis\n",
        "    import pyLDAvis\n",
        "\n",
        "# pickle\n",
        "import pickle\n",
        "\n",
        "# sklearn\n",
        "try:\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except ImportError:\n",
        "    !pip install scikit-learn\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Other necessary imports\n",
        "import time\n",
        "import itertools\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Logging\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "log_path = '/content/drive/MyDrive/Colab Notebooks/model_logs.txt'\n",
        "logging.basicConfig(\n",
        "    filename=log_path,\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Initial Setup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "!pip install --upgrade tensorflow tensorflow-text\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "logging.info(\"Initial setup completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuB21f58ptoj",
        "outputId": "6cb56dbe-da7e-4d1b-d541-646fd64df74b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.2)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOUiNzICFDj8"
      },
      "source": [
        "# 1 Architectures and Frameworks\n",
        "\n",
        "This document provides an overview of various architectures, models, and tools used in natural language processing tasks. Understanding the strengths and weaknesses of different approaches is crucial for designing effective NLP systems tailored to my specific use case and requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VkM_EQvV6sw"
      },
      "source": [
        "\n",
        "## 1.1 NLP Architectures\n",
        "\n",
        "### 1.1.1 Traditional Models\n",
        "\n",
        "**Solution:** Bag-of-Words (BoW)  \n",
        "**Description:** Represents text data as a collection of unique words and their frequencies.  \n",
        "**Example:** TfidfVectorizer  \n",
        "**Pros:**  \n",
        "- Simple and efficient representation.\n",
        "- Works well for tasks like sentiment analysis and document classification.\n",
        "\n",
        "**Cons:**  \n",
        "- Ignores word order and context.\n",
        "- Doesn't capture semantic meanings well.  \n",
        "\n",
        "**Solution:** N-gram Model  \n",
        "**Description:** Represents text data as a sequence of N consecutive words (N-grams).  \n",
        "**Examples:** Bigram, Trigram  \n",
        "**Pros:**  \n",
        "- Captures some local word order and context.\n",
        "- Simple and easy to implement.\n",
        "\n",
        "**Cons:**  \n",
        "- Limited in capturing long-range dependencies.\n",
        "- Can become computationally expensive with larger N values.\n",
        "\n",
        "**Solution:** Rule-Based Models  \n",
        "**Description:** Uses a set of manually crafted linguistic rules to process text data.  \n",
        "**Examples:** Regular Expressions, SpaCy Rule-Based Matching  \n",
        "**Pros:**  \n",
        "- High precision for well-defined tasks.\n",
        "- Transparent and interpretable.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires extensive domain knowledge and manual effort.\n",
        "- Not scalable for large or diverse datasets.\n",
        "\n",
        "### 1.1.2 Statistical NLP Models\n",
        "\n",
        "**Solution:** Hidden Markov Models (HMM)  \n",
        "**Description:** Sequential text models based on hidden state transitions.  \n",
        "**Example:** hmmlearn  \n",
        "**Pros:**  \n",
        "- Captures sequential dependencies effectively.\n",
        "- Suitable for tasks like part-of-speech tagging and named entity recognition.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- May struggle with capturing complex semantic relationships.\n",
        "\n",
        "**Solution:** Conditional Random Fields (CRF)  \n",
        "**Description:** Sequence labeling models.  \n",
        "**Example:** sklearn-crfsuite  \n",
        "**Pros:**  \n",
        "- Effective for sequential labeling tasks.\n",
        "- Incorporates feature dependencies between adjacent labels.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- Less effective for capturing long-range dependencies.\n",
        "\n",
        "**Solution:** Support Vector Machines (SVM)  \n",
        "**Description:** A supervised learning model used for classification and regression analysis.  \n",
        "**Example:** scikit-learn  \n",
        "**Pros:**  \n",
        "- Effective in high-dimensional spaces.\n",
        "- Versatile with different kernel functions for flexibility in decision boundaries.  \n",
        "\n",
        "**Cons:**  \n",
        "- Memory-intensive for large datasets.\n",
        "- May require careful selection of kernel functions and tuning parameters.\n",
        "\n",
        "### 1.1.3 Deep Learning Models\n",
        "\n",
        "**Solution:** Word Embeddings  \n",
        "**Description:** Represent words as dense vectors in a continuous vector space.  \n",
        "**Examples:** Word2Vec, GloVe  \n",
        "**Pros:**  \n",
        "- Captures semantic meanings and relationships between words.\n",
        "- Provides dense vector representations suitable for downstream tasks.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of data for training.\n",
        "- Struggles with out-of-vocabulary words.\n",
        "\n",
        "**Solution:** Recurrent Neural Networks (RNN)  \n",
        "**Description:** Neural networks that process sequences by iterating through elements.  \n",
        "**Examples:** Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)  \n",
        "**Pros:**  \n",
        "- Effective for capturing sequential dependencies in data.\n",
        "- Suitable for tasks like language modeling and machine translation.\n",
        "\n",
        "**Cons:**  \n",
        "- Vulnerable to vanishing and exploding gradient problems.\n",
        "- Computationally expensive to train.\n",
        "\n",
        "**Solution:** Convolutional Neural Networks (CNN) for Text  \n",
        "**Description:** Application of convolution operations to capture local dependencies in text.  \n",
        "**Examples:** TextCNN, KimCNN  \n",
        "**Pros:**  \n",
        "- Effective for tasks like sentence classification and text categorization.\n",
        "- Captures local patterns and relationships in text.  \n",
        "\n",
        "**Cons:**  \n",
        "- May not capture long-range dependencies as effectively as other solutions.\n",
        "- Requires careful tuning of convolutional filters and pooling strategies.\n",
        "\n",
        "### 1.1.4 Transformers\n",
        "\n",
        "**Solution:** Transformer Models  \n",
        "**Description:** Neural network architecture based entirely on self-attention mechanisms.  \n",
        "**Examples:** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer)  \n",
        "**Pros:**  \n",
        "- Captures long-range dependencies effectively.\n",
        "- Parallelizable training process.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of computational resources.\n",
        "- Limited interpretability compared to traditional models.\n",
        "\n",
        "**Solution:** Pre-trained Models  \n",
        "**Description:** Models pre-trained on large corpora and fine-tuned for specific tasks.  \n",
        "**Examples:** BERT, GPT, T5  \n",
        "**Pros:**  \n",
        "- Leverage large amounts of unlabeled data for pre-training.\n",
        "- Achieve state-of-the-art performance on various NLP tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Resource-intensive pre-training process.\n",
        "- May require substantial computational resources for fine-tuning.\n",
        "\n",
        "**Solution:** Attention Mechanisms  \n",
        "**Description:** Mechanisms that enable models to focus on specific parts of the input.  \n",
        "**Examples:** Self-Attention, Multi-Head Attention  \n",
        "**Pros:**  \n",
        "- Improves the ability to capture dependencies and relationships within the data.\n",
        "- Enhances performance in various machine translation and text summarization.\n",
        "\n",
        "**Cons:**  \n",
        "- Can be computationally intensive.\n",
        "- Complexity increases with the number of attention heads and layers.\n",
        "\n",
        "### 1.1.5 Additional Models and Techniques\n",
        "\n",
        "**Solution:** Retriever-Generator Models  \n",
        "**Description:** Models combine retrieval and generation components for text generation tasks.  \n",
        "**Examples:** RAG  \n",
        "**Pros:**  \n",
        "- Incorporates both structured and unstructured information for generation.\n",
        "- Produces more diverse and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires efficient retrieval mechanisms.\n",
        "- Increased complexity in model architecture.\n",
        "\n",
        "**Solution:** Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)  \n",
        "**Description:** A variant of RAG that incorporates knowledge graphs.  \n",
        "**Examples:** Graph-BERT  \n",
        "**Pros:**  \n",
        "- Integrates structured knowledge for improved understanding and generation.\n",
        "- Enables more coherent and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires high-quality and curated knowledge graphs.\n",
        "- Increased computational complexity compared to standard RAG.\n",
        "\n",
        "**Solution:** Elastic Search  \n",
        "**Description:** Distributed search and analytics engine for indexing and searching big data.  \n",
        "**Examples:** Elasticsearch, Apache Solr  \n",
        "**Pros:**  \n",
        "- Scalable and distributed architecture.\n",
        "- Supports full-text search and complex query structures.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires infrastructure for deployment and maintenance.\n",
        "- Indexing and search performance may degrade with large datasets.\n",
        "\n",
        "## Architecture Options Score Card\n",
        "\n",
        "| Model/Architecture  | Key Strength                   | CPU Compatibility | Ease of Use | Performance & Accuracy | Scalability | Integration | Total |\n",
        "|---------------------|-------------------------------|-------------------|-------------|------------------------|-------------|-------------|-------|\n",
        "| RAG                 | Context Understanding          | 1                 | 1           | 2                      | 2           | 2           | 8     |\n",
        "| BoW                 | Simplicity                     | 2                 | 2           | 0                      | 2           | 2           | 8     |\n",
        "| N-gram Model        | Local Context                  | 2                 | 2           | 1                      | 1           | 2           | 8     |\n",
        "| Pre-trained Models  | Accuracy                       | 1                 | 1           | 1                      | 2           | 2           | 7     |\n",
        "| Word Embeddings     | Semantic Understanding         | 1                 | 1           | 2                      | 1           | 2           | 7     |\n",
        "| Elastic Search      | Scalability                    | 2                 | 1           | 1                      | 2           | 1           | 7     |\n",
        "| CNN                 | Local Pattern Recognition      | 1                 | 1           | 2                      | 1           | 1           | 6     |\n",
        "| Rule Based          | High Precision                 | 2                 | 2           | 0                      | 1           | 1           | 6     |\n",
        "| Transformer Models  | State-of-the-Art               | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| KERAG               | Knowledge Integration          | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| HMM                 | Sequence Modeling              | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| CRF                 | Sequential Labeling            | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| SVM                 | Versatile                      | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Sequence Models     | Order Preservation             | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Attention Mechanisms| Focus on Specific Parts        | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| RNN                 | Sequential Dependencies        | 0                 | 1           | 2                      | 0           | 1           | 4     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The score card was a valuable tool to reduce options down to those most appropriate for this project. A final audit is as follows:\n",
        "\n",
        "### RAG (Retriever-Augmented Generation):\n",
        "- **Strengths:** Excellent at context understanding and contextually relevant responses.\n",
        "- **Weaknesses:** Computationally intensive and may require more resources.\n",
        "- **Suitability:** High, if you need detailed, context-aware answers and have the necessary computational resources.\n",
        "\n",
        "### BoW (Bag-of-Words):\n",
        "- **Strengths:** Simple and efficient, easy to implement, and works well for basic tasks.\n",
        "- **Weaknesses:** Ignores word order and context, may not capture semantic meaning well.\n",
        "- **Suitability:** Moderate, for straightforward tasks where simplicity and efficiency are prioritized over contextual understanding.\n",
        "\n",
        "### N-gram Model:\n",
        "- **Strengths:** Captures some local word order and context, relatively simple to implement.\n",
        "- **Weaknesses:** Limited in capturing long-range dependencies, can become computationally expensive with larger N values.\n",
        "- **Suitability:** Moderate, for tasks where local context is important, but computational efficiency is still needed.\n",
        "\n",
        "## Conclusion\n",
        "The RAG model is most appropriate for this effort given its heavy use of domain specific information (that may be missing from a stand-alone pre-trained model). Its contextual accuracy has a higher weighted value for this use case than Ease of Use. Having said that, it is a computationally heavy architecture, and it is unclear if free cloud CPU resources will be sufficient.\n",
        "\n",
        "## Example of RAG Model Implementation\n",
        "1. **Query:** \"What are the benefits of using a RAG model?\"\n",
        "2. **Retriever:**\n",
        "   - Searches a corpus for relevant documents or passages related to \"benefits of using a RAG model\".\n",
        "   - Retrieves top-k documents or passages that discuss the advantages of RAG models.\n",
        "3. **Generator:**\n",
        "   - Takes the retrieved documents and generates a response: \"A RAG model combines the strengths of information retrieval and generative modeling. It retrieves relevant documents to provide context and generates accurate and contextually appropriate responses. This makes it highly effective for tasks requiring detailed and specific information.\"\n",
        "\n",
        "For this project, the Retriever will be the corpus scraped from the online forum processed with Word Embeddings, and the Generator will be from a pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Options\n",
        "\n",
        "Example 1: LLM as the Primary Responder\n",
        "User asks a question.\n",
        "System retrieves relevant documents from a supplemental corpus using a retrieval system (e.g., Elasticsearch, FAISS).\n",
        "Retrieved documents are used to create a prompt that is fed into the LLM.\n",
        "LLM generates a response based on the retrieved documents.\n",
        "In this approach, the LLM takes the retrieved documents and synthesizes a response. The LLM is responsible for understanding the context, extracting relevant information from the documents, and generating a coherent answer.  \n",
        "<br>  \n",
        "\n",
        "Example 2: LLM as the Editor\n",
        "User asks a question.\n",
        "System retrieves relevant documents from a supplemental corpus using a retrieval system.\n",
        "System extracts the answer from the retrieved documents.\n",
        "LLM ensures the answer is grammatically correct and potentially enhances the response for fluency."
      ],
      "metadata": {
        "id": "8KNUjguEluC3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpRVNUJFDm-"
      },
      "source": [
        "## 1.2 Generator Options\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** GPT (GPT-2)  \n",
        "**Description:** Generative Pre-trained Transformer for generating text.  \n",
        "**Pros:**  \n",
        "- Highly capable of generating coherent and contextually relevant text.\n",
        "- Free to access and use.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources for fine-tuning.\n",
        "- GPT-2 is less powerful than newer models.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Claude (Claude 3)  \n",
        "**Description:** AI assistant designed for safety and ethical considerations.  \n",
        "**Pros:**  \n",
        "- Enhanced safety features and focus on ethical AI use.\n",
        "- Designed for robust handling of varying text lengths.\n",
        "\n",
        "**Cons:**  \n",
        "- Primarily available for research access, which may limit commercial use.\n",
        "- Conditional access, potentially limiting deployment flexibility.\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** DistilBART (sshleifer/distilbart-cnn-12-6)  \n",
        "**Description:** A distilled version of BART optimized for efficiency.  \n",
        "**Pros:**  \n",
        "- Optimized for CPU usage, making it efficient for resource usage.\n",
        "- Open-source and free, allowing for flexible use and customization.  \n",
        "\n",
        "**Cons:**  \n",
        "- Less powerful than the full BART model due to distillation.\n",
        "- May require additional integration efforts compared to more commercial models.\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** T5 (t5-small)  \n",
        "**Description:** Text-to-Text Transfer Transformer for various NLP tasks.  \n",
        "**Pros:**  \n",
        "- Highly flexible and powerful for a wide range of text-to-text tasks.\n",
        "- Open-source and free to use and fine-tune.\n",
        "\n",
        "**Cons:**  \n",
        "- May require additional preprocessing steps for certain tasks.\n",
        "- The small version is less powerful compared to larger T5 models.\n",
        "\n",
        "**Vendor:** Amazon  \n",
        "**Package:** AWS Comprehend  \n",
        "**Description:** Managed NLP service for text analysis and insights.  \n",
        "**Pros:**  \n",
        "- Fully managed service, reducing the burden of infrastructure management.\n",
        "- Tight integration with AWS ecosystem, offering scalability and ease of use.\n",
        "  \n",
        "**Cons:**  \n",
        "- API-dependent, limiting control over the underlying models.\n",
        "- Cloud performance may not be as optimized as specific CPU performance.\n",
        "\n",
        "## Generator Option Score Card\n",
        "\n",
        "| Vendor    | Package       | Key Strength               | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------|----------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | DistilBART    | CPU Efficiency             | 2                 | 2           | 1                      | 2                         | 2           | 9     |\n",
        "| Google    | T5-small      | Flexibility                | 2                 | 1           | 2                      | 2                         | 1           | 8     |\n",
        "| Amazon    | Comprehend    | Managed Service            | 1                 | 2           | 1                      | 2                         | 1           | 7     |\n",
        "| OpenAI    | GPT-2         | Coherent Text Generation   | 0                 | 2           | 2                      | 2                         | 0           | 6     |\n",
        "| Anthropic | Claude 3      | Ethical AI                 | 1                 | 1           | 2                      | 1                         | 1           | 6     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLhXj52bwtE"
      },
      "source": [
        "\n",
        "## 1.3 Frameworks and Tools\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** TensorFlow  \n",
        "**Description:** Open-source ML framework for building and deploying models.  \n",
        "**Pros:**  \n",
        "- Comprehensive ecosystem with deep learning support.\n",
        "- Scalable on both CPUs and GPUs.  \n",
        "\n",
        "**Cons:**  \n",
        "- Steeper learning curve than some other frameworks.\n",
        "- Limited support for dynamic computation graphs.\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** PyTorch  \n",
        "**Description:** Open-source deep learning framework by Meta AI Research.  \n",
        "**Pros:**  \n",
        "- Pythonic and intuitive interface for model development.\n",
        "- Dynamic computation graph for easier debugging and experimentation.  \n",
        "\n",
        "**Cons:**  \n",
        "- Less optimized for production deployment than TensorFlow.\n",
        "- Limited built-in support for distributed training.\n",
        "\n",
        "**Vendor:** AWS  \n",
        "**Package:** Amazon Bedrock  \n",
        "**Description:** Fully managed service for building, deploying, and scaling ML models.  \n",
        "**Pros:**  \n",
        "- Integrated support for various ML frameworks.\n",
        "- Scalable infrastructure with extensive AWS services integration.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires AWS-specific knowledge for optimal use.\n",
        "- Potentially high costs for extensive usage.\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "## Framework Score Card\n",
        "\n",
        "| Vendor    | Package             | Key Strength                  | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------------|-------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | PyTorch             | Pythonic Interface            | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Google    | TensorFlow          | Comprehensive Ecosystem       | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| AWS       | Bedrock             | Fully Managed Service         | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| OpenAI    | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Anthropic | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yafxx82Qbq17"
      },
      "source": [
        "\n",
        "## 1.4 Embedding\n",
        "\n",
        "**Solution:** Universal Sentence Encoder  \n",
        "**Provider:** Google  \n",
        "**Libraries:** TensorFlow Hub  \n",
        "**Pros:**  \n",
        "- Captures sentence-level embeddings, enhancing text understanding.\n",
        "- Efficient and easy to integrate with TensorFlow models.  \n",
        "**Cons:**  \n",
        "- May not capture fine-grained word-level nuances.\n",
        "- Performance can vary depending on the complexity of the sentences.\n",
        "\n",
        "**Solution:** FastText  \n",
        "**Provider:** Meta  \n",
        "**Libraries:** Gensim, TensorFlow, PyTorch  \n",
        "**Pros:**  \n",
        "- Handles out-of-vocabulary words as bags of character n-grams.\n",
        "- Captures subword information, enhancing the representation of rare words.  \n",
        "**Cons:**  \n",
        "- Increases computational complexity due to subword representations.\n",
        "- Larger model size compared to Word2Vec and GloVe.\n",
        "\n",
        "**Solution:** Amazon SageMaker Embeddings  \n",
        "**Provider:** AWS  \n",
        "**Libraries:** Amazon SageMaker  \n",
        "**Pros:**  \n",
        "- Provides pre-built models for embeddings, simplifying deployment.\n",
        "- Integrates seamlessly with other AWS services for scalability.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires familiarity with the AWS ecosystem.\n",
        "- Costs can increase with extensive usage.\n",
        "\n",
        "**Solution:** GPT-3 Embeddings  \n",
        "**Provider:** OpenAI  \n",
        "**Libraries:** OpenAI API  \n",
        "**Pros:**  \n",
        "- Generates high-quality, contextually relevant text embeddings.\n",
        "- Handles long-range dependencies and contextual information.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources.\n",
        "- Access may require API usage and associated costs.\n",
        "\n",
        "**Solution:** Claude Embeddings  \n",
        "**Provider:** Anthropic  \n",
        "**Libraries:** Anthropic API  \n",
        "**Pros:**  \n",
        "- Offers state-of-the-art embeddings with a focus on safety and ethics.\n",
        "- Handles context and nuances effectively for complex tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Primarily available for research access, limiting commercial use.\n",
        "- Access may require API usage and associated costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewprjkrLbicq"
      },
      "source": [
        "## 1.5 Tokenization\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "**Solution:** Word-level Tokenization  \n",
        "**Libraries:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers  \n",
        "**Pros:**  \n",
        "- Preserves word integrity and meaning, crucial for comprehension tasks.\n",
        "- Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "\n",
        "**Cons:**  \n",
        "- Can result in a large vocabulary, increasing memory and processing needs.\n",
        "- May overlook nuances in character-level variations.\n",
        "\n",
        "**Solution:** Character-level Tokenization  \n",
        "**Libraries:** Supported by deep learning frameworks like TensorFlow and Keras  \n",
        "**Pros:**  \n",
        "- Captures morphological nuances at the character level, aiding rich languages.\n",
        "- Simplifies vocabulary to unique characters, reducing model complexity.  \n",
        "\n",
        "**Cons:**  \n",
        "- Leads to longer input sequences, increasing computational costs.\n",
        "- Loses direct access to semantic information in words or phrases.\n",
        "\n",
        "**Solution:** Subword Tokenization  \n",
        "**Libraries:** A blend of word-level and character-level tokenization methods  \n",
        "**Pros:**  \n",
        "- Balances vocabulary size and semantic information preservation.\n",
        "- Handles rare or unknown words by breaking them into recognizable subwords.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "- Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "**Solution:** Model-Specific Tokenization  \n",
        "**Libraries:** Hugging Face's transformers library provides access to pre-built tokenizers  \n",
        "**Pros:**  \n",
        "- Ensures tokenization consistency with the model's original training data.\n",
        "- Reduces the need for extra preprocessing steps and custom tokenization.  \n",
        "\n",
        "**Cons:**  \n",
        "- Limited flexibility to change tokenization beyond the model's method.\n",
        "- May not be efficient for tasks outside the model's specific design.\n",
        "\n",
        "Tokenization and embedding must be considered together because tokenization directly impacts the quality of embedding. The choice of tokenization method determines how text is segmented, which in turn affects how embeddings capture context and meaning. Inconsistent tokenization can lead to poor embeddings and reduced model performance. Properly aligned tokenization and embedding processes ensure that the text's structure and semantics are preserved, enhancing overall model effectiveness.\n",
        "\n",
        "## Tokenization and Embedding Score Card\n",
        "\n",
        "| Vendor    | Embedder                    | Tokenizer                      | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-----------------------------|--------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | Universal Sentence Encoder  | TensorFlow Text                | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Meta      | FastText                    | Gensim Tokenizer or NLTK       | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| OpenAI    | GPT-2 Embeddings            | GPT-2 Tokenizer                | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Amazon    | SageMaker Embeddings        | SageMaker’s preprocessing tools| 1                 | 2           | 2                      | 2                         | 1           | 8     |\n",
        "| Anthropic | Claude Embeddings           | Built-in tokenization          | 1                 | 2           | 2                      | 2                         | 0           | 7     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvnMsI-bcPS"
      },
      "source": [
        "\n",
        "## 1.6 Solution Leader Board\n",
        "\n",
        "| Vendor    | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | 6                 | 4           | 6                      | 6                         | 5           | 27    |\n",
        "| Meta      | 6                 | 6           | 5                      | 6                         | 4           | 27    |\n",
        "| Amazon    | 4                 | 5           | 5                      | 6                         | 4           | 24    |\n",
        "| OpenAI    | 4                 | 6           | 6                      | 6                         | 2           | 24    |\n",
        "| Anthropic | 4                 | 5           | 6                      | 5                         | 2           | 22    |\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The results are interesting and paint a clearer picture of how the strengths of each option compare. While close, Google seems to have an edge on Performance and Accuracy but suffers a bit on Ease of Use and Scalability. Having said that, the use of Google would support my efforts to gain Google Cloud Certification—a highly desirable skill in the job market. With that in mind, I’ll be moving forward with a Google dominant stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBhF-lbTb6Q7"
      },
      "source": [
        "# 2 Develop Corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWbtT5GoFDpf"
      },
      "source": [
        "## 2.1 Data Ethics\n",
        "The data collected here is a collection of posts from widely available public forum. However, should this project move into public distribution, additional steps will be necessary to ensure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the project's goals and data gathering process.\n",
        "\n",
        "### Data Collection\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "#### Web Scraping\n",
        "**Tools:** Beautiful Soup, online SaaS products  \n",
        "**Pros:**  \n",
        "- Direct Access to Targeted Data: Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "- Efficiency in Data Collection: Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.  \n",
        "\n",
        "**Cons:**  \n",
        "- Potential for Incomplete Data: May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "- Ethical and Legal Considerations: Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "- Very Platform Dependent: Forum-specific solutions result in forum-specific data schemas that must be reverse engineered for successful text extraction.\n",
        "\n",
        "#### Forum-specific APIs\n",
        "**Tools:** Python (`requests` library for API calls and `json` library for handling responses)  \n",
        "**Pros:**  \n",
        "- Structured and Reliable Data Retrieval: APIs provide structured data, making it easier to process and integrate into your project.\n",
        "- Efficient and Direct Access: Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "- Compliance and Ethical Data Use: Utilizing APIs respects the forum's data policies and ensures access is in line with user agreements.  \n",
        "\n",
        "**Cons:**  \n",
        "- Rate Limiting: APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "- API Changes: Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "- Access Restrictions: Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lJ9umeIXNXy"
      },
      "source": [
        "## 2.2 Ingest Corpus from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y75VMgRttCo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8e48df4a-49c3-4a90-c97a-c90f879e0450",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Remove this line if you want to create a new corpus",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a88e785940a4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2.2 Ingest Corpus from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Remove this line if you want to create a new corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Remove this line if you want to create a new corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Remove this line if you want to create a new corpus"
          ]
        }
      ],
      "source": [
        "# 2.2 Ingest Corpus from scratch\n",
        "\n",
        "raise RuntimeError(\"Remove this line if you want to create a new corpus\")\n",
        "\n",
        "# Remove this line if you want to create a new corpus\n",
        "\n",
        "\n",
        "\n",
        "# Step 1 Create Corpus\n",
        "# Fetch and process forum threads\n",
        "# Corpus created in LDA notebook can be used.\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "def forum_thread_ids():\n",
        "    threads = 1  # Set the number of incremental threads to process here\n",
        "\n",
        "    file_path = os.path.join(BASE_PATH, 'e9_forum_thread_ids.csv')\n",
        "\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(e9_forum_thread_ids['thread_id'].iloc[-1])\n",
        "    else:\n",
        "        e9_forum_thread_ids = pd.DataFrame(columns=['thread_id'])\n",
        "        last_thread_id = 0\n",
        "\n",
        "    next_thread_id = last_thread_id + 1\n",
        "    new_urls = [{'thread_id': thread_id} for thread_id in range(next_thread_id, next_thread_id + threads)]\n",
        "\n",
        "    new_df = pd.DataFrame(new_urls)\n",
        "    e9_forum_thread_ids = pd.concat([e9_forum_thread_ids, new_df], ignore_index=True)\n",
        "    e9_forum_thread_ids.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Starting with thread_id {last_thread_id}\")\n",
        "    print(f\"Processing additional {threads} thread(s)\")\n",
        "    print(f\"Ending with thread_id {next_thread_id + threads - 1}\")\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def forum_thread_url(df):\n",
        "    if df.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "\n",
        "    df.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_url.csv'), index=False)\n",
        "    return df\n",
        "\n",
        "def forum_thread_first_post(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        post_content = first_post.get_text(strip=True) if first_post else \"No content found\"\n",
        "        data.append({'thread_id': thread_id, 'thread_first_post': post_content})\n",
        "\n",
        "    forum_first_post = pd.DataFrame(data)\n",
        "    forum_first_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_first_post.csv'), index=False)\n",
        "    return forum_first_post\n",
        "\n",
        "def forum_thread_all_post(df):\n",
        "    post_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "            post_data.append({'thread_id': row['thread_id'], 'post_raw': content})\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "    e9_forum_posts['thread_all_posts'] = e9_forum_posts['post_raw'].astype(str)\n",
        "    e9_forum_thread_all_post = e9_forum_posts.groupby('thread_id')['thread_all_posts'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "    e9_forum_thread_all_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_all_post.csv'), index=False)\n",
        "    return e9_forum_thread_all_post\n",
        "\n",
        "def forum_corpus(e9_forum_thread_url, e9_forum_thread_first_post, e9_forum_thread_all_post):\n",
        "    agg_df_1 = pd.merge(e9_forum_thread_url, e9_forum_thread_first_post, on='thread_id', how='left')\n",
        "    agg_df_2 = pd.merge(agg_df_1, e9_forum_thread_all_post, on='thread_id', how='left')\n",
        "\n",
        "    e9_forum_corpus = agg_df_2.dropna()\n",
        "    corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "    if os.path.exists(corpus_path) and os.path.getsize(corpus_path) > 0:\n",
        "        existing_corpus = pd.read_csv(corpus_path)\n",
        "        e9_forum_corpus = pd.concat([existing_corpus, e9_forum_corpus]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    e9_forum_corpus.columns = e9_forum_corpus.columns.str.upper()\n",
        "    e9_forum_corpus.to_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv'), index=False)\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    e9_forum_thread_ids = forum_thread_ids()\n",
        "    e9_forum_thread_url_df = forum_thread_url(e9_forum_thread_ids)\n",
        "    e9_forum_thread_first_post_df = forum_thread_first_post(e9_forum_thread_url_df)\n",
        "    e9_forum_thread_all_post_df = forum_thread_all_post(e9_forum_thread_url_df)\n",
        "    e9_forum_corpus_df = forum_corpus(e9_forum_thread_url_df, e9_forum_thread_first_post_df, e9_forum_thread_all_post_df)\n",
        "    print(f\"Output saved to {os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65J-_EM5DTlM"
      },
      "source": [
        "## 2.3 Ingest previously compiled corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ2e5CwqDYl2"
      },
      "outputs": [],
      "source": [
        "# 2.3 Ingest previously compiled corpus\n",
        "\n",
        "# Data here is from corpus workbook stored in Snowflake\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=')\n",
        "                    os.environ[key] = value\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    try:\n",
        "        conn = snowflake.connector.connect(\n",
        "            user=os.environ.get('USER'),\n",
        "            password=os.environ.get('PASSWORD'),\n",
        "            account=os.environ.get('ACCOUNT'),\n",
        "        )\n",
        "\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\"\n",
        "        order by 1 asc;\n",
        "        \"\"\"\n",
        "        cur.execute(query)\n",
        "        e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "        # Log the count of records retrieved\n",
        "        logging.info(f\"Number of records retrieved: {len(e9_forum_corpus)}\")\n",
        "        return e9_forum_corpus\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data from Snowflake: {str(e)}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n",
        "\n",
        "# Main sequence\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "# Load credentials\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Fetch data from Snowflake and print the count of records retrieved\n",
        "e9_forum_corpus = fetch_data_from_snowflake()\n",
        "\n",
        "if not e9_forum_corpus.empty:\n",
        "    try:\n",
        "        # Save the data to a CSV file\n",
        "        output_path = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_dirty.csv'\n",
        "        e9_forum_corpus.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Data saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving data to CSV: {str(e)}\")\n",
        "else:\n",
        "    logging.warning(\"No data retrieved to save.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A97WWzNlTEmj"
      },
      "source": [
        "# 3 Preprocessing Text\n",
        "\n",
        "The collected text is very unstructured and needs a reasonable amount of pre-processing to make it usable for NLP. This will address values that are either not localized, use slang, or do not have value from an NLP perspective.\n",
        "\n",
        "- **Clean the Text:**\n",
        "  - Remove HTML tags, extra whitespace, non-printable characters, and other irrelevant elements.\n",
        "\n",
        "- **Standardize the Text:**\n",
        "  - Convert all characters to lowercase to ensure uniformity.\n",
        "\n",
        "- **Filter Out Common Stop Words:**\n",
        "  - Remove stop words to focus on more meaningful content.\n",
        "\n",
        "- **Remove Duplicate Entries:**\n",
        "  - Ensure the uniqueness of the data by eliminating duplicates.\n",
        "\n",
        "- **Lemmatization or Stemming:**\n",
        "  - Convert words to their base or dictionary form to consolidate similar forms of a word.\n",
        "\n",
        "- **Anonymize Personal Information:**\n",
        "  - Identify and anonymize personal information or specific entity names to maintain privacy.\n",
        "\n",
        "- **Remove Irrelevant Sections:**\n",
        "  - Remove sections of the text that do not contribute to the knowledge base or are off-topic.\n",
        "\n",
        "- **Tokenization:**\n",
        "  - Break down the text into smaller units called tokens. Use a tokenizer compatible with your chosen model, such as the BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO22BY4_rScA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d11dc15-f790-4c26-f635-a171ffea8690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to /content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_clean.csv\n"
          ]
        }
      ],
      "source": [
        "# 3 Preprocessing Text\n",
        "# Clean and preprocess forum data\n",
        "\n",
        "# Define the path to the local CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_dirty.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "e9_forum_corpus_dirty = pd.read_csv(csv_file_path)\n",
        "\n",
        "\n",
        "def remove_urls(df):\n",
        "    \"\"\"Removes URLs from the text.\"\"\"\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: url_pattern.sub(r'', str(text)))\n",
        "    return df\n",
        "\n",
        "def alpha_numeric(df):\n",
        "    \"\"\"Removes non-alphanumeric characters and unwanted patterns from text.\"\"\"\n",
        "    pattern_email = re.compile(r'\\S*@\\S*\\s?')\n",
        "    pattern_non_alpha = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: pattern_non_alpha.sub('', pattern_email.sub('', str(text))))\n",
        "        df[column] = df[column].apply(lambda text: re.sub(r'\\s+', ' ', text).strip())  # Remove extra spaces\n",
        "    return df\n",
        "\n",
        "def spell_check(df):\n",
        "    \"\"\"Corrects spelling errors in the text with caching.\"\"\"\n",
        "    spell = SpellChecker()\n",
        "    cache = {}\n",
        "\n",
        "    def correct_word(word):\n",
        "        if word in cache:\n",
        "            return cache[word]\n",
        "        else:\n",
        "            correction = spell.correction(word) or word\n",
        "            cache[word] = correction\n",
        "            return correction\n",
        "\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([correct_word(word) for word in text.split()]))\n",
        "    return df\n",
        "\n",
        "def remove_stop_words(df):\n",
        "    \"\"\"Removes stop words from the text.\"\"\"\n",
        "    stop_words_set = set(stopwords.words('english')).union({'car', 'csi', 'cs', 'csl', 'e9', 'coupe', 'http', 'https', 'www', 'ebay', 'bmw', 'html'})\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in stop_words_set and len(word) > 2]))\n",
        "    return df\n",
        "\n",
        "def tokenize_and_lemmatize(df):\n",
        "    \"\"\"Tokenizes and lemmatizes the text in specified columns using TensorFlow Text.\"\"\"\n",
        "    tokenizer = tf_text.WhitespaceTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def tokenize_text(text):\n",
        "        tokens = tokenizer.tokenize(text).numpy().astype(str).tolist()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(tokenize_text)\n",
        "    return df\n",
        "\n",
        "def clean_nan_values(df):\n",
        "    \"\"\"Removes or replaces NaN values in the dataset and converts all entries to strings.\"\"\"\n",
        "    df.fillna('', inplace=True)\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].astype(str)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data processing pipeline.\"\"\"\n",
        "    # Load credentials\n",
        "    # path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "    # load_credentials(path_to_credentials)\n",
        "\n",
        "    # Fetch data from Snowflake\n",
        "    # e9_forum_corpus = fetch_data_from_snowflake()\n",
        "\n",
        "    # Process the data\n",
        "    df = remove_urls(e9_forum_corpus_dirty)\n",
        "    df = alpha_numeric(df)\n",
        "    # df = spell_check(df)  # Need to find a faster spell checking process\n",
        "    #df = remove_stop_words(df)\n",
        "    df = tokenize_and_lemmatize(df)\n",
        "    #df = clean_nan_values(df)  # Final NaN cleaning and type conversion step\n",
        "    df.columns = df.columns.str.upper()  # Convert column names to uppercase\n",
        "\n",
        "    # Save the cleaned data\n",
        "    output_path = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_clean.csv'\n",
        "    ''\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Cleaned data saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "# 4 Clustering and Summarization\n",
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through multiple options. For this effort, the following solutions were scored to reduce the potential solution set:\n",
        "\n",
        "| Provider  | Specific Package | Key Strength              | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|------------------|---------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | DistilBART       | Optimized for CPU usage   | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Google    | T5 (t5-small)    | Efficient CPU usage       | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Amazon    | AWS Comprehend   | Integrated AWS service    | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| OpenAI    | GPT (GPT-2)      | Optimal for CPUs          | 2                 | 2           | 2                      | 1                         | 1           | 8     |\n",
        "| Anthropic | Claude (Claude 3)| Enhanced safety features  | 2                 | 2           | 2                      | 1                         | 1           | 8     |\n",
        "\n",
        "\n",
        "\n",
        "A test of solutions can be found in the Appenix. T5 was chosen based on bettter ROUGE scores than BART. This also allows me to stick with Google centric stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2laEu8B7Yunw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "e2f4f0741fcd4f5b835b896b36c164b7",
            "008fe59b931f4473ab29b6925746aa37",
            "a763e2bfd7c9472fa508b8d73d0b8c02",
            "bed1017a8c464219acf48903c3d63e51",
            "dd82d89d452946ab8fce7dbede22c43e",
            "42ccf3e0bdea46df805feca46b41445e",
            "7917078cf91349b881589404d088600e",
            "0d126c25fd1e4787b7e8c1ba60ff96e2",
            "3f368f7754cd43feade3197d8395e066",
            "4a978f99a23e4036831a8673a07c9f0d",
            "55515ba1075147ea994228b20736c5bc",
            "3e9552f5b3cf48c18779c853d7c13b26",
            "959f90b147a74be792f3291ecf8127bd",
            "c187494b1ed342d2b19c4811d40a4989",
            "992e21e596754ae8b05d47e0d3ed5a1a",
            "a88a280e1e1043b9a0a28bb927725dbc",
            "57c1f2a62f9045ce9be75293b93cb481",
            "c5f11fe9897047abae734217d2fc845b",
            "b6bde1b41ae44c1eb40dcf8af76f6a31",
            "f6d245dd833543e4b215728776673af2",
            "3e41a33ff1d747c7a9a247f8c6942c10",
            "ead5dd6e075f45b5a912802a0b07b3a8",
            "790ef1d035b84078ba5e481f41a7749a",
            "10037926a42c42739321330742a04476",
            "3f5ba7358340411f8859c86d7443b56d",
            "8b3053ca5c4140c0b787ad9d1ee3c662",
            "e6a9d3ca4ff2423fb651c786a8b59d01",
            "272fde60ff4e450aa52c3fa0956f00e3",
            "11c4ae620db4471eb8530cd5608967d5",
            "cabaa7dfeae349da9b0fa3af7866884b",
            "875a815f42df4741b22cdf5a2e99f208",
            "a8998ed14e54408a9c8cde3ffcaa51a6",
            "1ab729f9e06c4a288967b056b2d65d5e",
            "a773c38597c645eb83062e61b1ceccc7",
            "cca9ec93c25f46b98b9a02ce8721ea12",
            "4d6360416a1f4eed9661adf83c05d379",
            "9a061fbb2403407096f663402e7afb8b",
            "d1f567cdae784ca19d66b858efd4b2a3",
            "0c96ef95e4b949c4ba2465a69a7f05f0",
            "b58f85a60fd8430abf69a59f12aeef3b",
            "6848022af9f84be8b7f89ede440178f6",
            "8ed0220e75534043bc3ca713fe1c2f52",
            "eaa7d883ce084ace971000ba8cc281fb",
            "3b784d129223435c8b2bfa6d4bc8f61a",
            "748bf2b6c0b64682a3243520572a72f7",
            "3bf7d425b6384aa0937881a4905a2721",
            "20423b3d7d6c49e9954a086c4672d69c",
            "7a457a436a1e435bacf9e3d4c2e189f9",
            "af96b1c9a09948efa17b99ef17922710",
            "a15c2b277cb94e0b9ff7c4d87cae3ea4",
            "effb2c7ae3da4dd7b61155c98d4dd952",
            "12f212fb37f6412ba8d421acc096e949",
            "74de11bd29ba4ea8afbae677563908ff",
            "7e04ec0bfb954d4c945521965570c605",
            "17dfd68bf70c4239a8b10c139aeaaff8",
            "c566a5e4275f4a4fa32419d888ee190a",
            "d804dc468b394689a63992c768067fd2",
            "66546e29eb764784bbfbbdbbbe2d4abf",
            "0436b1a8e26146c184be22f492385f1a",
            "c702435c07474c22a301ba502b5b676a",
            "d06181aa161c4b6b8453ee31bed1d229",
            "1e51ed3152d84872a1a52ade17934fb9",
            "c8d87e6a690841b484ac5f281cd03088",
            "c084856e958c49308ceea652aef660ef",
            "7defdebf5fcd4fbe9716127155ce9ec6",
            "26eb0cdced98420c93aca25e61be5407"
          ]
        },
        "outputId": "5bccf46c-3022-44d0-a831-9b8b1755035d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2f4f0741fcd4f5b835b896b36c164b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e9552f5b3cf48c18779c853d7c13b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "790ef1d035b84078ba5e481f41a7749a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a773c38597c645eb83062e61b1ceccc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "748bf2b6c0b64682a3243520572a72f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c566a5e4275f4a4fa32419d888ee190a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 2001\n",
            "Processing additional 0 thread(s)\n",
            "Ending with thread_id 2001\n",
            "Summarization completed and saved to /content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_summarized.csv\n"
          ]
        }
      ],
      "source": [
        "# 4 Clustering and Summarization\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Load e9_forum_corpus_clean DataFrame from the CSV\n",
        "e9_forum_corpus_clean = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv'))\n",
        "\n",
        "# Load the existing summarized corpus if it exists, otherwise create it\n",
        "summarized_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv')\n",
        "e9_forum_corpus_summarized = pd.read_csv(summarized_corpus_path) if os.path.exists(summarized_corpus_path) and os.path.getsize(summarized_corpus_path) > 0 else pd.DataFrame(columns=e9_forum_corpus_clean.columns)\n",
        "\n",
        "# Calculate the starting THREAD_ID of the summarized corpus\n",
        "starting_thread_id = e9_forum_corpus_summarized['THREAD_ID'].max() if not e9_forum_corpus_summarized.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = e9_forum_corpus_clean[~e9_forum_corpus_clean['THREAD_ID'].isin(e9_forum_corpus_summarized['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def T5_summarize(text):\n",
        "    \"\"\"Summarization using T5.\"\"\"\n",
        "    try:\n",
        "        if text.strip() == \"\":\n",
        "            return text\n",
        "\n",
        "        unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            unformatted_text,\n",
        "            max_length=900,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=50,\n",
        "            min_length=10,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary if summary else text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def main():\n",
        "    # Check if 'THREAD_ALL_POSTS' column exists in new entries\n",
        "    if 'THREAD_ALL_POSTS' in new_entries.columns:\n",
        "        unique_texts = new_entries['THREAD_ALL_POSTS'].drop_duplicates()\n",
        "        summaries = unique_texts.apply(T5_summarize)\n",
        "        summary_map = dict(zip(unique_texts, summaries))\n",
        "        new_entries.loc[:, 'SUMMARIZED_THREAD'] = new_entries['THREAD_ALL_POSTS'].map(summary_map)\n",
        "\n",
        "        # Append the new summarized data to the existing summarized corpus\n",
        "        updated_summarized_corpus = pd.concat([e9_forum_corpus_summarized, new_entries], ignore_index=True)\n",
        "\n",
        "        # Save the results with the new summarized column\n",
        "        updated_summarized_corpus.to_csv(summarized_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Summarization completed and saved to {summarized_corpus_path}\")\n",
        "    else:\n",
        "        print(\"Error: Column 'THREAD_ALL_POSTS' does not exist in the dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HjwEoXaR3y"
      },
      "source": [
        "# 5 Format Text for Training\n",
        "\n",
        "- Structure text into a question-answer format suitable for training a RAG model.\n",
        "- Ensure the question string ends with a question mark for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47CINHXsYvfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e316732-cc82-41f4-8692-984527c5ed07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 2001\n",
            "Processing additional 43 thread(s)\n",
            "Ending with thread_id 1972\n",
            "Output saved to /content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/e9_forum_corpus_qa.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-732a4cc61254>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'SUMMARIZED_THREAD': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# 5 Format Text for Training\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "# Load the dataset from Step 2\n",
        "df_summarized = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv'))\n",
        "\n",
        "# Load the existing QA corpus if it exists\n",
        "qa_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv')\n",
        "df_qa = pd.read_csv(qa_corpus_path) if os.path.exists(qa_corpus_path) and os.path.getsize(qa_corpus_path) > 0 else pd.DataFrame(columns=['THREAD_ID', 'QUESTION', 'ANSWER'])\n",
        "\n",
        "# Calculate the starting THREAD_ID of the QA corpus\n",
        "starting_thread_id = df_qa['THREAD_ID'].max() if not df_qa.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_summarized[~df_summarized['THREAD_ID'].isin(df_qa['THREAD_ID'])]\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def create_qa_schema(df):\n",
        "    \"\"\"Creates a QA schema by renaming and dropping specific columns.\"\"\"\n",
        "    df.rename(columns={'SUMMARIZED_THREAD': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "    #df.rename(columns={'THREAD_ALL_POSTS': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "    #df.drop(['THREAD_TITLE', 'THREAD_ALL_POSTS'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        # Process the new entries to create QA schema\n",
        "        df_qa_new = create_qa_schema(new_entries.dropna())\n",
        "\n",
        "        # Append the new QA data to the existing QA corpus\n",
        "        updated_qa_corpus = pd.concat([df_qa, df_qa_new], ignore_index=True)\n",
        "\n",
        "        # Save the updated QA corpus\n",
        "        updated_qa_corpus.to_csv(qa_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Output saved to {qa_corpus_path}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_mp1BMbFXp"
      },
      "source": [
        "# 6 Embedding and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK6L99nQKA_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9517ed-dc2c-4cbb-f5e3-322d66afa4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1958 entries, 0 to 1957\n",
            "Data columns (total 7 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   THREAD_ID            1958 non-null   int64 \n",
            " 1   QUESTION             1958 non-null   object\n",
            " 2   ANSWER               1958 non-null   object\n",
            " 3   Question_Tokens      1958 non-null   object\n",
            " 4   Answer_Tokens        1958 non-null   object\n",
            " 5   Question_Embeddings  1958 non-null   object\n",
            " 6   Answer_Embeddings    1958 non-null   object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 107.2+ KB\n",
            "Starting with thread_id 2001\n",
            "Processing additional 0 thread(s)\n",
            "Ending with thread_id 2001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new entries to process.\n"
          ]
        }
      ],
      "source": [
        "# 6 Embedding and Indexing\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "# Load the DataFrame from your CSV file\n",
        "df_tok = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv'))\n",
        "\n",
        "# Load the existing FAISS corpus if it exists, otherwise create an empty DataFrame\n",
        "faiss_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv')\n",
        "df_faiss = pd.read_csv(faiss_corpus_path) if os.path.exists(faiss_corpus_path) and os.path.getsize(faiss_corpus_path) > 0 else pd.DataFrame(columns=df_tok.columns)\n",
        "\n",
        "df_faiss.info()\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_tok[~df_tok['THREAD_ID'].isin(df_faiss['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate the starting THREAD_ID of the FAISS corpus\n",
        "starting_thread_id = df_faiss['THREAD_ID'].max() if not df_faiss.empty else 0\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to embed text using T5 encoder model\n",
        "def embed_text(tokens):\n",
        "    inputs = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "def process_new_entries(entries):\n",
        "    entries[\"Question_Tokens\"] = entries[\"QUESTION\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Answer_Tokens\"] = entries[\"ANSWER\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Question_Embeddings\"] = entries[\"Question_Tokens\"].apply(embed_text)\n",
        "    entries[\"Answer_Embeddings\"] = entries[\"Answer_Tokens\"].apply(embed_text)\n",
        "    return entries\n",
        "\n",
        "def filter_embeddings(embeddings_list, expected_shape):\n",
        "    \"\"\"Filter out embeddings that do not match the expected shape.\"\"\"\n",
        "    return [embedding for embedding in embeddings_list if len(embedding) == expected_shape]\n",
        "\n",
        "def build_faiss_index(embeddings, index_path):\n",
        "    embeddings_np = np.array(embeddings).astype('float32')  # Convert to NumPy array of type float32\n",
        "    d = embeddings_np.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(d)  # Build the index\n",
        "    index.add(embeddings_np)  # Add vectors to the index\n",
        "\n",
        "    # Save the index\n",
        "    faiss.write_index(index, index_path)\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        processed_entries = process_new_entries(new_entries)\n",
        "\n",
        "        # Append the new processed data to the existing FAISS corpus\n",
        "        updated_faiss_corpus = pd.concat([df_faiss, processed_entries], ignore_index=True)\n",
        "\n",
        "        # Save the updated FAISS corpus\n",
        "        updated_faiss_corpus.to_csv(faiss_corpus_path, index=False)\n",
        "\n",
        "        # Ensure all embeddings are of the same shape before saving\n",
        "        question_embeddings_list = updated_faiss_corpus[\"Question_Embeddings\"].to_list()\n",
        "        answer_embeddings_list = updated_faiss_corpus[\"Answer_Embeddings\"].to_list()\n",
        "\n",
        "        expected_shape = 512  # Expected embedding size (512 for T5 model)\n",
        "\n",
        "        question_embeddings_filtered = filter_embeddings(question_embeddings_list, expected_shape)\n",
        "        answer_embeddings_filtered = filter_embeddings(answer_embeddings_list, expected_shape)\n",
        "\n",
        "        question_embeddings = np.array(question_embeddings_filtered)\n",
        "        answer_embeddings = np.array(answer_embeddings_filtered)\n",
        "\n",
        "        np.save(os.path.join(BASE_PATH, 'question_embeddings_t5.npy'), question_embeddings)\n",
        "        np.save(os.path.join(BASE_PATH, 'answer_embeddings_t5.npy'), answer_embeddings)\n",
        "\n",
        "        # Build and save the FAISS index using the new answer embeddings\n",
        "        faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "        index = build_faiss_index(answer_embeddings, faiss_index_path)\n",
        "\n",
        "        print(f\"FAISS index has been rebuilt and saved to {faiss_index_path}\")\n",
        "        print(f\"Embeddings have been generated and saved to {BASE_PATH}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kRXQtQdm-b"
      },
      "source": [
        "# 7 Query Processing and Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueakMTFdd4k3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae48f29e-866c-44cc-888c-674fe5ac272a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# 7 Query Processing and Search\n",
        "\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "# Load the FAISS index\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Load the pre-trained embeddings\n",
        "question_embeddings = np.load(os.path.join(BASE_PATH, 'question_embeddings_t5.npy'))\n",
        "answer_embeddings = np.load(os.path.join(BASE_PATH, 'answer_embeddings_t5.npy'))\n",
        "\n",
        "# Load the corpus DataFrame\n",
        "e9_forum_corpus = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv'))\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "def generate_query_embeddings(query, tokenizer, model):\n",
        "    tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return np.array(embeddings).astype('float32')  # Convert to float32 for FAISS\n",
        "\n",
        "def search_similar_questions(query_embeddings, index, top_k=5):\n",
        "    D, I = index.search(query_embeddings, top_k)\n",
        "    return I, D\n",
        "\n",
        "def retrieve_and_rank(df, I, D):\n",
        "    results = []\n",
        "    for i, distances in zip(I, D):\n",
        "        for idx, distance in zip(i, distances):\n",
        "            result = {\n",
        "                'Thread ID': df.iloc[idx]['THREAD_ID'],\n",
        "                'Question': df.iloc[idx]['QUESTION'],  # Raw question text\n",
        "                'Answer': df.iloc[idx]['ANSWER'],  # Raw answer text\n",
        "                'Distance': distance\n",
        "            }\n",
        "            results.append(result)\n",
        "    return results\n",
        "\n",
        "# Example usage for step 8:\n",
        "query = \"I want a tool box?\"\n",
        "query_embeddings = generate_query_embeddings(query, tokenizer, model).reshape(1, -1)\n",
        "\n",
        "top_k = 5\n",
        "I, D = search_similar_questions(query_embeddings, index, top_k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSiN7POcPZL"
      },
      "source": [
        "\n",
        "\n",
        "# 8 Retrieve and Rank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1Ql-pHeI-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90573897-4d0e-4512-ef8d-28fccf12c033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Thread ID': 1, 'Question': 'New owner Goin drive home cant wait post experience read laugh cry critiqueI look forward coming lurker statusRegards', 'Answer': 'new owner Goin drive home cant wait post experience read laugh cry critiqueI look forward coming lurker statusRegards Congrats New OwnerCongrats pecsokfrom one new owner another another javascriptemoticon', 'Distance': 0.8356856}\n",
            "{'Thread ID': 2001, 'Question': 'Looking at the Photo Gallery for all of the modelsand it seems that very few have tinted windowsIs there a reason that more window are not tintedNot a total black out but something noticeableWhats your opion on tinting your e9 windowsThanks', 'Answer': 'tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the', 'Distance': 3.4028235e+38}\n",
            "{'Thread ID': 2001, 'Question': 'Looking at the Photo Gallery for all of the modelsand it seems that very few have tinted windowsIs there a reason that more window are not tintedNot a total black out but something noticeableWhats your opion on tinting your e9 windowsThanks', 'Answer': 'tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the', 'Distance': 3.4028235e+38}\n",
            "{'Thread ID': 2001, 'Question': 'Looking at the Photo Gallery for all of the modelsand it seems that very few have tinted windowsIs there a reason that more window are not tintedNot a total black out but something noticeableWhats your opion on tinting your e9 windowsThanks', 'Answer': 'tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the', 'Distance': 3.4028235e+38}\n",
            "{'Thread ID': 2001, 'Question': 'Looking at the Photo Gallery for all of the modelsand it seems that very few have tinted windowsIs there a reason that more window are not tintedNot a total black out but something noticeableWhats your opion on tinting your e9 windowsThanks', 'Answer': 'tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the', 'Distance': 3.4028235e+38}\n"
          ]
        }
      ],
      "source": [
        "# 8 Retrieve and Rank\n",
        "\n",
        "def retrieve_and_rank(df, I, D):\n",
        "    results = []\n",
        "    for i, distances in zip(I, D):\n",
        "        for idx, distance in zip(i, distances):\n",
        "            result = {\n",
        "                'Thread ID': df.iloc[idx]['THREAD_ID'],\n",
        "                'Question': df.iloc[idx]['QUESTION'],  # Raw question text\n",
        "                'Answer': df.iloc[idx]['ANSWER'],  # Raw answer text\n",
        "                'Distance': distance\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "    # Log the retrieved context\n",
        "    contexts = [result['Answer'] for result in results]\n",
        "    retrieved_context = \" \".join(contexts)\n",
        "    logging.info(f\"Query: {query}\\nRetrieved Context: {retrieved_context}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Retrieve and rank results\n",
        "ranked_results = retrieve_and_rank(e9_forum_corpus, I, D)\n",
        "for result in ranked_results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okjxc-Mn5qXv"
      },
      "source": [
        "\n",
        "# 9 Answer Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "# Required imports\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "# Define functions\n",
        "def generate_query_embeddings(query, tokenizer, model):\n",
        "    tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "    with torch.no_grad():\n",
        "        outputs = model.encoder(tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return np.array(embeddings).astype('float32')  # Convert to float32 for FAISS\n",
        "\n",
        "def search_similar_questions(query_embeddings, index, top_k=5):\n",
        "    query_embeddings = query_embeddings.reshape(1, -1)  # Ensure correct shape\n",
        "    D, I = index.search(query_embeddings, top_k)\n",
        "    return I, D\n",
        "\n",
        "def retrieve_and_rank(df, I, D):\n",
        "    results = []\n",
        "    for i, distances in zip(I, D):\n",
        "        for idx, distance in zip(i, distances):\n",
        "            result = {\n",
        "                'Thread ID': df.iloc[idx]['THREAD_ID'],\n",
        "                'Question': df.iloc[idx]['QUESTION'],  # Raw question text\n",
        "                'Answer': df.iloc[idx]['ANSWER'],  # Raw answer text\n",
        "                'Distance': distance\n",
        "            }\n",
        "            results.append(result)\n",
        "    return results\n",
        "\n",
        "def generate_answer(query, ranked_results, tokenizer, model):\n",
        "    # Print the original user question\n",
        "    print(f\"Original Question: {query}\")\n",
        "\n",
        "    # Concatenate the retrieved contexts\n",
        "    concatenated_context = \" \".join([result['Answer'] for result in ranked_results])\n",
        "\n",
        "    # Construct the prompt with the query and context\n",
        "    input_text = f\"answer: {query} context: {concatenated_context}\"\n",
        "\n",
        "    # Print the constructed prompt to see what is being sent to the LLM\n",
        "    print(f\"Engineered Prompt: {input_text}\")\n",
        "\n",
        "    # Generate the answer using the T5 model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_answer\n",
        "\n",
        "# Load the FAISS index and necessary data\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "e9_forum_corpus = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv'))\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to process a query and retrieve answers\n",
        "def process_query_and_generate_answer(query, index, df, tokenizer, model):\n",
        "    query_embeddings = generate_query_embeddings(query, tokenizer, model)\n",
        "    I, D = search_similar_questions(query_embeddings, index, top_k=5)\n",
        "    ranked_results = retrieve_and_rank(df, I, D)\n",
        "    generated_answer = generate_answer(query, ranked_results, tokenizer, model)\n",
        "    return generated_answer\n",
        "\n",
        "# Ensure this is part of your main execution code\n",
        "query = \"How do I fix the transmission issue in my car?\"  # Replace with the actual question input mechanism\n",
        "generated_answer = process_query_and_generate_answer(query, index, e9_forum_corpus, tokenizer, model)\n",
        "\n",
        "# Print the generated answer\n",
        "print(f\"Generated Answer: {generated_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRa1jfBz27Jx",
        "outputId": "59c639c4-7fe1-4df6-a652-52217e10bf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Question: How do I fix the transmission issue in my car?\n",
            "Engineered Prompt: answer: How do I fix the transmission issue in my car? context: new owner Goin drive home cant wait post experience read laugh cry critiqueI look forward coming lurker statusRegards Congrats New OwnerCongrats pecsokfrom one new owner another another javascriptemoticon tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the tinted window the tint the tint the glass the tint is 20 30 degree cooler and 10 12 dbquieterAnyone used this product the tint the tint of the models of the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: the tint the tint of the models of the tinted window the tint the glass the tint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to compare the origional text and how it changes throught the workstream\n",
        "\n",
        "# Define the base path\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "# Load the DataFrames from your CSV files\n",
        "e9_forum_corpus_dirty = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv'))\n",
        "e9_forum_corpus_clean = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv'))\n",
        "e9_forum_corpus_summarized = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv'))\n",
        "\n",
        "def compare_thread_id(thread_id):\n",
        "    # Check if the thread_id exists in all DataFrames\n",
        "    if thread_id in e9_forum_corpus_dirty['THREAD_ID'].values and \\\n",
        "       thread_id in e9_forum_corpus_clean['THREAD_ID'].values and \\\n",
        "       thread_id in e9_forum_corpus_summarized['THREAD_ID'].values:\n",
        "\n",
        "        # Get the rows corresponding to the thread_id from each DataFrame\n",
        "        dirty_row = e9_forum_corpus_dirty[e9_forum_corpus_dirty['THREAD_ID'] == thread_id]\n",
        "        clean_row = e9_forum_corpus_clean[e9_forum_corpus_clean['THREAD_ID'] == thread_id]\n",
        "        summarized_row = e9_forum_corpus_summarized[e9_forum_corpus_summarized['THREAD_ID'] == thread_id]\n",
        "\n",
        "        # Concatenate the rows into a single DataFrame for comparison\n",
        "        comparison_df = pd.concat([dirty_row, clean_row, summarized_row], keys=['Dirty', 'Clean', 'Summarized'])\n",
        "\n",
        "        return comparison_df\n",
        "    else:\n",
        "        print(f\"THREAD_ID {thread_id} not found in all DataFrames.\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "thread_id_to_compare = 4  # Replace with the actual THREAD_ID you want to compare\n",
        "comparison_result = compare_thread_id(thread_id_to_compare)\n",
        "\n",
        "if comparison_result is not None:\n",
        "    # Set display options to show full content of each cell\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    print(comparison_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8NlORXCEFv_",
        "outputId": "de5c23a1-0a55-43fd-8afb-f4bf2e86622b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              THREAD_ID    THREAD_TITLE  \\\n",
            "Dirty      3          4  5 Speed Tranny   \n",
            "Clean      3          4  5 Speed Tranny   \n",
            "Summarized 3          4    Speed Tranny   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                    THREAD_FIRST_POST  \\\n",
            "Dirty      3  I have just purchased a CS and discovered that the tranny was ran dry before the purchase.  What 5-speed tranny would work, the current is a Getrag 225 and the speedo cable is not attached, in fact there is a soft plug in place of the gear and that is the source of the fluid leak.  What would the Getrag numbers be?Thanks,Doug   \n",
            "Clean      3             I have just purchased a CS and discovered that the tranny wa ran dry before the purchase What 5speed tranny would work the current is a Getrag 225 and the speedo cable is not attached in fact there is a soft plug in place of the gear and that is the source of the fluid leak What would the Getrag number beThanksDoug   \n",
            "Summarized 3                                                                                                                                              purchased discovered tranny ran dry purchase 5speed tranny would work current Getrag 225 speedo cable attached fact soft plug place gear source fluid leak would Getrag number beThanksDoug   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         THREAD_ALL_POSTS  \\\n",
            "Dirty      3  I have just purchased a CS and discovered that the tranny was ran dry before the purchase.  What 5-speed tranny would work, the current is a Getrag 225 and the speedo cable is not attached, in fact there is a soft plug in place of the gear and that is the source of the fluid leak.  What would the Getrag numbers be?Thanks,Doug TrannyHi Doug,You want a 265 Getrag for your CS.  Bullet proof, reliable, fairly easy to find.  You'll need to shorten your driveshaft, but it's not a hard job to do if you're mechanically inclined. TrannyHi Doug,You want a 265 Getrag for your CS.  Bullet proof, reliable, fairly easy to find.  You'll need to shorten your driveshaft, but it's not a hard job to do if you're mechanically inclined. TrannyWill I need to do something special for the hook up for the speedo?  Thanks for the info!Doug TrannyI have a choice of two wrecking yards near me, one has a tranny out of a 633 the other out of a 533.  Does the tranny have markings to indicate it is the 265/6 which I believe is the tranny most recommend.  Most are recommending against the 265/5 dog leg do you agree?Doug Doug,The 265 is distinguished by a separate bell housing and a 3 section transmission case.  If the transmissions you are looking at have integral bell housings, they are 260's.  They will work, but you will have no option for speedo drive.265's are common in '79 and newer e12 5 series, and in e24 six series cars from about 1980-1982.  6's after that have 260's and only a few 1985 e28 5 series cars had the 265.  e30 M3's use the 265 as well, but are not easy to find in the u-pull-it.If the 265 you find has no speedo drive, it's a pretty easy job to install it.[Broken External Image]:http://www.rodang.ac.se/~fresko/02pics/Getrag_265_2.jpg If the transmissions you are looking at have integral bell housings, they are 260's. They will work, but you will have no option for speedo drive.Click to expand...Can anyone think of a way to make a pickup (VR or the like, similar to what's inside E28 diffs)?  I'll be more than happy to build a small circuit that takes a pickup signal and drives your speedometer cable via a small DC motor.  You can then use my board with whatever gearbox you feel like, 260, 280, whatever... TrannyTony, I have been reading some old threads on Roadfly and there is discussion on the 265/6 being the better tranny than a 265/5 dog leg how do I tell the difference between the two?Doug Re: Trannyslyedog said:Tony, I have been reading some old threads on Roadfly and there is discussion on the 265/6 being the better tranny than a 265/5 dog leg how do I tell the difference between the two?DougClick to expand...They look the same on the outside, the shift pattern is the obvious difference.  Dogleg close ratio pattern isl-H with R left and up, 1 left and down, and 2 through 5 on the H.OD pattern is'-H-'  with R left and up, 5 right and up, 1-4 on the H.The other way to tell is if you find a CR dogleg in a junkyard, you'll immediately win the lottery, your golf handicap will drop to 0, your boss will promote you to be his boss, and gorgeous twins from all corners of the planet will want to sleep with you.  And you'll be able to sell it and buy 4 OD boxes. TrannyTony, thanks now I know better... kinda sounds like looking for Alpina parts for a 02.  I tried to call you but no answer I would like to try and meet sometime and pick your brain on other items too.Doug one more thingit needs to be said, since no one else has mentioned it, that the shorter driveshaft you'll need can be one from an auto trans CS.  Works just fine; the telescoping section at the center bearing can be adjusted for overall length.Just don't take the front dshaft section completely off the rear section without marking their correct relative rotational positions.  Not many places can balance the two pieces together. I have also found that the driveshaft from the E12 fits. I have it on my CS, no cutting required.With the Getrag 265 as far as I am aware, the 265 always had a mechanical spedo pickup. The rest of the time, BMW used a Getrag 260, liek the one shown in the picture above. The two are externally identical, based on what I have seen, except for the mechanical spedo drive. I got my 265 from an 81 528e, and it has the spedo drive. Metric Mechanic has a transmission application guide on their website; it shows which cars had the 260 and which had the 265.[Broken External Image]:http://www.metricmechanic.com/m51.gifThey also have speed vs RPM charts for all the transmissions used, it's a great site overall. have a rebuilt driveshaftI have a driveshaft  rebuilt/balanced that may come handy for the 5-speed tranny. Here is the picture:[Broken External Image]:http://home.earthlink.net/~b_jamin/sitebuildercontent/sitebuilderpictures/dsc03412.jpgArde The rest of the time, BMW used a Getrag 260, liek the one shown in the picture above.Click to expand...The picture I posted above is a 265, not a 260.  Note the removable bell housing.They also have speed vs RPM charts for all the transmissions used, it's a great site overallClick to expand...There's a better reference for gear ratios and speed on the CS Registry website, created by Bruce Leggett.  It allows you to input the final drive ratio and tire sizes so you get accurate calculations.  Almost all Getrag OD 5 speeds BMW used on M30's have the same gear ratios, so the real variable is final drive.   \n",
            "Clean      3                                                                                                                                                                                                                                                                                                                                                                                                                                                                 I have just purchased a CS and discovered that the tranny wa ran dry before the purchase What 5speed tranny would work the current is a Getrag 225 and the speedo cable is not attached in fact there is a soft plug in place of the gear and that is the source of the fluid leak What would the Getrag number beThanksDoug TrannyHi DougYou want a 265 Getrag for your CS Bullet proof reliable fairly easy to find Youll need to shorten your driveshaft but it not a hard job to do if youre mechanically inclined TrannyHi DougYou want a 265 Getrag for your CS Bullet proof reliable fairly easy to find Youll need to shorten your driveshaft but it not a hard job to do if youre mechanically inclined TrannyWill I need to do something special for the hook up for the speedo Thanks for the infoDoug TrannyI have a choice of two wrecking yard near me one ha a tranny out of a 633 the other out of a 533 Does the tranny have marking to indicate it is the 2656 which I believe is the tranny most recommend Most are recommending against the 2655 dog leg do you agreeDoug DougThe 265 is distinguished by a separate bell housing and a 3 section transmission case If the transmission you are looking at have integral bell housing they are 260s They will work but you will have no option for speedo drive265s are common in 79 and newer e12 5 series and in e24 six series car from about 19801982 6 after that have 260s and only a few 1985 e28 5 series car had the 265 e30 M3s use the 265 a well but are not easy to find in the upullitIf the 265 you find ha no speedo drive it a pretty easy job to install itBroken External Image If the transmission you are looking at have integral bell housing they are 260s They will work but you will have no option for speedo driveClick to expandCan anyone think of a way to make a pickup VR or the like similar to whats inside E28 diffs Ill be more than happy to build a small circuit that take a pickup signal and drive your speedometer cable via a small DC motor You can then use my board with whatever gearbox you feel like 260 280 whatever TrannyTony I have been reading some old thread on Roadfly and there is discussion on the 2656 being the better tranny than a 2655 dog leg how do I tell the difference between the twoDoug Re Trannyslyedog saidTony I have been reading some old thread on Roadfly and there is discussion on the 2656 being the better tranny than a 2655 dog leg how do I tell the difference between the twoDougClick to expandThey look the same on the outside the shift pattern is the obvious difference Dogleg close ratio pattern islH with R left and up 1 left and down and 2 through 5 on the HOD pattern isH with R left and up 5 right and up 14 on the HThe other way to tell is if you find a CR dogleg in a junkyard youll immediately win the lottery your golf handicap will drop to 0 your bos will promote you to be his bos and gorgeous twin from all corner of the planet will want to sleep with you And youll be able to sell it and buy 4 OD box TrannyTony thanks now I know better kinda sound like looking for Alpina part for a 02 I tried to call you but no answer I would like to try and meet sometime and pick your brain on other item tooDoug one more thingit need to be said since no one else ha mentioned it that the shorter driveshaft youll need can be one from an auto trans CS Works just fine the telescoping section at the center bearing can be adjusted for overall lengthJust dont take the front dshaft section completely off the rear section without marking their correct relative rotational position Not many place can balance the two piece together I have also found that the driveshaft from the E12 fit I have it on my CS no cutting requiredWith the Getrag 265 a far a I am aware the 265 always had a mechanical spedo pickup The rest of the time BMW used a Getrag 260 liek the one shown in the picture above The two are externally identical based on what I have seen except for the mechanical spedo drive I got my 265 from an 81 528e and it ha the spedo drive Metric Mechanic ha a transmission application guide on their website it show which car had the 260 and which had the 265Broken External Image also have speed v RPM chart for all the transmission used it a great site overall have a rebuilt driveshaftI have a driveshaft rebuiltbalanced that may come handy for the 5speed tranny Here is the pictureBroken External Image The rest of the time BMW used a Getrag 260 liek the one shown in the picture aboveClick to expandThe picture I posted above is a 265 not a 260 Note the removable bell housingThey also have speed v RPM chart for all the transmission used it a great site overallClick to expandTheres a better reference for gear ratio and speed on the CS Registry website created by Bruce Leggett It allows you to input the final drive ratio and tire size so you get accurate calculation Almost all Getrag OD 5 speed BMW used on M30s have the same gear ratio so the real variable is final drive   \n",
            "Summarized 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        purchased discovered tranny ran dry purchase 5speed tranny would work current Getrag 225 speedo cable attached fact soft plug place gear source fluid leak would Getrag number beThanksDoug TrannyHi DougYou want 265 Getrag Bullet proof reliable fairly easy find Youll need shorten driveshaft hard job youre mechanically inclined TrannyHi DougYou want 265 Getrag Bullet proof reliable fairly easy find Youll need shorten driveshaft hard job youre mechanically inclined TrannyWill need something special hook speedo Thanks infoDoug TrannyI choice two wrecking yard near one tranny 633 533 tranny marking indicate 2656 believe tranny recommend recommending 2655 dog leg agreeDoug DougThe 265 distinguished separate bell housing section transmission case transmission looking integral bell housing 260s work option speedo drive265s common newer e12 series e24 six series car 19801982 260s 1985 e28 series car 265 e30 M3s use 265 well easy find upullitIf 265 find speedo drive pretty easy job install itBroken External Image transmission looking integral bell housing 260s work option speedo driveClick expandCan anyone think way make pickup like similar whats inside E28 diffs Ill happy build small circuit take pickup signal drive speedometer cable via small motor use board whatever gearbox feel like 260 280 whatever TrannyTony reading old thread Roadfly discussion 2656 better tranny 2655 dog leg tell difference twoDoug Trannyslyedog saidTony reading old thread Roadfly discussion 2656 better tranny 2655 dog leg tell difference twoDougClick expandThey look outside shift pattern obvious difference Dogleg close ratio pattern islH left left HOD pattern isH left right HThe way tell find dogleg junkyard youll immediately win lottery golf handicap drop bos promote bos gorgeous twin corner planet want sleep youll able sell buy box TrannyTony thanks know better kinda sound like looking Alpina part tried call answer would like try meet sometime pick brain item tooDoug one thingit need said since one else mentioned shorter driveshaft youll need one auto trans Works fine telescoping section center bearing adjusted overall lengthJust dont take front dshaft section completely rear section without marking correct relative rotational position many place balance two piece together also found driveshaft E12 fit cutting requiredWith Getrag 265 far aware 265 always mechanical spedo pickup rest time used Getrag 260 liek one shown picture two externally identical based seen except mechanical spedo drive got 265 528e spedo drive Metric Mechanic transmission application guide website show car 260 265Broken External Image also speed RPM chart transmission used great site overall rebuilt driveshaftI driveshaft rebuiltbalanced may come handy 5speed tranny pictureBroken External Image rest time used Getrag 260 liek one shown picture aboveClick expandThe picture posted 265 260 Note removable bell housingThey also speed RPM chart transmission used great site overallClick expandTheres better reference gear ratio speed Registry website created Bruce Leggett allows input final drive ratio tire size get accurate calculation Almost Getrag speed used M30s gear ratio real variable final drive   \n",
            "\n",
            "                                                                                                                                                                                         SUMMARIZED_THREAD  \n",
            "Dirty      3                                                                                                                                                                                           NaN  \n",
            "Clean      3                                                                                                                                                                                           NaN  \n",
            "Summarized 3  tranny ran dry purchase 5speed tranny would work current Getrag 225 speedo cable attached fact soft plug place gear source fluid leak would Getrag number beThanksDoug TrannyHi DougYou want  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAmDDBpchI6"
      },
      "source": [
        "# 10 Evaluation and Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQuFH6wleDuc"
      },
      "outputs": [],
      "source": [
        "# 10 Evaluation and Tuning\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/NLP/LLM_RAG/'\n",
        "\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus_faiss WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 11 Deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.  \n",
        "\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.  \n",
        "   \n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 12 Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bvouULWteB2"
      },
      "source": [
        "## Summarization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0r8gc0eJQtF"
      },
      "outputs": [],
      "source": [
        "# Summarization Comparison: T5\n",
        "\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def t5_summarize(text, max_length, min_length, num_beams):\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model_t5.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Define the reference summary (ground truth)\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = t5_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zal-aEkL_BFj"
      },
      "outputs": [],
      "source": [
        "# Summarization Comparison: DistilBART\n",
        "\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def distilbart_summarize(text, max_length, min_length, num_beams):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,  # Adjusted to fit within the model's constraints\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text and reference summary\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = distilbart_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RdIXKt4LPUB"
      },
      "source": [
        "Compare ROGE Summarization Scores:\n",
        "\n",
        "T5\n",
        "\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1778\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.1290\n",
        "*   Best parameters: max_length=50, min_length=10, num_beams=2\n",
        "\n",
        "\n",
        "\n",
        "BART\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1270\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.0816\n",
        "*   Best parameters: max_length=100, min_length=10, num_beams=4\n",
        "\n",
        "\n",
        "\n",
        "Based on these scores, Ill be using T5 for training due to its higher Precision and no discernable differnce in Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVaVmP2DVcjM"
      },
      "source": [
        "# Parking lot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HADiQJr4_WL"
      },
      "source": [
        "# Score query result quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ir9PZxznFQ"
      },
      "outputs": [],
      "source": [
        "# Parking Lot\n",
        "# Query Processing and Search of LDA derived topics\n",
        "# This step reuires LDA to have run first to generate topic sentences\n",
        "\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus_faiss WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 3.3 Data Storage and Database\n",
        "\n",
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### 3.3.1 Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "\n",
        "### 3.3.2 Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A9uW-2OyR9fa3DZlxEvVAAVJPf7jpGrw",
      "authorship_tag": "ABX9TyNbxEbqGTAYdCdSYcn4ZNy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2f4f0741fcd4f5b835b896b36c164b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_008fe59b931f4473ab29b6925746aa37",
              "IPY_MODEL_a763e2bfd7c9472fa508b8d73d0b8c02",
              "IPY_MODEL_bed1017a8c464219acf48903c3d63e51"
            ],
            "layout": "IPY_MODEL_dd82d89d452946ab8fce7dbede22c43e"
          }
        },
        "008fe59b931f4473ab29b6925746aa37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ccf3e0bdea46df805feca46b41445e",
            "placeholder": "​",
            "style": "IPY_MODEL_7917078cf91349b881589404d088600e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a763e2bfd7c9472fa508b8d73d0b8c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d126c25fd1e4787b7e8c1ba60ff96e2",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f368f7754cd43feade3197d8395e066",
            "value": 2324
          }
        },
        "bed1017a8c464219acf48903c3d63e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a978f99a23e4036831a8673a07c9f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_55515ba1075147ea994228b20736c5bc",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 40.5kB/s]"
          }
        },
        "dd82d89d452946ab8fce7dbede22c43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ccf3e0bdea46df805feca46b41445e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7917078cf91349b881589404d088600e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d126c25fd1e4787b7e8c1ba60ff96e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f368f7754cd43feade3197d8395e066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a978f99a23e4036831a8673a07c9f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55515ba1075147ea994228b20736c5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e9552f5b3cf48c18779c853d7c13b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_959f90b147a74be792f3291ecf8127bd",
              "IPY_MODEL_c187494b1ed342d2b19c4811d40a4989",
              "IPY_MODEL_992e21e596754ae8b05d47e0d3ed5a1a"
            ],
            "layout": "IPY_MODEL_a88a280e1e1043b9a0a28bb927725dbc"
          }
        },
        "959f90b147a74be792f3291ecf8127bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57c1f2a62f9045ce9be75293b93cb481",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f11fe9897047abae734217d2fc845b",
            "value": "spiece.model: 100%"
          }
        },
        "c187494b1ed342d2b19c4811d40a4989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6bde1b41ae44c1eb40dcf8af76f6a31",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6d245dd833543e4b215728776673af2",
            "value": 791656
          }
        },
        "992e21e596754ae8b05d47e0d3ed5a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e41a33ff1d747c7a9a247f8c6942c10",
            "placeholder": "​",
            "style": "IPY_MODEL_ead5dd6e075f45b5a912802a0b07b3a8",
            "value": " 792k/792k [00:00&lt;00:00, 1.15MB/s]"
          }
        },
        "a88a280e1e1043b9a0a28bb927725dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57c1f2a62f9045ce9be75293b93cb481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f11fe9897047abae734217d2fc845b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6bde1b41ae44c1eb40dcf8af76f6a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d245dd833543e4b215728776673af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e41a33ff1d747c7a9a247f8c6942c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead5dd6e075f45b5a912802a0b07b3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790ef1d035b84078ba5e481f41a7749a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10037926a42c42739321330742a04476",
              "IPY_MODEL_3f5ba7358340411f8859c86d7443b56d",
              "IPY_MODEL_8b3053ca5c4140c0b787ad9d1ee3c662"
            ],
            "layout": "IPY_MODEL_e6a9d3ca4ff2423fb651c786a8b59d01"
          }
        },
        "10037926a42c42739321330742a04476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_272fde60ff4e450aa52c3fa0956f00e3",
            "placeholder": "​",
            "style": "IPY_MODEL_11c4ae620db4471eb8530cd5608967d5",
            "value": "tokenizer.json: 100%"
          }
        },
        "3f5ba7358340411f8859c86d7443b56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cabaa7dfeae349da9b0fa3af7866884b",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_875a815f42df4741b22cdf5a2e99f208",
            "value": 1389353
          }
        },
        "8b3053ca5c4140c0b787ad9d1ee3c662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8998ed14e54408a9c8cde3ffcaa51a6",
            "placeholder": "​",
            "style": "IPY_MODEL_1ab729f9e06c4a288967b056b2d65d5e",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 7.63MB/s]"
          }
        },
        "e6a9d3ca4ff2423fb651c786a8b59d01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272fde60ff4e450aa52c3fa0956f00e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11c4ae620db4471eb8530cd5608967d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cabaa7dfeae349da9b0fa3af7866884b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875a815f42df4741b22cdf5a2e99f208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8998ed14e54408a9c8cde3ffcaa51a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab729f9e06c4a288967b056b2d65d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a773c38597c645eb83062e61b1ceccc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cca9ec93c25f46b98b9a02ce8721ea12",
              "IPY_MODEL_4d6360416a1f4eed9661adf83c05d379",
              "IPY_MODEL_9a061fbb2403407096f663402e7afb8b"
            ],
            "layout": "IPY_MODEL_d1f567cdae784ca19d66b858efd4b2a3"
          }
        },
        "cca9ec93c25f46b98b9a02ce8721ea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c96ef95e4b949c4ba2465a69a7f05f0",
            "placeholder": "​",
            "style": "IPY_MODEL_b58f85a60fd8430abf69a59f12aeef3b",
            "value": "config.json: 100%"
          }
        },
        "4d6360416a1f4eed9661adf83c05d379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6848022af9f84be8b7f89ede440178f6",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ed0220e75534043bc3ca713fe1c2f52",
            "value": 1206
          }
        },
        "9a061fbb2403407096f663402e7afb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa7d883ce084ace971000ba8cc281fb",
            "placeholder": "​",
            "style": "IPY_MODEL_3b784d129223435c8b2bfa6d4bc8f61a",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 48.9kB/s]"
          }
        },
        "d1f567cdae784ca19d66b858efd4b2a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c96ef95e4b949c4ba2465a69a7f05f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58f85a60fd8430abf69a59f12aeef3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6848022af9f84be8b7f89ede440178f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed0220e75534043bc3ca713fe1c2f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eaa7d883ce084ace971000ba8cc281fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b784d129223435c8b2bfa6d4bc8f61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748bf2b6c0b64682a3243520572a72f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bf7d425b6384aa0937881a4905a2721",
              "IPY_MODEL_20423b3d7d6c49e9954a086c4672d69c",
              "IPY_MODEL_7a457a436a1e435bacf9e3d4c2e189f9"
            ],
            "layout": "IPY_MODEL_af96b1c9a09948efa17b99ef17922710"
          }
        },
        "3bf7d425b6384aa0937881a4905a2721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a15c2b277cb94e0b9ff7c4d87cae3ea4",
            "placeholder": "​",
            "style": "IPY_MODEL_effb2c7ae3da4dd7b61155c98d4dd952",
            "value": "model.safetensors: 100%"
          }
        },
        "20423b3d7d6c49e9954a086c4672d69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12f212fb37f6412ba8d421acc096e949",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74de11bd29ba4ea8afbae677563908ff",
            "value": 242043056
          }
        },
        "7a457a436a1e435bacf9e3d4c2e189f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e04ec0bfb954d4c945521965570c605",
            "placeholder": "​",
            "style": "IPY_MODEL_17dfd68bf70c4239a8b10c139aeaaff8",
            "value": " 242M/242M [00:02&lt;00:00, 114MB/s]"
          }
        },
        "af96b1c9a09948efa17b99ef17922710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15c2b277cb94e0b9ff7c4d87cae3ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "effb2c7ae3da4dd7b61155c98d4dd952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12f212fb37f6412ba8d421acc096e949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74de11bd29ba4ea8afbae677563908ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e04ec0bfb954d4c945521965570c605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17dfd68bf70c4239a8b10c139aeaaff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c566a5e4275f4a4fa32419d888ee190a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d804dc468b394689a63992c768067fd2",
              "IPY_MODEL_66546e29eb764784bbfbbdbbbe2d4abf",
              "IPY_MODEL_0436b1a8e26146c184be22f492385f1a"
            ],
            "layout": "IPY_MODEL_c702435c07474c22a301ba502b5b676a"
          }
        },
        "d804dc468b394689a63992c768067fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06181aa161c4b6b8453ee31bed1d229",
            "placeholder": "​",
            "style": "IPY_MODEL_1e51ed3152d84872a1a52ade17934fb9",
            "value": "generation_config.json: 100%"
          }
        },
        "66546e29eb764784bbfbbdbbbe2d4abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d87e6a690841b484ac5f281cd03088",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c084856e958c49308ceea652aef660ef",
            "value": 147
          }
        },
        "0436b1a8e26146c184be22f492385f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7defdebf5fcd4fbe9716127155ce9ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_26eb0cdced98420c93aca25e61be5407",
            "value": " 147/147 [00:00&lt;00:00, 4.45kB/s]"
          }
        },
        "c702435c07474c22a301ba502b5b676a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d06181aa161c4b6b8453ee31bed1d229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e51ed3152d84872a1a52ade17934fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8d87e6a690841b484ac5f281cd03088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c084856e958c49308ceea652aef660ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7defdebf5fcd4fbe9716127155ce9ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26eb0cdced98420c93aca25e61be5407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}