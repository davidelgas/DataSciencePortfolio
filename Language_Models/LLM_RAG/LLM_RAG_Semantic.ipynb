{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3vmmpu/OUqCSgrVyj27Cu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/LLM_RAG/LLM_RAG_Semantic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2wC4i8p5Ho-"
      },
      "outputs": [],
      "source": [
        "# === 1. Mount Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# === 2. Install Required Packages ===\n",
        "!pip install -q sentence-transformers faiss-cpu pandas openai --upgrade\n",
        "!pip install snowflake\n",
        "!pip install hf_xet  # Add this line to silence warnings and improve performance\n",
        "\n",
        "\n",
        "# === 3. RAG Pipeline ===\n",
        "import snowflake.connector\n",
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import pickle\n",
        "\n",
        "\n",
        "# === 4. Configuration ===\n",
        "api_key_path = \"/content/drive/Othercomputers/My Mac/CSCI_104/credentials/openaikey.txt\"\n",
        "\n",
        "# Read OpenAI API key\n",
        "with open(api_key_path, 'r') as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "# Create OpenAI client\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# === 5. Load Corpus from Snowflake ===\n",
        "import snowflake.connector\n",
        "\n",
        "# Load credentials from a text file (format: KEY=VALUE per line)\n",
        "sf_creds_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n",
        "\n",
        "sf_env = {}\n",
        "with open(sf_creds_path, 'r') as f:\n",
        "    for line in f:\n",
        "        if '=' in line:\n",
        "            key, value = line.strip().split('=', 1)\n",
        "            sf_env[key.strip()] = value.strip()\n",
        "\n",
        "# Connect to Snowflake\n",
        "conn = snowflake.connector.connect(\n",
        "    user=sf_env['USER'],\n",
        "    password=sf_env['PASSWORD'],\n",
        "    account=sf_env['ACCOUNT'],\n",
        "    database='E9_CORPUS',\n",
        "    schema='E9_CORPUS_SCHEMA',\n",
        "    warehouse='COMPUTE_WH'  # or use your default warehouse\n",
        ")\n",
        "\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS FROM E9_CORPUS.E9_CORPUS_SCHEMA.E9_FORUM_CORPUS\")\n",
        "rows = cur.fetchall()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(rows, columns=['thread_id', 'thread_title', 'thread_first_post', 'thread_all_posts'])\n",
        "print(f\"Loaded {len(df)} threads from Snowflake.\")\n",
        "\n",
        "df[\"full_text\"] = (\n",
        "    df[\"thread_title\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_first_post\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_all_posts\"].fillna(\"\")\n",
        ")\n",
        "\n",
        "\n",
        "# === 6. Embed Corpus ===\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "# === 7. Create FAISS Index ===\n",
        "dimension = corpus_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# === 8. Ask a Question ===\n",
        "question = \"How do I remove the steering wheel in an E9?\"\n",
        "question_embedding = model.encode([question])\n",
        "\n",
        "# === 9. Retrieve Top Matches + Distances ===\n",
        "top_k = 3\n",
        "distances, indices = index.search(question_embedding, top_k)\n",
        "\n",
        "retrieved_texts = []\n",
        "print(\"\\n=== Retrieved Threads with Distances ===\\n\")\n",
        "for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
        "    title = df.iloc[idx][\"thread_title\"]\n",
        "    print(f\"[{i+1}] Distance: {dist:.4f} | Title: {title}\")\n",
        "    retrieved_texts.append(df.iloc[idx][\"full_text\"])\n",
        "\n",
        "# === 10. Format Prompt with Context ===\n",
        "context = \"\\n\\n\".join([f\"Thread {i+1}:\\n{text}\" for i, text in enumerate(retrieved_texts)])\n",
        "rag_prompt = f\"\"\"You are an expert on BMW E9 maintenance. Use the following forum threads to answer the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# === 11a. Generate Answer WITH context ===\n",
        "response_with_context = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# === 11b. Generate Answer WITHOUT context (baseline) ===\n",
        "baseline_prompt = f\"\"\"You are an expert on BMW E9 maintenance.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "response_without_context = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": baseline_prompt}],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# === 12. Export FAISS Index and Thread Data ===\n",
        "print(\"\\n=== Exporting FAISS Index and Thread Data ===\\n\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"/content/drive/Othercomputers/My Mac/Git/Language_Models/streamlit_rag/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Export FAISS index with the exact filenames Streamlit app expects\n",
        "faiss_path = os.path.join(output_dir, \"bmw_e9_index.faiss\")\n",
        "faiss.write_index(index, faiss_path)\n",
        "print(f\"FAISS index exported to: {faiss_path}\")\n",
        "\n",
        "# Export DataFrame with thread data with the exact filename Streamlit app expects\n",
        "df_path = os.path.join(output_dir, \"bmw_e9_threads.pkl\")\n",
        "with open(df_path, \"wb\") as f:\n",
        "    pickle.dump(df, f)\n",
        "print(f\"Thread data exported to: {df_path}\")\n",
        "\n",
        "# Also export a sample of the data as CSV for easy inspection\n",
        "sample_csv_path = os.path.join(output_dir, \"bmw_e9_sample.csv\")\n",
        "# Select subset of columns and only 100 rows for the sample\n",
        "df[['thread_id', 'thread_title']].head(100).to_csv(sample_csv_path, index=False)\n",
        "print(f\"Sample data exported to: {sample_csv_path}\")\n",
        "\n",
        "print(\"\\nExport complete! Files are now saved to the Git repository directory.\")\n",
        "print(f\"FAISS index: {faiss_path}\")\n",
        "print(f\"Thread data: {df_path}\")\n",
        "print(f\"Sample CSV: {sample_csv_path}\")\n",
        "\n",
        "# Verify files were created\n",
        "import os\n",
        "if os.path.exists(faiss_path) and os.path.exists(df_path):\n",
        "    print(f\"\\n SUCCESS: Files successfully created in {output_dir}\")\n",
        "    print(f\"FAISS index size: {os.path.getsize(faiss_path) / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"Thread data size: {os.path.getsize(df_path) / 1024 / 1024:.2f} MB\")\n",
        "else:\n",
        "    print(\"\\n ERROR: Files were not created successfully\")\n",
        "\n",
        "# === 13. Output both answers ===\n",
        "print(\"\\n=== ANSWER WITH RAG CONTEXT ===\\n\")\n",
        "print(response_with_context.choices[0].message.content)\n",
        "\n",
        "print(\"\\n=== BASELINE ANSWER (No RAG) ===\\n\")\n",
        "print(response_without_context.choices[0].message.content)"
      ]
    }
  ]
}