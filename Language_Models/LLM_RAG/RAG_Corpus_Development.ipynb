{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/LLM_RAG/RAG_Corpus_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        "#1 Project Description\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egcgaDGAzB6r"
      },
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the websiteâ€™s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ff_KyF7fz7"
      },
      "source": [
        "#2 Create Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWO8xyGhyy0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42741cb1-eca5-40bf-dc2f-7dfd7d96e427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "phiTC3nry3T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d76a58-47b7-4c5b-eac6-739986e1e4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowflake in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: snowflake-core==1.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake) (1.4.0)\n",
            "Requirement already satisfied: snowflake-legacy in /usr/local/lib/python3.11/dist-packages (from snowflake) (1.0.0)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.32.3)\n",
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (3.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from snowflake-core==1.4.0->snowflake) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->snowflake-core==1.4.0->snowflake) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->snowflake-core==1.4.0->snowflake) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->snowflake-core==1.4.0->snowflake) (2025.4.26)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.5.1)\n",
            "Requirement already satisfied: boto3>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.38.13)\n",
            "Requirement already satisfied: botocore>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.38.13)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (24.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (4.3.7)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python->snowflake-core==1.4.0->snowflake) (0.13.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python->snowflake-core==1.4.0->snowflake) (0.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python->snowflake-core==1.4.0->snowflake) (2.22)\n"
          ]
        }
      ],
      "source": [
        "#Packages and libraries\n",
        "\n",
        "!pip install snowflake\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import snowflake.connector\n",
        "import concurrent.futures\n",
        "import json\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Settings ---\n",
        "#credentials_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ACGDp-y8w-"
      },
      "source": [
        "#3 Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Configuration Parameters ---\n",
        "DEFAULT_BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "\n",
        "# --- Data Fetching Functions ---\n",
        "\n",
        "def create_urls(base_path: str, filename: str = 'e9_forum_thread_ids.csv', threads: int = 1):\n",
        "    \"\"\"\n",
        "    Creates and records thread IDs to fetch by incrementing from the last known thread ID.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory to store data files\n",
        "        filename: File to store thread IDs\n",
        "        threads: Number of new thread IDs to create\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with new thread IDs\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        existing_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(existing_ids['thread_id'].iloc[-1])\n",
        "        print(f\"Existing thread_ids found. Last thread_id: {last_thread_id}\")\n",
        "    else:\n",
        "        last_thread_id = 0\n",
        "        print(f\"No existing thread_ids. Starting from {last_thread_id}\")\n",
        "\n",
        "    new_ids = [{'thread_id': tid} for tid in range(last_thread_id + 1, last_thread_id + threads + 1)]\n",
        "    new_thread_ids = pd.DataFrame(new_ids)\n",
        "    new_thread_ids.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n",
        "\n",
        "    print(f\"Added {threads} new thread_ids. Ending at {new_ids[-1]['thread_id']}\")\n",
        "    return new_thread_ids\n",
        "\n",
        "def fetch_full_thread_data(df, base_path: str, posts_filename: str = 'e9_forum_posts.csv', decorated_filename: str = 'e9_forum_threads_decorated.csv'):\n",
        "    \"\"\"\n",
        "    Fetches full thread data for the given thread IDs from e9coupe forum.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with thread_id column\n",
        "        base_path: Directory to store output files\n",
        "        posts_filename: File to store individual posts\n",
        "        decorated_filename: File to store thread metadata\n",
        "    \"\"\"\n",
        "    # Ensure the base path exists\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "\n",
        "    # Load existing data if available\n",
        "    existing_posts = pd.read_csv(posts_file) if os.path.exists(posts_file) else pd.DataFrame(columns=['thread_id', 'post_timestamp', 'post_raw'])\n",
        "    existing_decorated = pd.read_csv(decorated_file) if os.path.exists(decorated_file) else pd.DataFrame(columns=['thread_id', 'thread_title', 'thread_first_post'])\n",
        "\n",
        "    # Find threads that we haven't processed yet\n",
        "    existing_thread_ids = set(existing_posts['thread_id'].tolist()) | set(existing_decorated['thread_id'].tolist())\n",
        "    new_threads = df[~df['thread_id'].isin(existing_thread_ids)]\n",
        "\n",
        "    if new_threads.empty:\n",
        "        print(\"No new threads to fetch.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Fetching data for {len(new_threads)} new threads...\")\n",
        "\n",
        "    post_data = []\n",
        "    decorated_data = []\n",
        "\n",
        "    for thread_id in new_threads['thread_id']:\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}/?page=1\"\n",
        "        try:\n",
        "            print(f\"Fetching thread {thread_id}...\")\n",
        "            response = requests.get(thread_url)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error {response.status_code} fetching {thread_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('article', class_='message--post')\n",
        "\n",
        "            if not articles:\n",
        "                print(f\"No posts found for thread {thread_id}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            post_count = len(articles)\n",
        "            print(f\"Found {post_count} posts in thread {thread_id}\")\n",
        "\n",
        "            # Extract thread title\n",
        "            title_element = soup.find('title')\n",
        "            thread_title = title_element.get_text().split('|')[0].strip() if title_element else \"No Title\"\n",
        "\n",
        "            # Extract first post content\n",
        "            first_post_element = soup.find('article', class_='message-body')\n",
        "            first_post = first_post_element.get_text(strip=True) if first_post_element else \"No content\"\n",
        "\n",
        "            decorated_data.append({\n",
        "                'thread_id': thread_id,\n",
        "                'thread_title': thread_title,\n",
        "                'thread_first_post': first_post\n",
        "            })\n",
        "\n",
        "            # Extract all posts\n",
        "            for article in articles:\n",
        "                timestamp_element = article.find('time')\n",
        "                content_element = article.find('div', class_='bbWrapper')\n",
        "\n",
        "                post_data.append({\n",
        "                    'thread_id': thread_id,\n",
        "                    'post_timestamp': timestamp_element['datetime'] if timestamp_element else \"N/A\",\n",
        "                    'post_raw': content_element.get_text(strip=True) if content_element else \"No content\"\n",
        "                })\n",
        "\n",
        "            # Be respectful to the server\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching thread {thread_id}: {e}\")\n",
        "\n",
        "    # Save new posts\n",
        "    if post_data:\n",
        "        new_posts_df = pd.DataFrame(post_data)\n",
        "        combined_posts = pd.concat([existing_posts, new_posts_df], ignore_index=True)\n",
        "        combined_posts.to_csv(posts_file, index=False)\n",
        "        print(f\"Saved {len(new_posts_df)} new posts. Total posts: {len(combined_posts)}\")\n",
        "\n",
        "    # Save new thread metadata\n",
        "    if decorated_data:\n",
        "        new_decorated_df = pd.DataFrame(decorated_data)\n",
        "        combined_decorated = pd.concat([existing_decorated, new_decorated_df], ignore_index=True)\n",
        "        combined_decorated.to_csv(decorated_file, index=False)\n",
        "        print(f\"Saved {len(new_decorated_df)} new decorated threads. Total threads: {len(combined_decorated)}\")\n",
        "\n",
        "def create_forum_corpus(base_path: str, posts_filename: str = 'e9_forum_posts.csv',\n",
        "                       decorated_filename: str = 'e9_forum_threads_decorated.csv',\n",
        "                       corpus_filename: str = 'e9_forum_corpus.csv',\n",
        "                       append_to_main_corpus: bool = True):\n",
        "    \"\"\"\n",
        "    Create or update the forum corpus file by combining posts and thread metadata.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing data files\n",
        "        posts_filename: File containing individual posts\n",
        "        decorated_filename: File containing thread metadata\n",
        "        corpus_filename: Output file for the batch corpus\n",
        "        append_to_main_corpus: Whether to append to the main corpus file\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing the complete corpus\n",
        "    \"\"\"\n",
        "    posts_file = os.path.join(base_path, posts_filename)\n",
        "    decorated_file = os.path.join(base_path, decorated_filename)\n",
        "    corpus_file = os.path.join(base_path, corpus_filename)\n",
        "    main_corpus_file = os.path.join(base_path, 'e9_forum_corpus.csv')\n",
        "\n",
        "    # Check if the required input files exist\n",
        "    if not os.path.exists(posts_file) or not os.path.exists(decorated_file):\n",
        "        print(f\"ERROR: Required input files not found. Cannot create corpus.\")\n",
        "        if not os.path.exists(posts_file):\n",
        "            print(f\"Missing: {posts_file}\")\n",
        "        if not os.path.exists(decorated_file):\n",
        "            print(f\"Missing: {decorated_file}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Read the full posts and decorated files\n",
        "    print(f\"Reading posts from {posts_file}\")\n",
        "    posts_df = pd.read_csv(posts_file)\n",
        "    print(f\"Reading thread metadata from {decorated_file}\")\n",
        "    decorated_df = pd.read_csv(decorated_file)\n",
        "\n",
        "    print(f\"Found {len(posts_df)} posts across {posts_df['thread_id'].nunique()} threads\")\n",
        "    print(f\"Found {len(decorated_df)} threads with metadata\")\n",
        "\n",
        "    # Aggregate posts by thread_id\n",
        "    print(\"Aggregating posts by thread ID...\")\n",
        "    aggregated = posts_df.groupby('thread_id')['post_raw'].agg(\n",
        "        lambda x: ' '.join(str(i) for i in x if pd.notna(i))).reset_index()\n",
        "    aggregated.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "    # Ensure data types match for joining\n",
        "    decorated_df['thread_id'] = decorated_df['thread_id'].astype('int64')\n",
        "    aggregated['thread_id'] = aggregated['thread_id'].astype('int64')\n",
        "\n",
        "    # Find threads with both metadata and posts\n",
        "    common_thread_ids = set(decorated_df['thread_id']) & set(aggregated['thread_id'])\n",
        "    print(f\"Found {len(common_thread_ids)} threads with both metadata and posts\")\n",
        "\n",
        "    # Filter to only include threads with both metadata and posts\n",
        "    filtered_decorated = decorated_df[decorated_df['thread_id'].isin(common_thread_ids)]\n",
        "    filtered_aggregated = aggregated[aggregated['thread_id'].isin(common_thread_ids)]\n",
        "\n",
        "    # Create the corpus by merging\n",
        "    batch_corpus = pd.merge(filtered_decorated, filtered_aggregated, on='thread_id', how='inner')\n",
        "    print(f\"Created corpus with {len(batch_corpus)} threads\")\n",
        "\n",
        "    # Save the batch corpus\n",
        "    batch_corpus.to_csv(corpus_file, index=False)\n",
        "    print(f\"Saved batch corpus to {corpus_file}\")\n",
        "\n",
        "    # If append_to_main_corpus is True, update the main corpus file\n",
        "    if append_to_main_corpus:\n",
        "        # Load existing main corpus if available\n",
        "        if os.path.exists(main_corpus_file):\n",
        "            main_corpus = pd.read_csv(main_corpus_file)\n",
        "            print(f\"Loaded existing main corpus with {len(main_corpus)} threads\")\n",
        "\n",
        "            # Find new threads not already in the main corpus\n",
        "            existing_main_thread_ids = set(main_corpus['thread_id'].tolist())\n",
        "            new_threads = batch_corpus[~batch_corpus['thread_id'].isin(existing_main_thread_ids)]\n",
        "\n",
        "            if new_threads.empty:\n",
        "                print(\"No new threads to add to main corpus\")\n",
        "            else:\n",
        "                # Append new threads to main corpus\n",
        "                combined_corpus = pd.concat([main_corpus, new_threads], ignore_index=True)\n",
        "                combined_corpus.to_csv(main_corpus_file, index=False)\n",
        "                print(f\"Added {len(new_threads)} new threads to main corpus. Total: {len(combined_corpus)}\")\n",
        "                return combined_corpus\n",
        "        else:\n",
        "            # If main corpus doesn't exist, create it with the batch corpus\n",
        "            batch_corpus.to_csv(main_corpus_file, index=False)\n",
        "            print(f\"Created new main corpus with {len(batch_corpus)} threads\")\n",
        "\n",
        "    return batch_corpus\n",
        "\n",
        "def update_local_corpus(base_path: str, threads_to_add: int = 5, corpus_filename: str = 'e9_forum_corpus_batch.csv'):\n",
        "    \"\"\"\n",
        "    Main function to update the local forum corpus by fetching new threads.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory to store all data files\n",
        "        threads_to_add: Number of new threads to fetch\n",
        "        corpus_filename: Filename for the batch corpus\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing the updated corpus\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting Local Forum Corpus Update ===\\n\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Get new thread IDs to fetch\n",
        "    new_thread_ids = create_urls(base_path, threads=threads_to_add)\n",
        "\n",
        "    # Fetch data for new threads\n",
        "    fetch_full_thread_data(new_thread_ids, base_path)\n",
        "\n",
        "    # Create or update the corpus, and append to main corpus\n",
        "    forum_corpus_df = create_forum_corpus(base_path, corpus_filename=corpus_filename, append_to_main_corpus=True)\n",
        "\n",
        "    print(\"\\n=== Local Forum Corpus Update Complete ===\\n\")\n",
        "    return forum_corpus_df\n",
        "\n",
        "def create_corpus_backup(base_path: str, corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    \"\"\"\n",
        "    Create a timestamped backup of the current corpus.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing the corpus file\n",
        "        corpus_filename: Filename of the corpus to backup\n",
        "    \"\"\"\n",
        "    corpus_path = os.path.join(base_path, corpus_filename)\n",
        "    if not os.path.exists(corpus_path):\n",
        "        print(f\"Corpus file not found: {corpus_path}\")\n",
        "        return\n",
        "\n",
        "    # Create backups directory\n",
        "    backup_dir = os.path.join(base_path, 'backups')\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamped filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    backup_filename = f\"{os.path.splitext(corpus_filename)[0]}_{timestamp}.csv\"\n",
        "    backup_path = os.path.join(backup_dir, backup_filename)\n",
        "\n",
        "    # Copy the file\n",
        "    import shutil\n",
        "    shutil.copy2(corpus_path, backup_path)\n",
        "\n",
        "    print(f\"Created backup: {backup_path}\")\n",
        "\n",
        "def save_corpus_to_json(base_path: str, corpus_filename: str = 'e9_forum_corpus.csv'):\n",
        "    \"\"\"\n",
        "    Save the corpus to a JSON file with better formatting.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing the corpus file\n",
        "        corpus_filename: Filename of the corpus to convert\n",
        "    \"\"\"\n",
        "    corpus_path = os.path.join(base_path, corpus_filename)\n",
        "    if not os.path.exists(corpus_path):\n",
        "        print(f\"Corpus file not found: {corpus_path}\")\n",
        "        return\n",
        "\n",
        "    # Load the corpus\n",
        "    corpus_df = pd.read_csv(corpus_path)\n",
        "\n",
        "    if corpus_df.empty:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Create JSON filename based on the CSV filename\n",
        "    json_filename = f\"{os.path.splitext(corpus_filename)[0]}.json\"\n",
        "    json_path = os.path.join(base_path, json_filename)\n",
        "\n",
        "    # Convert to records format for better JSON structure\n",
        "    records = corpus_df.to_dict(orient='records')\n",
        "\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(records, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Saved corpus to JSON: {json_path} ({len(records)} threads)\")\n",
        "\n",
        "def fetch_forum_data_in_batches(base_path: str, num_batches: int = 2, threads_per_batch: int = 10):\n",
        "    \"\"\"\n",
        "    Run multiple batch updates to fetch forum data and aggregate into a single corpus.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory to store all data files\n",
        "        num_batches: Number of batches to run\n",
        "        threads_per_batch: Number of threads to fetch in each batch\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing the complete corpus\n",
        "    \"\"\"\n",
        "    # Ensure base path exists\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Create backup of existing corpus if it exists\n",
        "    main_corpus_path = os.path.join(base_path, 'e9_forum_corpus.csv')\n",
        "    if os.path.exists(main_corpus_path):\n",
        "        create_corpus_backup(base_path)\n",
        "\n",
        "    print(f\"\\n=== Starting Forum Data Fetching: {num_batches} batches, {threads_per_batch} threads per batch ===\\n\")\n",
        "\n",
        "    for batch_num in range(num_batches):\n",
        "        print(f\"\\n=== Processing Batch {batch_num + 1}/{num_batches} ===\\n\")\n",
        "\n",
        "        batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n",
        "        update_local_corpus(base_path, threads_to_add=threads_per_batch, corpus_filename=batch_filename)\n",
        "\n",
        "    # Save the final corpus to JSON format as well\n",
        "    save_corpus_to_json(base_path)\n",
        "\n",
        "    # Load and return the final corpus\n",
        "    if os.path.exists(main_corpus_path):\n",
        "        final_corpus = pd.read_csv(main_corpus_path)\n",
        "        print(f\"\\n=== Forum Data Fetching Complete: {len(final_corpus)} total threads in corpus ===\\n\")\n",
        "        return final_corpus\n",
        "    else:\n",
        "        print(\"\\n=== Forum Data Fetching Complete, but no corpus was created ===\\n\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "yGEWd5gEHxlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Data Storage"
      ],
      "metadata": {
        "id": "-xhKA3MiT23t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Parameters ---\n",
        "DEFAULT_BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "DEFAULT_CREDENTIALS_PATH = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    \"\"\"\n",
        "    Load Snowflake credentials from a file and set them as environment variables.\n",
        "\n",
        "    Args:\n",
        "        path_to_credentials: Path to the credentials file\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_credentials):\n",
        "        raise FileNotFoundError(f\"Credentials file not found: {path_to_credentials}\")\n",
        "\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=', 1)\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Skipping invalid line {line_num}: {line}\")\n",
        "\n",
        "    for var in ['USER', 'PASSWORD', 'ACCOUNT']:\n",
        "        if not os.environ.get(var):\n",
        "            raise EnvironmentError(f\"Missing environment variable: {var}\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"\n",
        "    Connect to Snowflake using environment variables.\n",
        "\n",
        "    Returns:\n",
        "        Snowflake connection object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = snowflake.connector.connect(\n",
        "            user=os.environ.get('USER'),\n",
        "            password=os.environ.get('PASSWORD'),\n",
        "            account=os.environ.get('ACCOUNT')\n",
        "        )\n",
        "        print(f\"Connected to Snowflake account: {os.environ.get('ACCOUNT')}\")\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        raise ConnectionError(f\"Failed to connect to Snowflake: {e}\")\n",
        "\n",
        "def create_db_schema_table(cur):\n",
        "    \"\"\"\n",
        "    Create the database, schema, and table if they don't exist.\n",
        "\n",
        "    Args:\n",
        "        cur: Snowflake cursor\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "        cur.execute(\"USE DATABASE e9_corpus\")\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "                THREAD_ID NUMBER(38,0) PRIMARY KEY,\n",
        "                THREAD_TITLE STRING,\n",
        "                THREAD_FIRST_POST STRING,\n",
        "                THREAD_ALL_POSTS STRING\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"Database, schema, and table checked/created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating database/schema/table: {e}\")\n",
        "\n",
        "def fetch_existing_thread_ids(cur):\n",
        "    \"\"\"\n",
        "    Fetch the list of thread IDs that already exist in Snowflake.\n",
        "\n",
        "    Args:\n",
        "        cur: Snowflake cursor\n",
        "\n",
        "    Returns:\n",
        "        Set of existing thread IDs\n",
        "    \"\"\"\n",
        "    query = \"SELECT THREAD_ID FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\"\n",
        "    try:\n",
        "        cur.execute(query)\n",
        "        result = cur.fetchall()\n",
        "        return set(row[0] for row in result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching existing thread IDs: {e}\")\n",
        "        return set()\n",
        "\n",
        "def insert_missing_data(cur, df, existing_thread_ids):\n",
        "    \"\"\"\n",
        "    Insert only new data into Snowflake, skipping already loaded threads.\n",
        "\n",
        "    Args:\n",
        "        cur: Snowflake cursor\n",
        "        df: DataFrame with the corpus data\n",
        "        existing_thread_ids: Set of thread IDs already in the database\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No data to insert.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Original DataFrame has {len(df)} rows.\")\n",
        "    df.columns = [col.upper() for col in df.columns]\n",
        "\n",
        "    # Filter only missing threads\n",
        "    new_df = df[~df['THREAD_ID'].isin(existing_thread_ids)]\n",
        "    print(f\"{len(new_df)} new threads will be inserted into Snowflake.\")\n",
        "\n",
        "    if new_df.empty:\n",
        "        print(\"No new threads to insert.\")\n",
        "        return\n",
        "\n",
        "    # Replace NaN values with None\n",
        "    new_df = new_df.where(pd.notnull(new_df), None)\n",
        "\n",
        "    insert_query = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS)\n",
        "    VALUES (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list of tuples\n",
        "    rows_to_insert = [\n",
        "        (\n",
        "            row['THREAD_ID'],\n",
        "            row['THREAD_TITLE'],\n",
        "            row['THREAD_FIRST_POST'],\n",
        "            row['THREAD_ALL_POSTS']\n",
        "        )\n",
        "        for _, row in new_df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Batch insert all rows at once\n",
        "    cur.executemany(insert_query, rows_to_insert)\n",
        "    print(f\"Inserted {len(rows_to_insert)} new threads into Snowflake.\")\n",
        "\n",
        "def upload_corpus_to_snowflake(base_path=DEFAULT_BASE_PATH,\n",
        "                              credentials_path=DEFAULT_CREDENTIALS_PATH,\n",
        "                              filename=None):\n",
        "    \"\"\"\n",
        "    Upload the corpus file to Snowflake.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing the corpus file\n",
        "        credentials_path: Path to the Snowflake credentials file\n",
        "        filename: Name of the corpus file (if None, uses the default e9_forum_corpus.csv)\n",
        "    \"\"\"\n",
        "    # If no specific filename is provided, use the main corpus file\n",
        "    if filename is None:\n",
        "        filename = 'e9_forum_corpus.csv'\n",
        "\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Corpus file not found: {file_path}\")\n",
        "\n",
        "    # Load the corpus\n",
        "    forum_corpus_df = pd.read_csv(file_path)\n",
        "    print(f\"Loaded {len(forum_corpus_df)} rows from {file_path} to upload.\")\n",
        "\n",
        "    # Set up Snowflake connection\n",
        "    load_credentials(credentials_path)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        # Create database objects if needed\n",
        "        create_db_schema_table(cur)\n",
        "\n",
        "        # Check what threads already exist\n",
        "        existing_thread_ids = fetch_existing_thread_ids(cur)\n",
        "        print(f\"Snowflake already has {len(existing_thread_ids)} threads.\")\n",
        "\n",
        "        # Insert only new threads\n",
        "        insert_missing_data(cur, forum_corpus_df, existing_thread_ids)\n",
        "\n",
        "        # Commit the transaction\n",
        "        conn.commit()\n",
        "        print(f\"Data from {filename} committed successfully.\")\n",
        "\n",
        "        # Get the final count\n",
        "        cur.execute(\"SELECT COUNT(*) FROM e9_corpus.e9_corpus_schema.e9_forum_corpus\")\n",
        "        final_count = cur.fetchone()[0]\n",
        "        print(f\"Total threads now in Snowflake: {final_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upload: {e}\")\n",
        "        conn.rollback()\n",
        "        raise e\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "def upload_batch_files_to_snowflake(base_path=DEFAULT_BASE_PATH,\n",
        "                                   credentials_path=DEFAULT_CREDENTIALS_PATH,\n",
        "                                   pattern='e9_forum_corpus_batch_*.csv'):\n",
        "    \"\"\"\n",
        "    Upload all batch files to Snowflake.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing the batch files\n",
        "        credentials_path: Path to the Snowflake credentials file\n",
        "        pattern: Pattern to match batch files\n",
        "    \"\"\"\n",
        "    import glob\n",
        "\n",
        "    # Find all matching batch files\n",
        "    batch_files = glob.glob(os.path.join(base_path, pattern))\n",
        "\n",
        "    if not batch_files:\n",
        "        print(f\"No batch files found matching pattern: {pattern}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(batch_files)} batch files to upload.\")\n",
        "\n",
        "    # Upload each batch file\n",
        "    for batch_file in sorted(batch_files):\n",
        "        filename = os.path.basename(batch_file)\n",
        "        print(f\"\\n=== Uploading {filename} ===\\n\")\n",
        "        try:\n",
        "            upload_corpus_to_snowflake(base_path, credentials_path, filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading {filename}: {e}\")\n",
        "\n",
        "def upload_main_corpus_to_snowflake(base_path=DEFAULT_BASE_PATH,\n",
        "                                   credentials_path=DEFAULT_CREDENTIALS_PATH):\n",
        "    \"\"\"\n",
        "    Upload the main corpus file to Snowflake.\n",
        "\n",
        "    Args:\n",
        "        base_path: Directory containing the corpus file\n",
        "        credentials_path: Path to the Snowflake credentials file\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Uploading main corpus file ===\\n\")\n",
        "    try:\n",
        "        upload_corpus_to_snowflake(base_path, credentials_path, 'e9_forum_corpus.csv')\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading main corpus: {e}\")\n",
        "\n",
        "# For Colab execution\n",
        "if __name__ == \"__main__\":\n",
        "    # In Colab, create buttons for the user to choose what to upload\n",
        "    if 'google.colab' in globals():\n",
        "        try:\n",
        "            from google.colab import output\n",
        "            import ipywidgets as widgets\n",
        "            from IPython.display import display\n",
        "\n",
        "            print(\"E9 Forum Snowflake Uploader\")\n",
        "            print(\"---------------------------\")\n",
        "            print(f\"Default Base Path: {DEFAULT_BASE_PATH}\")\n",
        "            print(f\"Default Credentials Path: {DEFAULT_CREDENTIALS_PATH}\")\n",
        "            print(\"Choose an upload option:\")\n",
        "\n",
        "            def on_upload_main_clicked(b):\n",
        "                output.clear()\n",
        "                upload_main_corpus_to_snowflake()\n",
        "\n",
        "            def on_upload_batches_clicked(b):\n",
        "                output.clear()\n",
        "                upload_batch_files_to_snowflake()\n",
        "\n",
        "            def on_upload_all_clicked(b):\n",
        "                output.clear()\n",
        "                print(\"Uploading main corpus...\")\n",
        "                upload_main_corpus_to_snowflake()\n",
        "                print(\"\\nUploading batch files...\")\n",
        "                upload_batch_files_to_snowflake()\n",
        "\n",
        "            upload_main_button = widgets.Button(\n",
        "                description='Upload Main Corpus',\n",
        "                button_style='info',\n",
        "                tooltip='Upload only the main corpus file'\n",
        "            )\n",
        "            upload_main_button.on_click(on_upload_main_clicked)\n",
        "\n",
        "            upload_batches_button = widgets.Button(\n",
        "                description='Upload Batch Files',\n",
        "                button_style='warning',\n",
        "                tooltip='Upload all batch files'\n",
        "            )\n",
        "            upload_batches_button.on_click(on_upload_batches_clicked)\n",
        "\n",
        "            upload_all_button = widgets.Button(\n",
        "                description='Upload All',\n",
        "                button_style='success',\n",
        "                tooltip='Upload both main corpus and batch files'\n",
        "            )\n",
        "            upload_all_button.on_click(on_upload_all_clicked)\n",
        "\n",
        "            display(widgets.HBox([upload_main_button, upload_batches_button, upload_all_button]))\n",
        "        except:\n",
        "            # Fall back to regular execution if widgets not available\n",
        "            print(\"Interactive widgets not available. Using default options.\")\n",
        "            print(\"To upload the main corpus file:\")\n",
        "            print(\"  upload_main_corpus_to_snowflake()\")\n",
        "            print(\"To upload all batch files:\")\n",
        "            print(\"  upload_batch_files_to_snowflake()\")\n",
        "            print(\"To upload a specific file:\")\n",
        "            print(\"  upload_corpus_to_snowflake(filename='your_file.csv')\")\n",
        "    else:\n",
        "        # Not in Colab\n",
        "        print(\"To upload the main corpus file:\")\n",
        "        print(\"  upload_main_corpus_to_snowflake()\")\n",
        "        print(\"To upload all batch files:\")\n",
        "        print(\"  upload_batch_files_to_snowflake()\")\n",
        "        print(\"To upload a specific file:\")\n",
        "        print(\"  upload_corpus_to_snowflake(filename='your_file.csv')\")"
      ],
      "metadata": {
        "id": "Ao-68xqOSFXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461300eb-a28f-49f0-b79e-bbabe11666b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To upload the main corpus file:\n",
            "  upload_main_corpus_to_snowflake()\n",
            "To upload all batch files:\n",
            "  upload_batch_files_to_snowflake()\n",
            "To upload a specific file:\n",
            "  upload_corpus_to_snowflake(filename='your_file.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Orchestration"
      ],
      "metadata": {
        "id": "CCuARH_Thu_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "BASE_PATH = '/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n",
        "NUM_BATCHES = 2\n",
        "THREADS_PER_BATCH = 25\n",
        "MAX_WORKERS = 3\n",
        "\n",
        "# Create executor for concurrent uploads\n",
        "executor = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
        "futures = []\n",
        "\n",
        "# Process each batch\n",
        "for batch_num in range(NUM_BATCHES):\n",
        "    print(f\"\\n=== Starting batch {batch_num + 1} ===\\n\")\n",
        "\n",
        "    # Generate batch filename\n",
        "    batch_filename = f\"e9_forum_corpus_batch_{batch_num + 1}.csv\"\n",
        "\n",
        "    # Fetch data\n",
        "    forum_corpus_df = update_local_corpus(BASE_PATH, threads_to_add=THREADS_PER_BATCH, corpus_filename=batch_filename)\n",
        "\n",
        "    # Upload to Snowflake in the background\n",
        "    future = executor.submit(upload_corpus_to_snowflake, BASE_PATH, CREDENTIALS_PATH, batch_filename)\n",
        "\n",
        "    # Add callback for result handling\n",
        "    def create_callback(filename):\n",
        "        def handle_upload_result(fut):\n",
        "            try:\n",
        "                fut.result()\n",
        "                print(f\"Upload completed for {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"UPLOAD FAILED for {filename}: {e}\")\n",
        "        return handle_upload_result\n",
        "\n",
        "    future.add_done_callback(create_callback(batch_filename))\n",
        "    futures.append(future)\n",
        "\n",
        "# Wait for all uploads to complete\n",
        "executor.shutdown(wait=True)\n",
        "print(\"\\n=== All scraping and uploads complete ===\\n\")"
      ],
      "metadata": {
        "id": "I5QtR4GTSFd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de3cb66-9536-467c-a712-fbad8a6edec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting batch 1 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 15260\n",
            "Added 25 new thread_ids. Ending at 15285\n",
            "Fetching data for 25 new threads...\n",
            "Fetching thread 15261...\n",
            "Found 1 posts in thread 15261\n",
            "Fetching thread 15262...\n",
            "Found 3 posts in thread 15262\n",
            "Fetching thread 15263...\n",
            "Found 1 posts in thread 15263\n",
            "Fetching thread 15264...\n",
            "Found 7 posts in thread 15264\n",
            "Fetching thread 15265...\n",
            "Found 1 posts in thread 15265\n",
            "Fetching thread 15266...\n",
            "Found 9 posts in thread 15266\n",
            "Fetching thread 15267...\n",
            "Found 3 posts in thread 15267\n",
            "Fetching thread 15268...\n",
            "Found 20 posts in thread 15268\n",
            "Fetching thread 15269...\n",
            "Found 2 posts in thread 15269\n",
            "Fetching thread 15270...\n",
            "Found 4 posts in thread 15270\n",
            "Fetching thread 15271...\n",
            "Found 2 posts in thread 15271\n",
            "Fetching thread 15272...\n",
            "Found 20 posts in thread 15272\n",
            "Fetching thread 15273...\n",
            "Found 3 posts in thread 15273\n",
            "Fetching thread 15274...\n",
            "Found 1 posts in thread 15274\n",
            "Fetching thread 15275...\n",
            "Found 1 posts in thread 15275\n",
            "Fetching thread 15276...\n",
            "Found 1 posts in thread 15276\n",
            "Fetching thread 15277...\n",
            "Found 4 posts in thread 15277\n",
            "Fetching thread 15278...\n",
            "Found 20 posts in thread 15278\n",
            "Fetching thread 15279...\n",
            "Found 2 posts in thread 15279\n",
            "Fetching thread 15280...\n",
            "Found 7 posts in thread 15280\n",
            "Fetching thread 15281...\n",
            "Found 13 posts in thread 15281\n",
            "Fetching thread 15282...\n",
            "Found 6 posts in thread 15282\n",
            "Fetching thread 15283...\n",
            "Found 18 posts in thread 15283\n",
            "Fetching thread 15284...\n",
            "Found 3 posts in thread 15284\n",
            "Fetching thread 15285...\n",
            "Found 8 posts in thread 15285\n",
            "Saved 160 new posts. Total posts: 93654\n",
            "Saved 25 new decorated threads. Total threads: 15002\n",
            "Reading posts from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_posts.csv\n",
            "Reading thread metadata from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads_decorated.csv\n",
            "Found 93654 posts across 15002 threads\n",
            "Found 15002 threads with metadata\n",
            "Aggregating posts by thread ID...\n",
            "Found 15002 threads with both metadata and posts\n",
            "Created corpus with 15002 threads\n",
            "Saved batch corpus to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_1.csv\n",
            "Loaded existing main corpus with 14977 threads\n",
            "Added 25 new threads to main corpus. Total: 15002\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "\n",
            "=== Starting batch 2 ===\n",
            "\n",
            "\n",
            "=== Starting Local Forum Corpus Update ===\n",
            "\n",
            "Existing thread_ids found. Last thread_id: 15285\n",
            "Added 25 new thread_ids. Ending at 15310\n",
            "Loaded 15002 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_1.csv to upload.\n",
            "Fetching data for 25 new threads...\n",
            "Fetching thread 15286...\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Found 7 posts in thread 15286\n",
            "Database, schema, and table checked/created.\n",
            "Fetching thread 15287...\n",
            "Found 20 posts in thread 15287\n",
            "Snowflake already has 14977 threads.\n",
            "Original DataFrame has 15002 rows.\n",
            "25 new threads will be inserted into Snowflake.\n",
            "Fetching thread 15288...\n",
            "Found 5 posts in thread 15288\n",
            "Inserted 25 new threads into Snowflake.\n",
            "Data from e9_forum_corpus_batch_1.csv committed successfully.\n",
            "Fetching thread 15289...\n",
            "Total threads now in Snowflake: 15002\n",
            "Found 1 posts in thread 15289\n",
            "Upload completed for e9_forum_corpus_batch_1.csv\n",
            "Fetching thread 15290...\n",
            "Found 1 posts in thread 15290\n",
            "Fetching thread 15291...\n",
            "Found 6 posts in thread 15291\n",
            "Fetching thread 15292...\n",
            "Found 6 posts in thread 15292\n",
            "Fetching thread 15293...\n",
            "Found 20 posts in thread 15293\n",
            "Fetching thread 15294...\n",
            "Found 6 posts in thread 15294\n",
            "Fetching thread 15295...\n",
            "Found 5 posts in thread 15295\n",
            "Fetching thread 15296...\n",
            "Found 3 posts in thread 15296\n",
            "Fetching thread 15297...\n",
            "Found 2 posts in thread 15297\n",
            "Fetching thread 15298...\n",
            "Found 6 posts in thread 15298\n",
            "Fetching thread 15299...\n",
            "Found 10 posts in thread 15299\n",
            "Fetching thread 15300...\n",
            "Found 4 posts in thread 15300\n",
            "Fetching thread 15301...\n",
            "Found 10 posts in thread 15301\n",
            "Fetching thread 15302...\n",
            "Found 5 posts in thread 15302\n",
            "Fetching thread 15303...\n",
            "Found 6 posts in thread 15303\n",
            "Fetching thread 15304...\n",
            "Found 1 posts in thread 15304\n",
            "Fetching thread 15305...\n",
            "Found 2 posts in thread 15305\n",
            "Fetching thread 15306...\n",
            "Found 6 posts in thread 15306\n",
            "Fetching thread 15307...\n",
            "Found 3 posts in thread 15307\n",
            "Fetching thread 15308...\n",
            "Found 5 posts in thread 15308\n",
            "Fetching thread 15309...\n",
            "Found 1 posts in thread 15309\n",
            "Fetching thread 15310...\n",
            "Found 18 posts in thread 15310\n",
            "Saved 159 new posts. Total posts: 93813\n",
            "Saved 25 new decorated threads. Total threads: 15027\n",
            "Reading posts from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_posts.csv\n",
            "Reading thread metadata from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_threads_decorated.csv\n",
            "Found 93813 posts across 15027 threads\n",
            "Found 15027 threads with metadata\n",
            "Aggregating posts by thread ID...\n",
            "Found 15027 threads with both metadata and posts\n",
            "Created corpus with 15027 threads\n",
            "Saved batch corpus to /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_2.csv\n",
            "Loaded existing main corpus with 15002 threads\n",
            "Added 25 new threads to main corpus. Total: 15027\n",
            "\n",
            "=== Local Forum Corpus Update Complete ===\n",
            "\n",
            "Loaded 15027 rows from /content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus_batch_2.csv to upload.\n",
            "Connected to Snowflake account: MTOSDJI-QT86272\n",
            "Database, schema, and table checked/created.\n",
            "Snowflake already has 15002 threads.\n",
            "Original DataFrame has 15027 rows.\n",
            "25 new threads will be inserted into Snowflake.\n",
            "Inserted 25 new threads into Snowflake.\n",
            "Data from e9_forum_corpus_batch_2.csv committed successfully.\n",
            "Total threads now in Snowflake: 15027\n",
            "Upload completed for e9_forum_corpus_batch_2.csv\n",
            "\n",
            "=== All scraping and uploads complete ===\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}