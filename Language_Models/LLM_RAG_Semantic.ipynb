{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJaHUWCeqR+UCwmkk9WqEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models/LLM_RAG_Semantic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2wC4i8p5Ho-"
      },
      "outputs": [],
      "source": [
        "# === 1. Mount Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# === 2. Install Required Packages ===\n",
        "!pip install -q sentence-transformers faiss-cpu pandas openai --upgrade\n",
        "\n",
        "!pip install snowflake\n",
        "\n",
        "import snowflake.connector\n",
        "\n",
        "# === 3. RAG Pipeline ===\n",
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "\n",
        "# === 4. Configuration ===\n",
        "api_key_path = \"/content/drive/Othercomputers/My Mac/CSCI_104/credentials/openaikey.txt\"\n",
        "\n",
        "# Read OpenAI API key\n",
        "with open(api_key_path, 'r') as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "# Create OpenAI client\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# === 5. Load Corpus from Snowflake ===\n",
        "import snowflake.connector\n",
        "\n",
        "# Load credentials from a text file (format: KEY=VALUE per line)\n",
        "sf_creds_path = '/content/drive/Othercomputers/My Mac/Git/credentials/snowflake_credentials.txt'\n",
        "\n",
        "sf_env = {}\n",
        "with open(sf_creds_path, 'r') as f:\n",
        "    for line in f:\n",
        "        if '=' in line:\n",
        "            key, value = line.strip().split('=', 1)\n",
        "            sf_env[key.strip()] = value.strip()\n",
        "\n",
        "# Connect to Snowflake\n",
        "conn = snowflake.connector.connect(\n",
        "    user=sf_env['USER'],\n",
        "    password=sf_env['PASSWORD'],\n",
        "    account=sf_env['ACCOUNT'],\n",
        "    database='E9_CORPUS',\n",
        "    schema='E9_CORPUS_SCHEMA',\n",
        "    warehouse='COMPUTE_WH'  # or use your default warehouse\n",
        ")\n",
        "\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS FROM E9_CORPUS.E9_CORPUS_SCHEMA.E9_FORUM_CORPUS\")\n",
        "rows = cur.fetchall()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(rows, columns=['thread_id', 'thread_title', 'thread_first_post', 'thread_all_posts'])\n",
        "print(f\"Loaded {len(df)} threads from Snowflake.\")\n",
        "\n",
        "df[\"full_text\"] = (\n",
        "    df[\"thread_title\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_first_post\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_all_posts\"].fillna(\"\")\n",
        ")\n",
        "\n",
        "\n",
        "# === 6. Embed Corpus ===\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "# === 7. Create FAISS Index ===\n",
        "dimension = corpus_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# === 8. Ask a Question ===\n",
        "question = \"How do I remove the steering wheel in an E9?\"\n",
        "question_embedding = model.encode([question])\n",
        "\n",
        "# === 9. Retrieve Top Matches + Distances ===\n",
        "top_k = 5\n",
        "distances, indices = index.search(question_embedding, top_k)\n",
        "\n",
        "retrieved_texts = []\n",
        "print(\"\\n=== Retrieved Threads with Distances ===\\n\")\n",
        "for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
        "    title = df.iloc[idx][\"thread_title\"]\n",
        "    print(f\"[{i+1}] Distance: {dist:.4f} | Title: {title}\")\n",
        "    retrieved_texts.append(df.iloc[idx][\"full_text\"])\n",
        "\n",
        "# === 10. Format Prompt with Context ===\n",
        "context = \"\\n\\n\".join([f\"Thread {i+1}:\\n{text}\" for i, text in enumerate(retrieved_texts)])\n",
        "rag_prompt = f\"\"\"You are an expert on BMW E9 maintenance. Use the following forum threads to answer the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# === 11a. Generate Answer WITH context ===\n",
        "response_with_context = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# === 11b. Generate Answer WITHOUT context (baseline) ===\n",
        "baseline_prompt = f\"\"\"You are an expert on BMW E9 maintenance.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "response_without_context = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": baseline_prompt}],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# === 12. Output both answers ===\n",
        "print(\"\\n=== ANSWER WITH RAG CONTEXT ===\\n\")\n",
        "print(response_with_context.choices[0].message.content)\n",
        "\n",
        "print(\"\\n=== BASELINE ANSWER (No RAG) ===\\n\")\n",
        "print(response_without_context.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using llama"
      ],
      "metadata": {
        "id": "dRwEGmBOYdxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 1: INSTALL DEPENDENCIES ===\n",
        "!pip install -q sentence-transformers faiss-cpu pandas transformers accelerate bitsandbytes\n",
        "\n",
        "# === STEP 2: MOUNT GOOGLE DRIVE ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === STEP 3: IMPORTS ===\n",
        "import os\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# === STEP 4: LOAD EMBEDDING MODEL ===\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# === STEP 5: LOAD LLAMA-2 MODEL FROM HUGGING FACE ===\n",
        "llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # You must accept model access on Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_auth_token=True)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# === STEP 6: LOAD CSV CORPUS FROM DRIVE ===\n",
        "csv_path = \"/content/drive/Othercomputers/My Mac/Git/Language_Models/datasets/e9/e9_forum_corpus.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# === STEP 7: PREPARE TEXT COLUMN ===\n",
        "df[\"full_text\"] = (\n",
        "    df[\"thread_title\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_first_post\"].fillna(\"\") + \"\\n\\n\" +\n",
        "    df[\"thread_all_posts\"].fillna(\"\")\n",
        ")\n",
        "\n",
        "# === STEP 8: EMBED CORPUS ===\n",
        "corpus_embeddings = embedding_model.encode(df[\"full_text\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "# === STEP 9: CREATE FAISS INDEX ===\n",
        "dimension = corpus_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# === STEP 10: ASK A QUESTION ===\n",
        "question = \"How do I remove the steering wheel in an E9?\"\n",
        "question_embedding = embedding_model.encode([question])\n",
        "\n",
        "# === STEP 11: RETRIEVE RELEVANT THREADS ===\n",
        "top_k = 5\n",
        "distances, indices = index.search(question_embedding, top_k)\n",
        "retrieved_texts = df.iloc[indices[0]][\"full_text\"].tolist()\n",
        "retrieved_titles = df.iloc[indices[0]][\"thread_title\"].tolist()\n",
        "\n",
        "# === STEP 12: FORMAT PROMPT FOR LLAMA ===\n",
        "context = \"\\n\\n\".join([f\"Thread {i+1}: {retrieved_titles[i]}\\n{retrieved_texts[i]}\" for i in range(len(retrieved_texts))])\n",
        "rag_prompt = f\"\"\"### Instruction:\n",
        "You are an expert on BMW E9 maintenance. Use only the following forum threads to answer the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "# === STEP 13: GENERATE RAG-BASED ANSWER WITH LLAMA ===\n",
        "inputs = tokenizer(rag_prompt, return_tensors=\"pt\").to(llama_model.device)\n",
        "outputs = llama_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    top_p=0.95\n",
        ")\n",
        "response_with_context = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# === STEP 14: GENERATE BASELINE ANSWER WITHOUT CONTEXT ===\n",
        "baseline_prompt = f\"\"\"### Instruction:\n",
        "You are an expert on BMW E9 maintenance.\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "inputs_base = tokenizer(baseline_prompt, return_tensors=\"pt\").to(llama_model.device)\n",
        "outputs_base = llama_model.generate(\n",
        "    **inputs_base,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    top_p=0.95\n",
        ")\n",
        "response_without_context = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
        "\n",
        "# === STEP 15: DISPLAY RESULTS ===\n",
        "print(\"\\n=== ANSWER WITH RAG CONTEXT (LLaMA) ===\\n\")\n",
        "print(response_with_context)\n",
        "\n",
        "print(\"\\n=== BASELINE ANSWER (LLaMA) ===\\n\")\n",
        "print(response_without_context)\n"
      ],
      "metadata": {
        "id": "pIg_tz8wYcZc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}