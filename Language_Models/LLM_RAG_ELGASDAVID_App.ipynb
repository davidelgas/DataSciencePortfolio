{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqdtWY8Yt39e",
        "outputId": "a20c7eff-b6c4-4c73-c7b0-94179a20bb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output directory and file path\n",
        "output_dir = '/content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/App/'\n",
        "app_path = output_dir + 'LLM_RAG_App.py'\n",
        "requirements_path = output_dir + 'LLM_RAG_reqs.txt'\n",
        "\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create the Streamlit app and save it to the specified path\n",
        "with open(app_path, 'w') as f:\n",
        "    f.write('''import streamlit as st\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# Page configuration MUST be the first Streamlit command\n",
        "st.set_page_config(page_title=\"BMW 3.0 Knowledge Base - RAG vs Standard GPT\", layout=\"wide\")\n",
        "\n",
        "# Reset button in sidebar (moved after page config)\n",
        "if st.sidebar.button(\"Reset Application\"):\n",
        "    for key in list(st.session_state.keys()):\n",
        "        del st.session_state[key]\n",
        "    st.rerun()\n",
        "\n",
        "# Title\n",
        "st.title(\"BMW 3.0 Knowledge Base - RAG vs Standard GPT\")\n",
        "st.markdown(\"### Compare RAG-enhanced responses with standard GPT-3.5\")\n",
        "\n",
        "# Step 1: API Keys\n",
        "if \"api_key_set\" not in st.session_state:\n",
        "    st.header(\"Step 1: Enter API Key\")\n",
        "    with st.form(\"api_keys_form\"):\n",
        "        openai_key = st.text_input(\"OpenAI API Key:\", type=\"password\")\n",
        "        submitted = st.form_submit_button(\"Submit Key\")\n",
        "\n",
        "        if submitted:\n",
        "            if openai_key:\n",
        "                st.session_state.openai_key = openai_key\n",
        "                st.session_state.api_key_set = True\n",
        "                st.success(\"API key saved!\")\n",
        "                st.rerun()\n",
        "            else:\n",
        "                st.error(\"OpenAI API key is required\")\n",
        "\n",
        "# Step 2: File Upload\n",
        "elif \"files_loaded\" not in st.session_state:\n",
        "    st.header(\"Step 2: Upload Required Files\")\n",
        "\n",
        "    st.info(\"Please upload the following files:\")\n",
        "\n",
        "    # File upload widgets\n",
        "    faiss_file = st.file_uploader(\"Upload FAISS index file (index.faiss)\", type=[\"faiss\"])\n",
        "    threads_file = st.file_uploader(\"Upload threads data file (threads.pkl)\", type=[\"pkl\"])\n",
        "\n",
        "    if faiss_file and threads_file:\n",
        "        with st.spinner(\"Loading BMW E9 forum knowledge base...\"):\n",
        "            try:\n",
        "                # Create data directory if it doesn't exist\n",
        "                os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "                # Save uploaded files\n",
        "                with open(\"data/index.faiss\", \"wb\") as f:\n",
        "                    f.write(faiss_file.getvalue())\n",
        "\n",
        "                with open(\"data/threads.pkl\", \"wb\") as f:\n",
        "                    f.write(threads_file.getvalue())\n",
        "\n",
        "                # Load data\n",
        "                index = faiss.read_index(\"data/index.faiss\")\n",
        "\n",
        "                with open(\"data/threads.pkl\", \"rb\") as f:\n",
        "                    thread_data = pickle.load(f)\n",
        "\n",
        "                # Convert to DataFrame if needed\n",
        "                if isinstance(thread_data, pd.DataFrame):\n",
        "                    df = thread_data\n",
        "                else:\n",
        "                    # Simple conversion\n",
        "                    data = []\n",
        "                    for i, item in enumerate(thread_data):\n",
        "                        if hasattr(item, 'page_content'):\n",
        "                            data.append({\"id\": i, \"content\": item.page_content})\n",
        "                        elif isinstance(item, dict):\n",
        "                            data.append(item)\n",
        "                        else:\n",
        "                            data.append({\"id\": i, \"content\": str(item)})\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                # Load sentence transformer model\n",
        "                model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "                # Save to session state\n",
        "                st.session_state.index = index\n",
        "                st.session_state.df = df\n",
        "                st.session_state.model = model\n",
        "                st.session_state.client = OpenAI(api_key=st.session_state.openai_key)\n",
        "                st.session_state.files_loaded = True\n",
        "\n",
        "                st.success(\"Knowledge base loaded successfully!\")\n",
        "                st.info(f\"Loaded {len(df)} BMW E9 forum threads\")\n",
        "                st.rerun()\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error loading files: {str(e)}\")\n",
        "                st.warning(\"Please check that the files are in the correct format\")\n",
        "    else:\n",
        "        st.warning(\"Please upload all required files to continue\")\n",
        "\n",
        "# Step 3: Query Interface\n",
        "else:\n",
        "    # Add a \"Go Back\" button\n",
        "    if st.sidebar.button(\"Go Back to File Upload\"):\n",
        "        del st.session_state[\"files_loaded\"]\n",
        "        st.rerun()\n",
        "\n",
        "    # Setup display options\n",
        "    st.sidebar.title(\"Display Options\")\n",
        "    show_sources = st.sidebar.checkbox(\"Show sources\", value=False)\n",
        "\n",
        "    # Add model settings (only temperature and k)\n",
        "    st.sidebar.title(\"Model Settings\")\n",
        "    temperature = st.sidebar.slider(\"Temperature\", min_value=0.0, max_value=1.0, value=0.2, step=0.1)\n",
        "    k = st.sidebar.slider(\"Number of forum threads to retrieve\", min_value=1, max_value=10, value=3)\n",
        "\n",
        "    # Main query interface\n",
        "    query = st.text_input(\"Ask a question about BMW E9\")\n",
        "\n",
        "    if query:\n",
        "        with st.spinner(\"Processing your question...\"):\n",
        "            # Get RAG-enhanced response\n",
        "            # Encode query\n",
        "            query_embedding = st.session_state.model.encode([query])\n",
        "\n",
        "            # Search index (using k from slider)\n",
        "            distances, indices = st.session_state.index.search(query_embedding, k)\n",
        "\n",
        "            # Get forum context\n",
        "            context = \"\"\n",
        "            for i, idx in enumerate(indices[0]):\n",
        "                if idx < len(st.session_state.df):\n",
        "                    row = st.session_state.df.iloc[idx]\n",
        "                    # Try different column names\n",
        "                    content = \"\"\n",
        "                    for col in [\"full_text\", \"content\", \"text\", \"page_content\"]:\n",
        "                        if col in row:\n",
        "                            content = str(row[col])\n",
        "                            break\n",
        "\n",
        "                    if not content:\n",
        "                        # Just use the whole row as a string\n",
        "                        content = str(row.to_dict())\n",
        "\n",
        "                    # Limit length\n",
        "                    if len(content) > 1500:\n",
        "                        content = content[:1500] + \"...\"\n",
        "\n",
        "                    context += f\"\\\\nFORUM THREAD {i+1}:\\\\n{content}\\\\n\\\\n\"\n",
        "\n",
        "            # Get RAG-enhanced answer (using temperature from slider)\n",
        "            rag_prompt = f\"\"\"As a BMW E9 expert, answer this question using ONLY the information provided from the E9 forum:\n",
        "\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            rag_response = st.session_state.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            rag_answer = rag_response.choices[0].message.content\n",
        "\n",
        "            # Get standard GPT answer (using temperature from slider)\n",
        "            standard_prompt = f\"\"\"As a BMW E9 expert, answer this question based on your general knowledge:\n",
        "\n",
        "QUESTION: {query}\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            standard_response = st.session_state.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": standard_prompt}],\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            standard_answer = standard_response.choices[0].message.content\n",
        "\n",
        "            # Display answers in columns\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                st.header(\"RAG-Enhanced Response\")\n",
        "                st.markdown(\"*Using BMW E9 forum knowledge*\")\n",
        "                st.write(rag_answer)\n",
        "\n",
        "                # Show sources if requested\n",
        "                if show_sources:\n",
        "                    st.subheader(\"Forum Sources\")\n",
        "                    for i, idx in enumerate(indices[0]):\n",
        "                        if idx < len(st.session_state.df):\n",
        "                            st.markdown(f\"**Thread {i+1}:**\")\n",
        "                            st.text_area(f\"Content {i+1}\",\n",
        "                                        str(st.session_state.df.iloc[idx].get(\"full_text\",\n",
        "                                                                          st.session_state.df.iloc[idx].get(\"content\",\n",
        "                                                                                                         \"No content\")))[:1000],\n",
        "                                        height=150)\n",
        "\n",
        "            with col2:\n",
        "                st.header(\"Standard GPT Response\")\n",
        "                st.markdown(\"*Using GPT's general knowledge*\")\n",
        "                st.write(standard_answer)''')\n",
        "\n",
        "# Create the requirements.txt file\n",
        "with open(requirements_path, 'w') as f:\n",
        "    f.write('''streamlit\n",
        "faiss-cpu\n",
        "numpy\n",
        "sentence-transformers\n",
        "openai\n",
        "pandas\n",
        "requests''')\n",
        "\n",
        "\n",
        "print(f\"LLM_RAG_App.py saved to {app_path}\")\n",
        "print(f\"LLM_RAG_reqs.txt saved to {requirements_path}\")\n",
        "\n",
        "# After writing the files\n",
        "if os.path.exists(app_path):\n",
        "    print(f\"LLM_RAG_App.py was created successfully at {app_path}\")\n",
        "    print(f\"File size: {os.path.getsize(app_path)} bytes\")\n",
        "    print(f\"Last modified: {os.path.getmtime(app_path)}\")\n",
        "else:\n",
        "    print(f\"Failed to create {app_path}\")\n",
        "\n",
        "if os.path.exists(requirements_path):\n",
        "    print(f\"LLM_RAG_reqs.txt was created successfully at {requirements_path}\")\n",
        "else:\n",
        "    print(f\"Failed to create {requirements_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WLp12lBt7Xs",
        "outputId": "265b6577-b2fe-46c0-f1f8-87fb8e20692c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM_RAG_App.py saved to /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/App/LLM_RAG_App.py\n",
            "LLM_RAG_reqs.txt saved to /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/App/LLM_RAG_reqs.txt\n",
            "LLM_RAG_App.py was created successfully at /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/App/LLM_RAG_App.py\n",
            "File size: 7992 bytes\n",
            "Last modified: 1747191451.0\n",
            "LLM_RAG_reqs.txt was created successfully at /content/drive/Othercomputers/My Mac/CSCI_104/Week_Project/App/LLM_RAG_reqs.txt\n"
          ]
        }
      ]
    }
  ]
}