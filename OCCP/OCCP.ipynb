{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"markdown","metadata":{"id":"f4X4QT-W860Y"},"source":["Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1735959267845,"user":{"displayName":"David E.","userId":"12500566977266345478"},"user_tz":480},"id":"mhwSzFEWit8p","outputId":"b4ca3690-cb23-4178-e6a3-e8cf981be33a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import os\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import matplotlib.pyplot as p\n","\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fz_Nyx0M2KOU","outputId":"c9f44227-182b-4873-89c0-286528d9de37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.2.3)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: pyxlsb in /usr/local/lib/python3.10/dist-packages (1.0.10)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function with your parameters\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"],"metadata":{"id":"wvDdY57l4fxE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z403GIGNPXs"},"outputs":[],"source":["# Load Log files and concatinate\n","\n","def load_file(file_path):\n","    \"\"\"Load a CSV file into a Pandas DataFrame.\"\"\"\n","    try:\n","        df = pd.read_csv(file_path)\n","        print(f\"Loaded {file_path}: {df.shape}\")\n","        return df\n","    except Exception as e:\n","        print(f\"Error loading file {file_path}: {e}\")\n","        return pd.DataFrame()  # Return an empty DataFrame on failure\n","\n","def save_to_csv(df, output_path):\n","    \"\"\"Save DataFrame to a CSV file.\"\"\"\n","    try:\n","        df.to_csv(output_path, index=False)\n","        print(f\"Saved concatenated file to {output_path}\")\n","    except Exception as e:\n","        print(f\"Error saving file {output_path}: {e}\")\n","\n","# Main Function: Concatenate Files\n","def concatenate_files(file_paths, output_file):\n","    \"\"\"\n","    Load multiple files, concatenate them, and save as a single file.\n","    \"\"\"\n","    combined_df = pd.DataFrame()\n","\n","    for file_path in file_paths:\n","        print(f\"Loading file: {file_path}\")\n","        df = load_file(file_path)\n","\n","        if not df.empty:\n","            combined_df = pd.concat([combined_df, df], ignore_index=True)\n","        else:\n","            print(f\"Skipping empty file: {file_path}\")\n","\n","    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n","    save_to_csv(combined_df, output_file)\n","\n","# File Paths and Configuration\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv'\n","]\n","\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_raw.csv'\n","\n","# Run Concatenation\n","concatenate_files(file_paths, output_file)\n","\n","print(\"All files concatenated and saved successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rY_sk7bX59f"},"outputs":[],"source":["# Expand the JSON field in the logs\n","# Only pull in what I need to address memory issues\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","def load_csv(file_path):\n","    \"\"\"\n","    Load a CSV file into a DataFrame.\n","    \"\"\"\n","    try:\n","        df = pd.read_csv(file_path)\n","        logging.info(f\"Loaded file: {file_path}, Shape: {df.shape}\")\n","        return df\n","    except Exception as e:\n","        logging.error(f\"Error loading file {file_path}: {e}\")\n","        return pd.DataFrame()\n","\n","def flatten_row(row, json_column, fields_to_keep):\n","    \"\"\"\n","    Flatten a single row containing a JSON column into multiple rows.\n","    Only keep specified fields during the flattening process.\n","    \"\"\"\n","    try:\n","        if pd.isna(row[json_column]):\n","            return []\n","\n","        # Extract base data\n","        base_data = row.drop(json_column).to_dict()\n","\n","        # Parse the JSON column\n","        json_data = json.loads(row[json_column])\n","\n","        flattened_rows = []\n","        for meter_value in json_data.get(\"meterValue\", []):\n","            # Extract timestamp\n","            timestamp = meter_value.get(\"timestamp\", None)\n","\n","            for i, sampled_value in enumerate(meter_value.get(\"sampledValue\", [])):\n","                # Build a filtered flat_row with only fields_to_keep\n","                flat_row = {\n","                    \"property_id\": base_data.get(\"property_id\"),\n","                    \"user_id\": base_data.get(\"user_id\"),\n","                    \"timestamp\": timestamp,\n","                    f\"value_{i}\": sampled_value.get(\"value\", None),\n","                    f\"unit_{i}\": sampled_value.get(\"unit\", None),\n","                }\n","                # Keep only the fields specified in fields_to_keep\n","                flat_row_filtered = {key: flat_row[key] for key in fields_to_keep if key in flat_row}\n","                flattened_rows.append(flat_row_filtered)\n","        return flattened_rows\n","    except (json.JSONDecodeError, KeyError, TypeError) as e:\n","        logging.warning(f\"Error processing row: {e}\")\n","        return []\n","\n","def flatten_column(df, json_column, fields_to_keep):\n","    \"\"\"\n","    Flatten a JSON column in a DataFrame into multiple rows.\n","    Only keep specified fields during the flattening process.\n","    \"\"\"\n","    flattened_rows = []\n","    for _, row in df.iterrows():\n","        flattened_rows.extend(flatten_row(row, json_column, fields_to_keep))\n","    return pd.DataFrame(flattened_rows)\n","\n","def save_to_csv(df, output_file):\n","    \"\"\"\n","    Save a DataFrame to a CSV file.\n","    \"\"\"\n","    try:\n","        os.makedirs(os.path.dirname(output_file), exist_ok=True)  # Ensure the output directory exists\n","        df.to_csv(output_file, index=False)\n","        logging.info(f\"Saved file to: {output_file}\")\n","    except Exception as e:\n","        logging.error(f\"Error saving file {output_file}: {e}\")\n","\n","def process_and_flatten(file_path, json_column, output_file, fields_to_keep):\n","    \"\"\"\n","    Load a file, flatten its JSON column, filter required fields, and save the output.\n","    \"\"\"\n","    df = load_csv(file_path)\n","    if df.empty:\n","        logging.warning(f\"File {file_path} is empty. Skipping.\")\n","        return\n","\n","    if json_column not in df.columns:\n","        logging.error(f\"Column {json_column} does not exist in the DataFrame.\")\n","        return\n","\n","    flattened_df = flatten_column(df, json_column, fields_to_keep)\n","    if not flattened_df.empty:\n","        save_to_csv(flattened_df, output_file)\n","    else:\n","        logging.warning(f\"No data left after flattening {json_column}. Skipping save.\")\n","\n","# Example Usage\n","if __name__ == \"__main__\":\n","    file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_raw.csv'\n","    json_column = 'cleaned_message'\n","    output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_raw_expanded.csv'\n","\n","    # Define fields to keep\n","    fields_to_keep = ['property_id', 'user_id', 'timestamp', 'value_0', 'unit_0', 'value_1', 'unit_1']\n","\n","    process_and_flatten(file_path, json_column, output_file, fields_to_keep)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjQPMr4UVuBD"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='inner',\n","    left_on='property_type',\n","    right_on='id',\n","    suffixes=('_property', '_type')\n",")\n","\n","# Select the desired columns using the correct suffixes and rename 'name_type'\n","property_lookup = property_lookup[['id_property', 'name_type']].rename(columns={'name_type': 'property_type'})\n","\n","\n","# Rename the 'id_property' column\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPozkvOHH9tp"},"outputs":[],"source":["# Add property_type to log data\n","\n","import pandas as pd\n","import os\n","\n","df_prop = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup.csv')\n","df_logs = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_raw_expanded.csv')\n","\n","\n","# Perform the join\n","df_lookup = df_logs.merge(\n","    df_prop,\n","    how='left',            # Perform an inner join (only matching rows)\n","    on='property_id'        # Join on the common column 'property_id'\n",")\n","\n","# Save the result to a CSV file (optional)\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties.csv'\n","df_lookup.to_csv(output_file, index=False)\n","\n","print(f\"Joined DataFrame saved to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8i-NbTv47z8"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHAuNx_tKCa1"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# UDF to clean the 'user_id' column\n","def clean_user_id(df, column_name=\"user_id\"):\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'timestamp' column\n","def clean_timestamp(df, column_name=\"timestamp\"):\n","    df.loc[:, column_name] = pd.to_datetime(df[column_name], errors='coerce')  # Convert to datetime\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'property_id' column\n","def clean_property_id(df, column_name=\"property_id\"):\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'value_0' column\n","def clean_value_0(df, column_name=\"value_0\"):\n","    df[column_name] = df[column_name].fillna(0).astype(int)\n","    return df\n","\n","# UDF to clean the 'value_1' column\n","def clean_value_1(df, column_name=\"value_1\"):\n","    df[column_name] = df[column_name].fillna(0).astype(int)\n","    return df\n","\n","# UDF to remove rows where both unit_0 and unit_1 are null\n","def remove_null_units(df):\n","    return df.dropna(subset=['unit_0', 'unit_1'], how='all')\n","\n","# UDF to rename 'value_0' to 'Watthrs' and 'value_1' to 'Amps'\n","def rename_values(df):\n","    df = df.rename(columns={'value_0': 'Watthrs', 'value_1': 'Amps'})\n","    return df\n","\n","# UDF to drop 'unit_0' and 'unit_1' columns\n","def drop_units(df):\n","    df = df.drop(columns=['unit_0', 'unit_1'])\n","    return df\n","\n","# UDF to set data types\n","def set_data_types(df):\n","    dtype_mapping = {\n","        \"property_id\": \"object\",\n","        \"user_id\": \"object\",\n","        \"timestamp\": \"datetime64[ns]\",\n","        \"Watthrs\": \"int64\",  # updated to reflect the renamed column\n","        \"Amps\": \"int64\",     # updated to reflect the renamed column\n","    }\n","\n","    for column, dtype in dtype_mapping.items():\n","        try:\n","            if dtype == \"datetime64[ns]\":\n","                df.loc[:, column] = pd.to_datetime(df[column], errors='coerce')\n","                invalid_rows = df[column].isna().sum()\n","                if invalid_rows > 0:\n","                    print(f\"Warning: {invalid_rows} invalid timestamps found in '{column}' and coerced to NaT.\")\n","            else:\n","                df.loc[:, column] = df[column].astype(dtype)\n","        except KeyError:\n","            print(f\"Column '{column}' not found in DataFrame.\")\n","        except Exception as e:\n","            print(f\"Error converting column '{column}' to type '{dtype}': {e}\")\n","    return df\n","\n","# Function to compact the data and combine Amps and Watthrs in the same row\n","def compact_data(df):\n","    # Group by the relevant columns and aggregate to ensure Amps and Watthrs are in the same row\n","    df_compacted = df.groupby(['property_id', 'user_id', 'timestamp', 'property_type'], as_index=False).agg(\n","        Watthrs=('Watthrs', 'max'),  # Take the maximum (non-zero) value for Watthrs\n","        Amps=('Amps', 'max')         # Take the maximum (non-zero) value for Amps\n","    )\n","\n","    print(f\"After compacting data, data shape: {df_compacted.shape}\")\n","    return df_compacted\n","\n","# Generalized function to clean a DataFrame\n","def clean_data_with_udfs_and_dtypes(df, cleaning_functions):\n","    for func, col in cleaning_functions:\n","        print(f\"Applying cleaning rule: {func.__name__} on column: {col}\")\n","        if col:\n","            df = func(df, column_name=col)\n","        else:\n","            df = func(df)  # Call function without column_name argument\n","    print(\"Setting data types...\")\n","    df = set_data_types(df)\n","    return df\n","\n","# File paths\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties.csv'\n","]\n","\n","# Define cleaning rules\n","cleaning_rules = [\n","    (clean_user_id, \"user_id\"),\n","    (clean_timestamp, \"timestamp\"),\n","    (clean_property_id, \"property_id\"),\n","    (clean_value_0, \"value_0\"),\n","    (clean_value_1, \"value_1\"),\n","    (remove_null_units, None),\n","    (rename_values, None),  # Renaming values columns\n","    (drop_units, None),     # Dropping unit columns\n","]\n","\n","# Process each file\n","for file_path in file_paths:\n","    print(f\"Processing file: {file_path}\")\n","    try:\n","        df = pd.read_csv(file_path)\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","        continue  # Skip to the next file if this one isn't found\n","\n","    df_cleaned = clean_data_with_udfs_and_dtypes(df.copy(), cleaning_rules)\n","\n","    # Compact the data to ensure Watthrs and Amps are in the same row\n","    df_compacted = compact_data(df_cleaned)\n","\n","    print(\"\\nFinal DataFrame Info:\")\n","    print(df_compacted.info())\n","\n","    # Save the compacted DataFrame\n","    output_path = file_path.replace(\".csv\", \"_cleaned_compacted.csv\")\n","    df_compacted.to_csv(output_path, index=False)\n","\n","    print(f\"Compacted file saved to: {output_path}\")\n","    print(f\"Rows before cleaning: {len(df)}, Rows after cleaning: {len(df_compacted)}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk6sjC-mPN3W"},"outputs":[],"source":["# Data check\n","\n","import pandas as pd\n","import numpy as np\n","\n","# Function to count NaN and infinite values in the DataFrame\n","def count_nan_inf(df):\n","    # Count NaN values\n","    nan_count = df.isna().sum().sum()\n","\n","    # Count infinite values (positive and negative infinity)\n","    inf_count = (df == np.inf).sum().sum() + (df == -np.inf).sum().sum()\n","\n","    print(f\"NaN values: {nan_count}\")\n","    print(f\"Inf values: {inf_count}\")\n","\n","    # Optionally: Display count of NaN and Inf values per column\n","    print(\"\\nNaN and Inf values per column:\")\n","    print(df.isna().sum())  # Count of NaN per column\n","    print(\"\\nInfinite values per column:\")\n","    print((df == np.inf).sum() + (df == -np.inf).sum())  # Count of Inf per column\n","\n","    return nan_count, inf_count\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv'\n","df = pd.read_csv(file_path)\n","\n","# Run the check for NaN and Inf values\n","count_nan_inf(df)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R075jkxkSD2"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHW_OWXZbl-3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Utility Functions\n","def add_day_info(df, timestamp_col='timestamp'):\n","    df['day_of_week'] = df[timestamp_col].dt.dayofweek + 1  # Convert 0-6 (Monday-Sunday) to 1-7 (Sunday-Saturday)\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)  # Weekend: Saturday (6) and Sunday (7)\n","    return df\n","\n","def calculate_days_to_nearest_holiday(df, date_col, holiday_dates):\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    if df[date_col].dt.tz is not None:\n","        holiday_dates = [\n","            holiday if holiday.tz is not None else holiday.tz_localize('UTC')\n","            for holiday in holiday_dates\n","        ]\n","    else:\n","        holiday_dates = [\n","            holiday.tz_convert(None) if holiday.tz is not None else holiday\n","            for holiday in holiday_dates\n","        ]\n","\n","    df['days_to_nearest_holiday'] = df[date_col].apply(\n","        lambda x: min(abs((x - holiday).days) for holiday in holiday_dates)\n","    )\n","    return df\n","\n","def add_datetime_components(df, timestamp_col='timestamp'):\n","    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')  # Coerce invalid datetime to NaT\n","\n","    # Drop rows where the timestamp is NaT after coercion\n","    df = df.dropna(subset=[timestamp_col])\n","\n","    # Extract the datetime components\n","    df['year'] = df[timestamp_col].dt.year\n","    df['month'] = df[timestamp_col].dt.month\n","    df['day'] = df[timestamp_col].dt.day\n","    df['hour'] = df[timestamp_col].dt.hour\n","\n","    return df\n","\n","def encode_month_column(df, month_col='month'):\n","    month_mapping = {'September': 9, 'October': 10, 'November': 11}\n","    df['month_encoded'] = df[month_col].map(month_mapping).fillna(df[month_col]).astype(int)\n","    return df\n","\n","def add_unique_user_counts(df, group_cols, user_col='user_id'):\n","    unique_user_counts = (\n","        df.groupby(group_cols)[user_col]\n","        .nunique()\n","        .reset_index()\n","        .rename(columns={user_col: 'unique_user_count'})\n","    )\n","    df = df.merge(unique_user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols, value_cols):\n","    sums = df.groupby(group_cols)[value_cols].sum().reset_index()\n","    sums.rename(\n","        columns={\n","            value_cols[0]: 'hour_sum_value_Wh',\n","            value_cols[1]: 'hour_sum_value_A'\n","        },\n","        inplace=True\n","    )\n","    df = df.merge(sums, on=group_cols, how='left')\n","    return df\n","\n","# New function to encode 'property_type' column using label encoding\n","def encode_property_type(df):\n","    label_encoder = LabelEncoder()\n","    df['property_type'] = label_encoder.fit_transform(df['property_type'])\n","    return df\n","\n","def engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates=None):\n","    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce', utc=True)\n","    df = add_day_info(df, timestamp_col)\n","    if holiday_dates:\n","        df = calculate_days_to_nearest_holiday(df, timestamp_col, holiday_dates)\n","    df = add_datetime_components(df, timestamp_col)\n","    df = add_unique_user_counts(df, group_cols, user_col)\n","    df = add_usage_sums(df, group_cols, value_cols)\n","\n","    # Encode 'property_type' using label encoding\n","    df = encode_property_type(df)\n","\n","    return df\n","\n","def process_files(file_paths, output_dir, group_cols, value_cols, timestamp_col='timestamp', user_col='user_id', holiday_dates=None):\n","    processed_dfs = []\n","    for file_path in file_paths:\n","        file_name = os.path.splitext(os.path.basename(file_path))[0]\n","        output_file = os.path.join(output_dir, f\"{file_name}_eng_features.csv\")\n","        print(f\"Processing file: {file_path}\")\n","        df = pd.read_csv(file_path)\n","\n","        # Engineer data and handle NaT in timestamp column by dropping those rows\n","        processed_df = engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates)\n","\n","        # Optionally, drop any rows with NaT in the timestamp column before saving\n","        processed_df = processed_df.dropna(subset=[timestamp_col])\n","\n","        processed_df.to_csv(output_file, index=False)\n","        print(f\"Processed file saved to: {output_file}\")\n","        processed_dfs.append(processed_df)\n","\n","    return processed_dfs\n","\n","# Usage\n","file_paths = ['/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv']\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/'\n","group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","value_cols = ['Watthrs', 'Amps']  # Updated to reflect the compacted columns\n","holiday_dates = [\n","    pd.Timestamp(\"2024-09-04\", tz='UTC'),\n","    pd.Timestamp(\"2024-10-09\", tz='UTC'),\n","    pd.Timestamp(\"2024-11-23\", tz='UTC')\n","]\n","\n","# Process files\n","processed_dfs = process_files(\n","    file_paths=file_paths,\n","    output_dir=output_dir,\n","    group_cols=group_cols,\n","    value_cols=value_cols,\n","    timestamp_col='timestamp',\n","    user_col='user_id',\n","    holiday_dates=holiday_dates\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuS1cyCcPuAo"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","\n","# Function to count NaN and infinite values in the DataFrame\n","def count_nan_inf(df):\n","    # Count NaN values\n","    nan_count = df.isna().sum().sum()\n","\n","    # Count infinite values (positive and negative infinity)\n","    inf_count = (df == np.inf).sum().sum() + (df == -np.inf).sum().sum()\n","\n","    print(f\"NaN values: {nan_count}\")\n","    print(f\"Inf values: {inf_count}\")\n","\n","    # Optionally: Display count of NaN and Inf values per column\n","    print(\"\\nNaN and Inf values per column:\")\n","    print(df.isna().sum())  # Count of NaN per column\n","    print(\"\\nInfinite values per column:\")\n","    print((df == np.inf).sum() + (df == -np.inf).sum())  # Count of Inf per column\n","\n","    return nan_count, inf_count\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"hour_sum_value_A\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy (e.g., median, mean)\n","X.replace([np.inf, -np.inf], 0, inplace=True)  # Replace inf values with 0\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bgs5LYhzdls4"},"outputs":[],"source":["# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for regression/ANOVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import statsmodels.api as sm\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n","# Define the fields to drop\n","fields_to_drop = ['property_id', 'user_id', 'timestamp', 'hour_sum_value_A','Watthrs','Amps']\n","\n","# Drop non-numeric columns from the dataframe\n","X = df.drop(columns=fields_to_drop)  # All columns except the dependent variable\n","y = df['hour_sum_value_Wh']  # Dependent variable\n","\n","# Check types of X and y\n","print(\"X dtypes:\\n\", X.dtypes)\n","print(\"y dtype:\", y.dtypes)\n","\n","# Add a constant to the model (intercept)\n","X = sm.add_constant(X)\n","\n","# Fit the regression model\n","model = sm.OLS(y, X).fit()\n","\n","# Print the summary of the regression model\n","print(model.summary())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpSYEsmq3VUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umUOsBSn3VXA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-e5kpAZ3VZS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F20AGhU3Vbq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1F9VbEO3VhO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"J85SXxSCIwdY"},"source":["## Extract from Eddie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAlXiQuUIu76"},"outputs":[],"source":["file_path_a = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/2024-08-01.csv'\n","\n","df_big = pd.read_csv(file_path_a)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J89LPzgmdzuZ"},"source":["### Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMZPkMirgm9l"},"outputs":[],"source":["df_big.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOkPWChKg5k8"},"outputs":[],"source":["header = [\n","    \"qrcode\",  # Column 0\n","    \"connector\",  # Column 1\n","    \"serial_num\",  # Column 2\n","    \"org_id\",  # Column 3\n","    \"property_id\",  # Column 4\n","    \"station_id\",  # Column 5\n","    \"transaction_id\",  # Column 6\n","    \"metered_type\",  # Column 7\n","    \"timestamp\",  # Column 8\n","    \"metered_value\"   # Column 9\n","]\n","\n","df_big.columns = header"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gcxojux1h63w"},"outputs":[],"source":["df_big['Timestamp'] = pd.to_datetime(df_big['Timestamp'])\n"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"c6ZMTHHnZg9V"},"source":["### Tables I can access"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUKjVft2kxIk"},"outputs":[],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Xz1nKBmBM2nA"},"source":["###Create a table for all property info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7enpOKwkM2Gu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup_2.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names to lowercase and strip whitespace\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the left join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='left',  # Use 'left' join to keep all rows from properties and add property_type name where available\n","    left_on='property_type',  # Assuming 'property_type' is the column in properties.csv\n","    right_on='id',  # Assuming 'id' is the column in property_types.csv\n","    suffixes=('_property', '_type')\n",")\n","\n","# Keep all columns from properties and just add the 'name_type' column as 'property_type'\n","property_lookup['property_type'] = property_lookup['name_type']\n","\n","# Drop the 'name_type' column, since we already added it as 'property_type'\n","property_lookup = property_lookup.drop(columns=['name_type'])\n","\n","# Rename 'id_property' column to 'property_id'\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE633fu_M2Iz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrQU1kppM2LR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOTj1nOYM2N5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaoKGEHJw0uL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import logging\n","from itertools import combinations\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Path to the directory containing the CSV files\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n","\n","# List of tables (as per your previous code)\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Function to load CSV files into DataFrames\n","def load_dataframes(tables):\n","    dataframes = {}\n","    for table in tables:\n","        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","            dataframes[table] = df\n","            logging.info(f\"Loaded data for table: {table}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading data for table {table}: {e}\")\n","    return dataframes\n","\n","# Function to find strict join matches\n","def find_strict_joins(df1, df2, table1_name, table2_name):\n","    strict_joins = []\n","    # Iterate over all column pairs\n","    for col1 in df1.columns:\n","        for col2 in df2.columns:\n","            if df1[col1].dtype == df2[col2].dtype:\n","                # Perform the join\n","                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n","                # Check if all rows in df1 are in the joined DataFrame\n","                if len(joined_df) == len(df1):\n","                    strict_joins.append((col1, col2))\n","                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n","    return strict_joins\n","\n","# Main function to perform the strict join analysis\n","def analyze_strict_joins(tables):\n","    dataframes = load_dataframes(tables)\n","    results = {}\n","    table_pairs = combinations(tables, 2)\n","\n","    for table1, table2 in table_pairs:\n","        df1 = dataframes.get(table1)\n","        df2 = dataframes.get(table2)\n","\n","        if df1 is not None and df2 is not None:\n","            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n","            joins = find_strict_joins(df1, df2, table1, table2)\n","            if joins:\n","                results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n","\n","    return results\n","\n","# Run the strict join analysis\n","strict_join_results = analyze_strict_joins(tables)\n","\n","# Print the results\n","for table_pair, joins in strict_join_results.items():\n","    print(f\"\\nStrict joins for {table_pair}:\")\n","    for col1, col2 in joins:\n","        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n","\n","if not strict_join_results:\n","    print(\"No strict joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqQ_iKw5dn0M"},"outputs":[],"source":["import os\n","import logging\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection string for SQLAlchemy\n","connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n","engine = create_engine(connection_string)\n","\n","# Function to fetch column names for a table\n","def get_columns(table_name):\n","    try:\n","        query = f\"\"\"\n","        SELECT column_name, data_type\n","        FROM information_schema.columns\n","        WHERE table_name = '{table_name}';\n","        \"\"\"\n","        with engine.connect() as connection:\n","            df = pd.read_sql_query(query, connection)\n","        return df[['column_name', 'data_type']].to_dict('records')\n","    except Exception as e:\n","        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n","        return []\n","\n","# Function to test join logic between two tables\n","def test_joins(table1, table2, attempts=3):\n","    columns_table1 = get_columns(table1)\n","    columns_table2 = get_columns(table2)\n","    successful_joins = []\n","\n","    for col1 in columns_table1:\n","        for col2 in columns_table2:\n","            # Only test joins on matching data types\n","            if col1['data_type'] == col2['data_type']:\n","                success_count = 0\n","                for _ in range(attempts):  # Attempt the join multiple times\n","                    query = f\"\"\"\n","                    SELECT *\n","                    FROM {table1} t1\n","                    INNER JOIN {table2} t2\n","                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n","                    LIMIT 1;  -- Test with one row at a time\n","                    \"\"\"\n","                    try:\n","                        with engine.connect() as connection:\n","                            df = pd.read_sql_query(query, connection)\n","                            if not df.empty:\n","                                success_count += 1\n","                    except Exception as e:\n","                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n","\n","                if success_count == attempts:  # Only count as successful if all attempts work\n","                    successful_joins.append((col1['column_name'], col2['column_name']))\n","                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n","\n","    return successful_joins\n","\n","# Cross-check join fields for all table pairs\n","tables = [\n","    \"users\", \"ocpp_sub_session\"\n","]\n","\n","results = {}\n","\n","for i, table1 in enumerate(tables):\n","    for table2 in tables[i+1:]:\n","        logging.info(f\"Testing joins between {table1} and {table2}\")\n","        joins = test_joins(table1, table2)\n","        if joins:\n","            results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.info(f\"No join found between {table1} and {table2}\")\n","\n","# Print results\n","for table_pair, joins in results.items():\n","    print(f\"Successful joins for {table_pair}: {joins}\")\n","\n","if not results:\n","    print(\"No successful joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU41V4jQdn2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMXMJghgdn5H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuwvrH2ndn7Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VIJZs8fdn9k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kH2jY7TdoAK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ex8BAejxDoB"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Example dataframe (assuming df['message'] contains the raw strings)\n","# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n","def clean_message(msg):\n","    try:\n","        # Remove the prefix\n","        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n","\n","        # Attempt to load the cleaned message as JSON\n","        return json.loads(msg_cleaned)\n","    except (json.JSONDecodeError, TypeError):\n","        # If the message cannot be decoded as JSON, return None or handle as needed\n","        return None\n","\n","# Apply the function to the 'message' column\n","df['message'] = df['message'].apply(clean_message)\n","\n","# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n","df = df[df['message'].notna()]\n","\n","# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n","flattened_rows = []\n","\n","for idx, row in df.iterrows():\n","    message = row['message']  # Now this is a valid JSON object\n","\n","    # Extract top-level fields\n","    connector_id = message.get('connectorId')\n","    transaction_id = message.get('transactionId')\n","\n","    # Keep the 'meterValue' field as is (as a list of dicts)\n","    meter_value = message.get('meterValue', [])\n","\n","    # Add a row to the flattened list, including the nested 'meterValue' list\n","    flattened_rows.append({\n","        '_time': row['time'],  # Retain the original timestamp from the dataframe\n","        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","        'connectorId': connector_id,\n","        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n","    })\n","\n","# Step 2: Create a new DataFrame from the flattened rows\n","flattened_df = pd.DataFrame(flattened_rows)\n","\n","# Display the resulting DataFrame\n","print(flattened_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbk_sGLxZ_Z"},"outputs":[],"source":["import pandas as pd\n","\n","# Set pandas options to display the full content of any column (e.g., 'meterValue')\n","pd.set_option('display.max_colwidth', None)\n","\n","# Now, display the full content of the 'meterValue' column for the first 5 rows\n","print(flattened_df['meterValue'].head(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhRPDO5zJH0"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to hold the expanded rows\n","expanded_rows = []\n","\n","# Iterate over each row in the dataframe\n","for idx, row in flattened_df.iterrows():\n","    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n","\n","    # For each meter value entry (there should be one timestamp and a list of measurements)\n","    for meter in meter_values:\n","        timestamp = meter['timestamp']  # Extract the timestamp\n","\n","        # Initialize values for each measurement type\n","        watt_hours_value = None  # WattHours\n","        amps_value = None        # Amps (Current)\n","        voltage_value = None     # Voltage (Volts)\n","\n","        # Iterate over the sampledValue list (which contains the three measurements)\n","        for sample in meter['sampledValue']:\n","            # Check the 'unit' to assign the value to the correct column\n","            if sample['unit'] == 'Wh':  # WattHours\n","                watt_hours_value = sample['value']\n","            elif sample['unit'] == 'A':  # Amps (Current)\n","                amps_value = sample['value']\n","            elif sample['unit'] == 'V':  # Volts (Voltage)\n","                voltage_value = sample['value']\n","\n","        # Append the expanded row with the extracted values\n","        expanded_rows.append({\n","            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n","            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","            'connectorId': row['connectorId'],  # Connector ID\n","            'timestamp': timestamp,  # Timestamp from the meter value\n","            'WattHours': watt_hours_value,  # Renamed to WattHours\n","            'Amps': amps_value,  # Keep Amps as the column name\n","            'Voltage': voltage_value  # Value for Voltage (V)\n","        })\n","\n","# Create a new DataFrame from the expanded rows\n","expanded_df = pd.DataFrame(expanded_rows)\n","\n","# Convert the numeric columns to appropriate types (float)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Display the resulting DataFrame\n","print(expanded_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcgRo8Dz1Nm"},"outputs":[],"source":["# Ensure all columns are numeric (in case there are any string values left)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Classify values as 0 or > 0 for each of the measurements\n","expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n","\n","# Set up the plot\n","plt.figure(figsize=(18, 6))\n","\n","# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n","plt.subplot(1, 3, 1)\n","sns.countplot(data=expanded_df, x='WattHours_Class')\n","plt.title('Count of Rows with WattHours: 0 vs > 0')\n","plt.xlabel('WattHours Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 2)\n","sns.countplot(data=expanded_df, x='Amps_Class')\n","plt.title('Count of Rows with Amps: 0 vs > 0')\n","plt.xlabel('Amps Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 3)\n","sns.countplot(data=expanded_df, x='Voltage_Class')\n","plt.title('Count of Rows with Voltage: 0 vs > 0')\n","plt.xlabel('Voltage Class')\n","plt.ylabel('Count')\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsRrQLIz1QQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure that the '_time' column is in datetime format\n","expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n","\n","# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Drop rows where any of the values are missing\n","expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create the figure and axes for the plots\n","plt.figure(figsize=(18, 6))\n","\n","# Plot Amps over time\n","plt.subplot(1, 3, 1)\n","plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n","plt.title('Amps over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Amps')\n","plt.xticks(rotation=45)\n","\n","# Plot WattHours over time\n","plt.subplot(1, 3, 2)\n","plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n","plt.title('WattHours over Time')\n","plt.xlabel('Time')\n","plt.ylabel('WattHours')\n","plt.xticks(rotation=45)\n","\n","# Plot Voltage over time\n","plt.subplot(1, 3, 3)\n","plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n","plt.title('Voltage over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Voltage')\n","plt.xticks(rotation=45)\n","\n","# Adjust layout to avoid overlap of labels\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi2OjXK_2hXK"},"outputs":[],"source":["expanded_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TbILosnz1Sh"},"outputs":[],"source":["# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n","descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n","\n","# Display the statistics\n","print(descriptive_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zZUwehE16FW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure with 3 subplots (1 row, 3 columns)\n","plt.figure(figsize=(18, 6))\n","\n","# Plot for Amps\n","plt.subplot(1, 3, 1)\n","sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n","plt.title('Boxplot of Amps')\n","plt.ylabel('Amps')\n","\n","# Plot for WattHours\n","plt.subplot(1, 3, 2)\n","sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n","plt.title('Boxplot of WattHours')\n","plt.ylabel('WattHours')\n","\n","# Plot for Voltage\n","plt.subplot(1, 3, 3)\n","sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n","plt.title('Boxplot of Voltage')\n","plt.ylabel('Voltage')\n","\n","# Adjust layout to avoid overlap\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBnl60S2LIP"},"outputs":[],"source":["# count of propertyIDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9r14vD2LK6"},"outputs":[],"source":["#Count of cluster IDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F01IOcSb2LNZ"},"outputs":[],"source":["# counts of peropertyID and clusterIDimport os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'cluster_id' values for each 'property_id'\n","query = f\"\"\"\n","SELECT property_id, COUNT(DISTINCT cluster_id)\n","FROM {table}\n","GROUP BY property_id\n",";\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    property_id, cluster_count = row\n","    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values for each 'cluster_id'\n","query = f\"\"\"\n","SELECT cluster_id, COUNT(DISTINCT property_id)\n","FROM {table}\n","GROUP BY cluster_id;\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    cluster_id, property_count = row\n","    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n"]},{"cell_type":"markdown","metadata":{"id":"gJCn043ops5i"},"source":["OCPP_SessionID has a userID and TransactionID\n","I need to map to the cluster and property\n","\n","Stations has propertyID and cluster_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkQRHSF2pYLF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRERnF5HpYNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZNFFc8pYQY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pbxg6jVhpYS3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wI1hDsHpYVx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiA1AO1z2LSG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnNNDMY2LUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpS8FyTqEWgq"},"outputs":[],"source":["# Clean message field and port to a df\n","import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n","valid_df = df[df['parsed_message'].notnull()]\n","\n","# Flatten all the JSON objects and store them in a new DataFrame\n","flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n","new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n","\n","# Write the DataFrame to CSV with new naming convention\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n","new_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnkEtU3uDKj"},"outputs":[],"source":["new_df.info\n","new_df.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDqqE7Swt6vj"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df= new_df\n","\n","# Assuming your DataFrame is named df\n","# Step 1: Convert 'time' to datetime\n","df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n","\n","# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n","df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n","\n","# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n","df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n","\n","# Step 4: Plot the time series\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n","plt.xlabel('Time')\n","plt.ylabel('Meter Value')\n","plt.title('Meter Value Over Time')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlT8kevDasAZ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['message'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcwXooB_tXnf"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming new_df is already defined and contains the necessary columns\n","\n","# List of columns to keep\n","columns_to_keep = [\n","    #'time',\n","    'user_id',\n","    #'station_id',\n","    'property_id',\n","    'connectorId',\n","    'meterValue_0_timestamp',\n","    'meterValue_0_sampledValue_1_value',\n","    'meterValue_0_sampledValue_1_context',\n","    'meterValue_0_sampledValue_1_format',\n","    'meterValue_0_sampledValue_1_measurand',\n","    'meterValue_0_sampledValue_1_phase',\n","    'meterValue_0_sampledValue_1_location',\n","    'meterValue_0_sampledValue_1_unit'\n","]\n","\n","# Create new_df_2 with only the selected columns\n","new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","\n","# Convert 'time' to datetime\n","new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n","\n","# Check for any NaT values that may have resulted from the conversion\n","if new_df_2['meterValue_0_timestamp'].isnull().any():\n","    print(\"Some values could not be converted to datetime.\")\n","\n","# Extract day and hour using .loc to avoid warnings\n","new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n","new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLthqnU8u2eM"},"outputs":[],"source":["new_df_2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMxEqA8Qbkxz"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = new_df_2\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N29p7Kq_xWuu"},"outputs":[],"source":["# Assuming new_df_2 is the df\n","\n","unique_values = new_df_2['user_id'].unique()\n","\n","# To display the unique values\n","print(unique_values)\n","\n","\n","# Assuming new_df_2 is your DataFrame\n","unique_count = new_df_2['user_id'].nunique()\n","\n","# To display the count of unique user_id values\n","print(f\"Number of unique user_id values: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEjr917eu9Rb"},"outputs":[],"source":["import pandas as pd\n","\n","\n","new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n","\n","max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n","\n","result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n","\n","print(result_df)\n","result_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnNlof0taJFO"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of meterValue_0_timestamp')\n","plt.title('Count of meterValue_0_timestamp per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"842I60p_-VjA"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming result_df is your DataFrame\n","\n","# Print column names to verify\n","print(\"Column names in DataFrame:\")\n","print(result_df.columns.tolist())\n","\n","# Specify the user_id you're interested in\n","specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n","\n","# Filter the DataFrame for the specific user_id\n","filtered_df = result_df[result_df['user_id'] == specific_user_id]\n","\n","# Check if there are any rows for the specified user_id\n","if not filtered_df.empty:\n","    # Check for the timestamp column again\n","    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n","    value_col = 'meterValue_0_sampledValue_1_value'\n","\n","    # Ensure the column names are correct\n","    print(\"Filtered DataFrame columns:\")\n","    print(filtered_df.columns.tolist())\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n","    plt.title(f'Meter Values for User ID: {specific_user_id}')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Meter Value')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n","    plt.grid()\n","    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","    plt.show()\n","else:\n","    print(f\"No data found for user_id: {specific_user_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp5ImngUusix"},"outputs":[],"source":["\n","# Write the DataFrame to CSV\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n","result_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ZZ-b6HuslW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3cWsdlTusnX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TvKiocBsdvU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A0Tx9v-sdxw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKXZgFB5sd0F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBdB1TPsd2a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDJEMK-rsd4_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZmZ5lhTAhpZ"},"outputs":[],"source":["# Identify source of error in message JSON\n","import json\n","\n","# Function to recursively extract all keys\n","def get_all_keys(data, keys=set()):\n","    if isinstance(data, dict):\n","        for key, value in data.items():\n","            keys.add(key)\n","            get_all_keys(value, keys)\n","    elif isinstance(data, list):\n","        for item in data:\n","            get_all_keys(item, keys)\n","    return keys\n","\n","# Initialize set to collect unique keys\n","all_keys = set()\n","\n","# Variable to track the first invalid JSON example\n","invalid_json_index = None\n","invalid_json_sample = None\n","\n","# Iterate through df['message'] and try to process each row\n","for index, message in enumerate(df['message']):\n","    try:\n","        # Parse JSON string\n","        message_json = json.loads(message)\n","        # Extract keys from each valid message\n","        all_keys = get_all_keys(message_json, all_keys)\n","    except json.JSONDecodeError:\n","        # Capture the index and value of the first invalid JSON row\n","        invalid_json_index = index\n","        invalid_json_sample = df.iloc[index]\n","        break  # Stop after finding the first invalid JSON entry\n","\n","# Print all extracted keys\n","print(\"Extracted keys:\", all_keys)\n","\n","# Display the first invalid JSON example\n","if invalid_json_sample is not None:\n","    print(f\"First invalid JSON entry at row {invalid_json_index}:\")\n","    print(invalid_json_sample)\n","else:\n","    print(\"No invalid JSON entries found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"em9mU8OyCGDy"},"outputs":[],"source":["# Print the first 5 rows of the 'message' field to inspect for any issues\n","for i in range(5):\n","    print(f\"Row {i} message: {df['message'].iloc[i]}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnT-POY9rK8t"},"outputs":[],"source":["df.info()\n","meterValue_0_sampledValue_1_unit = A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-MhWFxWDpT0"},"outputs":[],"source":["import json\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]  # This removes everything before the first '{'\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to recursively extract all keys from JSON objects\n","def get_all_keys(data, keys=set()):\n","    if isinstance(data, dict):\n","        for key, value in data.items():\n","            keys.add(key)\n","            get_all_keys(value, keys)\n","    elif isinstance(data, list):\n","        for item in data:\n","            get_all_keys(item, keys)\n","    return keys\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Initialize a set to store unique keys from the parsed JSON objects\n","json_keys = set()\n","\n","# Extract keys from the 'parsed_message' field for all valid rows\n","for parsed_message in df['parsed_message'].dropna():  # Exclude rows with None (failed parsing)\n","    json_keys = get_all_keys(parsed_message, json_keys)\n","\n","# Get the original DataFrame headers (columns)\n","df_headers = set(df.columns)\n","\n","# Combine both sets: original DataFrame headers and extracted JSON keys\n","complete_headers = df_headers.union(json_keys)\n","\n","# Print the complete view of all data fields\n","print(\"Complete view of all data fields (headers + JSON keys):\")\n","print(complete_headers)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3T1rTXcE_tC"},"outputs":[],"source":["import json\n","\n","# Function to clean up the 'message' field by removing the prefix\n","def clean_message(message):\n","    # Strip the non-JSON prefix before the first '{'\n","    cleaned_message = message[message.find('{'):] if '{' in message else message\n","    return cleaned_message\n","\n","# Function to validate JSON and return whether it's valid\n","def validate_json(message):\n","    try:\n","        # Attempt to load the cleaned message as JSON\n","        json.loads(message)\n","        return True, None  # Valid JSON\n","    except json.JSONDecodeError as e:\n","        return False, str(e)  # Invalid JSON with error message\n","\n","# Apply the cleaning function to all rows in the 'message' field\n","df['cleaned_message'] = df['message'].apply(clean_message)\n","\n","# Apply the validation function to check JSON validity and capture errors\n","df['is_valid_json'], df['json_validation'] = zip(*df['cleaned_message'].apply(validate_json))\n","\n","# Display rows with invalid JSON\n","invalid_rows = df[df['is_valid_json'] == False]\n","\n","# Show a few invalid rows along with the error message for investigation\n","print(invalid_rows[['message', 'cleaned_message', 'json_validation']].head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Tjf4XMNFVc-"},"outputs":[],"source":["import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix\n","def clean_message(message):\n","    # Strip the non-JSON prefix before the first '{'\n","    cleaned_message = message[message.find('{'):] if '{' in message else message\n","    return cleaned_message\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Function to clean, parse and flatten JSON data\n","def clean_and_flatten_message(message):\n","    # Clean the message by removing non-JSON prefix\n","    cleaned_message = clean_message(message)\n","    # Parse the cleaned message into JSON\n","    try:\n","        parsed_message = json.loads(cleaned_message)\n","        # Flatten the parsed JSON\n","        return flatten_json(parsed_message)\n","    except json.JSONDecodeError:\n","        return None\n","\n","# Apply the clean, parse, and flatten function to all rows in the 'message' field\n","flattened_data = df['message'].apply(clean_and_flatten_message).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' field)\n","flattened_df = pd.concat([df.drop(columns=['message']), flattened_data], axis=1)\n","\n","# Display the first few rows of the flattened DataFrame\n","#print(flattened_df.head())\n","\n","\n","flattened_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_FMGlpjPcsJ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","#meterValue_0_sampledValue_0_unit = Wh\n","#meterValue_0_sampledValue_1_unit = A\n","#meterValue_0_sampledValue_2_unit = V\n","#meterValue_0_sampledValue_3_unit = Celsius\n","\n","\n","# Ensure 'time' column is in datetime format\n","flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n","\n","# Define the specific day to filter (replace with the desired date)\n","specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n","\n","# Filter the DataFrame for the specific day\n","filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n","\n","# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n","filtered_df['meterValue_0_sampledValue_1_value'] = filtered_df['meterValue_0_sampledValue_1_value'].astype(float)\n","\n","# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n","plt.figure(figsize=(10, 6))\n","sns.kdeplot(filtered_df['meterValue_0_sampledValue_1_value'], fill=True)\n","\n","# Formatting the plot\n","plt.xlabel('A') #Looking at amperage\n","plt.ylabel('Percentage (%)')\n","plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n","\n","# Convert y-axis to percentage\n","plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n","\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fX59jUMcQ2PX"},"outputs":[],"source":["df = flattened_df\n","df['calendar_day'] = df['time'].dt.date"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lZBmiLWRses"},"outputs":[],"source":["# Find the index of the maximum value for meterValue_0_sampledValue_1_value within each calendar_day\n","max_indices = df.groupby('calendar_day')['meterValue_0_sampledValue_1_value'].idxmax()\n","\n","# Select the hour from the time corresponding to those indices\n","df['max_hour'] = df['time'].dt.hour  # Extract the hour part\n","max_hour_per_day = df.loc[max_indices, ['calendar_day', 'max_hour']].reset_index(drop=True)\n","\n","print(max_hour_per_day)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtOwai_uPgrb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure 'time' column is in datetime format\n","flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n","\n","# Define the specific day to filter (replace with the desired date)\n","specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n","\n","# Filter the DataFrame for the specific day\n","filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n","\n","# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n","filtered_df['meterValue_0_sampledValue_1_value'] = filtered_df['meterValue_0_sampledValue_1_value'].astype(float)\n","\n","# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n","plt.figure(figsize=(10, 6))\n","sns.kdeplot(filtered_df['meterValue_0_sampledValue_1_value'], fill=True)\n","\n","# Formatting the plot\n","plt.xlabel('Amps')\n","plt.ylabel('Percentage (%)')\n","plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n","\n","# Convert y-axis to percentage\n","plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n","\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-GHkHCDO6TR"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure 'time' column is in datetime format\n","flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n","\n","# Define the specific day to filter (replace with the desired date)\n","specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n","\n","# Filter the DataFrame for the specific day\n","filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n","\n","# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n","filtered_df['meterValue_0_sampledValue_2_value'] = filtered_df['meterValue_0_sampledValue_2_value'].astype(float)\n","\n","# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n","plt.figure(figsize=(10, 6))\n","sns.kdeplot(filtered_df['meterValue_0_sampledValue_2_value'], fill=True)\n","\n","# Formatting the plot\n","plt.xlabel('Volts')\n","plt.ylabel('Percentage (%)')\n","plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n","\n","# Convert y-axis to percentage\n","plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n","\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyPWrWWXtjVf0NyYbLVAOIlG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}