{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"XznKesLDcf0o"},"source":["## Charging System Diagram\n","Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mhwSzFEWit8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737412324562,"user_tz":480,"elapsed":12865,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"94f89bcd-af4a-4ffb-ad28-b5c52b605b02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz_Nyx0M2KOU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"193fecd0-19db-435f-f046-6d3d983396f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyGithub in /usr/local/lib/python3.11/dist-packages (2.5.0)\n","Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.5.0)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.15)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: pyxlsb in /usr/local/lib/python3.11/dist-packages (1.0.10)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["# Packages and methods\n","\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import sys\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wvDdY57l4fxE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737324315302,"user_tz":480,"elapsed":2563,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"a3ab84f5-fc0b-4c73-b337-778c85ec04ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned token starts with: ghp_\n","Fetching GitHub token...\n","Token format check - starts with: ghp_\n","Token successfully retrieved.\n","Connected to repository: davidelgas/DataSciencePortfolio\n","Using commit message: Updated notebook from Colab\n","Notebook content read from /content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\n","Target file path in repo: OCCP/OCCP.ipynb\n","Checking if file exists at OCCP/OCCP.ipynb...\n","File updated successfully in branch 'main'.\n"]}],"source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function with your parameters\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest raw data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suHAcKhHVfV8"},"outputs":[],"source":["# import log data\n","\n","import pandas as pd\n","import numpy as np\n","\n","def load_file(file_path):\n","    \"\"\"Load a single CSV file.\"\"\"\n","    return pd.read_csv(file_path)\n","\n","def concatenate_files(file_paths):\n","    \"\"\"Load and combine multiple CSV files.\"\"\"\n","    dfs = []\n","    for file_path in file_paths:\n","        df = load_file(file_path)\n","        if not df.empty:\n","            dfs.append(df)\n","\n","    return pd.concat(dfs, ignore_index=True)\n","\n","if __name__ == \"__main__\":\n","    file_paths = [\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/dec_100_sample.csv'\n","    ]\n","\n","    # Concatenate all files\n","    df_logs = concatenate_files(file_paths)\n","\n","    # Save the combined raw data\n","    df_logs.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgAP-CX4LKV4"},"outputs":[],"source":["#Import property tables from AWS\n","\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","    with open(path_to_credentials, 'r') as file:\n","        for line in file:\n","            if '=' in line:\n","                key, value = line.split('=', 1)\n","                os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch properties table\n","cursor.execute(\"SELECT * FROM properties;\")\n","df_prop = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.csv', index=False)\n","df_prop.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl')\n","\n","# Fetch property_types table\n","cursor.execute(\"SELECT * FROM property_types;\")\n","df_prop_type = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop_type.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.csv', index=False)\n","df_prop_type.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl')\n","\n","# Close connection\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IjOthMDc4uL","executionInfo":{"status":"ok","timestamp":1737247582960,"user_tz":480,"elapsed":217,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"464c4bd6-98f8-4372-c973-2e89459e73c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Data types after conversion:\n","account_id             int64\n","managed_account_id     int64\n","Parking Space Count    int64\n","dtype: object\n"]}],"source":["#Import property table from SalesForce\n","\n","import pandas as pd\n","from pathlib import Path\n","import os\n","\n","def process_property_sizes(input_file: Path) -> pd.DataFrame:\n","    # Define fields to cast as int\n","    int_fields = [\n","        'account_id',\n","        'managed_account_id',\n","        'Parking Space Count'\n","        # Add any other fields that should be int\n","    ]\n","\n","    # Load CSV with explicit encoding\n","    df_prop_size = pd.read_csv(input_file, encoding='latin-1')\n","\n","    # Cast specified fields to int, handling any errors\n","    for field in int_fields:\n","        if field in df_prop_size.columns:\n","            df_prop_size[field] = pd.to_numeric(df_prop_size[field], errors='coerce').fillna(0).astype(int)\n","\n","    # Save pickle to same directory as input file\n","    prop_size_pickle_path = input_file.parent / 'df_prop_size.pkl'\n","    df_prop_size.to_pickle(prop_size_pickle_path)\n","\n","    # Print info about the conversions\n","    print(\"\\nData types after conversion:\")\n","    print(df_prop_size[int_fields].dtypes)\n","\n","    return df_prop_size\n","\n","if __name__ == \"__main__\":\n","    # Define base directory\n","    data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","    # Input path\n","    prop_size_input = data_dir / 'Properties Table jan2025.csv'\n","\n","    # Run workflow\n","    df_prop_size = process_property_sizes(prop_size_input)"]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"K0sBeZzEtE5O","executionInfo":{"status":"ok","timestamp":1737336302993,"user_tz":480,"elapsed":14138,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"outputs":[],"source":["# Here are the dfs Ill be working with\n","\n","df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl') # Event data from Splunk\n","df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl') # Property metadata from AWS\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl') # Property size data from SalesForce\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl') # Property metadata from AWS"]},{"cell_type":"markdown","metadata":{"id":"mtmzDPK5HSAw"},"source":["###This is what the JSON field looks like\n","\n","\n","\n","{\"connectorId\":1,\"transactionId\":1417592169,\"meterValue\":[{\"timestamp\":\"2025-01-14T13:27:37.145Z\",\"sampledValue\":[{\"value\":\"31323855.0\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Energy.Active.Import.Register\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"Wh\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"240.57\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Voltage\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"V\"},{\"value\":\"28\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Temperature\",\"phase\":null,\"location\":\"Body\",\"unit\":\"Celsius\"},{\"value\":\"6.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"1440.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Active.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"}]}]}"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XdW03n8B_At0","executionInfo":{"status":"ok","timestamp":1737336646076,"user_tz":480,"elapsed":339372,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"outputs":[],"source":["# Unpack the JSON field in the log file\n","import pandas as pd\n","import json\n","\n","def expand_message_json(df):\n","    rows = []\n","\n","    for idx, row in df.iterrows():\n","        # Parse the JSON message\n","        message = json.loads(row['cleaned_message']) if pd.notna(row['cleaned_message']) else {}\n","\n","        # Get transactionId from the message\n","        transaction_id = message.get('transactionId')\n","\n","        # Extract meter values\n","        meter_values = message.get('meterValue', [])\n","        for meter in meter_values:\n","            timestamp = meter.get('timestamp')\n","            sampled_values = meter.get('sampledValue', [])\n","\n","            # Filter for only A and W units\n","            for sample in sampled_values:\n","                unit = sample.get('unit')\n","                if unit in ['A', 'W']:\n","                    rows.append({\n","                        'property_id': row['property_id'],\n","                        'user_id': row['user_id'],\n","                        'transaction_id': transaction_id,  # Fixed variable name here\n","                        'timestamp': timestamp,\n","                        'value': sample.get('value'),\n","                        'unit': unit\n","                    })\n","\n","    return pd.DataFrame(rows)\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n"]},{"cell_type":"code","source":["# Create a sample of log data\n","\n","import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrame\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs_exp.pkl'\n","\n","# Read pickle and sample 10 records\n","df_logs_exp = pd.read_pickle(logs_pickle)\n","sample_df = df_logs_exp.sample(n=10, random_state=42)  # random_state for reproducibility\n","\n","# Save sample to CSV in same directory\n","sample_df.to_csv(data_dir / 'logs_sample.csv', index=False)\n","\n","print(\"10 record sample saved to logs_sample.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYYJCdYmLQNG","executionInfo":{"status":"ok","timestamp":1737336905145,"user_tz":480,"elapsed":3333,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"eb1b410e-f3bf-4f5e-8bd8-85a3f0325d9b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["10 record sample saved to logs_sample.csv\n"]}]},{"cell_type":"code","source":["# Fix field name and datatype inconsistencies\n","import pandas as pd\n","from pathlib import Path\n","\n","def normalize_timestamp(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Convert to datetime and truncate to seconds\n","   df['_time'] = pd.to_datetime(df['_time']).dt.floor('s')\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Timestamps normalized and saved\")\n","\n","def normalize_account_ids(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID fields to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","   df['account_id'] = pd.to_numeric(df['account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Account IDs normalized and saved\")\n","\n","def normalize_prop_size_id(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID field to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Property size ID normalized and saved\")\n","\n","if __name__ == \"__main__\":\n","   # Define base directory\n","   data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","   # Define pickle files\n","   logs_pickle = data_dir / 'df_logs.pkl'\n","   prop_pickle = data_dir / 'df_prop.pkl'\n","   prop_size_pickle = data_dir / 'df_prop_size.pkl'\n","\n","   # Run normalizations\n","   normalize_timestamp(logs_pickle)\n","   normalize_account_ids(prop_pickle)\n","   normalize_prop_size_id(prop_size_pickle)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSw2NT1WCaE7","executionInfo":{"status":"ok","timestamp":1737324421602,"user_tz":480,"elapsed":19171,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"b9d50b70-25f5-4427-f4c0-ca3e7f3c3d44"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Timestamps normalized and saved\n","Account IDs normalized and saved\n","Property size ID normalized and saved\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5lLvvK85MCce","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737324442572,"user_tz":480,"elapsed":14527,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"77460e76-cdd1-402e-a3bb-efdb9ddcc14c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique property_ids in logs: 700\n","Unique ids in properties: 807\n","\n","Found 20 property_ids in logs that don't exist in properties table\n","Sample of missing ids: [nan, '53e9cab2-aed2-4c35-ab39-e8375825e6fb\\n4cb8af8c-e0ed-41d9-8c08-6e66c4125cdd', 'f14f0f6a-1ec0-4136-8bf4-90b96ec0f604\\n0b372bad-6c5f-4ff7-ab2f-5f9b380d5e6f', '500c202d-143e-4e3d-854c-77fefb209253\\n406ec42d-df0b-401d-a0a2-5e21824cb3d0', 'e95523e6-3470-4a60-b586-ee715cd9f34b\\n9a06f545-1ab0-476d-8d43-1b5101cbd551']\n","\n","Unmatched rows after join: 8054 (0.45% of logs)\n"]}],"source":["# Check join logic\n","# Does df_logs.property_ud join with df_prop.id ?\n","# Sample value 3436570000094511023\n","# Found 20 property_ids in logs that don't exist in properties table\n","\n","import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrames\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs.pkl'\n","prop_pickle = data_dir / 'df_prop.pkl'\n","\n","df_logs = pd.read_pickle(logs_pickle)\n","df_prop = pd.read_pickle(prop_pickle)\n","\n","# Check unique values in each column\n","n_log_props = df_logs['property_id'].nunique()\n","n_prop_ids = df_prop['id'].nunique()\n","\n","print(f\"Unique property_ids in logs: {n_log_props}\")\n","print(f\"Unique ids in properties: {n_prop_ids}\")\n","\n","# Check which property_ids in logs don't exist in properties\n","missing_props = set(df_logs['property_id'].unique()) - set(df_prop['id'].unique())\n","\n","if missing_props:\n","    print(f\"\\nFound {len(missing_props)} property_ids in logs that don't exist in properties table\")\n","    print(\"Sample of missing ids:\", list(missing_props)[:5])\n","else:\n","    print(\"\\nAll property_ids in logs exist in properties table\")\n","\n","# Check actual join\n","merged_df = df_logs.merge(df_prop, left_on='property_id', right_on='id', how='left')\n","n_unmatched = merged_df['id'].isna().sum()\n","\n","print(f\"\\nUnmatched rows after join: {n_unmatched} ({(n_unmatched/len(df_logs))*100:.2f}% of logs)\")\n"]},{"cell_type":"code","source":["# Sample missing data\n","\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","   with open(path_to_credentials, 'r') as file:\n","       for line in file:\n","           if '=' in line:\n","               key, value = line.split('=', 1)\n","               os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","   'host': os.getenv('DB_HOST'),\n","   'dbname': os.getenv('DB_NAME'),\n","   'user': os.getenv('DB_USER'),\n","   'password': os.getenv('DB_PASSWORD'),\n","   'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch full rows where property_type is NULL\n","query = \"SELECT * FROM properties WHERE property_type IS NULL;\"\n","\n","cursor.execute(query)\n","columns = [desc[0] for desc in cursor.description]\n","missing_prop_types = pd.DataFrame(cursor.fetchall(), columns=columns)\n","\n","print(missing_prop_types.to_string())\n","\n","# Close connection\n","cursor.close()\n","connection.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrRG6PPLADrN","executionInfo":{"status":"ok","timestamp":1737324473007,"user_tz":480,"elapsed":1122,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"84fe9782-61a1-41fe-b574-843e0cff020a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["                                     id                       organization_id                                             name       phone contact            longitude            latitude  watts_soft_limit property_type  note   utility_provider gateway_type carrier_name               address_1  address_2         city state    zip email  has_editable_penalty   managed_account_id                       created_at                       updated_at reboot_cron_schedule account_id  hidden decommissioned_at exclusive_domain_1 exclusive_domain_2\n","0  522f3cfd-1a6a-4929-be9e-793642a7d1cd  e4c8d63e-b801-4d72-830a-0a5a8aad8097                    TEST - NewPropertyTestPricing                                     None                None               100          None                    LADWP         None         None                                                                                    False                      2024-10-02 04:45:34.289552+00:00 2025-01-08 21:12:29.442263+00:00                 None              False              None               None               None\n","1  f574df18-7928-4ea6-9845-09e814784053  a26a3318-8779-43ff-a8ca-ef5f694fc58f                   TEST - Chargie Test Property 2                                        0                   0               100          None                      SCE         None         None                                                                                    False                      2025-01-09 05:37:43.763938+00:00 2025-01-09 05:37:43.763938+00:00                 None               True              None               None               None\n","2  510a495d-a6e4-4518-892b-7b16674340d4  a26a3318-8779-43ff-a8ca-ef5f694fc58f                      TEST - Chargie Test Penalty  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:04:06.328713+00:00            * * * * *       None   False              None               None               None\n","3  047de1d3-0bd2-40fe-b92f-5155448ea171  69512d85-02da-42bc-b87b-85bd68df1783                          TEST - Calabasas Office                              -118.653976           34.155115           1000000          None  None                SCE         None         None  5038 Parkway Calabasas  Suite 101    Calabasas    CA  91302                       False  3436570000048718466 2024-03-06 05:23:08.827026+00:00 2024-06-12 15:30:40.209580+00:00                 None       None    True              None               None               None\n","4  50b8b410-ec8a-4656-a95c-659ef9aec749  a26a3318-8779-43ff-a8ca-ef5f694fc58f                     TEST - Chargie Test Property                               -118.52757            34.23555              1000          None        ROSEVILLEELECTRIC  CRADLEPOINT          ATT       3947 Landmark St.             Culver City    CA  90232                       False                    0 2023-05-03 05:01:09.099967+00:00 2024-09-10 23:04:55.447822+00:00                 None       None   False              None               None               None\n","5  49dead0e-f152-42ce-a47b-e2289bd1010f  a26a3318-8779-43ff-a8ca-ef5f694fc58f  TEST - Chargie Test No Transactions No Stations  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:03:34.470639+00:00            * * * * *       None   False              None               None               None\n"]}]},{"cell_type":"code","source":["# Check join logic\n","# Does df_logs.property_id join with df_prop_size.id\n","\n","# Check join keys\n","print(\"Unique property_id in df_logs:\", df_logs['property_id'].nunique())\n","print(\"Unique ids in df_prop_size:\", df_prop_size['id'].nunique())\n","\n","# Check overlap between keys\n","common_keys = set(df_logs['property_id']) & set(df_prop_size['id'])\n","print(\"\\nNumber of common property IDs:\", len(common_keys))\n","\n","# Percentage of matching keys\n","logs_match_percentage = len(common_keys) / df_logs['property_id'].nunique() * 100\n","prop_size_match_percentage = len(common_keys) / df_prop_size['id'].nunique() * 100\n","\n","print(f\"\\nPercentage of property_id in logs that match prop_size: {logs_match_percentage:.2f}%\")\n","print(f\"Percentage of ids in prop_size that match logs: {prop_size_match_percentage:.2f}%\")\n","\n","# Sample of matching and non-matching keys\n","print(\"\\nSample of matching property IDs (first 10):\")\n","print(list(common_keys)[:10])\n","\n","# Check for any non-matching keys\n","non_matching_in_logs = set(df_logs['property_id']) - set(df_prop_size['id'])\n","non_matching_in_prop_size = set(df_prop_size['id']) - set(df_logs['property_id'])\n","\n","print(\"\\nNumber of property_ids in logs not in prop_size:\", len(non_matching_in_logs))\n","print(\"\\nSample of non-matching property_ids in logs (first 10):\")\n","print(list(non_matching_in_logs)[:10])\n","\n","# Perform a left join to see unmatched rows\n","merged_df = df_logs.merge(df_prop_size, left_on='property_id', right_on='id', how='left', indicator=True)\n","print(\"\\nMerge result:\")\n","print(merged_df['_merge'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_WEqx1z_v78","executionInfo":{"status":"ok","timestamp":1737324483053,"user_tz":480,"elapsed":4564,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"30c12e87-9009-4ddc-a26f-3f9b06e6720d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique property_id in df_logs: 700\n","Unique ids in df_prop_size: 784\n","\n","Number of common property IDs: 676\n","\n","Percentage of property_id in logs that match prop_size: 96.57%\n","Percentage of ids in prop_size that match logs: 86.22%\n","\n","Sample of matching property IDs (first 10):\n","['2c4d3880-b3eb-4ff2-8a24-52c28f0d239d', 'b3915d54-19cb-43ff-b33d-3d4b3edc76f5', '7d972ac4-62cf-4c6f-a824-de6a91a077f6', 'f3c7c831-4fc8-4867-9bfe-940ce4b41abd', 'cee9d389-0a1c-4ae3-b4b5-751b9ad13395', '56fdf5a0-6f6a-4166-bfdc-f4544a8f0ec0', '0b372bad-6c5f-4ff7-ab2f-5f9b380d5e6f', 'ef33ba45-3393-49f0-bef2-84c5d251dfe3', '23cdfb03-d755-4e6e-a492-cde3efc52aa8', '46237996-96a3-4be9-acbf-957ec4259019']\n","\n","Number of property_ids in logs not in prop_size: 7208\n","\n","Sample of non-matching property_ids in logs (first 10):\n","[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n","\n","Merge result:\n","_merge\n","both          1768602\n","left_only       12505\n","right_only          0\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Check join logic\n","# Check join between df_prop.property_type and df_prop_type.id\n","\n","print(\"Unique property_type in df_prop:\", df_prop['property_type'].nunique())\n","print(\"Unique ids in df_prop_type:\", df_prop_type['id'].nunique())\n","\n","# Check overlap between keys\n","common_keys = set(df_prop['property_type']) & set(df_prop_type['id'])\n","print(\"\\nNumber of common property types:\", len(common_keys))\n","\n","# Percentage of matching keys\n","prop_match_percentage = len(common_keys) / df_prop['property_type'].nunique() * 100\n","prop_type_match_percentage = len(common_keys) / df_prop_type['id'].nunique() * 100\n","\n","print(f\"\\nPercentage of property_type in prop that match prop_type: {prop_match_percentage:.2f}%\")\n","print(f\"Percentage of ids in prop_type that match prop: {prop_type_match_percentage:.2f}%\")\n","\n","# Perform a left join to see unmatched rows\n","merged_df = df_prop.merge(df_prop_type, left_on='property_type', right_on='id', how='left', indicator=True)\n","print(\"\\nMerge result:\")\n","print(merged_df['_merge'].value_counts())\n","\n","# Check unmatched property types\n","unmatched = merged_df[merged_df['_merge'] == 'left_only']\n","print(\"\\nUnmatched property types:\")\n","print(unmatched['property_type'].unique())"],"metadata":{"id":"4Auysgoa7GaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737324497256,"user_tz":480,"elapsed":141,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"8c6ed6f6-a108-4ef9-e3dc-ba8f47240aec"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique property_type in df_prop: 13\n","Unique ids in df_prop_type: 20\n","\n","Number of common property types: 13\n","\n","Percentage of property_type in prop that match prop_type: 100.00%\n","Percentage of ids in prop_type that match prop: 65.00%\n","\n","Merge result:\n","_merge\n","both          801\n","left_only       6\n","right_only      0\n","Name: count, dtype: int64\n","\n","Unmatched property types:\n","[None]\n"]}]},{"cell_type":"code","source":["# Decorate log data with property metadata\n","\n","df_logs_exp = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","df_logs_metadata = df_logs_exp.copy()\n","\n","# Left join to append property_type from df_prop\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop[['id', 'property_type']],\n","   left_on='property_id',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Drop the redundant id column\n","df_logs_metadata = df_logs_metadata.drop(columns=['id'])\n","\n","# Left join to append name from df_prop_type\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop_type[['id', 'name']],\n","   left_on='property_type',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Drop the redundant id column\n","df_logs_metadata = df_logs_metadata.drop(columns=['id'])\n","\n","# Left join to append Parking Space Count from df_prop_size\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop_size[['id', 'Parking Space Count']],\n","   left_on='property_id',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Save the data\n","df_logs_metadata.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_metadata.pkl')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"IdD-cbAXnyCs","executionInfo":{"status":"error","timestamp":1737336280066,"user_tz":480,"elapsed":6216,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"44c42898-9152-48a5-9e6f-87495906736f"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_prop' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ad2cfc244c71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Left join to append property_type from df_prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m df_logs_metadata = df_logs_metadata.merge(\n\u001b[0;32m----> 9\u001b[0;31m    \u001b[0mdf_prop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'property_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m    \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'property_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m    \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_prop' is not defined"]}]},{"cell_type":"code","source":["# Data clean-up\n","\n","# Drop and re-name features\n","df_logs_metadata = df_logs_metadata.drop(columns=['id','property_type'])\n","df_logs_metadata = df_logs_metadata[['property_id','name','Parking Space Count','transaction_id','timestamp','user_id','unit','value']]\n","df_logs_metadata = df_logs_metadata.rename(columns={'name': 'property_type'})\n","df_logs_metadata = df_logs_metadata.rename(columns={'Parking Space Count': 'property_size'})\n","\n","\n","#Cast numeric formats\n","df_logs_metadata['property_size'] = pd.to_numeric(df_logs_metadata['property_size'], errors='coerce').fillna(0).astype(int)\n","df_logs_metadata['value'] = pd.to_numeric(df_logs_metadata['value'], errors='coerce').fillna(0).astype(int)\n","\n","# Fix timestamp format and convert to Pacific\n","def is_valid_timestamp(timestamp):\n","    try:\n","        pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%fZ')\n","        return True\n","    except:\n","        try:\n","            pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%SZ')\n","            return True\n","        except:\n","            try:\n","                pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%f%z')\n","                return True\n","            except:\n","                return False\n","\n","def remove_invalid_timestamps(df):\n","    return df[df['timestamp'].apply(is_valid_timestamp)]\n","\n","def truncate_timestamp(df):\n","    df['timestamp'] = df['timestamp'].str[:16]\n","    return df\n","\n","def convert_to_pacific_time(df):\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_convert('US/Pacific')\n","    return df\n","\n","def process_timestamps(df_logs_metadata):\n","    df_logs_metadata = remove_invalid_timestamps(df_logs_metadata)\n","    df_logs_metadata = truncate_timestamp(df_logs_metadata)\n","    df_logs_metadata = convert_to_pacific_time(df_logs_metadata)\n","\n","    return df_logs_metadata\n","\n","# Usage\n","df_logs_metadata = process_timestamps(df_logs_metadata)"],"metadata":{"id":"MznnCS7TnyFE","executionInfo":{"status":"ok","timestamp":1737327761099,"user_tz":480,"elapsed":3865,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# Fix timestamp formats and convert to Pacific\n","def is_valid_timestamp(timestamp):\n","    try:\n","        pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%fZ')\n","        return True\n","    except:\n","        try:\n","            pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%SZ')\n","            return True\n","        except:\n","            try:\n","                pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%f%z')\n","                return True\n","            except:\n","                return False\n","\n","def remove_invalid_timestamps(df):\n","    return df[df['timestamp'].apply(is_valid_timestamp)]\n","\n","def truncate_timestamp(df):\n","    df['timestamp'] = df['timestamp'].str[:16]\n","    return df\n","\n","def convert_to_pacific_time(df):\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_convert('US/Pacific')\n","    return df\n","\n","def process_timestamps(df_logs_metadata):\n","    df_logs_metadata = remove_invalid_timestamps(df_logs_metadata)\n","    df_logs_metadata = truncate_timestamp(df_logs_metadata)\n","    df_logs_metadata = convert_to_pacific_time(df_logs_metadata)\n","\n","    return df_logs_metadata\n","\n","# Usage\n","df_logs_metadata = process_timestamps(df_logs_metadata)"],"metadata":{"id":"rp-ZmCsGqvXk","executionInfo":{"status":"ok","timestamp":1737328233390,"user_tz":480,"elapsed":469484,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNc4o_n59G_G"},"source":["## Data Exploration"]},{"cell_type":"code","source":["df_logs_metadata.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzkrtvxn1ZO2","executionInfo":{"status":"ok","timestamp":1737328275257,"user_tz":480,"elapsed":173,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"fc14188e-a730-472e-9f27-bbb65c3810e4"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3289867 entries, 0 to 3289866\n","Data columns (total 8 columns):\n"," #   Column          Dtype                     \n","---  ------          -----                     \n"," 0   property_id     object                    \n"," 1   property_type   object                    \n"," 2   property_size   int64                     \n"," 3   transaction_id  int64                     \n"," 4   timestamp       datetime64[ns, US/Pacific]\n"," 5   user_id         object                    \n"," 6   unit            object                    \n"," 7   value           int64                     \n","dtypes: datetime64[ns, US/Pacific](1), int64(3), object(4)\n","memory usage: 200.8+ MB\n"]}]},{"cell_type":"code","source":["# Does each transaction_id have a unique timestamp:\n","\n","import pandas as pd\n","\n","\n","\n","# Create a new column with the combination of transaction_id and timestamp\n","df_logs_metadata['transaction_timestamp_pair'] = df_logs_metadata['transaction_id'].astype(str) + '_' + df_logs_metadata['timestamp'].astype(str)\n","\n","# Count the number of occurrences of each pair\n","pair_counts = df_logs_metadata['transaction_timestamp_pair'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = pair_counts[pair_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"transaction_id and timestamp pairs are unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate transaction_id and timestamp pairs.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique pairs: {len(pair_counts)}\")\n","print(f\"Are all pairs unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_pairs = duplicates.index[:5]  # Get the first 5 duplicate pairs\n","    for pair in duplicate_pairs:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'] == pair][['transaction_id', 'timestamp', 'property_id', 'user_id', 'value', 'unit']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate pairs:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","    print(f\"Number of unique units in duplicates: {duplicate_df['unit'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDqQkBB95XRx","executionInfo":{"status":"ok","timestamp":1737329396700,"user_tz":480,"elapsed":41243,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"d7c90d98-ec61-470e-cffb-f40bdf84d3a9"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 506191 duplicate transaction_id and timestamp pairs.\n","Examples of duplicates:\n","transaction_timestamp_pair\n","1938384688_2024-10-05 10:08:00-07:00    1540\n","464091764_2024-11-30 20:05:00-08:00     1440\n","1425252740_2024-11-23 16:18:00-08:00    1380\n","82110937_2024-12-24 13:24:00-08:00      1052\n","1388746211_2024-11-17 09:13:00-08:00     916\n","Name: count, dtype: int64\n","\n","Total rows: 3289867\n","Unique pairs: 1766678\n","Are all pairs unique? False\n","\n","Example of duplicate rows:\n","         transaction_id                 timestamp  \\\n","1341517      1938384688 2024-10-05 10:08:00-07:00   \n","1341518      1938384688 2024-10-05 10:08:00-07:00   \n","1341519      1938384688 2024-10-05 10:08:00-07:00   \n","1341520      1938384688 2024-10-05 10:08:00-07:00   \n","1341620      1938384688 2024-10-05 10:08:00-07:00   \n","...                 ...                       ...   \n","1436751      1938384688 2024-10-05 10:08:00-07:00   \n","1436777      1938384688 2024-10-05 10:08:00-07:00   \n","1436778      1938384688 2024-10-05 10:08:00-07:00   \n","1436779      1938384688 2024-10-05 10:08:00-07:00   \n","1436780      1938384688 2024-10-05 10:08:00-07:00   \n","\n","                                  property_id  \\\n","1341517  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341518  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341519  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341520  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341620  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","...                                       ...   \n","1436751  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436777  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436778  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436779  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436780  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","\n","                                      user_id  value unit  \n","1341517  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    A  \n","1341518  38f8ffdb-36bd-440e-8a1d-ffe859799046      6    A  \n","1341519  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248    W  \n","1341520  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    W  \n","1341620  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    A  \n","...                                       ...    ...  ...  \n","1436751  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    W  \n","1436777  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    A  \n","1436778  38f8ffdb-36bd-440e-8a1d-ffe859799046      6    A  \n","1436779  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248    W  \n","1436780  38f8ffdb-36bd-440e-8a1d-ffe859799046      0    W  \n","\n","[1540 rows x 6 columns]\n","\n","         transaction_id                 timestamp  \\\n","3179407       464091764 2024-11-30 20:05:00-08:00   \n","3179408       464091764 2024-11-30 20:05:00-08:00   \n","3179409       464091764 2024-11-30 20:05:00-08:00   \n","3179410       464091764 2024-11-30 20:05:00-08:00   \n","3179470       464091764 2024-11-30 20:05:00-08:00   \n","...                 ...                       ...   \n","3289637       464091764 2024-11-30 20:05:00-08:00   \n","3289691       464091764 2024-11-30 20:05:00-08:00   \n","3289692       464091764 2024-11-30 20:05:00-08:00   \n","3289693       464091764 2024-11-30 20:05:00-08:00   \n","3289694       464091764 2024-11-30 20:05:00-08:00   \n","\n","                                  property_id  \\\n","3179407  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179408  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179409  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179410  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179470  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","...                                       ...   \n","3289637  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289691  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289692  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289693  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289694  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","\n","                                      user_id  value unit  \n","3179407  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30    A  \n","3179408  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32    A  \n","3179409  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680    W  \n","3179410  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158    W  \n","3179470  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30    A  \n","...                                       ...    ...  ...  \n","3289637  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158    W  \n","3289691  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30    A  \n","3289692  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32    A  \n","3289693  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680    W  \n","3289694  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158    W  \n","\n","[1440 rows x 6 columns]\n","\n","         transaction_id                 timestamp  \\\n","1653275      1425252740 2024-11-23 16:18:00-08:00   \n","1653276      1425252740 2024-11-23 16:18:00-08:00   \n","1653277      1425252740 2024-11-23 16:18:00-08:00   \n","1653278      1425252740 2024-11-23 16:18:00-08:00   \n","1653372      1425252740 2024-11-23 16:18:00-08:00   \n","...                 ...                       ...   \n","1694341      1425252740 2024-11-23 16:18:00-08:00   \n","1694408      1425252740 2024-11-23 16:18:00-08:00   \n","1694409      1425252740 2024-11-23 16:18:00-08:00   \n","1694410      1425252740 2024-11-23 16:18:00-08:00   \n","1694411      1425252740 2024-11-23 16:18:00-08:00   \n","\n","                                  property_id  \\\n","1653275  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653276  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653277  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653278  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653372  87957b20-80e6-4441-8459-5e1f674de94e   \n","...                                       ...   \n","1694341  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694408  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694409  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694410  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694411  87957b20-80e6-4441-8459-5e1f674de94e   \n","\n","                                      user_id  value unit  \n","1653275  6061aa83-7027-4f83-9f05-a29ce617209f     31    A  \n","1653276  6061aa83-7027-4f83-9f05-a29ce617209f     32    A  \n","1653277  6061aa83-7027-4f83-9f05-a29ce617209f   6656    W  \n","1653278  6061aa83-7027-4f83-9f05-a29ce617209f   6628    W  \n","1653372  6061aa83-7027-4f83-9f05-a29ce617209f     31    A  \n","...                                       ...    ...  ...  \n","1694341  6061aa83-7027-4f83-9f05-a29ce617209f   6628    W  \n","1694408  6061aa83-7027-4f83-9f05-a29ce617209f     31    A  \n","1694409  6061aa83-7027-4f83-9f05-a29ce617209f     32    A  \n","1694410  6061aa83-7027-4f83-9f05-a29ce617209f   6656    W  \n","1694411  6061aa83-7027-4f83-9f05-a29ce617209f   6628    W  \n","\n","[1380 rows x 6 columns]\n","\n","         transaction_id                 timestamp  \\\n","2477881        82110937 2024-12-24 13:24:00-08:00   \n","2477882        82110937 2024-12-24 13:24:00-08:00   \n","2477883        82110937 2024-12-24 13:24:00-08:00   \n","2477884        82110937 2024-12-24 13:24:00-08:00   \n","2477914        82110937 2024-12-24 13:24:00-08:00   \n","...                 ...                       ...   \n","2585626        82110937 2024-12-24 13:24:00-08:00   \n","2585653        82110937 2024-12-24 13:24:00-08:00   \n","2585654        82110937 2024-12-24 13:24:00-08:00   \n","2585655        82110937 2024-12-24 13:24:00-08:00   \n","2585656        82110937 2024-12-24 13:24:00-08:00   \n","\n","                                  property_id  \\\n","2477881  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2477882  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2477883  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2477884  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2477914  b551cfb1-e777-48df-bd85-96c67b40a511   \n","...                                       ...   \n","2585626  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2585653  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2585654  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2585655  b551cfb1-e777-48df-bd85-96c67b40a511   \n","2585656  b551cfb1-e777-48df-bd85-96c67b40a511   \n","\n","                                      user_id  value unit  \n","2477881  c07af594-a1d3-4a18-a439-fa96a8c1a33d     30    A  \n","2477882  c07af594-a1d3-4a18-a439-fa96a8c1a33d     32    A  \n","2477883  c07af594-a1d3-4a18-a439-fa96a8c1a33d   6656    W  \n","2477884  c07af594-a1d3-4a18-a439-fa96a8c1a33d   6251    W  \n","2477914  c07af594-a1d3-4a18-a439-fa96a8c1a33d     30    A  \n","...                                       ...    ...  ...  \n","2585626  c07af594-a1d3-4a18-a439-fa96a8c1a33d   6251    W  \n","2585653  c07af594-a1d3-4a18-a439-fa96a8c1a33d     30    A  \n","2585654  c07af594-a1d3-4a18-a439-fa96a8c1a33d     32    A  \n","2585655  c07af594-a1d3-4a18-a439-fa96a8c1a33d   6656    W  \n","2585656  c07af594-a1d3-4a18-a439-fa96a8c1a33d   6251    W  \n","\n","[1052 rows x 6 columns]\n","\n","         transaction_id                 timestamp  \\\n","1858295      1388746211 2024-11-17 09:13:00-08:00   \n","1858296      1388746211 2024-11-17 09:13:00-08:00   \n","1858297      1388746211 2024-11-17 09:13:00-08:00   \n","1858298      1388746211 2024-11-17 09:13:00-08:00   \n","1858378      1388746211 2024-11-17 09:13:00-08:00   \n","...                 ...                       ...   \n","1894257      1388746211 2024-11-17 09:13:00-08:00   \n","1894262      1388746211 2024-11-17 09:13:00-08:00   \n","1894263      1388746211 2024-11-17 09:13:00-08:00   \n","1894264      1388746211 2024-11-17 09:13:00-08:00   \n","1894265      1388746211 2024-11-17 09:13:00-08:00   \n","\n","                                  property_id  \\\n","1858295  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1858296  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1858297  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1858298  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1858378  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","...                                       ...   \n","1894257  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1894262  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1894263  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1894264  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","1894265  4be10c4f-a0ef-418a-b75f-be1ec53eb1ae   \n","\n","                                      user_id  value unit  \n","1858295  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    A  \n","1858296  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      6    A  \n","1858297  f7e2b7a4-acb6-4489-8e73-1bff6ee77784   1248    W  \n","1858298  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    W  \n","1858378  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    A  \n","...                                       ...    ...  ...  \n","1894257  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    W  \n","1894262  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    A  \n","1894263  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      6    A  \n","1894264  f7e2b7a4-acb6-4489-8e73-1bff6ee77784   1248    W  \n","1894265  f7e2b7a4-acb6-4489-8e73-1bff6ee77784      0    W  \n","\n","[916 rows x 6 columns]\n","\n","\n","Analysis of duplicate pairs:\n","Number of unique property_ids in duplicates: 628\n","Number of unique user_ids in duplicates: 7082\n","Number of unique values in duplicates: 7808\n","Number of unique units in duplicates: 2\n","\n","Most common property_types in duplicates:\n","property_type\n","Multifamily     1095015\n","Education        299830\n","Mixed Use        160104\n","Office           138144\n","Condominiums     119459\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Is the combination of transaction_id, timestamp and unit unique ?\n","import pandas as pd\n","\n","# Assuming df_logs_metadata is your existing DataFrame\n","\n","# Create a new column with the combination of transaction_id, timestamp, and unit\n","df_logs_metadata['transaction_timestamp_unit_combo'] = (\n","    df_logs_metadata['transaction_id'].astype(str) + '_' +\n","    df_logs_metadata['timestamp'].astype(str) + '_' +\n","    df_logs_metadata['unit']\n",")\n","\n","# Count the number of occurrences of each combination\n","combo_counts = df_logs_metadata['transaction_timestamp_unit_combo'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = combo_counts[combo_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"The combination of transaction_id, timestamp, and unit is unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate combinations of transaction_id, timestamp, and unit.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique combinations: {len(combo_counts)}\")\n","print(f\"Are all combinations unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_combos = duplicates.index[:5]  # Get the first 5 duplicate combinations\n","    for combo in duplicate_combos:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_unit_combo'] == combo][['transaction_id', 'timestamp', 'unit', 'property_id', 'user_id', 'value']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_unit_combo'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate combinations:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())\n","\n","\n","\n","\n","# Example of a \"duplicate\" transaction_id: 1938384688\n","result = df_logs_metadata[df_logs_metadata['transaction_id'] == 1938384688]\n","print(result)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Te6OtkQz6eSZ","executionInfo":{"status":"ok","timestamp":1737329664995,"user_tz":480,"elapsed":48146,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"2c08a418-3111-4078-bd42-ddc01ddf6688"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 995109 duplicate combinations of transaction_id, timestamp, and unit.\n","Examples of duplicates:\n","transaction_timestamp_unit_combo\n","1938384688_2024-10-05 10:08:00-07:00_A    770\n","1938384688_2024-10-05 10:08:00-07:00_W    770\n","464091764_2024-11-30 20:05:00-08:00_A     720\n","464091764_2024-11-30 20:05:00-08:00_W     720\n","1425252740_2024-11-23 16:18:00-08:00_A    690\n","Name: count, dtype: int64\n","\n","Total rows: 3289867\n","Unique combinations: 2266379\n","Are all combinations unique? False\n","\n","Example of duplicate rows:\n","         transaction_id                 timestamp unit  \\\n","1341517      1938384688 2024-10-05 10:08:00-07:00    A   \n","1341518      1938384688 2024-10-05 10:08:00-07:00    A   \n","1341620      1938384688 2024-10-05 10:08:00-07:00    A   \n","1341621      1938384688 2024-10-05 10:08:00-07:00    A   \n","1341714      1938384688 2024-10-05 10:08:00-07:00    A   \n","...                 ...                       ...  ...   \n","1436724      1938384688 2024-10-05 10:08:00-07:00    A   \n","1436748      1938384688 2024-10-05 10:08:00-07:00    A   \n","1436749      1938384688 2024-10-05 10:08:00-07:00    A   \n","1436777      1938384688 2024-10-05 10:08:00-07:00    A   \n","1436778      1938384688 2024-10-05 10:08:00-07:00    A   \n","\n","                                  property_id  \\\n","1341517  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341518  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341620  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341621  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341714  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","...                                       ...   \n","1436724  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436748  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436749  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436777  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436778  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","\n","                                      user_id  value  \n","1341517  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1341518  38f8ffdb-36bd-440e-8a1d-ffe859799046      6  \n","1341620  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1341621  38f8ffdb-36bd-440e-8a1d-ffe859799046      6  \n","1341714  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","...                                       ...    ...  \n","1436724  38f8ffdb-36bd-440e-8a1d-ffe859799046      6  \n","1436748  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1436749  38f8ffdb-36bd-440e-8a1d-ffe859799046      6  \n","1436777  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1436778  38f8ffdb-36bd-440e-8a1d-ffe859799046      6  \n","\n","[770 rows x 6 columns]\n","\n","         transaction_id                 timestamp unit  \\\n","1341519      1938384688 2024-10-05 10:08:00-07:00    W   \n","1341520      1938384688 2024-10-05 10:08:00-07:00    W   \n","1341622      1938384688 2024-10-05 10:08:00-07:00    W   \n","1341623      1938384688 2024-10-05 10:08:00-07:00    W   \n","1341716      1938384688 2024-10-05 10:08:00-07:00    W   \n","...                 ...                       ...  ...   \n","1436726      1938384688 2024-10-05 10:08:00-07:00    W   \n","1436750      1938384688 2024-10-05 10:08:00-07:00    W   \n","1436751      1938384688 2024-10-05 10:08:00-07:00    W   \n","1436779      1938384688 2024-10-05 10:08:00-07:00    W   \n","1436780      1938384688 2024-10-05 10:08:00-07:00    W   \n","\n","                                  property_id  \\\n","1341519  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341520  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341622  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341623  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1341716  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","...                                       ...   \n","1436726  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436750  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436751  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436779  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","1436780  51865921-029c-47fa-a3e8-2e8d3e45f26d   \n","\n","                                      user_id  value  \n","1341519  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248  \n","1341520  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1341622  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248  \n","1341623  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1341716  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248  \n","...                                       ...    ...  \n","1436726  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1436750  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248  \n","1436751  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","1436779  38f8ffdb-36bd-440e-8a1d-ffe859799046   1248  \n","1436780  38f8ffdb-36bd-440e-8a1d-ffe859799046      0  \n","\n","[770 rows x 6 columns]\n","\n","         transaction_id                 timestamp unit  \\\n","3179407       464091764 2024-11-30 20:05:00-08:00    A   \n","3179408       464091764 2024-11-30 20:05:00-08:00    A   \n","3179470       464091764 2024-11-30 20:05:00-08:00    A   \n","3179471       464091764 2024-11-30 20:05:00-08:00    A   \n","3179658       464091764 2024-11-30 20:05:00-08:00    A   \n","...                 ...                       ...  ...   \n","3289351       464091764 2024-11-30 20:05:00-08:00    A   \n","3289634       464091764 2024-11-30 20:05:00-08:00    A   \n","3289635       464091764 2024-11-30 20:05:00-08:00    A   \n","3289691       464091764 2024-11-30 20:05:00-08:00    A   \n","3289692       464091764 2024-11-30 20:05:00-08:00    A   \n","\n","                                  property_id  \\\n","3179407  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179408  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179470  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179471  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179658  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","...                                       ...   \n","3289351  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289634  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289635  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289691  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289692  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","\n","                                      user_id  value  \n","3179407  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30  \n","3179408  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32  \n","3179470  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30  \n","3179471  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32  \n","3179658  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30  \n","...                                       ...    ...  \n","3289351  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32  \n","3289634  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30  \n","3289635  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32  \n","3289691  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     30  \n","3289692  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731     32  \n","\n","[720 rows x 6 columns]\n","\n","         transaction_id                 timestamp unit  \\\n","3179409       464091764 2024-11-30 20:05:00-08:00    W   \n","3179410       464091764 2024-11-30 20:05:00-08:00    W   \n","3179472       464091764 2024-11-30 20:05:00-08:00    W   \n","3179473       464091764 2024-11-30 20:05:00-08:00    W   \n","3179660       464091764 2024-11-30 20:05:00-08:00    W   \n","...                 ...                       ...  ...   \n","3289353       464091764 2024-11-30 20:05:00-08:00    W   \n","3289636       464091764 2024-11-30 20:05:00-08:00    W   \n","3289637       464091764 2024-11-30 20:05:00-08:00    W   \n","3289693       464091764 2024-11-30 20:05:00-08:00    W   \n","3289694       464091764 2024-11-30 20:05:00-08:00    W   \n","\n","                                  property_id  \\\n","3179409  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179410  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179472  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179473  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3179660  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","...                                       ...   \n","3289353  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289636  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289637  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289693  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","3289694  621b41d9-91b4-42d2-8520-89ad2e4ba75f   \n","\n","                                      user_id  value  \n","3179409  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680  \n","3179410  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158  \n","3179472  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680  \n","3179473  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158  \n","3179660  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680  \n","...                                       ...    ...  \n","3289353  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158  \n","3289636  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680  \n","3289637  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158  \n","3289693  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7680  \n","3289694  d9e3c58d-cb3d-4c65-ae72-e00a1cd78731   7158  \n","\n","[720 rows x 6 columns]\n","\n","         transaction_id                 timestamp unit  \\\n","1653275      1425252740 2024-11-23 16:18:00-08:00    A   \n","1653276      1425252740 2024-11-23 16:18:00-08:00    A   \n","1653372      1425252740 2024-11-23 16:18:00-08:00    A   \n","1653373      1425252740 2024-11-23 16:18:00-08:00    A   \n","1653390      1425252740 2024-11-23 16:18:00-08:00    A   \n","...                 ...                       ...  ...   \n","1694293      1425252740 2024-11-23 16:18:00-08:00    A   \n","1694338      1425252740 2024-11-23 16:18:00-08:00    A   \n","1694339      1425252740 2024-11-23 16:18:00-08:00    A   \n","1694408      1425252740 2024-11-23 16:18:00-08:00    A   \n","1694409      1425252740 2024-11-23 16:18:00-08:00    A   \n","\n","                                  property_id  \\\n","1653275  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653276  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653372  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653373  87957b20-80e6-4441-8459-5e1f674de94e   \n","1653390  87957b20-80e6-4441-8459-5e1f674de94e   \n","...                                       ...   \n","1694293  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694338  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694339  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694408  87957b20-80e6-4441-8459-5e1f674de94e   \n","1694409  87957b20-80e6-4441-8459-5e1f674de94e   \n","\n","                                      user_id  value  \n","1653275  6061aa83-7027-4f83-9f05-a29ce617209f     31  \n","1653276  6061aa83-7027-4f83-9f05-a29ce617209f     32  \n","1653372  6061aa83-7027-4f83-9f05-a29ce617209f     31  \n","1653373  6061aa83-7027-4f83-9f05-a29ce617209f     32  \n","1653390  6061aa83-7027-4f83-9f05-a29ce617209f     31  \n","...                                       ...    ...  \n","1694293  6061aa83-7027-4f83-9f05-a29ce617209f     32  \n","1694338  6061aa83-7027-4f83-9f05-a29ce617209f     31  \n","1694339  6061aa83-7027-4f83-9f05-a29ce617209f     32  \n","1694408  6061aa83-7027-4f83-9f05-a29ce617209f     31  \n","1694409  6061aa83-7027-4f83-9f05-a29ce617209f     32  \n","\n","[690 rows x 6 columns]\n","\n","\n","Analysis of duplicate combinations:\n","Number of unique property_ids in duplicates: 628\n","Number of unique user_ids in duplicates: 7082\n","Number of unique values in duplicates: 7425\n","\n","Most common property_types in duplicates:\n","property_type\n","Multifamily     1085478\n","Education        299830\n","Mixed Use        160104\n","Office           138078\n","Condominiums     119446\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Example of a \"duplicate\" transaction_id: 1938384688\n","result = df_logs_metadata[df_logs_metadata['transaction_id'] == 1938384688]\n","print(result)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTD987pt7Ugu","executionInfo":{"status":"ok","timestamp":1737329825610,"user_tz":480,"elapsed":173,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"f286550f-ddbf-460e-cd82-4e5d9c40f20f"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["                                  property_id property_type  property_size  \\\n","1341517  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1341518  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1341519  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1341520  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1341620  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","...                                       ...           ...            ...   \n","1489393  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1490456  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1490457  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1490458  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","1490459  51865921-029c-47fa-a3e8-2e8d3e45f26d  Condominiums            100   \n","\n","         transaction_id                 timestamp  \\\n","1341517      1938384688 2024-10-05 10:08:00-07:00   \n","1341518      1938384688 2024-10-05 10:08:00-07:00   \n","1341519      1938384688 2024-10-05 10:08:00-07:00   \n","1341520      1938384688 2024-10-05 10:08:00-07:00   \n","1341620      1938384688 2024-10-05 10:08:00-07:00   \n","...                 ...                       ...   \n","1489393      1938384688 2024-10-03 20:08:00-07:00   \n","1490456      1938384688 2024-10-04 00:42:00-07:00   \n","1490457      1938384688 2024-10-04 00:42:00-07:00   \n","1490458      1938384688 2024-10-04 00:42:00-07:00   \n","1490459      1938384688 2024-10-04 00:42:00-07:00   \n","\n","                                      user_id unit  value  \\\n","1341517  38f8ffdb-36bd-440e-8a1d-ffe859799046    A      0   \n","1341518  38f8ffdb-36bd-440e-8a1d-ffe859799046    A      6   \n","1341519  38f8ffdb-36bd-440e-8a1d-ffe859799046    W   1248   \n","1341520  38f8ffdb-36bd-440e-8a1d-ffe859799046    W      0   \n","1341620  38f8ffdb-36bd-440e-8a1d-ffe859799046    A      0   \n","...                                       ...  ...    ...   \n","1489393  38f8ffdb-36bd-440e-8a1d-ffe859799046    W      0   \n","1490456  38f8ffdb-36bd-440e-8a1d-ffe859799046    A      0   \n","1490457  38f8ffdb-36bd-440e-8a1d-ffe859799046    A      6   \n","1490458  38f8ffdb-36bd-440e-8a1d-ffe859799046    W   1248   \n","1490459  38f8ffdb-36bd-440e-8a1d-ffe859799046    W      0   \n","\n","                   transaction_timestamp_pair  \\\n","1341517  1938384688_2024-10-05 10:08:00-07:00   \n","1341518  1938384688_2024-10-05 10:08:00-07:00   \n","1341519  1938384688_2024-10-05 10:08:00-07:00   \n","1341520  1938384688_2024-10-05 10:08:00-07:00   \n","1341620  1938384688_2024-10-05 10:08:00-07:00   \n","...                                       ...   \n","1489393  1938384688_2024-10-03 20:08:00-07:00   \n","1490456  1938384688_2024-10-04 00:42:00-07:00   \n","1490457  1938384688_2024-10-04 00:42:00-07:00   \n","1490458  1938384688_2024-10-04 00:42:00-07:00   \n","1490459  1938384688_2024-10-04 00:42:00-07:00   \n","\n","               transaction_timestamp_unit_combo  \n","1341517  1938384688_2024-10-05 10:08:00-07:00_A  \n","1341518  1938384688_2024-10-05 10:08:00-07:00_A  \n","1341519  1938384688_2024-10-05 10:08:00-07:00_W  \n","1341520  1938384688_2024-10-05 10:08:00-07:00_W  \n","1341620  1938384688_2024-10-05 10:08:00-07:00_A  \n","...                                         ...  \n","1489393  1938384688_2024-10-03 20:08:00-07:00_W  \n","1490456  1938384688_2024-10-04 00:42:00-07:00_A  \n","1490457  1938384688_2024-10-04 00:42:00-07:00_A  \n","1490458  1938384688_2024-10-04 00:42:00-07:00_W  \n","1490459  1938384688_2024-10-04 00:42:00-07:00_W  \n","\n","[1936 rows x 10 columns]\n"]}]},{"cell_type":"code","source":["# How often does transaction_id have multiple user_ids ?\n","# Only 1 example was found.\n","\n","# Count unique user_ids for each transaction_id\n","user_id_counts = df_logs_metadata.groupby('transaction_id')['user_id'].nunique()\n","\n","# Count unique property_ids for each transaction_id\n","property_id_counts = df_logs_metadata.groupby('transaction_id')['property_id'].nunique()\n","\n","# Calculate results\n","total_transactions = len(df_logs_metadata['transaction_id'].unique())\n","transactions_with_multiple_users = (user_id_counts > 1).sum()\n","transactions_with_multiple_properties = (property_id_counts > 1).sum()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions with multiple user_ids: {transactions_with_multiple_users}\")\n","print(f\"Percentage of transactions with multiple user_ids: {transactions_with_multiple_users/total_transactions*100:.2f}%\")\n","print(f\"Transactions with multiple property_ids: {transactions_with_multiple_properties}\")\n","print(f\"Percentage of transactions with multiple property_ids: {transactions_with_multiple_properties/total_transactions*100:.2f}%\")\n","\n","# If you want to see examples of transactions with multiple user_ids or property_ids:\n","print(\"\\nExamples of transactions with multiple user_ids:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(user_id_counts[user_id_counts > 1].index)].groupby('transaction_id')[['user_id', 'property_id']].head())\n","\n","print(\"\\nExamples of transactions with multiple property_ids:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(property_id_counts[property_id_counts > 1].index)].groupby('transaction_id')[['user_id', 'property_id']].head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btpv4cC37UmA","executionInfo":{"status":"ok","timestamp":1737330200133,"user_tz":480,"elapsed":3482,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"1c920c06-9f37-49fe-f54a-b1e85b8a77e3"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Total unique transactions: 158479\n","Transactions with multiple user_ids: 1\n","Percentage of transactions with multiple user_ids: 0.00%\n","Transactions with multiple property_ids: 1\n","Percentage of transactions with multiple property_ids: 0.00%\n","\n","Examples of transactions with multiple user_ids:\n","                                      user_id  \\\n","1356823  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1356852  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1362345  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1362805  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1363469  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","\n","                                  property_id  \n","1356823  21faa010-7a71-488a-a77c-30c463055104  \n","1356852  21faa010-7a71-488a-a77c-30c463055104  \n","1362345  21faa010-7a71-488a-a77c-30c463055104  \n","1362805  21faa010-7a71-488a-a77c-30c463055104  \n","1363469  21faa010-7a71-488a-a77c-30c463055104  \n","\n","Examples of transactions with multiple property_ids:\n","                                      user_id  \\\n","1356823  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1356852  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1362345  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1362805  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","1363469  78a7ecf3-b926-4a18-b9fe-34a4c24be7dd   \n","\n","                                  property_id  \n","1356823  21faa010-7a71-488a-a77c-30c463055104  \n","1356852  21faa010-7a71-488a-a77c-30c463055104  \n","1362345  21faa010-7a71-488a-a77c-30c463055104  \n","1362805  21faa010-7a71-488a-a77c-30c463055104  \n","1363469  21faa010-7a71-488a-a77c-30c463055104  \n"]}]},{"cell_type":"code","source":["# How often does transaction_id have multiple year or month values ?\n","# Looks like its concentrated at the begining or ending of a month\n","\n","# Extract year and month from the timestamp\n","df_logs_metadata['year'] = df_logs_metadata['timestamp'].dt.year\n","df_logs_metadata['month'] = df_logs_metadata['timestamp'].dt.month\n","\n","# Count unique years for each transaction_id\n","year_counts = df_logs_metadata.groupby('transaction_id')['year'].nunique()\n","\n","# Count unique months for each transaction_id\n","month_counts = df_logs_metadata.groupby('transaction_id')['month'].nunique()\n","\n","# Calculate results\n","total_transactions = len(df_logs_metadata['transaction_id'].unique())\n","transactions_with_multiple_years = (year_counts > 1).sum()\n","transactions_with_multiple_months = (month_counts > 1).sum()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions spanning multiple years: {transactions_with_multiple_years}\")\n","print(f\"Percentage of transactions spanning multiple years: {transactions_with_multiple_years/total_transactions*100:.2f}%\")\n","print(f\"Transactions spanning multiple months: {transactions_with_multiple_months}\")\n","print(f\"Percentage of transactions spanning multiple months: {transactions_with_multiple_months/total_transactions*100:.2f}%\")\n","\n","# If you want to see examples of transactions spanning multiple years or months:\n","print(\"\\nExamples of transactions spanning multiple years:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(year_counts[year_counts > 1].index)].groupby('transaction_id')[['timestamp', 'year', 'month']].head())\n","\n","print(\"\\nExamples of transactions spanning multiple months:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(month_counts[month_counts > 1].index)].groupby('transaction_id')[['timestamp', 'year', 'month']].head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1a3e54F7Uoq","executionInfo":{"status":"ok","timestamp":1737330363449,"user_tz":480,"elapsed":2157,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"fd2390cc-f3a2-43b5-e6cd-14e7fd5d8605"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Total unique transactions: 158479\n","Transactions spanning multiple years: 0\n","Percentage of transactions spanning multiple years: 0.00%\n","Transactions spanning multiple months: 1985\n","Percentage of transactions spanning multiple months: 1.25%\n","\n","Examples of transactions spanning multiple years:\n","Empty DataFrame\n","Columns: [timestamp, year, month]\n","Index: []\n","\n","Examples of transactions spanning multiple months:\n","                        timestamp  year  month\n","2       2024-09-30 02:38:00-07:00  2024      9\n","3       2024-09-30 02:38:00-07:00  2024      9\n","4       2024-09-30 02:38:00-07:00  2024      9\n","5       2024-09-30 02:38:00-07:00  2024      9\n","16      2024-09-30 02:37:00-07:00  2024      9\n","...                           ...   ...    ...\n","3264709 2024-12-01 03:18:00-08:00  2024     12\n","3264848 2024-12-01 03:12:00-08:00  2024     12\n","3265402 2024-12-01 02:45:00-08:00  2024     12\n","3265652 2024-12-01 11:26:00-08:00  2024     12\n","3267425 2024-12-01 09:48:00-08:00  2024     12\n","\n","[9906 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["# How often does transaction_id have multiple A or W non-0 values  ?\n","\n","# Filter for non-zero 'A' and 'W' values\n","df_non_zero = df_logs_metadata[(df_logs_metadata['value'] != 0) &\n","                               (df_logs_metadata['unit'].isin(['A', 'W']))]\n","\n","# Group by transaction_id and unit, then count non-zero values\n","value_counts = df_non_zero.groupby(['transaction_id', 'unit'])['value'].count().unstack(fill_value=0)\n","\n","# Count transactions with multiple non-zero 'A' values\n","multiple_A = (value_counts['A'] > 1).sum()\n","\n","# Count transactions with multiple non-zero 'W' values\n","multiple_W = (value_counts['W'] > 1).sum()\n","\n","# Total unique transactions\n","total_transactions = df_logs_metadata['transaction_id'].nunique()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions with multiple non-zero 'A' values: {multiple_A}\")\n","print(f\"Percentage of transactions with multiple non-zero 'A' values: {multiple_A/total_transactions*100:.2f}%\")\n","print(f\"Transactions with multiple non-zero 'W' values: {multiple_W}\")\n","print(f\"Percentage of transactions with multiple non-zero 'W' values: {multiple_W/total_transactions*100:.2f}%\")\n","\n","# Examples of transactions with multiple non-zero 'A' or 'W' values\n","print(\"\\nExamples of transactions with multiple non-zero 'A' values:\")\n","print(df_non_zero[df_non_zero['transaction_id'].isin(value_counts[value_counts['A'] > 1].index) &\n","                  (df_non_zero['unit'] == 'A')].groupby('transaction_id').head())\n","\n","print(\"\\nExamples of transactions with multiple non-zero 'W' values:\")\n","print(df_non_zero[df_non_zero['transaction_id'].isin(value_counts[value_counts['W'] > 1].index) &\n","                  (df_non_zero['unit'] == 'W')].groupby('transaction_id').head())\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"pWQlq-do7Uq-","executionInfo":{"status":"error","timestamp":1737335876615,"user_tz":480,"elapsed":329,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"f2696021-db48-4515-8e47-56b4a3d173dc"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_logs_metadata' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cd7ea9b43610>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Filter for non-zero 'A' and 'W' values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m df_non_zero = df_logs_metadata[(df_logs_metadata['value'] != 0) & \n\u001b[0m\u001b[1;32m      5\u001b[0m                                (df_logs_metadata['unit'].isin(['A', 'W']))]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_logs_metadata' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"maoVHBYB91fB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ja2XI3Dy91ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2_jWYUOP91me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YwqSEDfD7UwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"VcZxp8ni89AZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737329355461,"user_tz":480,"elapsed":20275,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"66fab6d7-3ae3-4c6b-dc63-bcfe4f9465a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Uniqueness check for ['user_id', 'timestamp', 'property_id', 'value', 'unit']:\n","Number of unique transaction_ids per group: 2\n","\n","Groups with multiple transaction_ids:\n","user_id                               timestamp                  property_id                           value  unit\n","18bf9385-29e3-4d8b-8b5b-406e80ed0d5a  2024-09-07 09:44:00-07:00  ab1035ee-0f5e-445a-bc53-b40c3cc5d8ff  0      A       2\n","4268c527-5452-4c7c-b10d-25373bafc1e3  2024-09-09 20:59:00-07:00  ac76db18-231d-4dad-928c-c42fcb9ae2f9  0      A       2\n","                                      2024-10-28 09:18:00-07:00  348c41a5-baaa-4275-b8c5-4d9e6ba8f97d  10     A       2\n","                                      2024-11-27 10:34:00-08:00  795cc566-8d58-4c54-b9ea-501671bb2b45  0      A       2\n","5378280e-d6d3-4c22-8304-37ee3993ca7e  2024-10-15 12:20:00-07:00  9af54d24-5521-46d4-8db8-6d42227efb04  30     A       2\n","8fd904c0-b577-4fd5-9409-5e35a6440337  2024-09-21 23:39:00-07:00  53e9cab2-aed2-4c35-ab39-e8375825e6fb  0      A       2\n","be0a2ba3-69bd-4518-a64a-166e12e3116c  2024-12-04 12:38:00-08:00  b5da81dd-e999-4e23-a8d0-466e92b34576  10     A       2\n","                                      2024-12-04 14:01:00-08:00  b5da81dd-e999-4e23-a8d0-466e92b34576  0      A       2\n","ceb38458-67d4-4325-a448-939dd4d4c4e2  2024-11-06 08:24:00-08:00  d2ac08a9-647d-48ed-8979-f3450bee2fbf  31     A       2\n","dfae965d-32c9-4f60-926e-48fb2f80ada4  2024-10-16 11:25:00-07:00  795cc566-8d58-4c54-b9ea-501671bb2b45  15     A       2\n","                                      2024-10-23 14:29:00-07:00  795cc566-8d58-4c54-b9ea-501671bb2b45  0      A       2\n","Name: transaction_id, dtype: int64\n","\n","Uniqueness check for ['user_id', 'property_id', 'value', 'unit']:\n","Number of unique transaction_ids per group: 223\n","\n","Groups with multiple transaction_ids:\n","user_id                               property_id                           value  unit\n","00007f82-7d02-4dda-bfe3-75df83b44e21  4cb8af8c-e0ed-41d9-8c08-6e66c4125cdd  0      A        5\n","                                                                            1      A        7\n","                                                                            10     A        2\n","                                                                            15     A        7\n","                                                                            16     A        2\n","                                                                                           ..\n","ffde88c1-41da-43f7-a1aa-96400d6629ac  c6b9d065-867d-41c8-b586-1e287d5179c8  3545   W        2\n","                                                                            6656   W       43\n","ffecb0ea-c479-4311-b958-c5a29a64992f  aa7e505a-1d72-4a3a-b944-70955c418977  0      A        3\n","                                                                            31     A        4\n","                                                                            32     A        8\n","Name: transaction_id, Length: 78562, dtype: int64\n","\n","Uniqueness check for ['timestamp', 'property_id', 'value', 'unit']:\n","Number of unique transaction_ids per group: 5\n","\n","Groups with multiple transaction_ids:\n","timestamp                  property_id                           value  unit\n","2024-08-31 18:05:00-07:00  9a06f545-1ab0-476d-8d43-1b5101cbd551  0      A       2\n","2024-08-31 18:23:00-07:00  8bbdcff9-06b0-44cf-8c9f-a9ddce51a317  31     A       2\n","2024-08-31 18:39:00-07:00  e97af51d-a077-4af8-8061-56a530dc6259  32     A       2\n","2024-08-31 20:42:00-07:00  6c026229-a314-4e01-8a01-4838782e7d76  0      A       2\n","2024-08-31 20:53:00-07:00  24d7e3f1-9d63-457c-9f0c-a50b0bc08ebc  32     A       2\n","                                                                               ..\n","2024-12-31 14:07:00-08:00  fb58c76d-2cd9-4095-b693-b3d6b14becf0  6656   W       2\n","2024-12-31 14:30:00-08:00  b5da81dd-e999-4e23-a8d0-466e92b34576  16     A       2\n","2024-12-31 14:43:00-08:00  907a7e25-d890-4953-97ea-9ee9b994bc21  0      A       2\n","2024-12-31 14:47:00-08:00  b58b8730-a532-4fe6-93fc-4bc9e822d3ea  0      A       2\n","2024-12-31 14:50:00-08:00  b5da81dd-e999-4e23-a8d0-466e92b34576  32     A       2\n","Name: transaction_id, Length: 47934, dtype: int64\n","\n","Uniqueness check for ['user_id', 'timestamp', 'value', 'unit']:\n","Number of unique transaction_ids per group: 2\n","\n","Groups with multiple transaction_ids:\n","user_id                               timestamp                  value  unit\n","18bf9385-29e3-4d8b-8b5b-406e80ed0d5a  2024-09-07 09:44:00-07:00  0      A       2\n","4268c527-5452-4c7c-b10d-25373bafc1e3  2024-09-09 20:59:00-07:00  0      A       2\n","                                      2024-10-08 19:18:00-07:00  0      A       2\n","                                      2024-10-09 00:16:00-07:00  0      A       2\n","                                      2024-10-15 19:47:00-07:00  0      A       2\n","                                      2024-10-28 09:18:00-07:00  10     A       2\n","                                      2024-11-12 10:50:00-08:00  31     A       2\n","                                                                 32     A       2\n","                                      2024-11-27 10:34:00-08:00  0      A       2\n","5378280e-d6d3-4c22-8304-37ee3993ca7e  2024-10-15 12:20:00-07:00  30     A       2\n","62485ad3-c97c-49d6-9be7-2a1073a38916  2024-09-03 11:20:00-07:00  7      A       2\n","                                      2024-09-03 11:36:00-07:00  8      A       2\n","                                      2024-09-03 12:31:00-07:00  8      A       2\n","8fd904c0-b577-4fd5-9409-5e35a6440337  2024-09-21 23:39:00-07:00  0      A       2\n","be0a2ba3-69bd-4518-a64a-166e12e3116c  2024-12-04 12:38:00-08:00  10     A       2\n","                                      2024-12-04 14:01:00-08:00  0      A       2\n","ceb38458-67d4-4325-a448-939dd4d4c4e2  2024-11-06 08:24:00-08:00  31     A       2\n","dfae965d-32c9-4f60-926e-48fb2f80ada4  2024-10-16 11:25:00-07:00  15     A       2\n","                                      2024-10-23 14:29:00-07:00  0      A       2\n","Name: transaction_id, dtype: int64\n","\n","Overall unique counts:\n","Total rows: 3289867\n","Unique transaction_ids: 158479\n","Unique combinations: 2\n"]}],"source":["# Check uniqueness of transaction_id across different column combinations\n","def check_uniqueness(df, columns):\n","    unique_combinations = df.groupby(columns)['transaction_id'].nunique()\n","    print(f\"\\nUniqueness check for {columns}:\")\n","    print(\"Number of unique transaction_ids per group:\", unique_combinations.max())\n","\n","    if unique_combinations.max() > 1:\n","        print(\"\\nGroups with multiple transaction_ids:\")\n","        print(unique_combinations[unique_combinations > 1])\n","\n","# Check various column combinations\n","check_columns = [\n","    ['user_id', 'timestamp', 'property_id', 'value', 'unit'],\n","    ['user_id', 'property_id', 'value', 'unit'],\n","    ['timestamp', 'property_id', 'value', 'unit'],\n","    ['user_id', 'timestamp', 'value', 'unit']\n","]\n","\n","for cols in check_columns:\n","    check_uniqueness(df_logs_metadata, cols)\n","\n","# Additional overall statistics\n","print(\"\\nOverall unique counts:\")\n","print(\"Total rows:\", len(df_logs_metadata))\n","print(\"Unique transaction_ids:\", df_logs_metadata['transaction_id'].nunique())\n","print(\"Unique combinations:\",\n","    df_logs_metadata.groupby(['user_id', 'timestamp', 'property_id', 'value', 'unit'])['transaction_id'].nunique().max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeuNqAdJ59_z"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming df_logs_metadata is your existing DataFrame\n","\n","# Create a new column with the combination of transaction_id and timestamp\n","df_logs_metadata['transaction_timestamp_pair'] = df_logs_metadata['transaction_id'].astype(str) + '_' + df_logs_metadata['timestamp'].astype(str)\n","\n","# Count the number of occurrences of each pair\n","pair_counts = df_logs_metadata['transaction_timestamp_pair'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = pair_counts[pair_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"transaction_id and timestamp pairs are unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate transaction_id and timestamp pairs.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique pairs: {len(pair_counts)}\")\n","print(f\"Are all pairs unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_pairs = duplicates.index[:5]  # Get the first 5 duplicate pairs\n","    for pair in duplicate_pairs:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'] == pair][['transaction_id', 'timestamp', 'property_id', 'user_id', 'value', 'unit']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate pairs:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","    print(f\"Number of unique units in duplicates: {duplicate_df['unit'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Histogram of timestamps per transaction_id\n","timestamps_per_transaction = df_logs_enriched.groupby('transaction_id')['timestamp'].nunique()\n","\n","plt.figure(figsize=(12, 5))\n","\n","# Left plot - log scale\n","plt.subplot(1, 2, 1)\n","plt.hist(timestamps_per_transaction, bins=np.logspace(np.log10(1), np.log10(timestamps_per_transaction.max()), 50))\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.title('Unique Timestamps per Transaction ID (Log Scale)')\n","plt.xlabel('Number of Unique Timestamps')\n","plt.ylabel('Count of Transaction IDs')\n","plt.grid(True)\n","\n","# Right plot - properties per transaction\n","properties_per_transaction = df_logs_enriched.groupby('transaction_id')['property_id'].nunique()\n","plt.subplot(1, 2, 2)\n","plt.hist(properties_per_transaction, bins=50)\n","plt.title('Unique Properties per Transaction ID')\n","plt.xlabel('Number of Unique Properties')\n","plt.ylabel('Count of Transaction IDs')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTimestamps per transaction summary:\")\n","print(timestamps_per_transaction.describe())\n","print(\"\\nProperties per transaction summary:\")\n","print(properties_per_transaction.describe())\n","\n","# Print additional context\n","print(\"\\nTotal number of unique transactions:\", len(timestamps_per_transaction))\n","print(\"Total number of unique properties:\", df_logs_enriched['property_id'].nunique())\n","print(\"Total number of timestamps:\", df_logs_enriched['timestamp'].nunique())"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","source":["import pandas as pd\n","import pytz\n","import numpy as np\n","from datetime import datetime\n","\n","def convert_to_pst_components(df):\n","    \"\"\"Convert timestamp to PST and extract components.\"\"\"\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')\n","    df['timestamp'] = df['timestamp'].dt.tz_convert('US/Pacific')\n","\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","    df['minute'] = df['timestamp'].dt.minute\n","\n","    return df\n","\n","def add_day_info(df):\n","    \"\"\"Add day of week and weekend indicator.\"\"\"\n","    df['day_of_week'] = df['timestamp'].dt.dayofweek + 1\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)\n","    return df\n","\n","def get_nearest_holiday(df):\n","    \"\"\"Calculate proximity to major US holidays.\"\"\"\n","    major_holidays = {\n","        '2024-01-01': \"New Year's Day\",\n","        '2024-01-15': \"Martin Luther King Jr. Day\",\n","        '2024-02-19': \"Presidents Day\",\n","        '2024-05-27': \"Memorial Day\",\n","        '2024-07-04': \"Independence Day\",\n","        '2024-09-02': \"Labor Day\",\n","        '2024-11-28': \"Thanksgiving\",\n","        '2024-12-25': \"Christmas\",\n","        '2025-01-01': \"New Year's Day\",\n","        '2025-01-20': \"Martin Luther King Jr. Day\",\n","        '2025-02-17': \"Presidents Day\",\n","        '2025-05-26': \"Memorial Day\",\n","        '2025-07-04': \"Independence Day\",\n","        '2025-09-01': \"Labor Day\",\n","        '2025-11-27': \"Thanksgiving\",\n","        '2025-12-25': \"Christmas\"\n","    }\n","\n","    holiday_dates = pd.to_datetime(list(major_holidays.keys())).sort_values()\n","    holiday_dates_array = holiday_dates.values\n","    dates_array = pd.to_datetime(df['timestamp'].dt.date.unique()).values\n","    holiday_lookup = {}\n","\n","    for date in dates_array:\n","        days_diff = np.abs((holiday_dates_array - date).astype('timedelta64[D]').astype(int))\n","        closest_idx = np.argmin(days_diff)\n","        closest_date = holiday_dates[closest_idx]\n","\n","        holiday_lookup[pd.Timestamp(date).date()] = {\n","            'days_to_nearest_holiday': days_diff[closest_idx],\n","            'nearest_holiday_date': closest_date,\n","            'nearest_holiday_name': major_holidays[closest_date.strftime('%Y-%m-%d')]\n","        }\n","\n","    # Create and merge holiday information\n","    df['date'] = df['timestamp'].dt.date\n","    result = pd.DataFrame.from_dict(holiday_lookup, orient='index')\n","    result.index = pd.to_datetime(result.index).date\n","    df = df.merge(result, left_on='date', right_index=True)\n","    df = df.drop('date', axis=1)\n","\n","    return df\n","\n","def process_timestamps(input_path, output_path):\n","    \"\"\"Main function to process all timestamp-related features.\"\"\"\n","    df = pd.read_pickle(input_path)\n","    df = convert_to_pst_components(df)\n","    df = add_day_info(df)\n","    df = get_nearest_holiday(df)\n","    df.to_pickle(output_path)\n","    return df\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","\n","    df_processed = process_timestamps(input_path, output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"th5Y2zwzXBm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Add user and usage data\n","import pandas as pd\n","\n","def add_unique_user_counts(df, group_cols, user_col):\n","    \"\"\"Count unique users per group.\"\"\"\n","    user_counts = df.groupby(group_cols)[user_col].nunique().reset_index()\n","    user_counts.rename(columns={user_col: 'unique_user_count'}, inplace=True)\n","    df = df.merge(user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols):\n","    \"\"\"Add sums of values for each unit type by group.\"\"\"\n","    # Calculate sums for each unit type\n","    sums = df.groupby([*group_cols, 'unit_encoded'])['value'].sum().reset_index()\n","\n","    # Pivot to create separate columns for A and W\n","    sums = sums.pivot(\n","        index=group_cols,\n","        columns='unit_encoded',\n","        values='value'\n","    ).reset_index()\n","\n","    # Rename columns\n","    sums.rename(\n","        columns={\n","            0: 'sum_value_A',   # Amps were encoded as 0\n","            1: 'sum_value_Wh'   # Watts were encoded as 1\n","        },\n","        inplace=True\n","    )\n","\n","    return df.merge(sums, on=group_cols, how='left')\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl'\n","\n","    # Read data\n","    df = pd.read_pickle(input_path)\n","\n","    # Extract datetime components\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","\n","    # Define grouping columns after datetime components are created\n","    group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","\n","    # Add user and usage metrics\n","    df = add_unique_user_counts(df, group_cols, 'user_id')\n","    df = add_usage_sums(df, group_cols)\n","\n","    # Save results\n","    df.to_pickle(output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"5d2mRKE6U2VA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')"],"metadata":{"id":"l3w1u1r0nADm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"kT15u3G6nWmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWLnoqvaKEB6"},"outputs":[],"source":["# Halt\n","sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric, handling errors\n","columns_to_analyze = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Create a clean dataframe for analysis\n","df_clean = df[columns_to_analyze].copy()\n","\n","# Convert each column to numeric, handling errors\n","for col in columns_to_analyze:\n","    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n","\n","    # Print info about conversion\n","    print(f\"\\nColumn: {col}\")\n","    print(f\"Null values after conversion: {df_clean[col].isnull().sum()}\")\n","    print(f\"Sample unique values: {df_clean[col].dropna().sample(5).tolist()}\")\n","\n","# Calculate correlations for cleaned numeric columns\n","correlations = df_clean.corr()\n","\n","# Create correlation heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","# Basic summary statistics\n","print(\"\\nSummary Statistics:\")\n","print(df_clean.describe())\n","\n","# Check for patterns across categorical variables\n","print(\"\\nMean energy consumption (sum_value_Wh) by:\")\n","print(\"\\nDay of Week:\")\n","print(df_clean.groupby('day_of_week')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nHour of Day:\")\n","print(df_clean.groupby('hour')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nMonth:\")\n","print(df_clean.groupby('month')['sum_value_Wh'].mean().sort_values(ascending=False))"],"metadata":{"id":"UsCbfrlAE1-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load and clean data\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric\n","df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Group by hour and calculate various statistics\n","hourly_stats = df.groupby('hour').agg({\n","    'sum_value_Wh': ['mean', 'median', 'std', 'count'],\n","    'unique_user_count': 'mean'\n","}).round(2)\n","\n","# Create a figure with two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n","\n","# Plot 1: Energy consumption pattern\n","ax1.plot(hourly_stats[('sum_value_Wh', 'mean')], marker='o')\n","ax1.fill_between(range(24),\n","                 hourly_stats[('sum_value_Wh', 'mean')] - hourly_stats[('sum_value_Wh', 'std')],\n","                 hourly_stats[('sum_value_Wh', 'mean')] + hourly_stats[('sum_value_Wh', 'std')],\n","                 alpha=0.2)\n","ax1.set_title('24-Hour Energy Consumption Pattern')\n","ax1.set_xlabel('Hour of Day')\n","ax1.set_ylabel('Average Energy Consumption (Wh)')\n","ax1.grid(True)\n","\n","# Plot 2: Users vs Energy\n","ax2.scatter(hourly_stats[('unique_user_count', 'mean')],\n","           hourly_stats[('sum_value_Wh', 'mean')],\n","           alpha=0.6)\n","# Add hour labels to each point\n","for i in range(24):\n","    ax2.annotate(str(i),\n","                (hourly_stats[('unique_user_count', 'mean')][i],\n","                 hourly_stats[('sum_value_Wh', 'mean')][i]))\n","ax2.set_title('Users vs Energy Consumption by Hour')\n","ax2.set_xlabel('Average Number of Users')\n","ax2.set_ylabel('Average Energy Consumption (Wh)')\n","ax2.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print detailed statistics for peak hours\n","peak_hours = hourly_stats.nlargest(5, ('sum_value_Wh', 'mean'))\n","print(\"\\nPeak Hours Analysis:\")\n","print(peak_hours)\n","\n","# Calculate relative increase during peak vs off-peak\n","off_peak_mean = hourly_stats.loc[0:6, ('sum_value_Wh', 'mean')].mean()\n","peak_mean = hourly_stats.loc[16:19, ('sum_value_Wh', 'mean')].mean()\n","increase_factor = peak_mean / off_peak_mean\n","\n","print(f\"\\nPeak vs Off-peak Analysis:\")\n","print(f\"Average off-peak consumption (midnight-6am): {off_peak_mean:.2f} Wh\")\n","print(f\"Average peak consumption (4pm-7pm): {peak_mean:.2f} Wh\")\n","print(f\"Peak hours consume {increase_factor:.1f}x more energy than off-peak hours\")"],"metadata":{"id":"kuoFSz6pGHoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert sum_value_Wh to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","\n","# Create a single property type column for easier plotting\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Create box plot\n","plt.figure(figsize=(15, 8))\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df)\n","plt.xticks(rotation=45, ha='right')\n","plt.title('Energy Consumption Distribution by Property Type')\n","plt.xlabel('Property Type')\n","plt.ylabel('Energy Consumption (Wh)')\n","plt.tight_layout()\n","plt.show()\n","\n","# Print basic stats for each property type\n","print(\"\\nBasic statistics by property type:\")\n","stats = df.groupby('property_type')['sum_value_Wh'].describe()\n","print(stats)\n","\n","# Count number of observations for each property type\n","print(\"\\nNumber of observations per property type:\")\n","counts = df['property_type'].value_counts()\n","print(counts)"],"metadata":{"id":"2-BGTCfAGHwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Create a single property type column\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Calculate energy per user\n","df['energy_per_user'] = df['sum_value_Wh'] / df['unique_user_count']\n","\n","# Create two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n","\n","# Plot 1: Total Energy\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df, ax=ax1)\n","ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n","ax1.set_title('Total Energy Consumption by Property Type')\n","ax1.set_xlabel('Property Type')\n","ax1.set_ylabel('Energy Consumption (Wh)')\n","\n","# Plot 2: Energy per User\n","sns.boxplot(x='property_type', y='energy_per_user', data=df, ax=ax2)\n","ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n","ax2.set_title('Energy Consumption per User by Property Type')\n","ax2.set_xlabel('Property Type')\n","ax2.set_ylabel('Energy Consumption per User (Wh/user)')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print statistics\n","print(\"\\nStatistics by property type:\")\n","stats = df.groupby('property_type').agg({\n","    'sum_value_Wh': ['count', 'mean'],\n","    'unique_user_count': 'mean',\n","    'energy_per_user': 'mean'\n","}).round(2)\n","\n","print(stats)"],"metadata":{"id":"ZLVLTbCcGH2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First, let's look at the raw data before any processing\n","print(\"Original data counts by property type:\")\n","for col in prop_type_cols:\n","    print(f\"\\n{col}:\")\n","    print(f\"Number of 1s: {df[col].sum()}\")\n","    print(f\"Number of non-null values: {df[col].count()}\")\n","\n","# Let's also check for nulls in key columns\n","print(\"\\nNull values in key columns:\")\n","print(df[['sum_value_Wh', 'unique_user_count']].isnull().sum())\n","\n","# Let's look at the data before any type conversion\n","print(\"\\nSample of raw data before conversion:\")\n","sample_data = df[['property_type', 'sum_value_Wh', 'unique_user_count']].head(10)\n","print(sample_data)\n","\n","# Check data types of key columns\n","print(\"\\nData types of columns:\")\n","print(df.dtypes)"],"metadata":{"id":"Xt5KFkm2GH92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8ZOVtYORGH_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZHlAIGuNGICQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","def clean_numeric_string(value):\n","    \"\"\"Clean numeric string by taking the first valid number.\"\"\"\n","    if pd.isna(value):\n","        return 0\n","    # Convert to string if not already\n","    value = str(value)\n","    # Find first number (integer or decimal)\n","    parts = value.split('.')\n","    if not parts:\n","        return 0\n","    try:\n","        # Take first valid number\n","        return float(parts[0])\n","    except ValueError:\n","        return 0\n","\n","# Clean and convert sum_value columns\n","df['sum_value_A'] = df['sum_value_A'].apply(clean_numeric_string).astype(int)\n","df['sum_value_Wh'] = df['sum_value_Wh'].apply(clean_numeric_string).astype(int)\n","df['unique_user_count'] = df['unique_user_count'].astype(int)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy\n","X.replace([np.inf, -np.inf], 0, inplace=True)\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)"]},{"cell_type":"code","source":["# Drop suspect features and run VIF again\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Drop suspect features\n","df = df.drop(['day_weekend', 'year'], axis=1)\n","\n","# Define numerical columns\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[numerical_columns].copy()\n","for column in numerical_columns:\n","    # Convert to numeric, coerce any non-numeric values to NaN\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","\n","    # Fill NaN values with the median of the column\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Sort VIF values in descending order\n","vif_data = vif_data.sort_values('VIF', ascending=False)\n","\n","# Display the VIF values\n","print(\"\\nVariance Inflation Factors:\")\n","print(vif_data)"],"metadata":{"id":"oMvFjeGOqEtc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for regression/ANOVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Define the dependent variable (y) - in this case let's use sum_value_Wh as our target\n","y = \"sum_value_Wh\"\n","\n","# Define independent variables (features to compare groups)\n","independent_vars = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\"\n","    #\"sum_value_A\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[independent_vars].copy()\n","for column in independent_vars:\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Convert y to numeric\n","df[y] = pd.to_numeric(df[y], errors='coerce')\n","df[y] = df[y].fillna(df[y].median())\n","\n","# Function to run one-way ANOVA for each independent variable against y\n","def run_anova_with_target(df, independent_vars, y):\n","    results = []\n","\n","    for var in independent_vars:\n","        # Create groups based on the independent variable\n","        groups = []\n","        # Create 5 groups using quantiles for continuous variables\n","        df['group'] = pd.qcut(df[var], q=5, labels=['G1', 'G2', 'G3', 'G4', 'G5'])\n","\n","        # Get the y values for each group\n","        for group in df['group'].unique():\n","            groups.append(df[df['group'] == group][y].values)\n","\n","        # Perform one-way ANOVA\n","        f_stat, p_val = stats.f_oneway(*groups)\n","\n","        results.append({\n","            'Independent Variable': var,\n","            'F-statistic': f_stat,\n","            'p-value': p_val\n","        })\n","\n","    return pd.DataFrame(results)\n","\n","# Run ANOVA\n","anova_results = run_anova_with_target(df, independent_vars, y)\n","\n","# Sort results by p-value\n","anova_results_sorted = anova_results.sort_values('p-value')\n","\n","# Display results\n","pd.set_option('display.float_format', lambda x: '{:.10f}'.format(x) if x < 0.0001 else '{:.4f}'.format(x))\n","print(f\"\\nOne-way ANOVA Results (dependent variable: {y}):\")\n","print(anova_results_sorted)\n","\n","# Add significance indicators\n","anova_results_sorted['Significance'] = ['***' if p < 0.001\n","                                      else '**' if p < 0.01\n","                                      else '*' if p < 0.05\n","                                      else 'ns' for p in anova_results_sorted['p-value']]\n","\n","print(\"\\nSignificance levels:\")\n","print(\"***: p < 0.001\")\n","print(\"**: p < 0.01\")\n","print(\"*: p < 0.05\")\n","print(\"ns: not significant\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpSYEsmq3VUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umUOsBSn3VXA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-e5kpAZ3VZS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F20AGhU3Vbq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1F9VbEO3VhO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"J89LPzgmdzuZ"},"source":["### Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"xvFZX6JKKyHa"},"source":["####AWS Tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUKjVft2kxIk"},"outputs":[],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Xz1nKBmBM2nA"},"source":["###Create a table for all property info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7enpOKwkM2Gu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup_2.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names to lowercase and strip whitespace\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the left join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='left',  # Use 'left' join to keep all rows from properties and add property_type name where available\n","    left_on='property_type',  # Assuming 'property_type' is the column in properties.csv\n","    right_on='id',  # Assuming 'id' is the column in property_types.csv\n","    suffixes=('_property', '_type')\n",")\n","\n","# Keep all columns from properties and just add the 'name_type' column as 'property_type'\n","property_lookup['property_type'] = property_lookup['name_type']\n","\n","# Drop the 'name_type' column, since we already added it as 'property_type'\n","property_lookup = property_lookup.drop(columns=['name_type'])\n","\n","# Rename 'id_property' column to 'property_id'\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE633fu_M2Iz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrQU1kppM2LR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOTj1nOYM2N5"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaoKGEHJw0uL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import logging\n","from itertools import combinations\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Path to the directory containing the CSV files\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n","\n","# List of tables (as per your previous code)\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Function to load CSV files into DataFrames\n","def load_dataframes(tables):\n","    dataframes = {}\n","    for table in tables:\n","        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","            dataframes[table] = df\n","            logging.info(f\"Loaded data for table: {table}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading data for table {table}: {e}\")\n","    return dataframes\n","\n","# Function to find strict join matches\n","def find_strict_joins(df1, df2, table1_name, table2_name):\n","    strict_joins = []\n","    # Iterate over all column pairs\n","    for col1 in df1.columns:\n","        for col2 in df2.columns:\n","            if df1[col1].dtype == df2[col2].dtype:\n","                # Perform the join\n","                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n","                # Check if all rows in df1 are in the joined DataFrame\n","                if len(joined_df) == len(df1):\n","                    strict_joins.append((col1, col2))\n","                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n","    return strict_joins\n","\n","# Main function to perform the strict join analysis\n","def analyze_strict_joins(tables):\n","    dataframes = load_dataframes(tables)\n","    results = {}\n","    table_pairs = combinations(tables, 2)\n","\n","    for table1, table2 in table_pairs:\n","        df1 = dataframes.get(table1)\n","        df2 = dataframes.get(table2)\n","\n","        if df1 is not None and df2 is not None:\n","            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n","            joins = find_strict_joins(df1, df2, table1, table2)\n","            if joins:\n","                results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n","\n","    return results\n","\n","# Run the strict join analysis\n","strict_join_results = analyze_strict_joins(tables)\n","\n","# Print the results\n","for table_pair, joins in strict_join_results.items():\n","    print(f\"\\nStrict joins for {table_pair}:\")\n","    for col1, col2 in joins:\n","        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n","\n","if not strict_join_results:\n","    print(\"No strict joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqQ_iKw5dn0M"},"outputs":[],"source":["import os\n","import logging\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection string for SQLAlchemy\n","connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n","engine = create_engine(connection_string)\n","\n","# Function to fetch column names for a table\n","def get_columns(table_name):\n","    try:\n","        query = f\"\"\"\n","        SELECT column_name, data_type\n","        FROM information_schema.columns\n","        WHERE table_name = '{table_name}';\n","        \"\"\"\n","        with engine.connect() as connection:\n","            df = pd.read_sql_query(query, connection)\n","        return df[['column_name', 'data_type']].to_dict('records')\n","    except Exception as e:\n","        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n","        return []\n","\n","# Function to test join logic between two tables\n","def test_joins(table1, table2, attempts=3):\n","    columns_table1 = get_columns(table1)\n","    columns_table2 = get_columns(table2)\n","    successful_joins = []\n","\n","    for col1 in columns_table1:\n","        for col2 in columns_table2:\n","            # Only test joins on matching data types\n","            if col1['data_type'] == col2['data_type']:\n","                success_count = 0\n","                for _ in range(attempts):  # Attempt the join multiple times\n","                    query = f\"\"\"\n","                    SELECT *\n","                    FROM {table1} t1\n","                    INNER JOIN {table2} t2\n","                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n","                    LIMIT 1;  -- Test with one row at a time\n","                    \"\"\"\n","                    try:\n","                        with engine.connect() as connection:\n","                            df = pd.read_sql_query(query, connection)\n","                            if not df.empty:\n","                                success_count += 1\n","                    except Exception as e:\n","                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n","\n","                if success_count == attempts:  # Only count as successful if all attempts work\n","                    successful_joins.append((col1['column_name'], col2['column_name']))\n","                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n","\n","    return successful_joins\n","\n","# Cross-check join fields for all table pairs\n","tables = [\n","    \"users\", \"ocpp_sub_session\"\n","]\n","\n","results = {}\n","\n","for i, table1 in enumerate(tables):\n","    for table2 in tables[i+1:]:\n","        logging.info(f\"Testing joins between {table1} and {table2}\")\n","        joins = test_joins(table1, table2)\n","        if joins:\n","            results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.info(f\"No join found between {table1} and {table2}\")\n","\n","# Print results\n","for table_pair, joins in results.items():\n","    print(f\"Successful joins for {table_pair}: {joins}\")\n","\n","if not results:\n","    print(\"No successful joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU41V4jQdn2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMXMJghgdn5H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuwvrH2ndn7Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VIJZs8fdn9k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kH2jY7TdoAK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ex8BAejxDoB"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Example dataframe (assuming df['message'] contains the raw strings)\n","# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n","def clean_message(msg):\n","    try:\n","        # Remove the prefix\n","        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n","\n","        # Attempt to load the cleaned message as JSON\n","        return json.loads(msg_cleaned)\n","    except (json.JSONDecodeError, TypeError):\n","        # If the message cannot be decoded as JSON, return None or handle as needed\n","        return None\n","\n","# Apply the function to the 'message' column\n","df['message'] = df['message'].apply(clean_message)\n","\n","# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n","df = df[df['message'].notna()]\n","\n","# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n","flattened_rows = []\n","\n","for idx, row in df.iterrows():\n","    message = row['message']  # Now this is a valid JSON object\n","\n","    # Extract top-level fields\n","    connector_id = message.get('connectorId')\n","    transaction_id = message.get('transactionId')\n","\n","    # Keep the 'meterValue' field as is (as a list of dicts)\n","    meter_value = message.get('meterValue', [])\n","\n","    # Add a row to the flattened list, including the nested 'meterValue' list\n","    flattened_rows.append({\n","        '_time': row['time'],  # Retain the original timestamp from the dataframe\n","        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","        'connectorId': connector_id,\n","        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n","    })\n","\n","# Step 2: Create a new DataFrame from the flattened rows\n","flattened_df = pd.DataFrame(flattened_rows)\n","\n","# Display the resulting DataFrame\n","print(flattened_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbk_sGLxZ_Z"},"outputs":[],"source":["import pandas as pd\n","\n","# Set pandas options to display the full content of any column (e.g., 'meterValue')\n","pd.set_option('display.max_colwidth', None)\n","\n","# Now, display the full content of the 'meterValue' column for the first 5 rows\n","print(flattened_df['meterValue'].head(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhRPDO5zJH0"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to hold the expanded rows\n","expanded_rows = []\n","\n","# Iterate over each row in the dataframe\n","for idx, row in flattened_df.iterrows():\n","    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n","\n","    # For each meter value entry (there should be one timestamp and a list of measurements)\n","    for meter in meter_values:\n","        timestamp = meter['timestamp']  # Extract the timestamp\n","\n","        # Initialize values for each measurement type\n","        watt_hours_value = None  # WattHours\n","        amps_value = None        # Amps (Current)\n","        voltage_value = None     # Voltage (Volts)\n","\n","        # Iterate over the sampledValue list (which contains the three measurements)\n","        for sample in meter['sampledValue']:\n","            # Check the 'unit' to assign the value to the correct column\n","            if sample['unit'] == 'Wh':  # WattHours\n","                watt_hours_value = sample['value']\n","            elif sample['unit'] == 'A':  # Amps (Current)\n","                amps_value = sample['value']\n","            elif sample['unit'] == 'V':  # Volts (Voltage)\n","                voltage_value = sample['value']\n","\n","        # Append the expanded row with the extracted values\n","        expanded_rows.append({\n","            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n","            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","            'connectorId': row['connectorId'],  # Connector ID\n","            'timestamp': timestamp,  # Timestamp from the meter value\n","            'WattHours': watt_hours_value,  # Renamed to WattHours\n","            'Amps': amps_value,  # Keep Amps as the column name\n","            'Voltage': voltage_value  # Value for Voltage (V)\n","        })\n","\n","# Create a new DataFrame from the expanded rows\n","expanded_df = pd.DataFrame(expanded_rows)\n","\n","# Convert the numeric columns to appropriate types (float)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Display the resulting DataFrame\n","print(expanded_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcgRo8Dz1Nm"},"outputs":[],"source":["# Ensure all columns are numeric (in case there are any string values left)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Classify values as 0 or > 0 for each of the measurements\n","expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n","\n","# Set up the plot\n","plt.figure(figsize=(18, 6))\n","\n","# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n","plt.subplot(1, 3, 1)\n","sns.countplot(data=expanded_df, x='WattHours_Class')\n","plt.title('Count of Rows with WattHours: 0 vs > 0')\n","plt.xlabel('WattHours Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 2)\n","sns.countplot(data=expanded_df, x='Amps_Class')\n","plt.title('Count of Rows with Amps: 0 vs > 0')\n","plt.xlabel('Amps Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 3)\n","sns.countplot(data=expanded_df, x='Voltage_Class')\n","plt.title('Count of Rows with Voltage: 0 vs > 0')\n","plt.xlabel('Voltage Class')\n","plt.ylabel('Count')\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsRrQLIz1QQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure that the '_time' column is in datetime format\n","expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n","\n","# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Drop rows where any of the values are missing\n","expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create the figure and axes for the plots\n","plt.figure(figsize=(18, 6))\n","\n","# Plot Amps over time\n","plt.subplot(1, 3, 1)\n","plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n","plt.title('Amps over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Amps')\n","plt.xticks(rotation=45)\n","\n","# Plot WattHours over time\n","plt.subplot(1, 3, 2)\n","plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n","plt.title('WattHours over Time')\n","plt.xlabel('Time')\n","plt.ylabel('WattHours')\n","plt.xticks(rotation=45)\n","\n","# Plot Voltage over time\n","plt.subplot(1, 3, 3)\n","plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n","plt.title('Voltage over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Voltage')\n","plt.xticks(rotation=45)\n","\n","# Adjust layout to avoid overlap of labels\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi2OjXK_2hXK"},"outputs":[],"source":["expanded_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TbILosnz1Sh"},"outputs":[],"source":["# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n","descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n","\n","# Display the statistics\n","print(descriptive_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zZUwehE16FW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure with 3 subplots (1 row, 3 columns)\n","plt.figure(figsize=(18, 6))\n","\n","# Plot for Amps\n","plt.subplot(1, 3, 1)\n","sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n","plt.title('Boxplot of Amps')\n","plt.ylabel('Amps')\n","\n","# Plot for WattHours\n","plt.subplot(1, 3, 2)\n","sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n","plt.title('Boxplot of WattHours')\n","plt.ylabel('WattHours')\n","\n","# Plot for Voltage\n","plt.subplot(1, 3, 3)\n","sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n","plt.title('Boxplot of Voltage')\n","plt.ylabel('Voltage')\n","\n","# Adjust layout to avoid overlap\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBnl60S2LIP"},"outputs":[],"source":["# count of propertyIDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9r14vD2LK6"},"outputs":[],"source":["#Count of cluster IDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F01IOcSb2LNZ"},"outputs":[],"source":["# counts of peropertyID and clusterIDimport os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'cluster_id' values for each 'property_id'\n","query = f\"\"\"\n","SELECT property_id, COUNT(DISTINCT cluster_id)\n","FROM {table}\n","GROUP BY property_id\n",";\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    property_id, cluster_count = row\n","    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values for each 'cluster_id'\n","query = f\"\"\"\n","SELECT cluster_id, COUNT(DISTINCT property_id)\n","FROM {table}\n","GROUP BY cluster_id;\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    cluster_id, property_count = row\n","    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n"]},{"cell_type":"markdown","metadata":{"id":"gJCn043ops5i"},"source":["OCPP_SessionID has a userID and TransactionID\n","I need to map to the cluster and property\n","\n","Stations has propertyID and cluster_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkQRHSF2pYLF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRERnF5HpYNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZNFFc8pYQY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pbxg6jVhpYS3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wI1hDsHpYVx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiA1AO1z2LSG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnNNDMY2LUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpS8FyTqEWgq"},"outputs":[],"source":["# Clean message field and port to a df\n","import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n","valid_df = df[df['parsed_message'].notnull()]\n","\n","# Flatten all the JSON objects and store them in a new DataFrame\n","flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n","new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n","\n","# Write the DataFrame to CSV with new naming convention\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n","new_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnkEtU3uDKj"},"outputs":[],"source":["new_df.info\n","new_df.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDqqE7Swt6vj"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df= new_df\n","\n","# Assuming your DataFrame is named df\n","# Step 1: Convert 'time' to datetime\n","df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n","\n","# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n","df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n","\n","# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n","df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n","\n","# Step 4: Plot the time series\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n","plt.xlabel('Time')\n","plt.ylabel('Meter Value')\n","plt.title('Meter Value Over Time')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlT8kevDasAZ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['message'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcwXooB_tXnf"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming new_df is already defined and contains the necessary columns\n","\n","# List of columns to keep\n","columns_to_keep = [\n","    #'time',\n","    'user_id',\n","    #'station_id',\n","    'property_id',\n","    'connectorId',\n","    'meterValue_0_timestamp',\n","    'meterValue_0_sampledValue_1_value',\n","    'meterValue_0_sampledValue_1_context',\n","    'meterValue_0_sampledValue_1_format',\n","    'meterValue_0_sampledValue_1_measurand',\n","    'meterValue_0_sampledValue_1_phase',\n","    'meterValue_0_sampledValue_1_location',\n","    'meterValue_0_sampledValue_1_unit'\n","]\n","\n","# Create new_df_2 with only the selected columns\n","new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","\n","# Convert 'time' to datetime\n","new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n","\n","# Check for any NaT values that may have resulted from the conversion\n","if new_df_2['meterValue_0_timestamp'].isnull().any():\n","    print(\"Some values could not be converted to datetime.\")\n","\n","# Extract day and hour using .loc to avoid warnings\n","new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n","new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLthqnU8u2eM"},"outputs":[],"source":["new_df_2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMxEqA8Qbkxz"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = new_df_2\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N29p7Kq_xWuu"},"outputs":[],"source":["# Assuming new_df_2 is the df\n","\n","unique_values = new_df_2['user_id'].unique()\n","\n","# To display the unique values\n","print(unique_values)\n","\n","\n","# Assuming new_df_2 is your DataFrame\n","unique_count = new_df_2['user_id'].nunique()\n","\n","# To display the count of unique user_id values\n","print(f\"Number of unique user_id values: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEjr917eu9Rb"},"outputs":[],"source":["import pandas as pd\n","\n","\n","new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n","\n","max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n","\n","result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n","\n","print(result_df)\n","result_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnNlof0taJFO"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of meterValue_0_timestamp')\n","plt.title('Count of meterValue_0_timestamp per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"842I60p_-VjA"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming result_df is your DataFrame\n","\n","# Print column names to verify\n","print(\"Column names in DataFrame:\")\n","print(result_df.columns.tolist())\n","\n","# Specify the user_id you're interested in\n","specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n","\n","# Filter the DataFrame for the specific user_id\n","filtered_df = result_df[result_df['user_id'] == specific_user_id]\n","\n","# Check if there are any rows for the specified user_id\n","if not filtered_df.empty:\n","    # Check for the timestamp column again\n","    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n","    value_col = 'meterValue_0_sampledValue_1_value'\n","\n","    # Ensure the column names are correct\n","    print(\"Filtered DataFrame columns:\")\n","    print(filtered_df.columns.tolist())\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n","    plt.title(f'Meter Values for User ID: {specific_user_id}')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Meter Value')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n","    plt.grid()\n","    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","    plt.show()\n","else:\n","    print(f\"No data found for user_id: {specific_user_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp5ImngUusix"},"outputs":[],"source":["\n","# Write the DataFrame to CSV\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n","result_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ZZ-b6HuslW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3cWsdlTusnX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TvKiocBsdvU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A0Tx9v-sdxw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKXZgFB5sd0F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBdB1TPsd2a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDJEMK-rsd4_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyM/2ifFo6mI3xpWt3vDy+8M"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}