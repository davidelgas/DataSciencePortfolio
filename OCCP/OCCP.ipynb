{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/OCCP/OCCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylcs9vE6TRgG"
      },
      "source": [
        "## Prepare Enviornment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4X4QT-W860Y"
      },
      "source": [
        "Organization < Property < Location < Cluster < Station < UserID\n",
        "\n",
        "A cluster is a grouping of chargers/stations. This for convenience/load balancing\n",
        "\n",
        "Each circuit can have multiple clusters.\n",
        "\n",
        "Each cluster has its own breaker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwSzFEWit8p",
        "outputId": "509d6f40-db2d-4424-9ecb-9ab0b4166551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d-ObLi-aMOWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276cbf6f-60ba-423e-9d71-a0a3df7abd10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyxlsb in /usr/local/lib/python3.10/dist-packages (1.0.10)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (2.0.36)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.10)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (3.1.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas pyxlsb\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import psycopg2\n",
        "\n",
        "!pip install SQLAlchemy psycopg2-binary\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as p\n",
        "\n",
        "import json\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "\n",
        "!pip install statsmodels\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My2ExD4GMgls"
      },
      "source": [
        "## Ingest data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Log files and concatinate\n",
        "\n",
        "def load_file(file_path):\n",
        "    \"\"\"Load a CSV file into a Pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {file_path}: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {file_path}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
        "\n",
        "def save_to_csv(df, output_path):\n",
        "    \"\"\"Save DataFrame to a CSV file.\"\"\"\n",
        "    try:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved concatenated file to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file {output_path}: {e}\")\n",
        "\n",
        "# Main Function: Concatenate Files\n",
        "def concatenate_files(file_paths, output_file):\n",
        "    \"\"\"\n",
        "    Load multiple files, concatenate them, and save as a single file.\n",
        "    \"\"\"\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        print(f\"Loading file: {file_path}\")\n",
        "        df = load_file(file_path)\n",
        "\n",
        "        if not df.empty:\n",
        "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Skipping empty file: {file_path}\")\n",
        "\n",
        "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
        "    save_to_csv(combined_df, output_file)\n",
        "\n",
        "# File Paths and Configuration\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv'\n",
        "]\n",
        "\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_data.csv'\n",
        "\n",
        "# Run Concatenation\n",
        "concatenate_files(file_paths, output_file)\n",
        "\n",
        "print(\"All files concatenated and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z403GIGNPXs",
        "outputId": "8714fa11-721f-4e4d-8bcd-d718fff86658"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file: /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv\n",
            "Loaded /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv: (425918, 4)\n",
            "Loading file: /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv\n",
            "Loaded /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv: (451574, 4)\n",
            "Loading file: /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv\n",
            "Loaded /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv: (444630, 4)\n",
            "Combined DataFrame shape: (1322122, 4)\n",
            "Saved concatenated file to /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_data.csv\n",
            "All files concatenated and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Properties table\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    \"\"\"Load credentials and set as environment variables.\"\"\"\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection parameters\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n",
        "}\n",
        "\n",
        "# Export PostgreSQL table to CSV\n",
        "def export_table_to_csv(connection_params, table_name, output_csv_path):\n",
        "    \"\"\"Export table data from PostgreSQL to CSV.\"\"\"\n",
        "    try:\n",
        "        # Connect to the database\n",
        "        connection = psycopg2.connect(**connection_params)\n",
        "        cursor = connection.cursor()\n",
        "\n",
        "        # Query to fetch all rows\n",
        "        query = f\"SELECT * FROM {table_name};\"\n",
        "        cursor.execute(query)\n",
        "\n",
        "        # Fetch rows and column headers\n",
        "        rows = cursor.fetchall()\n",
        "        column_names = [desc[0] for desc in cursor.description]\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(rows, columns=column_names)\n",
        "        logging.info(f\"Loaded table '{table_name}' with shape: {df.shape}\")\n",
        "\n",
        "        # Save raw table data to CSV\n",
        "        df.to_csv(output_csv_path, index=False)\n",
        "        logging.info(f\"Data exported to {output_csv_path} successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing table '{table_name}': {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        if 'connection' in locals() and connection:\n",
        "            cursor.close()\n",
        "            connection.close()\n",
        "            logging.info(\"Database connection closed.\")\n",
        "\n",
        "# Configuration\n",
        "tables = [\"properties\"]\n",
        "output_csv_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n",
        "\n",
        "# Process tables\n",
        "for table in tables:\n",
        "    export_table_to_csv(connection_params, table, output_csv_path)\n"
      ],
      "metadata": {
        "id": "bpqNPX2w-B8l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean data"
      ],
      "metadata": {
        "id": "hN-XTvd8X3eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand the JSON data in the logs\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_csv(file_path):\n",
        "    \"\"\"\n",
        "    Load a CSV file into a DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        logging.info(f\"Loaded file: {file_path}, Shape: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading file {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def flatten_row(row, json_column):\n",
        "    \"\"\"\n",
        "    Helper function to flatten a single row with JSON data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(row[json_column]):\n",
        "            return []\n",
        "\n",
        "        # Extract base data\n",
        "        base_data = row.drop(json_column).to_dict()\n",
        "\n",
        "        # Parse the JSON column\n",
        "        json_data = json.loads(row[json_column])\n",
        "\n",
        "        flattened_rows = []\n",
        "        for meter_value in json_data.get(\"meterValue\", []):\n",
        "            # Extract timestamp\n",
        "            timestamp = meter_value.get(\"timestamp\", None)\n",
        "\n",
        "            for i, sampled_value in enumerate(meter_value.get(\"sampledValue\", [])):\n",
        "                # Extract only the required fields\n",
        "                flat_row = {\n",
        "                    **base_data,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    f\"value_{i}\": sampled_value.get(\"value\", None),\n",
        "                    f\"unit_{i}\": sampled_value.get(\"unit\", None)\n",
        "                }\n",
        "                flattened_rows.append(flat_row)\n",
        "        return flattened_rows\n",
        "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "        logging.warning(f\"Error processing row: {e}\")\n",
        "        return []\n",
        "\n",
        "def flatten_column(df, json_column):\n",
        "    \"\"\"\n",
        "    Flatten a JSON column in a DataFrame.\n",
        "    \"\"\"\n",
        "    flattened_rows = []\n",
        "    for index, row in df.iterrows():\n",
        "        flattened_rows.extend(flatten_row(row, json_column))\n",
        "    return pd.DataFrame(flattened_rows)\n",
        "\n",
        "def save_to_csv(df, output_file):\n",
        "    \"\"\"\n",
        "    Save a DataFrame to a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)  # Ensure the output directory exists\n",
        "        df.to_csv(output_file, index=False)\n",
        "        logging.info(f\"Saved file to: {output_file}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving file {output_file}: {e}\")\n",
        "\n",
        "def process_and_flatten(file_path, json_column, output_dir):\n",
        "    \"\"\"\n",
        "    Load a file, flatten its JSON column, and save the output.\n",
        "    \"\"\"\n",
        "    df = load_csv(file_path)\n",
        "    if df.empty:\n",
        "        logging.warning(f\"File {file_path} is empty. Skipping.\")\n",
        "        return\n",
        "\n",
        "    if json_column not in df.columns:\n",
        "        logging.error(f\"Column {json_column} does not exist in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    flattened_df = flatten_column(df, json_column)\n",
        "    if not flattened_df.empty:\n",
        "        # Construct the output file path\n",
        "        output_file = os.path.join(output_dir, f\"{os.path.basename(file_path).split('.')[0]}_expanded.csv\")\n",
        "        save_to_csv(flattened_df, output_file)\n",
        "    else:\n",
        "        logging.warning(f\"No data left after flattening {json_column}. Skipping save.\")\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_data.csv'\n",
        "    json_column = 'cleaned_message'\n",
        "    output_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP'\n",
        "\n",
        "    process_and_flatten(file_path, json_column, output_dir)\n"
      ],
      "metadata": {
        "id": "2rY_sk7bX59f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the two\n",
        "\n",
        "# File paths\n",
        "combined_data_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_data_flattened.csv'\n",
        "properties_data_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_with_properties.csv'\n",
        "\n",
        "def left_join_data(combined_path, properties_path, output_path):\n",
        "    \"\"\"\n",
        "    Perform a left join between combined_data.csv and properties.csv.\n",
        "    - combined_data.property_id joins properties.id.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load datasets\n",
        "        combined_df = pd.read_csv(combined_path)\n",
        "        properties_df = pd.read_csv(properties_path)\n",
        "        print(f\"Loaded combined_data: {combined_df.shape}\")\n",
        "        print(f\"Loaded properties: {properties_df.shape}\")\n",
        "\n",
        "        # Perform left join\n",
        "        result_df = pd.merge(\n",
        "            combined_df,\n",
        "            properties_df,\n",
        "            left_on='property_id',\n",
        "            right_on='id',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Save the result\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved joined data to {output_path}\")\n",
        "        print(f\"Final DataFrame shape: {result_df.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during join: {e}\")\n",
        "\n",
        "# Run the join operation\n",
        "left_join_data(combined_data_path, properties_data_path, output_path)\n"
      ],
      "metadata": {
        "id": "__kCCMVLNPZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/combined_with_properties.csv')\n",
        "\n",
        "fields_to_keep = [\n",
        "'timestamp',\n",
        "'property_id',\n",
        "'property_type',\n",
        "'user_id',\n",
        "'value_0',\n",
        "'unit_0',\n",
        "'unit_1',\n",
        "'value_1'\n",
        "]\n",
        "\n",
        "df = df_1[fields_to_keep]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4bOX3BzgDFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "17wD83Ff518J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean data\n",
        "\n",
        "# UDF for cleaning the 'timestamp' column\n",
        "def clean_timestamp(df, column_name=\"timestamp\"):\n",
        "    \"\"\"\n",
        "    Cleans the timestamp column:\n",
        "    - Converts to datetime.\n",
        "    - Drops rows with invalid or null timestamps.\n",
        "    \"\"\"\n",
        "    df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
        "    df_cleaned = df.dropna(subset=[column_name])\n",
        "    return df_cleaned\n",
        "\n",
        "# UDF for cleaning the 'property_id' column\n",
        "def clean_property_id(df, column_name=\"property_id\"):\n",
        "    \"\"\"\n",
        "    Cleans the property_id column:\n",
        "    - Drops rows where 'property_id' is null.\n",
        "    \"\"\"\n",
        "    df_cleaned = df.dropna(subset=[column_name])\n",
        "    return df_cleaned\n",
        "\n",
        "# UDF for cleaning 'value_0' column\n",
        "def clean_value_0(df, column_name=\"value_0\"):\n",
        "    \"\"\"\n",
        "    Cleans the value_0 column:\n",
        "    - Replaces NaN/Null values with 0.\n",
        "    - Ensures the column data type is int.\n",
        "    \"\"\"\n",
        "    df[column_name] = df[column_name].fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "# UDF for cleaning 'value_1' column\n",
        "def clean_value_1(df, column_name=\"value_1\"):\n",
        "    \"\"\"\n",
        "    Cleans the value_1 column:\n",
        "    - Replaces NaN/Null values with 0.\n",
        "    - Ensures the column data type is int.\n",
        "    \"\"\"\n",
        "    df[column_name] = df[column_name].fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "# Generalized function to clean a DataFrame using a set of UDFs\n",
        "def clean_data_with_udfs(df, cleaning_functions):\n",
        "    \"\"\"\n",
        "    Applies a sequence of cleaning UDFs to the DataFrame.\n",
        "    :param df: Input DataFrame.\n",
        "    :param cleaning_functions: List of (function, column_name) tuples.\n",
        "    \"\"\"\n",
        "    for func, col in cleaning_functions:\n",
        "        print(f\"Applying cleaning rule: {func.__name__} on column: {col}\")\n",
        "        df = func(df, column_name=col)\n",
        "    return df\n",
        "\n",
        "# File paths\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/sept_100_sample_flat.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/oct_100_sample_flat.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/nov_100_sample_flat.csv'\n",
        "]\n",
        "\n",
        "# Define cleaning rules as a list of (UDF, column_name) pairs\n",
        "cleaning_rules = [\n",
        "    (clean_timestamp, \"timestamp\"),\n",
        "    (clean_property_id, \"property_id\"),  # Updated to property_id\n",
        "    (clean_value_0, \"value_0\"),\n",
        "    (clean_value_1, \"value_1\")\n",
        "]\n",
        "\n",
        "# Process each file\n",
        "for file_path in file_paths:\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "    df = pd.read_csv(file_path)  # Load the data\n",
        "\n",
        "    # Apply cleaning rules\n",
        "    df_cleaned = clean_data_with_udfs(df, cleaning_rules)\n",
        "\n",
        "    # Save cleaned data\n",
        "    output_path = file_path.replace(\".csv\", \"_cleaned.csv\")\n",
        "    df_cleaned.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Cleaned file saved to: {output_path}\")\n",
        "    print(f\"Rows before cleaning: {len(df)}, Rows after cleaning: {len(df_cleaned)}\\n\")\n"
      ],
      "metadata": {
        "id": "OHAuNx_tKCa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engineer Features"
      ],
      "metadata": {
        "id": "uGWTmO7jkzys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Utility Functions\n",
        "def add_day_info(df, timestamp_col='timestamp'):\n",
        "    \"\"\"\n",
        "    Add day information including:\n",
        "    - Day of the week as integers (Sunday=1, Monday=2, ..., Saturday=7)\n",
        "    - Weekend indicator (1 if weekend, 0 otherwise)\n",
        "    \"\"\"\n",
        "    # Extract day of the week (Sunday=1, Monday=2, ..., Saturday=7)\n",
        "    df['day_of_week'] = df[timestamp_col].dt.dayofweek + 1  # Convert 0-6 (Monday-Sunday) to 1-7 (Sunday-Saturday)\n",
        "\n",
        "    # Add weekend indicator\n",
        "    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)  # Weekend: Saturday (6) and Sunday (7)\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_days_to_nearest_holiday(df, date_col, holiday_dates):\n",
        "    \"\"\"\n",
        "    Calculate the minimum number of days to the nearest holiday for each date.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the date column\n",
        "    - date_col: Name of the column with datetime values\n",
        "    - holiday_dates: List of holiday dates (as datetime objects)\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with a new column 'days_to_nearest_holiday'\n",
        "    \"\"\"\n",
        "    # Ensure date column is in datetime format\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "\n",
        "    # Normalize holidays to match the timezone-awareness of the column\n",
        "    if df[date_col].dt.tz is not None:\n",
        "        # Make holidays timezone-aware if timestamps are timezone-aware\n",
        "        holiday_dates = [pd.Timestamp(holiday).tz_localize(df[date_col].dt.tz) for holiday in holiday_dates]\n",
        "    else:\n",
        "        # Ensure holidays are timezone-naive if timestamps are timezone-naive\n",
        "        holiday_dates = [pd.Timestamp(holiday).tz_localize(None) for holiday in holiday_dates]\n",
        "\n",
        "    # Calculate days to each holiday and find the minimum\n",
        "    df['days_to_nearest_holiday'] = df[date_col].apply(\n",
        "        lambda x: min(abs((x - holiday).days) for holiday in holiday_dates)\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_datetime_components(df, timestamp_col='timestamp'):\n",
        "    \"\"\"Add year, month, day, and hour components as separate columns.\"\"\"\n",
        "    df['year'] = df[timestamp_col].dt.year\n",
        "    df['month'] = df[timestamp_col].dt.month\n",
        "    df['day'] = df[timestamp_col].dt.day\n",
        "    df['hour'] = df[timestamp_col].dt.hour\n",
        "    return df\n",
        "\n",
        "\n",
        "def encode_month_column(df, month_col='month'):\n",
        "    \"\"\"\n",
        "    Encode the month column as integers (September=9, October=10, November=11).\n",
        "    \"\"\"\n",
        "    # Map month names (if needed) to integers\n",
        "    month_mapping = {'September': 9, 'October': 10, 'November': 11}\n",
        "    df['month_encoded'] = df[month_col].map(month_mapping).fillna(df[month_col]).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_unique_user_counts(df, group_cols, user_col='user_id'):\n",
        "    \"\"\"Add a column with the count of unique users per group.\"\"\"\n",
        "    unique_user_counts = (\n",
        "        df.groupby(group_cols)[user_col]\n",
        "        .nunique()\n",
        "        .reset_index()\n",
        "        .rename(columns={user_col: 'unique_user_count'})\n",
        "    )\n",
        "    df = df.merge(unique_user_counts, on=group_cols, how='left')\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_usage_sums(df, group_cols, value_cols):\n",
        "    \"\"\"Add sum of usage per group.\"\"\"\n",
        "    sums = df.groupby(group_cols)[value_cols].sum().reset_index()\n",
        "    sums.rename(\n",
        "        columns={\n",
        "            value_cols[0]: 'hour_sum_value_Wh',\n",
        "            value_cols[1]: 'hour_sum_value_A'\n",
        "        },\n",
        "        inplace=True\n",
        "    )\n",
        "    df = df.merge(sums, on=group_cols, how='left')\n",
        "    return df\n",
        "\n",
        "\n",
        "def engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates=None):\n",
        "    \"\"\"\n",
        "    Engineer new features for the dataset:\n",
        "    - Add day information\n",
        "    - Add holiday proximity\n",
        "    - Extract datetime components\n",
        "    - Add unique user counts\n",
        "    - Add usage sums\n",
        "    \"\"\"\n",
        "    # Ensure timestamp is in datetime format\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce', utc=True)\n",
        "\n",
        "    # Add day information\n",
        "    df = add_day_info(df, timestamp_col)\n",
        "\n",
        "    # Add holiday proximity if holiday dates are provided\n",
        "    if holiday_dates:\n",
        "        df = calculate_days_to_nearest_holiday(df, timestamp_col, holiday_dates)\n",
        "\n",
        "    # Extract datetime components\n",
        "    df = add_datetime_components(df, timestamp_col)\n",
        "\n",
        "    # Flatten Month\n",
        "    df = encode_month_column(df)\n",
        "\n",
        "    # Add unique user counts\n",
        "    df = add_unique_user_counts(df, group_cols, user_col)\n",
        "\n",
        "    # Add usage sums\n",
        "    df = add_usage_sums(df, group_cols, value_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_files(file_paths, output_dir, group_cols, value_cols, timestamp_col='timestamp', user_col='user_id', holiday_dates=None, properties_file=None):\n",
        "    \"\"\"\n",
        "    Process a list of files:\n",
        "    - Load CSV files\n",
        "    - Engineer features\n",
        "    - Merge metadata from properties CSV\n",
        "    - Save processed files to an output directory\n",
        "    \"\"\"\n",
        "    processed_dfs = []\n",
        "\n",
        "    # Load properties metadata if provided\n",
        "    if properties_file:\n",
        "        properties_df = pd.read_csv(properties_file)\n",
        "        print(f\"Loaded properties metadata from {properties_file}\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Extract file name without extension\n",
        "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        output_file = os.path.join(output_dir, f\"{file_name}_processed.csv\")\n",
        "\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Apply feature engineering\n",
        "        processed_df = engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates)\n",
        "\n",
        "        # Merge with properties metadata if available\n",
        "        if properties_file:\n",
        "            processed_df = pd.merge(processed_df, properties_df, left_on='property_id', right_on='id', how='left')\n",
        "            print(f\"Metadata merged for {file_name}\")\n",
        "\n",
        "        # Save the processed DataFrame\n",
        "        processed_df.to_csv(output_file, index=False)\n",
        "        print(f\"Processed file saved to: {output_file}\")\n",
        "\n",
        "        # Append to list of processed DataFrames\n",
        "        processed_dfs.append(processed_df)\n",
        "\n",
        "    return processed_dfs\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/sept_100_sample_flat.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/oct_100_sample_flat.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/nov_100_sample_flat.csv'\n",
        "]\n",
        "\n",
        "properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/properties.csv'  # Path to properties CSV\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n",
        "\n",
        "group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n",
        "value_cols = ['value_0', 'value_1']\n",
        "holiday_dates = [\n",
        "    pd.Timestamp(\"2024-09-04\"),\n",
        "    pd.Timestamp(\"2024-10-09\"),\n",
        "    pd.Timestamp(\"2024-11-23\")\n",
        "]\n",
        "\n",
        "processed_dfs = process_files(\n",
        "    file_paths=file_paths,\n",
        "    output_dir=output_dir,\n",
        "    group_cols=group_cols,\n",
        "    value_cols=value_cols,\n",
        "    timestamp_col='timestamp',\n",
        "    user_col='user_id',\n",
        "    holiday_dates=holiday_dates,\n",
        "    properties_file=properties_file  # Pass properties file here\n",
        ")\n",
        "\n",
        "# Combine all processed DataFrames into one\n",
        "final_combined_df = pd.concat(processed_dfs, axis=0).reset_index(drop=True)\n",
        "\n",
        "# Save the combined DataFrame\n",
        "final_combined_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/final_combined.csv', index=False)\n",
        "\n",
        "print(\"Processing complete. Combined DataFrame saved.\")\n"
      ],
      "metadata": {
        "id": "bHW_OWXZbl-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OY_xU4ngDMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zQCd1bKgDPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APt44LDmgDRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uvBYZC_gDUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "nB6QSWEvYZZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DM9fshqYZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O88H0tVqYZeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRnacKJtYZgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_miHeTkUYZjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVgSE5d7YZlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XJ6-wX8YZoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pczhWlbFYZqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFNaNJF4YZtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvG90NZMYZv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingest log tables that were exported from Splunk\n",
        "\n",
        "\n",
        "# Utility Functions\n",
        "def load_file(file_path):\n",
        "    \"\"\"Load a CSV file into a Pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {file_path}: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {file_path}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
        "\n",
        "\n",
        "def flatten_column(df, json_column):\n",
        "    \"\"\"Flatten a JSON column in a DataFrame.\"\"\"\n",
        "    flattened_rows = []\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            if pd.isna(row[json_column]):\n",
        "                continue  # Skip rows where the JSON column is missing or NaN\n",
        "\n",
        "            base_data = row.drop(json_column).to_dict()  # Base columns (_time, property_id, user_id)\n",
        "            json_data = json.loads(row[json_column])  # Parse JSON\n",
        "\n",
        "            for meter_value in json_data.get(\"meterValue\", []):\n",
        "                timestamp = meter_value.get(\"timestamp\", None)\n",
        "                for i, sampled_value in enumerate(meter_value.get(\"sampledValue\", [])):\n",
        "                    flat_row = {\n",
        "                        **base_data,\n",
        "                        \"timestamp\": timestamp,\n",
        "                        f\"value_{i}\": sampled_value.get(\"value\", None),\n",
        "                        f\"context_{i}\": sampled_value.get(\"context\", None),\n",
        "                        f\"format_{i}\": sampled_value.get(\"format\", None),\n",
        "                        f\"measurand_{i}\": sampled_value.get(\"measurand\", None),\n",
        "                        f\"phase_{i}\": sampled_value.get(\"phase\", None),\n",
        "                        f\"location_{i}\": sampled_value.get(\"location\", None),\n",
        "                        f\"unit_{i}\": sampled_value.get(\"unit\", None),\n",
        "                    }\n",
        "                    flattened_rows.append(flat_row)\n",
        "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "            print(f\"Error processing row {index}: {e}\")\n",
        "    flattened_df = pd.DataFrame(flattened_rows)\n",
        "    print(f\"Flattened DataFrame shape: {flattened_df.shape}\")\n",
        "    return flattened_df\n",
        "\n",
        "\n",
        "def drop_missing_rows(df, required_columns):\n",
        "    \"\"\"Drop rows with missing values (NaN, Null, or inf) in the specified columns.\"\"\"\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Missing required columns {missing_columns}. Skipping drop.\")\n",
        "        return df\n",
        "\n",
        "    print(f\"Checking for missing values in columns: {required_columns}\")\n",
        "    initial_row_count = len(df)\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df = df.dropna(subset=required_columns).copy()\n",
        "    final_row_count = len(df)\n",
        "    print(f\"Dropped {initial_row_count - final_row_count} rows with missing values.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean all data by handling NaN and inf globally.\"\"\"\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_parsing_errors(df, unit_0_column, unit_1_column):\n",
        "    \"\"\"Drop rows where unit_0 is not 'Wh' and unit_1 is not 'A'.\"\"\"\n",
        "    if unit_0_column not in df.columns or unit_1_column not in df.columns:\n",
        "        print(f\"Skipping parsing errors: Missing columns {unit_0_column} or {unit_1_column}.\")\n",
        "        return df\n",
        "\n",
        "    conditions = (\n",
        "        ((df[unit_0_column] == 'Wh') | (pd.isna(df[unit_0_column]))) &\n",
        "        ((df[unit_1_column] == 'A') | (pd.isna(df[unit_1_column])))\n",
        "    )\n",
        "    initial_row_count = len(df)\n",
        "    cleaned_df = df[conditions].copy()\n",
        "    print(f\"Cleaned parsing errors: {initial_row_count - len(cleaned_df)} rows dropped.\")\n",
        "    return cleaned_df\n",
        "\n",
        "\n",
        "def filter_columns(df, fields_to_keep):\n",
        "    \"\"\"Keep only specified columns in the DataFrame.\"\"\"\n",
        "    missing_columns = [col for col in fields_to_keep if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Missing columns {missing_columns}. Skipping those.\")\n",
        "    return df[[col for col in fields_to_keep if col in df.columns]].copy()\n",
        "\n",
        "\n",
        "def convert_to_datetime(df, columns):\n",
        "    \"\"\"Convert specified columns to datetime.\"\"\"\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def convert_to_int(df, columns):\n",
        "    \"\"\"Convert specified columns to integers, replacing NaN with 0 and inf with large values.\"\"\"\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            # Replace NaN or inf/-inf with 0 or a large number as appropriate\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, coerce errors to NaN\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], 0)  # Replace infinities with 0 or another value\n",
        "            df[col] = df[col].fillna(0)  # Fill NaN with 0\n",
        "            df[col] = df[col].astype(int)  # Convert to int after replacing NaN and inf\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_to_csv(df, output_path):\n",
        "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "# Main Processing Function\n",
        "def process_files(file_paths, json_column, fields_to_keep, output_dir):\n",
        "    \"\"\"\n",
        "    Process a list of files:\n",
        "    - Load\n",
        "    - Flatten JSON\n",
        "    - Drop rows with missing values\n",
        "    - Clean parsing errors\n",
        "    - Filter out columns\n",
        "    - Convert datatypes\n",
        "    - Save to CSV\n",
        "    \"\"\"\n",
        "    for file_path in file_paths:\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        df = load_file(file_path)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"File {file_path} is empty. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        df = flatten_column(df, json_column)\n",
        "        df = clean_data(df)\n",
        "        df = drop_missing_rows(df, ['timestamp', 'user_id'])\n",
        "        df = clean_parsing_errors(df, 'unit_0', 'unit_1')\n",
        "        df = filter_columns(df, fields_to_keep)\n",
        "        df = convert_to_datetime(df, ['timestamp', '_time'])\n",
        "\n",
        "        if not df.empty:\n",
        "            output_path = os.path.join(output_dir, f\"{os.path.basename(file_path).split('.')[0]}_flat.csv\")\n",
        "            save_to_csv(df, output_path)\n",
        "            print(f\"Processed file saved to: {output_path}\")\n",
        "        else:\n",
        "            print(f\"No data left after processing {file_path}. Skipping.\")\n",
        "\n",
        "\n",
        "# File Paths and Configuration\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/sept_100_sample.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/oct_100_sample.csv',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/nov_100_sample.csv'\n",
        "]\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie'\n",
        "\n",
        "# Trimming data here due to file sizes\n",
        "fields_to_keep = ['timestamp', 'user_id', 'property_id', 'value_0', 'value_1', 'unit_0', 'unit_1']\n",
        "\n",
        "# Process All Files\n",
        "process_files(file_paths, 'cleaned_message', fields_to_keep, output_dir)\n",
        "\n",
        "print(\"All files processed successfully.\")\n"
      ],
      "metadata": {
        "id": "TjTtusuOF-Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_son.head(5)"
      ],
      "metadata": {
        "id": "P4AOcdg_wUN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns I dont need\n",
        "\n",
        "columns_to_drop = ['_time', 'property_id','user_id','timestamp','unit_0','value_0','unit_1','value_1','day_weekend','year','month','hour','hour_sum_value_A']\n",
        "\n",
        "# Drop the columns from the DataFrame\n",
        "\n",
        "df_son = final_combined_df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Convert object type columns with string 'True'/'False' to boolean\n",
        "df_son[df_son.select_dtypes('object').columns] = df_son.select_dtypes('object').applymap(lambda x: x == 'True')\n",
        "\n",
        "# Verify the changes\n",
        "print(df_son.dtypes)  # To check if the columns are now boolean\n",
        "\n",
        "# Save the combined DataFrame\n",
        "\n",
        "df_son.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_son.csv', index=False)"
      ],
      "metadata": {
        "id": "uqfHkuSMTwJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for colinearity"
      ],
      "metadata": {
        "id": "69Y399-MUhvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VIF\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "\n",
        "# Convert boolean columns to integers for VIF calculation\n",
        "X = X.astype(int)\n",
        "\n",
        "# Add a constant column to the DataFrame for the intercept\n",
        "X_with_const = add_constant(X)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i + 1) for i in range(len(X.columns))]\n",
        "\n",
        "# Display VIF\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "ZWAI3CA0XPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep df for regression/ANOVA"
      ],
      "metadata": {
        "id": "o3XDub4aqdrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define predictors (X) and dependent variable (y)\n",
        "X = df_son.drop(columns=['hour_sum_value_Wh'])  # All columns except the dependent variable\n",
        "y = df_son['hour_sum_value_Wh']\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Check types of X and y\n",
        "print(X.dtypes)\n",
        "print(y.dtypes)\n",
        "\n",
        "# Fit the regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the summary of the regression model\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "sTFhCBuk3VRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpSYEsmq3VUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umUOsBSn3VXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-e5kpAZ3VZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_F20AGhU3Vbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U1F9VbEO3VhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Fit a regression model\n",
        "X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n",
        "model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n",
        "\n",
        "# Predict values for regression line\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Plot scatter with regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n",
        "plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n",
        "plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n",
        "plt.xlabel('Unique User Count')\n",
        "plt.ylabel('Hour hour_sum_value_A')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Ensure property_id is treated as a categorical variable\n",
        "df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n",
        "\n",
        "# Prepare the formula for ANOVA\n",
        "independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n",
        "formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n",
        "\n",
        "# Fit the model\n",
        "model = ols(formula, data=df_sampled).fit()\n",
        "\n",
        "# Perform ANOVA\n",
        "anova_results = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Display the ANOVA results\n",
        "print(anova_results)\n"
      ],
      "metadata": {
        "id": "SkEIez9-o-Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the IQR for the column with potential outliers\n",
        "Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n",
        "Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter out the outliers\n",
        "df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"
      ],
      "metadata": {
        "id": "ZJU6o-8Jo-ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a property lookup\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Call the function to load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection parameters from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n",
        "}\n",
        "\n",
        "# Function to fetch non-transposed data\n",
        "def fetch_non_transposed_data(cursor, table):\n",
        "    query = f\"SELECT * FROM {table};\"\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    column_names = [desc[0] for desc in cursor.description]\n",
        "\n",
        "    # Create a DataFrame from the fetched data\n",
        "    df = pd.DataFrame(rows, columns=column_names)\n",
        "    return df\n",
        "\n",
        "# List of tables to process\n",
        "tables = [\n",
        "    \"location\",\n",
        "    \"properties\"\n",
        "]\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "try:\n",
        "    connection = psycopg2.connect(**connection_params)\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Loop through each table name\n",
        "    for table in tables:\n",
        "        logging.info(f\"Processing table: {table}\")\n",
        "\n",
        "        # Fetch non-transposed data\n",
        "        df_non_transposed = fetch_non_transposed_data(cursor, table)\n",
        "\n",
        "        # Write the DataFrame to CSV with new naming convention\n",
        "        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n",
        "        df_non_transposed.to_csv(output_csv_path, index=False)\n",
        "        logging.info(f\"Data written to {output_csv_path} successfully.\")\n",
        "\n",
        "except Exception as error:\n",
        "    logging.error(f\"Error connecting to the database: {error}\")\n",
        "\n",
        "finally:\n",
        "    if 'connection' in locals() and connection:\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        logging.info(\"Connection closed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Uft9sM-JFrIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Fit a regression model\n",
        "X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n",
        "model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n",
        "\n",
        "# Predict values for regression line\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Plot scatter with regression line\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n",
        "plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n",
        "plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n",
        "plt.xlabel('Unique User Count')\n",
        "plt.ylabel('Hour hour_sum_value_A')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Ensure property_id is treated as a categorical variable\n",
        "df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n",
        "\n",
        "# Prepare the formula for ANOVA\n",
        "independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n",
        "formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n",
        "\n",
        "# Fit the model\n",
        "model = ols(formula, data=df_filtered).fit()\n",
        "\n",
        "# Perform ANOVA\n",
        "anova_results = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Display the ANOVA results\n",
        "print(anova_results)"
      ],
      "metadata": {
        "id": "avvahCDuo-bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using an extract from Eng"
      ],
      "metadata": {
        "id": "Z3-76nrR5HD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/2024-08-01.csv'\n",
        "\n",
        "df_Aug_1_24 = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "REvO0jspo-gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Aug_1_24.info()"
      ],
      "metadata": {
        "id": "z_amy8Anjqht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need confirmation that it is watts and not watt hours."
      ],
      "metadata": {
        "id": "D3cN_tSi7rzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding or modifying the header\n",
        "df_Aug_1_24.columns = ['qrcode','connector', 'serial_num', 'org_id', 'property_id', 'station_id', 'transaction_id', 'metered_type', 'timestamp', 'metered_value']\n",
        "print(df_Aug_1_24)"
      ],
      "metadata": {
        "id": "DCuAXRadjqmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGVrT8vSjqoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ljroTrTjq0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECCr_CuwjqrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMYRBhDejqtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB5XlE0ahulJ"
      },
      "source": [
        "## Start Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl6nDdBtQ8hS"
      },
      "outputs": [],
      "source": [
        "connector_id = message_data['connectorId']\n",
        "transaction_id = message_data['transactionId']\n",
        "meter_values = message_data['meterValue']\n",
        "\n",
        "print(f\"Connector ID: {connector_id}\")\n",
        "print(f\"Transaction ID: {transaction_id}\")\n",
        "\n",
        "# Loop through meter values\n",
        "for mv in meter_values:\n",
        "    timestamp = mv['timestamp']\n",
        "    print(f\"Timestamp: {timestamp}\")\n",
        "    for sv in mv['sampledValue']:\n",
        "        print(f\"  Measurand: {sv['measurand']}, Value: {sv['value']}, Unit: {sv['unit']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueesq-tBxixy"
      },
      "outputs": [],
      "source": [
        "# Aggregate all three months of data\n",
        "\n",
        "file_path_a = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/e95523e6-3470-4a60-b586-ee715cd9f34b_aug.csv'\n",
        "\n",
        "df_aug = pd.read_csv(file_path_a)\n",
        "\n",
        "file_path_s = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/e95523e6-3470-4a60-b586-ee715cd9f34b_sept.csv'\n",
        "\n",
        "df_sep = pd.read_csv(file_path_s)\n",
        "\n",
        "file_path_o = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/e95523e6-3470-4a60-b586-ee715cd9f34b_oct.csv'\n",
        "\n",
        "df_oct = pd.read_csv(file_path_o)\n",
        "\n",
        "# Combine vertically\n",
        "df_a_s_o = pd.concat([df_aug, df_sep, df_oct], axis=0, ignore_index=True)\n",
        "\n",
        "print(df_a_s_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_Om21VbYQzx"
      },
      "outputs": [],
      "source": [
        "df_a_s_o.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v3gF806WG-Y"
      },
      "outputs": [],
      "source": [
        "# Data Check\n",
        "\n",
        "# Calculate the overall count of unique user IDs\n",
        "unique_user_count = df_a_s_o['user_id'].nunique()\n",
        "\n",
        "# Calculate the sum of unit_a\n",
        "sum_of_unit_a = df_a_s_o['unit_a'].sum()\n",
        "\n",
        "# Calculate the sum of watt_h\n",
        "sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Unique User Count: {unique_user_count}\")\n",
        "print(f\"Sum of unit_a: {sum_of_unit_a}\")\n",
        "print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n",
        "\n",
        "# Unique User Count: 1028\n",
        "# Sum of unit_a: 84714332.39000002\n",
        "# Sum of unit_wh: 57182938816884.78\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EAhWIUhxi0B"
      },
      "outputs": [],
      "source": [
        "# Decorate data with engineered values\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Function to convert to PST and extract datetime\n",
        "def convert_to_pst_as_datetime(timestamp):\n",
        "    # Parse the UTC timestamp\n",
        "    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "    # Set timezone to UTC\n",
        "    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n",
        "    # Convert to PST\n",
        "    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n",
        "    # Truncate to day, month, year, and hour (zero minutes and seconds)\n",
        "    return pst_time.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "# Apply the function to convert timestamp\n",
        "df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n",
        "\n",
        "# Add a column for day of the week (0 = Monday, 6 = Sunday)\n",
        "df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n",
        "\n",
        "# Add a column for hour of the day (24hr format)\n",
        "df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n",
        "\n",
        "# Add a column for ISO week number\n",
        "df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n",
        "\n",
        "# Add in count of unique users\n",
        "df_a_s_o['unique_user_count'] = (\n",
        "    df_a_s_o\n",
        "    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n",
        "    .transform('nunique')\n",
        ")\n",
        "\n",
        "# Add in sum of unit_a\n",
        "df_a_s_o['sum_of_unit_a'] = (\n",
        "    df_a_s_o\n",
        "    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n",
        "    .transform('sum')\n",
        ")\n",
        "\n",
        "# Add in sum of watt_h\n",
        "df_a_s_o['sum_of_unit_wh'] = (\n",
        "    df_a_s_o\n",
        "    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n",
        "    .transform('sum')\n",
        ")\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(df_a_s_o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC6HV07FXtTr"
      },
      "outputs": [],
      "source": [
        "# Data Check\n",
        "print(df_a_s_o['week_number'].unique())\n",
        "\n",
        "\n",
        "# Calculate the overall count of unique user IDs\n",
        "unique_user_count = df_a_s_o['user_id'].nunique()\n",
        "\n",
        "# Calculate the sum of unit_a\n",
        "sum_of_unit_a = df_a_s_o['unit_a'].sum()\n",
        "\n",
        "# Calculate the sum of watt_h\n",
        "sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Unique User Count: {unique_user_count}\")\n",
        "print(f\"Sum of unit_a: {sum_of_unit_a}\")\n",
        "print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n",
        "\n",
        "# Unique User Count: 1028\n",
        "# Sum of unit_a: 84714332.39000002\n",
        "# Sum of unit_wh: 57182938816884.78"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUNjyr1Nxi4u"
      },
      "outputs": [],
      "source": [
        "df_a_s_o.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqX5wj0rxi7F"
      },
      "outputs": [],
      "source": [
        "# Reduce the df\n",
        "df_a_s_o.drop(columns=['_time','transactionId','timestamp','station_id','serial_number','cluster','time_sample'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vaGBSdjg0_I"
      },
      "outputs": [],
      "source": [
        "# Reduce the DataFrame to unique rows based on the specified columns\n",
        "reduced_df = df_a_s_o.drop_duplicates(\n",
        "    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",
        ")\n",
        "\n",
        "# Keep only the specified columns\n",
        "reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(reduced_df.info())\n",
        "print(reduced_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBxpWJMKg4z7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the sum of unit_a\n",
        "sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n",
        "\n",
        "# Calculate the sum of watt_h\n",
        "sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n",
        "\n",
        "# Print the results\n",
        "\n",
        "print(f\"Sum of unit_a: {sum_of_unit_a}\")\n",
        "print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n",
        "\n",
        "# Unique User Count: 1028\n",
        "# Sum of unit_a: 84714332.39000002\n",
        "# Sum of unit_wh: 57182938816884.78\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7oaFCsfLjT1"
      },
      "outputs": [],
      "source": [
        "# Write a local file to take a look\n",
        "\n",
        "df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM80beG-xi9j"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n",
        "plt.xlabel('User unique_user_count Count')\n",
        "plt.ylabel('Total Unit WH')\n",
        "plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F__BqafGHzpU"
      },
      "outputs": [],
      "source": [
        "df_a_s_o.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIVH6Ob5LlXb"
      },
      "outputs": [],
      "source": [
        "# Data is week 32 through week 44 (12)\n",
        "# So below, there is no week lag1 value for week 32 because it is the first\n",
        "\n",
        "# Identify the peak total_unit_wh for each week\n",
        "peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n",
        "\n",
        "# Sort by week number to ensure correct lagging\n",
        "peak_weekly_data = peak_weekly_data.sort_values('week_number')\n",
        "\n",
        "# Add only lag_1 features\n",
        "peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n",
        "peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n",
        "\n",
        "# Drop rows with insufficient lag (week 1)\n",
        "peak_weekly_data = peak_weekly_data.dropna()\n",
        "\n",
        "# Retain only relevant columns\n",
        "peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n",
        "\n",
        "print(\"Updated DataFrame:\")\n",
        "print(peak_weekly_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoEouHpMLlce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Features (lagged day of week and hour) and target (day of week)\n",
        "X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n",
        "y = peak_weekly_data['day_of_week']  # Target: Day of the week\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Day of Week Prediction Accuracy:\", accuracy)\n",
        "\n",
        "# Display true vs predicted values\n",
        "results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n",
        "print(\"\\nTrue vs Predicted Days of the Week:\")\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozIzbbVKLlew"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Feature importance for day_of_week classification\n",
        "clf_importances = clf.feature_importances_\n",
        "plt.barh(X.columns, clf_importances)\n",
        "plt.title(\"Feature Importance for Day of Week Prediction\")\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for hour regression\n",
        "reg_importances = reg.feature_importances_\n",
        "plt.barh(X.columns, reg_importances)\n",
        "plt.title(\"Feature Importance for Hour Prediction\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J85SXxSCIwdY"
      },
      "source": [
        "## Extract from Eddie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAlXiQuUIu76"
      },
      "outputs": [],
      "source": [
        "file_path_a = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/2024-08-01.csv'\n",
        "\n",
        "df_big = pd.read_csv(file_path_a)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J89LPzgmdzuZ"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMZPkMirgm9l"
      },
      "outputs": [],
      "source": [
        "df_big.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOkPWChKg5k8"
      },
      "outputs": [],
      "source": [
        "header = [\n",
        "    \"qrcode\",  # Column 0\n",
        "    \"connector\",  # Column 1\n",
        "    \"serial_num\",  # Column 2\n",
        "    \"org_id\",  # Column 3\n",
        "    \"property_id\",  # Column 4\n",
        "    \"station_id\",  # Column 5\n",
        "    \"transaction_id\",  # Column 6\n",
        "    \"metered_type\",  # Column 7\n",
        "    \"timestamp\",  # Column 8\n",
        "    \"metered_value\"   # Column 9\n",
        "]\n",
        "\n",
        "df_big.columns = header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcxojux1h63w"
      },
      "outputs": [],
      "source": [
        "df_big['Timestamp'] = pd.to_datetime(df_big['Timestamp'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCxVMwubTAaC"
      },
      "source": [
        "## List of tables I can access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUKjVft2kxIk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Call the function to load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection parameters from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n",
        "}\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "try:\n",
        "    connection = psycopg2.connect(**connection_params)\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Execute a query to fetch all table names\n",
        "    query = \"\"\"\n",
        "    SELECT table_name\n",
        "    FROM information_schema.tables\n",
        "    WHERE table_schema = 'public';\n",
        "    \"\"\"\n",
        "\n",
        "    cursor.execute(query)\n",
        "    tables = cursor.fetchall()\n",
        "\n",
        "    # Print the table names\n",
        "    for table in tables:\n",
        "        print(table[0])\n",
        "\n",
        "except Exception as error:\n",
        "    print(f\"Error connecting to the database: {error}\")\n",
        "\n",
        "finally:\n",
        "    if 'connection' in locals() and connection:\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        print(\"Connection closed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz7Y_6f1MZM0"
      },
      "source": [
        "## Look at each table I can access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwifoFzdUDwh"
      },
      "outputs": [],
      "source": [
        "# This creates a table of field names and sample values\n",
        "import os\n",
        "import logging\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Call the function to load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection parameters from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n",
        "}\n",
        "\n",
        "# List of tables to process\n",
        "tables = [\n",
        "    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n",
        "    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n",
        "    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n",
        "    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n",
        "    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n",
        "    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n",
        "    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n",
        "    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n",
        "    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n",
        "    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n",
        "    \"connectors\", \"clusters\"\n",
        "]\n",
        "\n",
        "\n",
        "# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "try:\n",
        "    connection = psycopg2.connect(**connection_params)\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Loop through each table name\n",
        "    for table in tables:\n",
        "        logging.info(f\"Processing table: {table}\")\n",
        "\n",
        "        # Query to fetch the first few rows from the current table\n",
        "        query = f\"SELECT * FROM {table} LIMIT 10;\"\n",
        "        cursor.execute(query)\n",
        "\n",
        "        # Fetch the rows\n",
        "        rows = cursor.fetchall()\n",
        "        # Fetch the column headers\n",
        "        column_names = [desc[0] for desc in cursor.description]\n",
        "\n",
        "        # Create a DataFrame from the fetched data\n",
        "        df = pd.DataFrame(rows, columns=column_names)\n",
        "\n",
        "        # Prepare the transposed DataFrame\n",
        "        transposed_data = {\n",
        "            'Header': column_names,\n",
        "            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n",
        "            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n",
        "        }\n",
        "\n",
        "        df_transposed = pd.DataFrame(transposed_data)\n",
        "\n",
        "        # Write the DataFrame to CSV\n",
        "        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_fields.csv'\n",
        "        df_transposed.to_csv(output_csv_path, index=False)\n",
        "        logging.info(f\"Data written to {output_csv_path} successfully.\")\n",
        "\n",
        "except Exception as error:\n",
        "    logging.error(f\"Error connecting to the database: {error}\")\n",
        "\n",
        "finally:\n",
        "    if 'connection' in locals() and connection:\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        logging.info(\"Connection closed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btMrMBD0jHEN"
      },
      "outputs": [],
      "source": [
        "# This creates a table of sample records\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Call the function to load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection parameters from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n",
        "}\n",
        "\n",
        "# Function to fetch non-transposed data\n",
        "def fetch_non_transposed_data(cursor, table):\n",
        "    query = f\"SELECT * FROM {table} LIMIT 10;\"\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    column_names = [desc[0] for desc in cursor.description]\n",
        "\n",
        "    # Create a DataFrame from the fetched data\n",
        "    df = pd.DataFrame(rows, columns=column_names)\n",
        "    return df\n",
        "\n",
        "# List of tables to process\n",
        "tables = [\n",
        "    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n",
        "    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n",
        "    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n",
        "    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n",
        "    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n",
        "    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n",
        "    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n",
        "    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n",
        "    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n",
        "    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n",
        "    \"connectors\", \"clusters\"\n",
        "]\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "try:\n",
        "    connection = psycopg2.connect(**connection_params)\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Loop through each table name\n",
        "    for table in tables:\n",
        "        logging.info(f\"Processing table: {table}\")\n",
        "\n",
        "        # Fetch non-transposed data\n",
        "        df_non_transposed = fetch_non_transposed_data(cursor, table)\n",
        "\n",
        "        # Write the DataFrame to CSV with new naming convention\n",
        "        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_example_data.csv'\n",
        "        df_non_transposed.to_csv(output_csv_path, index=False)\n",
        "        logging.info(f\"Data written to {output_csv_path} successfully.\")\n",
        "\n",
        "except Exception as error:\n",
        "    logging.error(f\"Error connecting to the database: {error}\")\n",
        "\n",
        "finally:\n",
        "    if 'connection' in locals() and connection:\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        logging.info(\"Connection closed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVJVYYv8s_tV"
      },
      "source": [
        "# Now I need to build the correct table directly from RS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaoKGEHJw0uL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from itertools import combinations\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Path to the directory containing the CSV files\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n",
        "\n",
        "# List of tables (as per your previous code)\n",
        "tables = [\n",
        "    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n",
        "    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n",
        "    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n",
        "    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n",
        "    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n",
        "    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n",
        "    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n",
        "    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n",
        "    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n",
        "    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n",
        "    \"connectors\", \"clusters\"\n",
        "]\n",
        "\n",
        "# Function to load CSV files into DataFrames\n",
        "def load_dataframes(tables):\n",
        "    dataframes = {}\n",
        "    for table in tables:\n",
        "        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            dataframes[table] = df\n",
        "            logging.info(f\"Loaded data for table: {table}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading data for table {table}: {e}\")\n",
        "    return dataframes\n",
        "\n",
        "# Function to find strict join matches\n",
        "def find_strict_joins(df1, df2, table1_name, table2_name):\n",
        "    strict_joins = []\n",
        "    # Iterate over all column pairs\n",
        "    for col1 in df1.columns:\n",
        "        for col2 in df2.columns:\n",
        "            if df1[col1].dtype == df2[col2].dtype:\n",
        "                # Perform the join\n",
        "                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n",
        "                # Check if all rows in df1 are in the joined DataFrame\n",
        "                if len(joined_df) == len(df1):\n",
        "                    strict_joins.append((col1, col2))\n",
        "                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n",
        "    return strict_joins\n",
        "\n",
        "# Main function to perform the strict join analysis\n",
        "def analyze_strict_joins(tables):\n",
        "    dataframes = load_dataframes(tables)\n",
        "    results = {}\n",
        "    table_pairs = combinations(tables, 2)\n",
        "\n",
        "    for table1, table2 in table_pairs:\n",
        "        df1 = dataframes.get(table1)\n",
        "        df2 = dataframes.get(table2)\n",
        "\n",
        "        if df1 is not None and df2 is not None:\n",
        "            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n",
        "            joins = find_strict_joins(df1, df2, table1, table2)\n",
        "            if joins:\n",
        "                results[f\"{table1} <-> {table2}\"] = joins\n",
        "        else:\n",
        "            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the strict join analysis\n",
        "strict_join_results = analyze_strict_joins(tables)\n",
        "\n",
        "# Print the results\n",
        "for table_pair, joins in strict_join_results.items():\n",
        "    print(f\"\\nStrict joins for {table_pair}:\")\n",
        "    for col1, col2 in joins:\n",
        "        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n",
        "\n",
        "if not strict_join_results:\n",
        "    print(\"No strict joins found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqQ_iKw5dn0M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load credentials from file\n",
        "def load_credentials(path_to_credentials):\n",
        "    try:\n",
        "        with open(path_to_credentials, 'r') as file:\n",
        "            for line_num, line in enumerate(file, start=1):\n",
        "                line = line.strip()\n",
        "                if line and '=' in line:\n",
        "                    key, value = line.split('=', 1)  # Split only on the first '='\n",
        "                    os.environ[key.strip()] = value.strip()\n",
        "                else:\n",
        "                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "        logging.info(\"Credentials loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading credentials: {str(e)}\")\n",
        "\n",
        "# Call the function to load credentials\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Create connection string for SQLAlchemy\n",
        "connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
        "engine = create_engine(connection_string)\n",
        "\n",
        "# Function to fetch column names for a table\n",
        "def get_columns(table_name):\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        SELECT column_name, data_type\n",
        "        FROM information_schema.columns\n",
        "        WHERE table_name = '{table_name}';\n",
        "        \"\"\"\n",
        "        with engine.connect() as connection:\n",
        "            df = pd.read_sql_query(query, connection)\n",
        "        return df[['column_name', 'data_type']].to_dict('records')\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to test join logic between two tables\n",
        "def test_joins(table1, table2, attempts=3):\n",
        "    columns_table1 = get_columns(table1)\n",
        "    columns_table2 = get_columns(table2)\n",
        "    successful_joins = []\n",
        "\n",
        "    for col1 in columns_table1:\n",
        "        for col2 in columns_table2:\n",
        "            # Only test joins on matching data types\n",
        "            if col1['data_type'] == col2['data_type']:\n",
        "                success_count = 0\n",
        "                for _ in range(attempts):  # Attempt the join multiple times\n",
        "                    query = f\"\"\"\n",
        "                    SELECT *\n",
        "                    FROM {table1} t1\n",
        "                    INNER JOIN {table2} t2\n",
        "                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n",
        "                    LIMIT 1;  -- Test with one row at a time\n",
        "                    \"\"\"\n",
        "                    try:\n",
        "                        with engine.connect() as connection:\n",
        "                            df = pd.read_sql_query(query, connection)\n",
        "                            if not df.empty:\n",
        "                                success_count += 1\n",
        "                    except Exception as e:\n",
        "                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n",
        "\n",
        "                if success_count == attempts:  # Only count as successful if all attempts work\n",
        "                    successful_joins.append((col1['column_name'], col2['column_name']))\n",
        "                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n",
        "\n",
        "    return successful_joins\n",
        "\n",
        "# Cross-check join fields for all table pairs\n",
        "tables = [\n",
        "    \"users\", \"ocpp_sub_session\"\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for i, table1 in enumerate(tables):\n",
        "    for table2 in tables[i+1:]:\n",
        "        logging.info(f\"Testing joins between {table1} and {table2}\")\n",
        "        joins = test_joins(table1, table2)\n",
        "        if joins:\n",
        "            results[f\"{table1} <-> {table2}\"] = joins\n",
        "        else:\n",
        "            logging.info(f\"No join found between {table1} and {table2}\")\n",
        "\n",
        "# Print results\n",
        "for table_pair, joins in results.items():\n",
        "    print(f\"Successful joins for {table_pair}: {joins}\")\n",
        "\n",
        "if not results:\n",
        "    print(\"No successful joins found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU41V4jQdn2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMXMJghgdn5H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwvrH2ndn7Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VIJZs8fdn9k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kH2jY7TdoAK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ex8BAejxDoB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Example dataframe (assuming df['message'] contains the raw strings)\n",
        "# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n",
        "def clean_message(msg):\n",
        "    try:\n",
        "        # Remove the prefix\n",
        "        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n",
        "\n",
        "        # Attempt to load the cleaned message as JSON\n",
        "        return json.loads(msg_cleaned)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        # If the message cannot be decoded as JSON, return None or handle as needed\n",
        "        return None\n",
        "\n",
        "# Apply the function to the 'message' column\n",
        "df['message'] = df['message'].apply(clean_message)\n",
        "\n",
        "# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n",
        "df = df[df['message'].notna()]\n",
        "\n",
        "# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n",
        "flattened_rows = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    message = row['message']  # Now this is a valid JSON object\n",
        "\n",
        "    # Extract top-level fields\n",
        "    connector_id = message.get('connectorId')\n",
        "    transaction_id = message.get('transactionId')\n",
        "\n",
        "    # Keep the 'meterValue' field as is (as a list of dicts)\n",
        "    meter_value = message.get('meterValue', [])\n",
        "\n",
        "    # Add a row to the flattened list, including the nested 'meterValue' list\n",
        "    flattened_rows.append({\n",
        "        '_time': row['time'],  # Retain the original timestamp from the dataframe\n",
        "        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n",
        "        'connectorId': connector_id,\n",
        "        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n",
        "    })\n",
        "\n",
        "# Step 2: Create a new DataFrame from the flattened rows\n",
        "flattened_df = pd.DataFrame(flattened_rows)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(flattened_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKbk_sGLxZ_Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set pandas options to display the full content of any column (e.g., 'meterValue')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Now, display the full content of the 'meterValue' column for the first 5 rows\n",
        "print(flattened_df['meterValue'].head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQhRPDO5zJH0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a list to hold the expanded rows\n",
        "expanded_rows = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for idx, row in flattened_df.iterrows():\n",
        "    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n",
        "\n",
        "    # For each meter value entry (there should be one timestamp and a list of measurements)\n",
        "    for meter in meter_values:\n",
        "        timestamp = meter['timestamp']  # Extract the timestamp\n",
        "\n",
        "        # Initialize values for each measurement type\n",
        "        watt_hours_value = None  # WattHours\n",
        "        amps_value = None        # Amps (Current)\n",
        "        voltage_value = None     # Voltage (Volts)\n",
        "\n",
        "        # Iterate over the sampledValue list (which contains the three measurements)\n",
        "        for sample in meter['sampledValue']:\n",
        "            # Check the 'unit' to assign the value to the correct column\n",
        "            if sample['unit'] == 'Wh':  # WattHours\n",
        "                watt_hours_value = sample['value']\n",
        "            elif sample['unit'] == 'A':  # Amps (Current)\n",
        "                amps_value = sample['value']\n",
        "            elif sample['unit'] == 'V':  # Volts (Voltage)\n",
        "                voltage_value = sample['value']\n",
        "\n",
        "        # Append the expanded row with the extracted values\n",
        "        expanded_rows.append({\n",
        "            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n",
        "            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n",
        "            'connectorId': row['connectorId'],  # Connector ID\n",
        "            'timestamp': timestamp,  # Timestamp from the meter value\n",
        "            'WattHours': watt_hours_value,  # Renamed to WattHours\n",
        "            'Amps': amps_value,  # Keep Amps as the column name\n",
        "            'Voltage': voltage_value  # Value for Voltage (V)\n",
        "        })\n",
        "\n",
        "# Create a new DataFrame from the expanded rows\n",
        "expanded_df = pd.DataFrame(expanded_rows)\n",
        "\n",
        "# Convert the numeric columns to appropriate types (float)\n",
        "expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n",
        "expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n",
        "expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(expanded_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWcgRo8Dz1Nm"
      },
      "outputs": [],
      "source": [
        "# Ensure all columns are numeric (in case there are any string values left)\n",
        "expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n",
        "expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n",
        "expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n",
        "\n",
        "# Classify values as 0 or > 0 for each of the measurements\n",
        "expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n",
        "expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n",
        "expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.countplot(data=expanded_df, x='WattHours_Class')\n",
        "plt.title('Count of Rows with WattHours: 0 vs > 0')\n",
        "plt.xlabel('WattHours Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.countplot(data=expanded_df, x='Amps_Class')\n",
        "plt.title('Count of Rows with Amps: 0 vs > 0')\n",
        "plt.xlabel('Amps Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.countplot(data=expanded_df, x='Voltage_Class')\n",
        "plt.title('Count of Rows with Voltage: 0 vs > 0')\n",
        "plt.xlabel('Voltage Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVsRrQLIz1QQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure that the '_time' column is in datetime format\n",
        "expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n",
        "\n",
        "# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n",
        "expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n",
        "expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n",
        "expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n",
        "\n",
        "# Drop rows where any of the values are missing\n",
        "expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n",
        "\n",
        "# Set the style for the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the figure and axes for the plots\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot Amps over time\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n",
        "plt.title('Amps over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Amps')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot WattHours over time\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n",
        "plt.title('WattHours over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('WattHours')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot Voltage over time\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n",
        "plt.title('Voltage over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Voltage')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust layout to avoid overlap of labels\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi2OjXK_2hXK"
      },
      "outputs": [],
      "source": [
        "expanded_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TbILosnz1Sh"
      },
      "outputs": [],
      "source": [
        "# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n",
        "descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n",
        "\n",
        "# Display the statistics\n",
        "print(descriptive_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zZUwehE16FW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style for the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a figure with 3 subplots (1 row, 3 columns)\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot for Amps\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n",
        "plt.title('Boxplot of Amps')\n",
        "plt.ylabel('Amps')\n",
        "\n",
        "# Plot for WattHours\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n",
        "plt.title('Boxplot of WattHours')\n",
        "plt.ylabel('WattHours')\n",
        "\n",
        "# Plot for Voltage\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n",
        "plt.title('Boxplot of Voltage')\n",
        "plt.ylabel('Voltage')\n",
        "\n",
        "# Adjust layout to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcBnl60S2LIP"
      },
      "outputs": [],
      "source": [
        "# count of propertyIDs\n",
        "\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "# Load credentials from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "\n",
        "# Table to process (the 'stations' table)\n",
        "table = \"stations\"\n",
        "\n",
        "# Connect to the PostgreSQL database and run the query\n",
        "connection = psycopg2.connect(**connection_params)\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Query to count unique 'property_id' values\n",
        "query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetch the result\n",
        "result = cursor.fetchone()\n",
        "\n",
        "# Extract and print the count\n",
        "unique_property_id_count = result[0] if result else 0\n",
        "print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n",
        "\n",
        "# Clean up\n",
        "cursor.close()\n",
        "connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc9r14vD2LK6"
      },
      "outputs": [],
      "source": [
        "#Count of cluster IDs\n",
        "\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "# Load credentials from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "\n",
        "# Table to process (the 'stations' table)\n",
        "table = \"stations\"\n",
        "\n",
        "# Connect to the PostgreSQL database and run the query\n",
        "connection = psycopg2.connect(**connection_params)\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Query to count unique 'property_id' values\n",
        "query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetch the result\n",
        "result = cursor.fetchone()\n",
        "\n",
        "# Extract and print the count\n",
        "unique_property_id_count = result[0] if result else 0\n",
        "print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n",
        "\n",
        "# Clean up\n",
        "cursor.close()\n",
        "connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F01IOcSb2LNZ"
      },
      "outputs": [],
      "source": [
        "# counts of peropertyID and clusterIDimport os\n",
        "import psycopg2\n",
        "\n",
        "# Load credentials from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "\n",
        "# Table to process (the 'stations' table)\n",
        "table = \"stations\"\n",
        "\n",
        "# Connect to the PostgreSQL database and run the query\n",
        "connection = psycopg2.connect(**connection_params)\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Query to count unique 'cluster_id' values for each 'property_id'\n",
        "query = f\"\"\"\n",
        "SELECT property_id, COUNT(DISTINCT cluster_id)\n",
        "FROM {table}\n",
        "GROUP BY property_id\n",
        ";\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetch all the results\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    property_id, cluster_count = row\n",
        "    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n",
        "\n",
        "# Clean up\n",
        "cursor.close()\n",
        "connection.close()\n",
        "\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "# Load credentials from environment variables\n",
        "connection_params = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "\n",
        "# Table to process (the 'stations' table)\n",
        "table = \"stations\"\n",
        "\n",
        "# Connect to the PostgreSQL database and run the query\n",
        "connection = psycopg2.connect(**connection_params)\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Query to count unique 'property_id' values for each 'cluster_id'\n",
        "query = f\"\"\"\n",
        "SELECT cluster_id, COUNT(DISTINCT property_id)\n",
        "FROM {table}\n",
        "GROUP BY cluster_id;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetch all the results\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    cluster_id, property_count = row\n",
        "    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n",
        "\n",
        "# Clean up\n",
        "cursor.close()\n",
        "connection.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJCn043ops5i"
      },
      "source": [
        "OCPP_SessionID has a userID and TransactionID\n",
        "I need to map to the cluster and property\n",
        "\n",
        "Stations has propertyID and cluster_id\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkQRHSF2pYLF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRERnF5HpYNr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzZNFFc8pYQY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbxg6jVhpYS3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wI1hDsHpYVx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiA1AO1z2LSG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmnNNDMY2LUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpS8FyTqEWgq"
      },
      "outputs": [],
      "source": [
        "# Clean message field and port to a df\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean up the 'message' field by removing the prefix and parsing JSON\n",
        "def clean_and_parse_message(message):\n",
        "    try:\n",
        "        # Strip the non-JSON prefix before the first '{'\n",
        "        cleaned_message = message[message.find('{'):]\n",
        "        # Parse the cleaned JSON string\n",
        "        return json.loads(cleaned_message)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to flatten nested JSON\n",
        "def flatten_json(y):\n",
        "    out = {}\n",
        "\n",
        "    def flatten(x, name=''):\n",
        "        if isinstance(x, dict):\n",
        "            for a in x:\n",
        "                flatten(x[a], name + a + '_')\n",
        "        elif isinstance(x, list):\n",
        "            i = 0\n",
        "            for a in x:\n",
        "                flatten(a, name + str(i) + '_')\n",
        "                i += 1\n",
        "        else:\n",
        "            out[name[:-1]] = x\n",
        "\n",
        "    flatten(y)\n",
        "    return out\n",
        "\n",
        "# Apply the cleaning and parsing function to all rows in the 'message' field\n",
        "df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n",
        "\n",
        "# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n",
        "valid_df = df[df['parsed_message'].notnull()]\n",
        "\n",
        "# Flatten all the JSON objects and store them in a new DataFrame\n",
        "flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n",
        "\n",
        "# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n",
        "new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n",
        "\n",
        "# Write the DataFrame to CSV with new naming convention\n",
        "output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n",
        "new_df.to_csv(output_csv_path, index=False)\n",
        "logging.info(f\"Data written to {output_csv_path} successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PnkEtU3uDKj"
      },
      "outputs": [],
      "source": [
        "new_df.info\n",
        "new_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDqqE7Swt6vj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df= new_df\n",
        "\n",
        "# Assuming your DataFrame is named df\n",
        "# Step 1: Convert 'time' to datetime\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n",
        "\n",
        "# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n",
        "df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n",
        "\n",
        "# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n",
        "df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n",
        "\n",
        "# Step 4: Plot the time series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Meter Value')\n",
        "plt.title('Meter Value Over Time')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlT8kevDasAZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Group by user_id and count the occurrences of meterValue_0_timestamp\n",
        "hist_data = df.groupby('user_id')['message'].count().reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "hist_data.columns = ['user_id', 'count']\n",
        "\n",
        "# Sort the data by user_id for better visualization\n",
        "hist_data = hist_data.sort_values('user_id')\n",
        "\n",
        "# Plotting the normal line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n",
        "plt.xlabel('User ID')\n",
        "plt.ylabel('Count of message')\n",
        "plt.title('Count of message per User ID')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcwXooB_tXnf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming new_df is already defined and contains the necessary columns\n",
        "\n",
        "# List of columns to keep\n",
        "columns_to_keep = [\n",
        "    #'time',\n",
        "    'user_id',\n",
        "    #'station_id',\n",
        "    'property_id',\n",
        "    'connectorId',\n",
        "    'meterValue_0_timestamp',\n",
        "    'meterValue_0_sampledValue_1_value',\n",
        "    'meterValue_0_sampledValue_1_context',\n",
        "    'meterValue_0_sampledValue_1_format',\n",
        "    'meterValue_0_sampledValue_1_measurand',\n",
        "    'meterValue_0_sampledValue_1_phase',\n",
        "    'meterValue_0_sampledValue_1_location',\n",
        "    'meterValue_0_sampledValue_1_unit'\n",
        "]\n",
        "\n",
        "# Create new_df_2 with only the selected columns\n",
        "new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Convert 'time' to datetime\n",
        "new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n",
        "\n",
        "# Check for any NaT values that may have resulted from the conversion\n",
        "if new_df_2['meterValue_0_timestamp'].isnull().any():\n",
        "    print(\"Some values could not be converted to datetime.\")\n",
        "\n",
        "# Extract day and hour using .loc to avoid warnings\n",
        "new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n",
        "new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLthqnU8u2eM"
      },
      "outputs": [],
      "source": [
        "new_df_2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMxEqA8Qbkxz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = new_df_2\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Group by user_id and count the occurrences of meterValue_0_timestamp\n",
        "hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "hist_data.columns = ['user_id', 'count']\n",
        "\n",
        "# Sort the data by user_id for better visualization\n",
        "hist_data = hist_data.sort_values('user_id')\n",
        "\n",
        "# Plotting the normal line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n",
        "plt.xlabel('User ID')\n",
        "plt.ylabel('Count of message')\n",
        "plt.title('Count of message per User ID')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N29p7Kq_xWuu"
      },
      "outputs": [],
      "source": [
        "# Assuming new_df_2 is the df\n",
        "\n",
        "unique_values = new_df_2['user_id'].unique()\n",
        "\n",
        "# To display the unique values\n",
        "print(unique_values)\n",
        "\n",
        "\n",
        "# Assuming new_df_2 is your DataFrame\n",
        "unique_count = new_df_2['user_id'].nunique()\n",
        "\n",
        "# To display the count of unique user_id values\n",
        "print(f\"Number of unique user_id values: {unique_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEjr917eu9Rb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n",
        "\n",
        "max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n",
        "\n",
        "result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n",
        "\n",
        "print(result_df)\n",
        "result_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnNlof0taJFO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Group by user_id and count the occurrences of meterValue_0_timestamp\n",
        "hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "hist_data.columns = ['user_id', 'count']\n",
        "\n",
        "# Sort the data by user_id for better visualization\n",
        "hist_data = hist_data.sort_values('user_id')\n",
        "\n",
        "# Plotting the normal line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n",
        "plt.xlabel('User ID')\n",
        "plt.ylabel('Count of meterValue_0_timestamp')\n",
        "plt.title('Count of meterValue_0_timestamp per User ID')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "842I60p_-VjA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming result_df is your DataFrame\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in DataFrame:\")\n",
        "print(result_df.columns.tolist())\n",
        "\n",
        "# Specify the user_id you're interested in\n",
        "specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n",
        "\n",
        "# Filter the DataFrame for the specific user_id\n",
        "filtered_df = result_df[result_df['user_id'] == specific_user_id]\n",
        "\n",
        "# Check if there are any rows for the specified user_id\n",
        "if not filtered_df.empty:\n",
        "    # Check for the timestamp column again\n",
        "    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n",
        "    value_col = 'meterValue_0_sampledValue_1_value'\n",
        "\n",
        "    # Ensure the column names are correct\n",
        "    print(\"Filtered DataFrame columns:\")\n",
        "    print(filtered_df.columns.tolist())\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n",
        "    plt.title(f'Meter Values for User ID: {specific_user_id}')\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel('Meter Value')\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
        "    plt.grid()\n",
        "    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"No data found for user_id: {specific_user_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp5ImngUusix"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Write the DataFrame to CSV\n",
        "output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n",
        "result_df.to_csv(output_csv_path, index=False)\n",
        "logging.info(f\"Data written to {output_csv_path} successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8ZZ-b6HuslW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3cWsdlTusnX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TvKiocBsdvU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A0Tx9v-sdxw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKXZgFB5sd0F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGBdB1TPsd2a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDJEMK-rsd4_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZmZ5lhTAhpZ"
      },
      "outputs": [],
      "source": [
        "# Identify source of error in message JSON\n",
        "import json\n",
        "\n",
        "# Function to recursively extract all keys\n",
        "def get_all_keys(data, keys=set()):\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            keys.add(key)\n",
        "            get_all_keys(value, keys)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            get_all_keys(item, keys)\n",
        "    return keys\n",
        "\n",
        "# Initialize set to collect unique keys\n",
        "all_keys = set()\n",
        "\n",
        "# Variable to track the first invalid JSON example\n",
        "invalid_json_index = None\n",
        "invalid_json_sample = None\n",
        "\n",
        "# Iterate through df['message'] and try to process each row\n",
        "for index, message in enumerate(df['message']):\n",
        "    try:\n",
        "        # Parse JSON string\n",
        "        message_json = json.loads(message)\n",
        "        # Extract keys from each valid message\n",
        "        all_keys = get_all_keys(message_json, all_keys)\n",
        "    except json.JSONDecodeError:\n",
        "        # Capture the index and value of the first invalid JSON row\n",
        "        invalid_json_index = index\n",
        "        invalid_json_sample = df.iloc[index]\n",
        "        break  # Stop after finding the first invalid JSON entry\n",
        "\n",
        "# Print all extracted keys\n",
        "print(\"Extracted keys:\", all_keys)\n",
        "\n",
        "# Display the first invalid JSON example\n",
        "if invalid_json_sample is not None:\n",
        "    print(f\"First invalid JSON entry at row {invalid_json_index}:\")\n",
        "    print(invalid_json_sample)\n",
        "else:\n",
        "    print(\"No invalid JSON entries found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em9mU8OyCGDy"
      },
      "outputs": [],
      "source": [
        "# Print the first 5 rows of the 'message' field to inspect for any issues\n",
        "for i in range(5):\n",
        "    print(f\"Row {i} message: {df['message'].iloc[i]}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnT-POY9rK8t"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "meterValue_0_sampledValue_1_unit = A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-MhWFxWDpT0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Function to clean up the 'message' field by removing the prefix and parsing JSON\n",
        "def clean_and_parse_message(message):\n",
        "    try:\n",
        "        # Strip the non-JSON prefix before the first '{'\n",
        "        cleaned_message = message[message.find('{'):]  # This removes everything before the first '{'\n",
        "        # Parse the cleaned JSON string\n",
        "        return json.loads(cleaned_message)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to recursively extract all keys from JSON objects\n",
        "def get_all_keys(data, keys=set()):\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            keys.add(key)\n",
        "            get_all_keys(value, keys)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            get_all_keys(item, keys)\n",
        "    return keys\n",
        "\n",
        "# Apply the cleaning and parsing function to all rows in the 'message' field\n",
        "df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n",
        "\n",
        "# Initialize a set to store unique keys from the parsed JSON objects\n",
        "json_keys = set()\n",
        "\n",
        "# Extract keys from the 'parsed_message' field for all valid rows\n",
        "for parsed_message in df['parsed_message'].dropna():  # Exclude rows with None (failed parsing)\n",
        "    json_keys = get_all_keys(parsed_message, json_keys)\n",
        "\n",
        "# Get the original DataFrame headers (columns)\n",
        "df_headers = set(df.columns)\n",
        "\n",
        "# Combine both sets: original DataFrame headers and extracted JSON keys\n",
        "complete_headers = df_headers.union(json_keys)\n",
        "\n",
        "# Print the complete view of all data fields\n",
        "print(\"Complete view of all data fields (headers + JSON keys):\")\n",
        "print(complete_headers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3T1rTXcE_tC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Function to clean up the 'message' field by removing the prefix\n",
        "def clean_message(message):\n",
        "    # Strip the non-JSON prefix before the first '{'\n",
        "    cleaned_message = message[message.find('{'):] if '{' in message else message\n",
        "    return cleaned_message\n",
        "\n",
        "# Function to validate JSON and return whether it's valid\n",
        "def validate_json(message):\n",
        "    try:\n",
        "        # Attempt to load the cleaned message as JSON\n",
        "        json.loads(message)\n",
        "        return True, None  # Valid JSON\n",
        "    except json.JSONDecodeError as e:\n",
        "        return False, str(e)  # Invalid JSON with error message\n",
        "\n",
        "# Apply the cleaning function to all rows in the 'message' field\n",
        "df['cleaned_message'] = df['message'].apply(clean_message)\n",
        "\n",
        "# Apply the validation function to check JSON validity and capture errors\n",
        "df['is_valid_json'], df['json_validation'] = zip(*df['cleaned_message'].apply(validate_json))\n",
        "\n",
        "# Display rows with invalid JSON\n",
        "invalid_rows = df[df['is_valid_json'] == False]\n",
        "\n",
        "# Show a few invalid rows along with the error message for investigation\n",
        "print(invalid_rows[['message', 'cleaned_message', 'json_validation']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tjf4XMNFVc-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean up the 'message' field by removing the prefix\n",
        "def clean_message(message):\n",
        "    # Strip the non-JSON prefix before the first '{'\n",
        "    cleaned_message = message[message.find('{'):] if '{' in message else message\n",
        "    return cleaned_message\n",
        "\n",
        "# Function to flatten nested JSON\n",
        "def flatten_json(y):\n",
        "    out = {}\n",
        "\n",
        "    def flatten(x, name=''):\n",
        "        if isinstance(x, dict):\n",
        "            for a in x:\n",
        "                flatten(x[a], name + a + '_')\n",
        "        elif isinstance(x, list):\n",
        "            i = 0\n",
        "            for a in x:\n",
        "                flatten(a, name + str(i) + '_')\n",
        "                i += 1\n",
        "        else:\n",
        "            out[name[:-1]] = x\n",
        "\n",
        "    flatten(y)\n",
        "    return out\n",
        "\n",
        "# Function to clean, parse and flatten JSON data\n",
        "def clean_and_flatten_message(message):\n",
        "    # Clean the message by removing non-JSON prefix\n",
        "    cleaned_message = clean_message(message)\n",
        "    # Parse the cleaned message into JSON\n",
        "    try:\n",
        "        parsed_message = json.loads(cleaned_message)\n",
        "        # Flatten the parsed JSON\n",
        "        return flatten_json(parsed_message)\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "# Apply the clean, parse, and flatten function to all rows in the 'message' field\n",
        "flattened_data = df['message'].apply(clean_and_flatten_message).apply(pd.Series)\n",
        "\n",
        "# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' field)\n",
        "flattened_df = pd.concat([df.drop(columns=['message']), flattened_data], axis=1)\n",
        "\n",
        "# Display the first few rows of the flattened DataFrame\n",
        "#print(flattened_df.head())\n",
        "\n",
        "\n",
        "flattened_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_FMGlpjPcsJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "#meterValue_0_sampledValue_0_unit = Wh\n",
        "#meterValue_0_sampledValue_1_unit = A\n",
        "#meterValue_0_sampledValue_2_unit = V\n",
        "#meterValue_0_sampledValue_3_unit = Celsius\n",
        "\n",
        "\n",
        "# Ensure 'time' column is in datetime format\n",
        "flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n",
        "\n",
        "# Define the specific day to filter (replace with the desired date)\n",
        "specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n",
        "\n",
        "# Filter the DataFrame for the specific day\n",
        "filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n",
        "\n",
        "# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n",
        "filtered_df['meterValue_0_sampledValue_1_value'] = filtered_df['meterValue_0_sampledValue_1_value'].astype(float)\n",
        "\n",
        "# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(filtered_df['meterValue_0_sampledValue_1_value'], fill=True)\n",
        "\n",
        "# Formatting the plot\n",
        "plt.xlabel('A') #Looking at amperage\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n",
        "\n",
        "# Convert y-axis to percentage\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX59jUMcQ2PX"
      },
      "outputs": [],
      "source": [
        "df = flattened_df\n",
        "df['calendar_day'] = df['time'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lZBmiLWRses"
      },
      "outputs": [],
      "source": [
        "# Find the index of the maximum value for meterValue_0_sampledValue_1_value within each calendar_day\n",
        "max_indices = df.groupby('calendar_day')['meterValue_0_sampledValue_1_value'].idxmax()\n",
        "\n",
        "# Select the hour from the time corresponding to those indices\n",
        "df['max_hour'] = df['time'].dt.hour  # Extract the hour part\n",
        "max_hour_per_day = df.loc[max_indices, ['calendar_day', 'max_hour']].reset_index(drop=True)\n",
        "\n",
        "print(max_hour_per_day)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtOwai_uPgrb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure 'time' column is in datetime format\n",
        "flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n",
        "\n",
        "# Define the specific day to filter (replace with the desired date)\n",
        "specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n",
        "\n",
        "# Filter the DataFrame for the specific day\n",
        "filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n",
        "\n",
        "# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n",
        "filtered_df['meterValue_0_sampledValue_1_value'] = filtered_df['meterValue_0_sampledValue_1_value'].astype(float)\n",
        "\n",
        "# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(filtered_df['meterValue_0_sampledValue_1_value'], fill=True)\n",
        "\n",
        "# Formatting the plot\n",
        "plt.xlabel('Amps')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n",
        "\n",
        "# Convert y-axis to percentage\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-GHkHCDO6TR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure 'time' column is in datetime format\n",
        "flattened_df['time'] = pd.to_datetime(flattened_df['time'])\n",
        "\n",
        "# Define the specific day to filter (replace with the desired date)\n",
        "specific_day = '2024-10-20'  # Example date in 'YYYY-MM-DD' format\n",
        "\n",
        "# Filter the DataFrame for the specific day\n",
        "filtered_df = flattened_df[flattened_df['time'].dt.date == pd.to_datetime(specific_day).date()]\n",
        "\n",
        "# Explicitly cast 'meterValue_0_sampledValue_2_value' to float\n",
        "filtered_df['meterValue_0_sampledValue_2_value'] = filtered_df['meterValue_0_sampledValue_2_value'].astype(float)\n",
        "\n",
        "# Create the distribution plot for the filtered data (meterValue_0_sampledValue_2_value)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(filtered_df['meterValue_0_sampledValue_2_value'], fill=True)\n",
        "\n",
        "# Formatting the plot\n",
        "plt.xlabel('Volts')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title(f'Distribution of meterValue_0_sampledValue_2_value on {specific_day}')\n",
        "\n",
        "# Convert y-axis to percentage\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x)))\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11y38iI97BbjLgUt8QX8sx0DYgkr60xWp",
      "authorship_tag": "ABX9TyPLwPga4R+GPqB893d5sgiV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}