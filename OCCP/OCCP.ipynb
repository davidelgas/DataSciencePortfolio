{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"XznKesLDcf0o"},"source":["## Charging System Diagram\n","Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13985,"status":"ok","timestamp":1737154723024,"user":{"displayName":"David E.","userId":"12500566977266345478"},"user_tz":480},"id":"mhwSzFEWit8p","outputId":"44a8fb85-cf6b-4d26-ee9f-29a5ec03b155"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32353,"status":"error","timestamp":1737154757209,"user":{"displayName":"David E.","userId":"12500566977266345478"},"user_tz":480},"id":"fz_Nyx0M2KOU","outputId":"4ae9c84a-fc8e-4b9c-b8da-defefa62ded1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyGithub\n","  Downloading PyGithub-2.5.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pynacl>=1.4.0 (from PyGithub)\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.15)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Downloading PyGithub-2.5.0-py3-none-any.whl (375 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.9/375.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynacl, PyGithub\n","Successfully installed PyGithub-2.5.0 pynacl-1.5.0\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Collecting pyxlsb\n","  Downloading pyxlsb-1.0.10-py2.py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading pyxlsb-1.0.10-py2.py3-none-any.whl (23 kB)\n","Installing collected packages: pyxlsb\n","Successfully installed pyxlsb-1.0.10\n","Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.11/dist-packages (2.0.37)\n","Collecting psycopg2-binary\n","  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy) (4.12.2)\n","Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: psycopg2-binary\n","Successfully installed psycopg2-binary-2.9.10\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["psycopg2"]},"id":"284f5feae456412d984da11d5d65da82"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-437313f35991>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mfamilies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgofplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProbPlot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqplot_2samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayes_mi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesGaussMI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/graphics/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcorrelation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_corr_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfactorplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minteraction_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfboxplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrboxplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrainbowplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgofplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqqplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplottools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrainbow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/graphics/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_import_mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonparametric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_density\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDEMultivariate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/nonparametric/kernel_density.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kernel_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenericKDE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEstimatorSettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_adjust_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Packages and methods\n","\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import sys\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import matplotlib.pyplot as p\n","\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvDdY57l4fxE"},"outputs":[],"source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function with your parameters\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suHAcKhHVfV8"},"outputs":[],"source":["# import log data\n","\n","import pandas as pd\n","import numpy as np\n","\n","def load_file(file_path):\n","    \"\"\"Load a single CSV file.\"\"\"\n","    return pd.read_csv(file_path)\n","\n","def concatenate_files(file_paths):\n","    \"\"\"Load and combine multiple CSV files.\"\"\"\n","    dfs = []\n","    for file_path in file_paths:\n","        df = load_file(file_path)\n","        if not df.empty:\n","            dfs.append(df)\n","\n","    return pd.concat(dfs, ignore_index=True)\n","\n","if __name__ == \"__main__\":\n","    file_paths = [\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/dec_100_sample.csv'\n","    ]\n","\n","    # Concatenate all files\n","    df_logs = concatenate_files(file_paths)\n","\n","    # Save the combined raw data\n","    df_logs.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgAP-CX4LKV4"},"outputs":[],"source":["# Ingest property tables\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","    with open(path_to_credentials, 'r') as file:\n","        for line in file:\n","            if '=' in line:\n","                key, value = line.split('=', 1)\n","                os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch properties table\n","cursor.execute(\"SELECT * FROM properties;\")\n","properties_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","properties_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv', index=False)\n","\n","# Fetch property_types table\n","cursor.execute(\"SELECT * FROM property_types;\")\n","property_types_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","property_types_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv', index=False)\n","\n","# Close connection\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jni86zvuc4ym"},"outputs":[],"source":["# import property data\n","import pandas as pd\n","\n","def process_properties(file_path, output_path):\n","    \"\"\"Process properties file workflow.\"\"\"\n","    # Load CSV\n","    df_prop = pd.read_csv(file_path)\n","\n","    # Clean IDs\n","    df_prop = df_prop[df_prop['id'].notna()].reset_index(drop=True)\n","\n","    # Rename id column\n","    df_prop = df_prop.rename(columns={'id': 'property_id'})\n","\n","    # Save processed data\n","    df_prop.to_pickle(output_path_3 + '.pkl')\n","\n","    return df_prop\n","\n","if __name__ == \"__main__\":\n","    input_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","    output_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop'\n","\n","    # Run workflow\n","    df_prop = process_properties(input_path_3, output_path_3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IjOthMDc4uL"},"outputs":[],"source":["# import prop size data\n","# This is from Salesforce\n","\n","import pandas as pd\n","\n","def clean_record_id(record_id):\n","   return str(record_id).replace('zcrm_', '') if pd.notna(record_id) else record_id\n","\n","def process_property_sizes(file_path, output_path):\n","   \"\"\"Process property size file workflow.\"\"\"\n","   # Load CSV with explicit encoding\n","   df_prop_size = pd.read_csv(file_path, encoding='latin-1')\n","\n","   # Rename columns\n","   df_prop_size = df_prop_size.rename(columns={\n","       'Record Id': 'Record_id_lg',\n","       'Record Id (Managed Account)': 'Record_id_js'\n","   })\n","\n","   # Clean IDs by removing 'zcrm_' prefix directly in the existing column\n","   df_prop_size['Record_id_js'] = df_prop_size['Record_id_js'].apply(clean_record_id)\n","\n","   # Clean IDs\n","   df_prop_size = df_prop_size[df_prop_size['Record_id_js'].notna()].reset_index(drop=True)\n","\n","   # Cast id to object type\n","   df_prop_size = df_prop_size.astype({'Record_id_js': 'object'})\n","\n","   # Save processed data\n","   df_prop_size.to_pickle(output_path + '.pkl')\n","\n","   return df_prop_size\n","\n","if __name__ == \"__main__\":\n","   input_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/All_Viable_Accounts_JS.csv'\n","   output_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size'\n","\n","   # Run workflow\n","   df_prop_size = process_property_sizes(input_path_2, output_path_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7gGYKeQIlYm"},"outputs":[],"source":["# import prop type data\n","# This is from AWS\n","\n","import pandas as pd\n","\n","def clean_record_id(record_id):\n","   \"\"\"\n","   Remove 'zcrm_' prefix from Record Id\n","   \"\"\"\n","   return str(record_id).replace('zcrm_', '') if pd.notna(record_id) else record_id\n","\n","def process_property_types(file_path, output_path):\n","   # Load CSV with explicit encoding\n","   df_types = pd.read_csv(file_path, encoding='latin-1')\n","\n","   # Rename columns\n"," #  df_prop_size = df_prop_size.rename(columns={\n","  #     'Record Id': 'Record_id_lg',\n"," #      'Record Id (Managed Account)': 'Record_id_js'\n","  # })\n","\n","  # # Clean IDs by removing 'zcrm_' prefix directly in the existing column\n"," #  df_prop_size['Record_id_js'] = df_prop_size['Record_id_js'].apply(clean_record_id)\n","\n","  # # Clean IDs\n","  # df_prop_size = df_prop_size[df_prop_size['Record_id_js'].notna()].reset_index(drop=True)\n","\n","   # Cast id to object type\n","  # df_prop_size = df_prop_size.astype({'Record_id_js': 'object'})\n","\n","   # Save processed data\n","   df_types.to_pickle(output_path + '.pkl')\n","\n","   return df_types\n","\n","if __name__ == \"__main__\":\n","   input_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","   output_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type'\n","\n","   # Run workflow\n","   df_types = process_property_types(input_path_2, output_path_2)"]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0sBeZzEtE5O"},"outputs":[],"source":["# Here are the dfs Ill be working with\n","\n","df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')\n","df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl')\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl')\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl')"]},{"cell_type":"markdown","metadata":{"id":"mtmzDPK5HSAw"},"source":["###This is what the JSON field looks like\n","\n","\n","\n","{\"connectorId\":1,\"transactionId\":1417592169,\"meterValue\":[{\"timestamp\":\"2025-01-14T13:27:37.145Z\",\"sampledValue\":[{\"value\":\"31323855.0\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Energy.Active.Import.Register\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"Wh\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"240.57\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Voltage\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"V\"},{\"value\":\"28\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Temperature\",\"phase\":null,\"location\":\"Body\",\"unit\":\"Celsius\"},{\"value\":\"6.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"1440.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Active.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"}]}]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdW03n8B_At0"},"outputs":[],"source":["# Unpack the JSON in the log file\n","import pandas as pd\n","import json\n","\n","def expand_message_json(df):\n","    rows = []\n","\n","    for idx, row in df.iterrows():\n","        # Parse the JSON message\n","        message = json.loads(row['cleaned_message']) if pd.notna(row['cleaned_message']) else {}\n","\n","        # Get transactionId from the message\n","        transaction_id = message.get('transactionId')\n","\n","        # Extract meter values\n","        meter_values = message.get('meterValue', [])\n","        for meter in meter_values:\n","            timestamp = meter.get('timestamp')\n","            sampled_values = meter.get('sampledValue', [])\n","\n","            # Filter for only A and W units\n","            for sample in sampled_values:\n","                unit = sample.get('unit')\n","                if unit in ['A', 'W']:\n","                    rows.append({\n","                        'property_id': row['property_id'],\n","                        'user_id': row['user_id'],\n","                        'transaction_id': transaction_id,  # Fixed variable name here\n","                        'timestamp': timestamp,\n","                        'value': sample.get('value'),\n","                        'unit': unit\n","                    })\n","\n","    return pd.DataFrame(rows)\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSOQ6IB3OzuL"},"outputs":[],"source":["# Normalize column naming\n","\n","df_prop_type = df_prop_type.rename(columns={'id': 'property_id'})\n","df_prop_type = df_prop_type.rename(columns={'name': 'prop_type'})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lLvvK85MCce"},"outputs":[],"source":["# Check join logic\n","# Does df_prop.managed_account_id join with df_prop_size.Record_id_js\n","\n","# Check overlap between the two columns\n","total_managed_accounts = len(df_prop['managed_account_id'].unique())\n","total_record_ids = len(df_prop_size['Record_id_js'].unique())\n","\n","# Check for matches\n","common_ids = set(df_prop['managed_account_id']) & set(df_prop_size['Record_id_js'])\n","total_matches = len(common_ids)\n","\n","print(f\"Unique values in df_prop.managed_account_id: {total_managed_accounts:,}\")\n","print(f\"Unique values in df_prop_size.Record_id_js: {total_record_ids:,}\")\n","print(f\"Number of matching IDs: {total_matches:,}\")\n","\n","# Calculate percentages\n","match_pct_prop = (total_matches / total_managed_accounts) * 100\n","match_pct_size = (total_matches / total_record_ids) * 100\n","\n","print(f\"\\nPercentage of df_prop.managed_account_id with matches: {match_pct_prop:.1f}%\")\n","print(f\"Percentage of df_prop_size.Record_id_js with matches: {match_pct_size:.1f}%\")\n","\n","# Generally speaking, yes\n","\n","\n","\n","# Check join logic\n","# Does df_prop.property_type join with df_prop_type.property_id\n","\n","total_prop_types = len(df_prop['property_type'].unique())\n","total_prop_ids = len(df_prop_type['property_id'].unique())\n","\n","# Check for matches\n","common_ids = set(df_prop['property_type']) & set(df_prop_type['property_id'])\n","total_matches = len(common_ids)\n","\n","print(f\"Unique values in df_prop.property_type: {total_prop_types:,}\")\n","print(f\"Unique values in df_prop_type.property_id: {total_prop_ids:,}\")\n","print(f\"Number of matching IDs: {total_matches:,}\")\n","\n","# Calculate percentages\n","match_pct_prop = (total_matches / total_prop_types) * 100\n","match_pct_type = (total_matches / total_prop_ids) * 100\n","\n","print(f\"\\nPercentage of df_prop.property_type with matches: {match_pct_prop:.1f}%\")\n","print(f\"Percentage of df_prop_type.property_id with matches: {match_pct_type:.1f}%\")\n","\n","# Generally speaking, yes\n","\n","\n","\n","# Check join logic\n","# Does df_logs.property_id join with df_prop.property_id\n","\n","# Check overlap between the two columns\n","total_log_ids = len(df_logs['property_id'].unique())\n","total_type_ids = len(df_prop['property_id'].unique())\n","\n","# Check for matches\n","common_ids = set(df_logs['property_id']) & set(df_prop['property_id'])\n","total_matches = len(common_ids)\n","\n","print(f\"Unique values in df_logs.property_id: {total_log_ids:,}\")\n","print(f\"Unique values in df_prop_type.property_id: {total_type_ids:,}\")\n","print(f\"Number of matching IDs: {total_matches:,}\")\n","\n","# Calculate percentages\n","match_pct_logs = (total_matches / total_log_ids) * 100\n","match_pct_type = (total_matches / total_type_ids) * 100\n","\n","print(f\"\\nPercentage of df_logs.property_id with matches: {match_pct_logs:.1f}%\")\n","print(f\"Percentage of df_prop_type.property_id with matches: {match_pct_type:.1f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kazsa0EcHcT"},"outputs":[],"source":["# Update log df with property metadata\n","\n","# Load the logs data from pickle\n","df_logs_exp = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","\n","def create_property_metadata(df_prop):\n","    # Select only the required columns\n","    df_metadata = df_prop[['property_id', 'property_type', 'managed_account_id']]\n","    return df_metadata\n","\n","def append_property_type_name(df_metadata, df_prop_type):\n","    df_metadata = df_metadata.merge(\n","        df_prop_type[['property_id', 'prop_type']],\n","        left_on='property_type',\n","        right_on='property_id',\n","        how='left'\n","    )\n","\n","    # Drop redundant property_id column and rename property_id_x\n","    df_metadata = df_metadata.drop(columns=['property_id_y'])\n","    df_metadata = df_metadata.rename(columns={'property_id_x': 'property_id'})\n","\n","    return df_metadata\n","\n","def append_parking_spaces(df_metadata, df_prop_size):\n","    df_metadata = df_metadata.merge(\n","        df_prop_size[['Record_id_js', 'Total Parking Space Count']],\n","        left_on='managed_account_id',\n","        right_on='Record_id_js',\n","        how='left'\n","    )\n","\n","    # Drop redundant join key and rename parking spaces column\n","    df_metadata = df_metadata.drop(columns=['Record_id_js'])\n","    df_metadata = df_metadata.rename(columns={'Total Parking Space Count': 'parking_spaces'})\n","\n","    return df_metadata\n","\n","def append_metadata_to_logs(df_logs, df_prop_metadata):\n","    \"\"\"\n","    Append property metadata to logs using property_id as join key.\n","    Left join preserves all logs records.\n","    \"\"\"\n","    df_logs_enriched = df_logs.merge(\n","        df_prop_metadata,\n","        on='property_id',\n","        how='left'\n","    )\n","\n","    return df_logs_enriched\n","\n","def process_and_save_enriched_logs(df_prop, df_prop_type, df_prop_size, df_logs, output_path):\n","    \"\"\"Process metadata and logs workflow and save to pickle.\"\"\"\n","\n","    # Ingest df\n","    df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","    # Create metadata table\n","    df_prop_metadata = create_property_metadata(df_prop)\n","\n","    # Append property type name\n","    df_prop_metadata = append_property_type_name(df_prop_metadata, df_prop_type)\n","\n","    # Append parking spaces\n","    df_prop_metadata = append_parking_spaces(df_prop_metadata, df_prop_size)\n","\n","    # Append metadata to logs\n","    df_logs_enriched = append_metadata_to_logs(df_logs, df_prop_metadata)\n","\n","    # Save enriched logs to pickle\n","    df_logs_enriched.to_pickle(output_path + '.pkl')\n","\n","    return df_logs_enriched\n","\n","if __name__ == \"__main__\":\n","    # Define output path\n","    enriched_logs_output = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched'\n","\n","    # Run workflow and save enriched logs\n","    df_logs_enriched = process_and_save_enriched_logs(\n","        df_prop,\n","        df_prop_type,\n","        df_prop_size,\n","        df_logs_exp,\n","        enriched_logs_output\n","    )\n","\n","    # Print final shapes to verify\n","    print(\"Original logs shape:\", df_logs.shape)\n","    print(\"Enriched logs shape:\", df_logs_enriched.shape)\n","    print(\"\\nNew columns added from metadata:\", sorted(set(df_logs_enriched.columns) - set(df_logs.columns)))\n","\n","df_logs_enriched.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xTeoi_jSVTw"},"outputs":[],"source":["# Remove rows with Null data and convert timestamp to UTC\n","import numpy as np\n","\n","df_logs_enriched = df_logs_enriched.replace([np.inf, -np.inf], np.nan)\n","df_logs_enriched = df_logs_enriched.dropna()\n","df_logs_enriched = df_logs_enriched[['property_id','prop_type','parking_spaces','user_id','transaction_id','timestamp','unit','value']]\n","\n","# Convert timestamp to UTC\n","df_logs_enriched['timestamp'] = pd.to_datetime(df_logs_enriched['timestamp'], format='ISO8601', utc=True)\n","\n","# Save the cleaned dataframe\n","df_logs_enriched.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl')\n","\n","# Flatten, encode\n","# One-hot encode 'prop_type' with 1/0 values\n","df_logs_enriched = pd.get_dummies(df_logs_enriched, columns=['prop_type'], prefix='prop_type', dtype=int)\n","\n","# Binary encode unit (0 for 'A', 1 for 'W') - being explicit about handling NaN\n","df_logs_enriched['unit_encoded'] = df_logs_enriched['unit'].map({'A': 0, 'W': 1}).fillna(0).astype(int)\n","df_logs_enriched = df_logs_enriched.drop('unit', axis=1)\n","\n","\n","# Save processed data\n","output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched'\n","\n","df_logs_enriched.to_pickle(output_path + '.pkl')"]},{"cell_type":"markdown","metadata":{"id":"UNc4o_n59G_G"},"source":["## Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5fnXoeF70NJ"},"outputs":[],"source":["# Check unique timestamp formats\n","print(\"Sample of unique timestamp formats:\")\n","print(df_logs_enriched['timestamp'].drop_duplicates().head(10))\n","\n","# Group by transaction_id and get min/max timestamps\n","time_spans = df_logs_enriched.groupby('transaction_id').agg({\n","   'timestamp': ['min', 'max']\n","}).reset_index()\n","\n","# Calculate duration for each transaction\n","time_spans['duration'] = time_spans['timestamp']['max'] - time_spans['timestamp']['min']\n","\n","# Calculate different time spans\n","spans = {\n","   'different_days': time_spans['duration'] >= pd.Timedelta(days=1),\n","   'different_hours': time_spans['duration'] >= pd.Timedelta(hours=1),\n","   'different_minutes': time_spans['duration'] >= pd.Timedelta(minutes=1),\n","   'different_seconds': time_spans['duration'] >= pd.Timedelta(seconds=1),\n","   'different_milliseconds': time_spans['duration'] >= pd.Timedelta(milliseconds=1)\n","}\n","\n","# Count transactions in each category\n","for span_type, mask in spans.items():\n","   count = mask.sum()\n","   percent = (count / len(time_spans)) * 100\n","   print(f\"Transactions spanning {span_type}: {count:,} ({percent:.2f}%)\")\n","\n","print(\"\\nDuration statistics:\")\n","print(time_spans['duration'].describe())\n","\n","# Additional context\n","print(\"\\nTotal transactions analyzed:\", len(time_spans))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcZxp8ni89AZ"},"outputs":[],"source":["# Compare usae data across property types\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# First ensure timestamps are datetime\n","df_logs_enriched['timestamp'] = pd.to_datetime(df_logs_enriched['timestamp'], format='ISO8601')\n","\n","# Create time_spans from df_logs_enriched\n","time_spans = df_logs_enriched.groupby('transaction_id').agg({\n","    'timestamp': ['min', 'max'],\n","    'property_id': 'first'  # keep property_id for merging\n","}).reset_index()\n","\n","time_spans.columns = ['transaction_id', 'min_time', 'max_time', 'property_id']\n","time_spans['duration'] = time_spans['max_time'] - time_spans['min_time']\n","time_spans['duration_hours'] = time_spans['duration'].dt.total_seconds() / 3600\n","\n","# Merge with property types from df_logs_enriched\n","prop_type_cols = [col for col in df_logs_enriched.columns if col.startswith('prop_type_')]\n","durations_with_type = time_spans.merge(\n","    df_logs_enriched[['property_id'] + prop_type_cols].drop_duplicates(),\n","    on='property_id'\n",")\n","\n","# Create subplot for each property type\n","n_types = len(prop_type_cols)\n","n_cols = 3\n","n_rows = (n_types + n_cols - 1) // n_cols\n","\n","plt.figure(figsize=(15, 4*n_rows))\n","\n","for idx, prop_type in enumerate(prop_type_cols, 1):\n","    plt.subplot(n_rows, n_cols, idx)\n","\n","    # Get durations for this property type\n","    type_durations = durations_with_type[durations_with_type[prop_type] == 1]['duration_hours']\n","    type_durations = type_durations[type_durations > 0]  # Filter positive durations\n","\n","    if len(type_durations) > 0:  # Only plot if we have data\n","        plt.hist(type_durations,\n","                bins=np.logspace(np.log10(type_durations.min()),\n","                               np.log10(type_durations.max()),\n","                               50))\n","        plt.xscale('log')\n","        plt.yscale('log')\n","\n","        plt.title(f'{prop_type.replace(\"prop_type_\", \"\")} (n={len(type_durations):,})')\n","        plt.xlabel('Duration (hours) - Log Scale')\n","        plt.ylabel('Number of Sessions - Log Scale')\n","        plt.grid(True)\n","\n","        # Add statistics\n","        plt.text(0.02, 0.95,\n","                f'Mean: {type_durations.mean():.1f}h\\n'\n","                f'Median: {type_durations.median():.1f}h',\n","                transform=plt.gca().transAxes,\n","                bbox=dict(facecolor='white', alpha=0.8))\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary statistics by property type\n","print(\"\\nSummary statistics by property type:\")\n","for prop_type in prop_type_cols:\n","    type_durations = durations_with_type[durations_with_type[prop_type] == 1]['duration_hours']\n","    type_durations = type_durations[type_durations > 0]\n","    if len(type_durations) > 0:\n","        print(f\"\\n{prop_type.replace('prop_type_', '')}:\")\n","        print(type_durations.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeuNqAdJ59_z"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Histogram of timestamps per transaction_id\n","timestamps_per_transaction = df_logs_enriched.groupby('transaction_id')['timestamp'].nunique()\n","\n","plt.figure(figsize=(12, 5))\n","\n","# Left plot - log scale\n","plt.subplot(1, 2, 1)\n","plt.hist(timestamps_per_transaction, bins=np.logspace(np.log10(1), np.log10(timestamps_per_transaction.max()), 50))\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.title('Unique Timestamps per Transaction ID (Log Scale)')\n","plt.xlabel('Number of Unique Timestamps')\n","plt.ylabel('Count of Transaction IDs')\n","plt.grid(True)\n","\n","# Right plot - properties per transaction\n","properties_per_transaction = df_logs_enriched.groupby('transaction_id')['property_id'].nunique()\n","plt.subplot(1, 2, 2)\n","plt.hist(properties_per_transaction, bins=50)\n","plt.title('Unique Properties per Transaction ID')\n","plt.xlabel('Number of Unique Properties')\n","plt.ylabel('Count of Transaction IDs')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTimestamps per transaction summary:\")\n","print(timestamps_per_transaction.describe())\n","print(\"\\nProperties per transaction summary:\")\n","print(properties_per_transaction.describe())\n","\n","# Print additional context\n","print(\"\\nTotal number of unique transactions:\", len(timestamps_per_transaction))\n","print(\"Total number of unique properties:\", df_logs_enriched['property_id'].nunique())\n","print(\"Total number of timestamps:\", df_logs_enriched['timestamp'].nunique())"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","source":["import pandas as pd\n","import pytz\n","import numpy as np\n","from datetime import datetime\n","\n","def convert_to_pst_components(df):\n","    \"\"\"Convert timestamp to PST and extract components.\"\"\"\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')\n","    df['timestamp'] = df['timestamp'].dt.tz_convert('US/Pacific')\n","\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","    df['minute'] = df['timestamp'].dt.minute\n","\n","    return df\n","\n","def add_day_info(df):\n","    \"\"\"Add day of week and weekend indicator.\"\"\"\n","    df['day_of_week'] = df['timestamp'].dt.dayofweek + 1\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)\n","    return df\n","\n","def get_nearest_holiday(df):\n","    \"\"\"Calculate proximity to major US holidays.\"\"\"\n","    major_holidays = {\n","        '2024-01-01': \"New Year's Day\",\n","        '2024-01-15': \"Martin Luther King Jr. Day\",\n","        '2024-02-19': \"Presidents Day\",\n","        '2024-05-27': \"Memorial Day\",\n","        '2024-07-04': \"Independence Day\",\n","        '2024-09-02': \"Labor Day\",\n","        '2024-11-28': \"Thanksgiving\",\n","        '2024-12-25': \"Christmas\",\n","        '2025-01-01': \"New Year's Day\",\n","        '2025-01-20': \"Martin Luther King Jr. Day\",\n","        '2025-02-17': \"Presidents Day\",\n","        '2025-05-26': \"Memorial Day\",\n","        '2025-07-04': \"Independence Day\",\n","        '2025-09-01': \"Labor Day\",\n","        '2025-11-27': \"Thanksgiving\",\n","        '2025-12-25': \"Christmas\"\n","    }\n","\n","    holiday_dates = pd.to_datetime(list(major_holidays.keys())).sort_values()\n","    holiday_dates_array = holiday_dates.values\n","    dates_array = pd.to_datetime(df['timestamp'].dt.date.unique()).values\n","    holiday_lookup = {}\n","\n","    for date in dates_array:\n","        days_diff = np.abs((holiday_dates_array - date).astype('timedelta64[D]').astype(int))\n","        closest_idx = np.argmin(days_diff)\n","        closest_date = holiday_dates[closest_idx]\n","\n","        holiday_lookup[pd.Timestamp(date).date()] = {\n","            'days_to_nearest_holiday': days_diff[closest_idx],\n","            'nearest_holiday_date': closest_date,\n","            'nearest_holiday_name': major_holidays[closest_date.strftime('%Y-%m-%d')]\n","        }\n","\n","    # Create and merge holiday information\n","    df['date'] = df['timestamp'].dt.date\n","    result = pd.DataFrame.from_dict(holiday_lookup, orient='index')\n","    result.index = pd.to_datetime(result.index).date\n","    df = df.merge(result, left_on='date', right_index=True)\n","    df = df.drop('date', axis=1)\n","\n","    return df\n","\n","def process_timestamps(input_path, output_path):\n","    \"\"\"Main function to process all timestamp-related features.\"\"\"\n","    df = pd.read_pickle(input_path)\n","    df = convert_to_pst_components(df)\n","    df = add_day_info(df)\n","    df = get_nearest_holiday(df)\n","    df.to_pickle(output_path)\n","    return df\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","\n","    df_processed = process_timestamps(input_path, output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"th5Y2zwzXBm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add user and usage data\n","# Add user and usage data\n","import pandas as pd\n","\n","def add_unique_user_counts(df, group_cols, user_col):\n","    \"\"\"Count unique users per group.\"\"\"\n","    user_counts = df.groupby(group_cols)[user_col].nunique().reset_index()\n","    user_counts.rename(columns={user_col: 'unique_user_count'}, inplace=True)\n","    df = df.merge(user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols):\n","    \"\"\"Add sums of values for each unit type by group.\"\"\"\n","    # Calculate sums for each unit type\n","    sums = df.groupby([*group_cols, 'unit_encoded'])['value'].sum().reset_index()\n","\n","    # Pivot to create separate columns for A and W\n","    sums = sums.pivot(\n","        index=group_cols,\n","        columns='unit_encoded',\n","        values='value'\n","    ).reset_index()\n","\n","    # Rename columns\n","    sums.rename(\n","        columns={\n","            0: 'sum_value_A',   # Amps were encoded as 0\n","            1: 'sum_value_Wh'   # Watts were encoded as 1\n","        },\n","        inplace=True\n","    )\n","\n","    return df.merge(sums, on=group_cols, how='left')\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl'\n","\n","    # Read data\n","    df = pd.read_pickle(input_path)\n","\n","    # Extract datetime components\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","\n","    # Define grouping columns after datetime components are created\n","    group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","\n","    # Add user and usage metrics\n","    df = add_unique_user_counts(df, group_cols, 'user_id')\n","    df = add_usage_sums(df, group_cols)\n","\n","    # Save results\n","    df.to_pickle(output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"5d2mRKE6U2VA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWLnoqvaKEB6"},"outputs":[],"source":["# Halt\n","sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"hour_sum_value_A\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy (e.g., median, mean)\n","X.replace([np.inf, -np.inf], 0, inplace=True)  # Replace inf values with 0\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)\n"]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for regression/ANOVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import scipy.stats as stats\n","from statsmodels.stats.multicomp import pairwise_tukeyhsd\n","\n","# Prepare data for ANOVA\n","# 1. Property Type ANOVA\n","prop_type_cols = [col for col in events.columns if col.startswith('prop_type_')]\n","prop_type_watts = []\n","prop_type_labels = []\n","\n","for col in prop_type_cols:\n","    watts = events[events[col] == 1]['total_watts']\n","    prop_type_watts.extend(watts)\n","    prop_type_labels.extend([col.replace('prop_type_', '')] * len(watts))\n","\n","# One-way ANOVA for Property Type\n","f_stat, p_val = stats.f_oneway(*[events[events[col] == 1]['total_watts']\n","                                for col in prop_type_cols])\n","\n","print(\"One-way ANOVA results for Property Type:\")\n","print(f\"F-statistic: {f_stat:.2f}\")\n","print(f\"p-value: {p_val:.2e}\")\n","\n","# Tukey's HSD test for property types\n","tukey = pairwise_tukeyhsd(prop_type_watts, prop_type_labels)\n","print(\"\\nTukey's HSD test results for property types:\")\n","print(tukey)\n","\n","# 2. Hour of Day ANOVA\n","f_stat, p_val = stats.f_oneway(*[events[events['hour'] == h]['total_watts']\n","                                for h in range(24)])\n","\n","print(\"\\nOne-way ANOVA results for Hour of Day:\")\n","print(f\"F-statistic: {f_stat:.2f}\")\n","print(f\"p-value: {p_val:.2e}\")\n","\n","# 3. Day of Week ANOVA\n","f_stat, p_val = stats.f_oneway(*[events[events['day_of_week'] == d]['total_watts']\n","                                for d in range(7)])\n","\n","print(\"\\nOne-way ANOVA results for Day of Week:\")\n","print(f\"F-statistic: {f_stat:.2f}\")\n","print(f\"p-value: {p_val:.2e}\")\n","\n","# Calculate effect sizes (eta-squared)\n","def calculate_eta_squared(groups):\n","    \"\"\"Calculate eta-squared for one-way ANOVA\"\"\"\n","    all_values = np.concatenate(groups)\n","    grand_mean = np.mean(all_values)\n","\n","    ss_between = sum(len(g) * (np.mean(g) - grand_mean)**2 for g in groups)\n","    ss_total = sum((x - grand_mean)**2 for x in all_values)\n","\n","    return ss_between / ss_total\n","\n","print(\"\\nEffect sizes (eta-squared):\")\n","print(\"Property Type:\", calculate_eta_squared([events[events[col] == 1]['total_watts']\n","                                            for col in prop_type_cols]))\n","print(\"Hour of Day:\", calculate_eta_squared([events[events['hour'] == h]['total_watts']\n","                                           for h in range(24)]))\n","print(\"Day of Week:\", calculate_eta_squared([events[events['day_of_week'] == d]['total_watts']\n","                                           for d in range(7)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpSYEsmq3VUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umUOsBSn3VXA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-e5kpAZ3VZS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F20AGhU3Vbq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1F9VbEO3VhO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"J89LPzgmdzuZ"},"source":["### Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"xvFZX6JKKyHa"},"source":["####AWS Tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUKjVft2kxIk"},"outputs":[],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Xz1nKBmBM2nA"},"source":["###Create a table for all property info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7enpOKwkM2Gu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup_2.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names to lowercase and strip whitespace\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the left join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='left',  # Use 'left' join to keep all rows from properties and add property_type name where available\n","    left_on='property_type',  # Assuming 'property_type' is the column in properties.csv\n","    right_on='id',  # Assuming 'id' is the column in property_types.csv\n","    suffixes=('_property', '_type')\n",")\n","\n","# Keep all columns from properties and just add the 'name_type' column as 'property_type'\n","property_lookup['property_type'] = property_lookup['name_type']\n","\n","# Drop the 'name_type' column, since we already added it as 'property_type'\n","property_lookup = property_lookup.drop(columns=['name_type'])\n","\n","# Rename 'id_property' column to 'property_id'\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE633fu_M2Iz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrQU1kppM2LR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOTj1nOYM2N5"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaoKGEHJw0uL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import logging\n","from itertools import combinations\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Path to the directory containing the CSV files\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n","\n","# List of tables (as per your previous code)\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Function to load CSV files into DataFrames\n","def load_dataframes(tables):\n","    dataframes = {}\n","    for table in tables:\n","        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","            dataframes[table] = df\n","            logging.info(f\"Loaded data for table: {table}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading data for table {table}: {e}\")\n","    return dataframes\n","\n","# Function to find strict join matches\n","def find_strict_joins(df1, df2, table1_name, table2_name):\n","    strict_joins = []\n","    # Iterate over all column pairs\n","    for col1 in df1.columns:\n","        for col2 in df2.columns:\n","            if df1[col1].dtype == df2[col2].dtype:\n","                # Perform the join\n","                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n","                # Check if all rows in df1 are in the joined DataFrame\n","                if len(joined_df) == len(df1):\n","                    strict_joins.append((col1, col2))\n","                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n","    return strict_joins\n","\n","# Main function to perform the strict join analysis\n","def analyze_strict_joins(tables):\n","    dataframes = load_dataframes(tables)\n","    results = {}\n","    table_pairs = combinations(tables, 2)\n","\n","    for table1, table2 in table_pairs:\n","        df1 = dataframes.get(table1)\n","        df2 = dataframes.get(table2)\n","\n","        if df1 is not None and df2 is not None:\n","            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n","            joins = find_strict_joins(df1, df2, table1, table2)\n","            if joins:\n","                results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n","\n","    return results\n","\n","# Run the strict join analysis\n","strict_join_results = analyze_strict_joins(tables)\n","\n","# Print the results\n","for table_pair, joins in strict_join_results.items():\n","    print(f\"\\nStrict joins for {table_pair}:\")\n","    for col1, col2 in joins:\n","        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n","\n","if not strict_join_results:\n","    print(\"No strict joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqQ_iKw5dn0M"},"outputs":[],"source":["import os\n","import logging\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection string for SQLAlchemy\n","connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n","engine = create_engine(connection_string)\n","\n","# Function to fetch column names for a table\n","def get_columns(table_name):\n","    try:\n","        query = f\"\"\"\n","        SELECT column_name, data_type\n","        FROM information_schema.columns\n","        WHERE table_name = '{table_name}';\n","        \"\"\"\n","        with engine.connect() as connection:\n","            df = pd.read_sql_query(query, connection)\n","        return df[['column_name', 'data_type']].to_dict('records')\n","    except Exception as e:\n","        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n","        return []\n","\n","# Function to test join logic between two tables\n","def test_joins(table1, table2, attempts=3):\n","    columns_table1 = get_columns(table1)\n","    columns_table2 = get_columns(table2)\n","    successful_joins = []\n","\n","    for col1 in columns_table1:\n","        for col2 in columns_table2:\n","            # Only test joins on matching data types\n","            if col1['data_type'] == col2['data_type']:\n","                success_count = 0\n","                for _ in range(attempts):  # Attempt the join multiple times\n","                    query = f\"\"\"\n","                    SELECT *\n","                    FROM {table1} t1\n","                    INNER JOIN {table2} t2\n","                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n","                    LIMIT 1;  -- Test with one row at a time\n","                    \"\"\"\n","                    try:\n","                        with engine.connect() as connection:\n","                            df = pd.read_sql_query(query, connection)\n","                            if not df.empty:\n","                                success_count += 1\n","                    except Exception as e:\n","                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n","\n","                if success_count == attempts:  # Only count as successful if all attempts work\n","                    successful_joins.append((col1['column_name'], col2['column_name']))\n","                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n","\n","    return successful_joins\n","\n","# Cross-check join fields for all table pairs\n","tables = [\n","    \"users\", \"ocpp_sub_session\"\n","]\n","\n","results = {}\n","\n","for i, table1 in enumerate(tables):\n","    for table2 in tables[i+1:]:\n","        logging.info(f\"Testing joins between {table1} and {table2}\")\n","        joins = test_joins(table1, table2)\n","        if joins:\n","            results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.info(f\"No join found between {table1} and {table2}\")\n","\n","# Print results\n","for table_pair, joins in results.items():\n","    print(f\"Successful joins for {table_pair}: {joins}\")\n","\n","if not results:\n","    print(\"No successful joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU41V4jQdn2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMXMJghgdn5H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuwvrH2ndn7Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VIJZs8fdn9k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kH2jY7TdoAK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ex8BAejxDoB"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Example dataframe (assuming df['message'] contains the raw strings)\n","# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n","def clean_message(msg):\n","    try:\n","        # Remove the prefix\n","        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n","\n","        # Attempt to load the cleaned message as JSON\n","        return json.loads(msg_cleaned)\n","    except (json.JSONDecodeError, TypeError):\n","        # If the message cannot be decoded as JSON, return None or handle as needed\n","        return None\n","\n","# Apply the function to the 'message' column\n","df['message'] = df['message'].apply(clean_message)\n","\n","# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n","df = df[df['message'].notna()]\n","\n","# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n","flattened_rows = []\n","\n","for idx, row in df.iterrows():\n","    message = row['message']  # Now this is a valid JSON object\n","\n","    # Extract top-level fields\n","    connector_id = message.get('connectorId')\n","    transaction_id = message.get('transactionId')\n","\n","    # Keep the 'meterValue' field as is (as a list of dicts)\n","    meter_value = message.get('meterValue', [])\n","\n","    # Add a row to the flattened list, including the nested 'meterValue' list\n","    flattened_rows.append({\n","        '_time': row['time'],  # Retain the original timestamp from the dataframe\n","        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","        'connectorId': connector_id,\n","        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n","    })\n","\n","# Step 2: Create a new DataFrame from the flattened rows\n","flattened_df = pd.DataFrame(flattened_rows)\n","\n","# Display the resulting DataFrame\n","print(flattened_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbk_sGLxZ_Z"},"outputs":[],"source":["import pandas as pd\n","\n","# Set pandas options to display the full content of any column (e.g., 'meterValue')\n","pd.set_option('display.max_colwidth', None)\n","\n","# Now, display the full content of the 'meterValue' column for the first 5 rows\n","print(flattened_df['meterValue'].head(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhRPDO5zJH0"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to hold the expanded rows\n","expanded_rows = []\n","\n","# Iterate over each row in the dataframe\n","for idx, row in flattened_df.iterrows():\n","    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n","\n","    # For each meter value entry (there should be one timestamp and a list of measurements)\n","    for meter in meter_values:\n","        timestamp = meter['timestamp']  # Extract the timestamp\n","\n","        # Initialize values for each measurement type\n","        watt_hours_value = None  # WattHours\n","        amps_value = None        # Amps (Current)\n","        voltage_value = None     # Voltage (Volts)\n","\n","        # Iterate over the sampledValue list (which contains the three measurements)\n","        for sample in meter['sampledValue']:\n","            # Check the 'unit' to assign the value to the correct column\n","            if sample['unit'] == 'Wh':  # WattHours\n","                watt_hours_value = sample['value']\n","            elif sample['unit'] == 'A':  # Amps (Current)\n","                amps_value = sample['value']\n","            elif sample['unit'] == 'V':  # Volts (Voltage)\n","                voltage_value = sample['value']\n","\n","        # Append the expanded row with the extracted values\n","        expanded_rows.append({\n","            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n","            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","            'connectorId': row['connectorId'],  # Connector ID\n","            'timestamp': timestamp,  # Timestamp from the meter value\n","            'WattHours': watt_hours_value,  # Renamed to WattHours\n","            'Amps': amps_value,  # Keep Amps as the column name\n","            'Voltage': voltage_value  # Value for Voltage (V)\n","        })\n","\n","# Create a new DataFrame from the expanded rows\n","expanded_df = pd.DataFrame(expanded_rows)\n","\n","# Convert the numeric columns to appropriate types (float)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Display the resulting DataFrame\n","print(expanded_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcgRo8Dz1Nm"},"outputs":[],"source":["# Ensure all columns are numeric (in case there are any string values left)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Classify values as 0 or > 0 for each of the measurements\n","expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n","\n","# Set up the plot\n","plt.figure(figsize=(18, 6))\n","\n","# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n","plt.subplot(1, 3, 1)\n","sns.countplot(data=expanded_df, x='WattHours_Class')\n","plt.title('Count of Rows with WattHours: 0 vs > 0')\n","plt.xlabel('WattHours Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 2)\n","sns.countplot(data=expanded_df, x='Amps_Class')\n","plt.title('Count of Rows with Amps: 0 vs > 0')\n","plt.xlabel('Amps Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 3)\n","sns.countplot(data=expanded_df, x='Voltage_Class')\n","plt.title('Count of Rows with Voltage: 0 vs > 0')\n","plt.xlabel('Voltage Class')\n","plt.ylabel('Count')\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsRrQLIz1QQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure that the '_time' column is in datetime format\n","expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n","\n","# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Drop rows where any of the values are missing\n","expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create the figure and axes for the plots\n","plt.figure(figsize=(18, 6))\n","\n","# Plot Amps over time\n","plt.subplot(1, 3, 1)\n","plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n","plt.title('Amps over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Amps')\n","plt.xticks(rotation=45)\n","\n","# Plot WattHours over time\n","plt.subplot(1, 3, 2)\n","plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n","plt.title('WattHours over Time')\n","plt.xlabel('Time')\n","plt.ylabel('WattHours')\n","plt.xticks(rotation=45)\n","\n","# Plot Voltage over time\n","plt.subplot(1, 3, 3)\n","plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n","plt.title('Voltage over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Voltage')\n","plt.xticks(rotation=45)\n","\n","# Adjust layout to avoid overlap of labels\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi2OjXK_2hXK"},"outputs":[],"source":["expanded_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TbILosnz1Sh"},"outputs":[],"source":["# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n","descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n","\n","# Display the statistics\n","print(descriptive_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zZUwehE16FW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure with 3 subplots (1 row, 3 columns)\n","plt.figure(figsize=(18, 6))\n","\n","# Plot for Amps\n","plt.subplot(1, 3, 1)\n","sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n","plt.title('Boxplot of Amps')\n","plt.ylabel('Amps')\n","\n","# Plot for WattHours\n","plt.subplot(1, 3, 2)\n","sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n","plt.title('Boxplot of WattHours')\n","plt.ylabel('WattHours')\n","\n","# Plot for Voltage\n","plt.subplot(1, 3, 3)\n","sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n","plt.title('Boxplot of Voltage')\n","plt.ylabel('Voltage')\n","\n","# Adjust layout to avoid overlap\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBnl60S2LIP"},"outputs":[],"source":["# count of propertyIDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9r14vD2LK6"},"outputs":[],"source":["#Count of cluster IDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F01IOcSb2LNZ"},"outputs":[],"source":["# counts of peropertyID and clusterIDimport os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'cluster_id' values for each 'property_id'\n","query = f\"\"\"\n","SELECT property_id, COUNT(DISTINCT cluster_id)\n","FROM {table}\n","GROUP BY property_id\n",";\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    property_id, cluster_count = row\n","    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values for each 'cluster_id'\n","query = f\"\"\"\n","SELECT cluster_id, COUNT(DISTINCT property_id)\n","FROM {table}\n","GROUP BY cluster_id;\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    cluster_id, property_count = row\n","    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n"]},{"cell_type":"markdown","metadata":{"id":"gJCn043ops5i"},"source":["OCPP_SessionID has a userID and TransactionID\n","I need to map to the cluster and property\n","\n","Stations has propertyID and cluster_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkQRHSF2pYLF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRERnF5HpYNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZNFFc8pYQY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pbxg6jVhpYS3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wI1hDsHpYVx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiA1AO1z2LSG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnNNDMY2LUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpS8FyTqEWgq"},"outputs":[],"source":["# Clean message field and port to a df\n","import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n","valid_df = df[df['parsed_message'].notnull()]\n","\n","# Flatten all the JSON objects and store them in a new DataFrame\n","flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n","new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n","\n","# Write the DataFrame to CSV with new naming convention\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n","new_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnkEtU3uDKj"},"outputs":[],"source":["new_df.info\n","new_df.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDqqE7Swt6vj"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df= new_df\n","\n","# Assuming your DataFrame is named df\n","# Step 1: Convert 'time' to datetime\n","df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n","\n","# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n","df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n","\n","# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n","df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n","\n","# Step 4: Plot the time series\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n","plt.xlabel('Time')\n","plt.ylabel('Meter Value')\n","plt.title('Meter Value Over Time')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlT8kevDasAZ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['message'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcwXooB_tXnf"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming new_df is already defined and contains the necessary columns\n","\n","# List of columns to keep\n","columns_to_keep = [\n","    #'time',\n","    'user_id',\n","    #'station_id',\n","    'property_id',\n","    'connectorId',\n","    'meterValue_0_timestamp',\n","    'meterValue_0_sampledValue_1_value',\n","    'meterValue_0_sampledValue_1_context',\n","    'meterValue_0_sampledValue_1_format',\n","    'meterValue_0_sampledValue_1_measurand',\n","    'meterValue_0_sampledValue_1_phase',\n","    'meterValue_0_sampledValue_1_location',\n","    'meterValue_0_sampledValue_1_unit'\n","]\n","\n","# Create new_df_2 with only the selected columns\n","new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","\n","# Convert 'time' to datetime\n","new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n","\n","# Check for any NaT values that may have resulted from the conversion\n","if new_df_2['meterValue_0_timestamp'].isnull().any():\n","    print(\"Some values could not be converted to datetime.\")\n","\n","# Extract day and hour using .loc to avoid warnings\n","new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n","new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLthqnU8u2eM"},"outputs":[],"source":["new_df_2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMxEqA8Qbkxz"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = new_df_2\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N29p7Kq_xWuu"},"outputs":[],"source":["# Assuming new_df_2 is the df\n","\n","unique_values = new_df_2['user_id'].unique()\n","\n","# To display the unique values\n","print(unique_values)\n","\n","\n","# Assuming new_df_2 is your DataFrame\n","unique_count = new_df_2['user_id'].nunique()\n","\n","# To display the count of unique user_id values\n","print(f\"Number of unique user_id values: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEjr917eu9Rb"},"outputs":[],"source":["import pandas as pd\n","\n","\n","new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n","\n","max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n","\n","result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n","\n","print(result_df)\n","result_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnNlof0taJFO"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of meterValue_0_timestamp')\n","plt.title('Count of meterValue_0_timestamp per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"842I60p_-VjA"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming result_df is your DataFrame\n","\n","# Print column names to verify\n","print(\"Column names in DataFrame:\")\n","print(result_df.columns.tolist())\n","\n","# Specify the user_id you're interested in\n","specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n","\n","# Filter the DataFrame for the specific user_id\n","filtered_df = result_df[result_df['user_id'] == specific_user_id]\n","\n","# Check if there are any rows for the specified user_id\n","if not filtered_df.empty:\n","    # Check for the timestamp column again\n","    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n","    value_col = 'meterValue_0_sampledValue_1_value'\n","\n","    # Ensure the column names are correct\n","    print(\"Filtered DataFrame columns:\")\n","    print(filtered_df.columns.tolist())\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n","    plt.title(f'Meter Values for User ID: {specific_user_id}')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Meter Value')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n","    plt.grid()\n","    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","    plt.show()\n","else:\n","    print(f\"No data found for user_id: {specific_user_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp5ImngUusix"},"outputs":[],"source":["\n","# Write the DataFrame to CSV\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n","result_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ZZ-b6HuslW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3cWsdlTusnX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TvKiocBsdvU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A0Tx9v-sdxw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKXZgFB5sd0F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBdB1TPsd2a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDJEMK-rsd4_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyNn0Sx6scfWDSRr6Uz/9lGA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}