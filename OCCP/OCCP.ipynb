{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"XznKesLDcf0o"},"source":["## Charging System Diagram\n","Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mhwSzFEWit8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738775734943,"user_tz":480,"elapsed":120347,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"4a80a4fa-a2ad-46bb-c024-9aa4b21fbdf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz_Nyx0M2KOU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"850f182c-cbcd-494c-d9e6-3942267efad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyGithub\n","  Downloading PyGithub-2.5.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pynacl>=1.4.0 (from PyGithub)\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.18)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Downloading PyGithub-2.5.0-py3-none-any.whl (375 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.9/375.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Packages and methods\n","\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import sys\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvDdY57l4fxE"},"outputs":[],"source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest raw data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suHAcKhHVfV8"},"outputs":[],"source":["# import log data\n","\n","import pandas as pd\n","import numpy as np\n","\n","def load_file(file_path):\n","    \"\"\"Load a single CSV file.\"\"\"\n","    return pd.read_csv(file_path)\n","\n","def concatenate_files(file_paths):\n","    \"\"\"Load and combine multiple CSV files.\"\"\"\n","    dfs = []\n","    for file_path in file_paths:\n","        df = load_file(file_path)\n","        if not df.empty:\n","            dfs.append(df)\n","\n","    return pd.concat(dfs, ignore_index=True)\n","\n","if __name__ == \"__main__\":\n","    file_paths = [\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/dec_100_sample.csv'\n","    ]\n","\n","    # Concatenate all files\n","    df_logs = concatenate_files(file_paths)\n","\n","    # Save the combined raw data\n","    df_logs.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgAP-CX4LKV4"},"outputs":[],"source":["#Import property tables from AWS\n","\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","    with open(path_to_credentials, 'r') as file:\n","        for line in file:\n","            if '=' in line:\n","                key, value = line.split('=', 1)\n","                os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch properties table\n","cursor.execute(\"SELECT * FROM properties;\")\n","df_prop = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.csv', index=False)\n","df_prop.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl')\n","\n","# Fetch property_types table\n","cursor.execute(\"SELECT * FROM property_types;\")\n","df_prop_type = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop_type.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.csv', index=False)\n","df_prop_type.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl')\n","\n","# Close connection\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IjOthMDc4uL"},"outputs":[],"source":["#Import property table from SalesForce\n","\n","import pandas as pd\n","from pathlib import Path\n","import os\n","\n","def process_property_sizes(input_file: Path) -> pd.DataFrame:\n","    # Define fields to cast as int\n","    int_fields = [\n","        'account_id',\n","        'managed_account_id',\n","        'Parking Space Count'\n","        # Add any other fields that should be int\n","    ]\n","\n","    # Load CSV with explicit encoding\n","    df_prop_size = pd.read_csv(input_file, encoding='latin-1')\n","\n","    # Cast specified fields to int, handling any errors\n","    for field in int_fields:\n","        if field in df_prop_size.columns:\n","            df_prop_size[field] = pd.to_numeric(df_prop_size[field], errors='coerce').fillna(0).astype(int)\n","\n","    # Save pickle to same directory as input file\n","    prop_size_pickle_path = input_file.parent / 'df_prop_size.pkl'\n","    df_prop_size.to_pickle(prop_size_pickle_path)\n","\n","    # Print info about the conversions\n","    print(\"\\nData types after conversion:\")\n","    print(df_prop_size[int_fields].dtypes)\n","\n","    return df_prop_size\n","\n","if __name__ == \"__main__\":\n","    # Define base directory\n","    data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","    # Input path\n","    prop_size_input = data_dir / 'Properties Table jan2025.csv'\n","\n","    # Run workflow\n","    df_prop_size = process_property_sizes(prop_size_input)"]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0sBeZzEtE5O"},"outputs":[],"source":["# Here are the dfs Ill be working with\n","\n","df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl') # Event data from Splunk\n","df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl') # Property metadata from AWS\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl') # Property size data from SalesForce\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl') # Property metadata from AWS"]},{"cell_type":"markdown","metadata":{"id":"mtmzDPK5HSAw"},"source":["###This is what the JSON field looks like\n","\n","\n","\n","{\"connectorId\":1,\"transactionId\":1417592169,\"meterValue\":[{\"timestamp\":\"2025-01-14T13:27:37.145Z\",\"sampledValue\":[{\"value\":\"31323855.0\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Energy.Active.Import.Register\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"Wh\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"240.57\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Voltage\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"V\"},{\"value\":\"28\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Temperature\",\"phase\":null,\"location\":\"Body\",\"unit\":\"Celsius\"},{\"value\":\"6.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"1440.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Active.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"}]}]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdW03n8B_At0"},"outputs":[],"source":["# Unpack the JSON field in the log file\n","import pandas as pd\n","import json\n","\n","def expand_message_json(df):\n","    rows = []\n","\n","    for idx, row in df.iterrows():\n","        # Parse the JSON message\n","        message = json.loads(row['cleaned_message']) if pd.notna(row['cleaned_message']) else {}\n","\n","        # Get transactionId from the message\n","        transaction_id = message.get('transactionId')\n","\n","        # Extract meter values\n","        meter_values = message.get('meterValue', [])\n","        for meter in meter_values:\n","            timestamp = meter.get('timestamp')\n","            sampled_values = meter.get('sampledValue', [])\n","\n","            # Filter for only A and W units\n","            for sample in sampled_values:\n","                unit = sample.get('unit')\n","                if unit in ['A', 'W']:\n","                    rows.append({\n","                        'property_id': row['property_id'],\n","                        'user_id': row['user_id'],\n","                        'transaction_id': transaction_id,  # Fixed variable name here\n","                        'timestamp': timestamp,\n","                        'value': sample.get('value'),\n","                        'unit': unit\n","                    })\n","\n","    return pd.DataFrame(rows)\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n"]},{"cell_type":"code","source":["# Create a sample of log data\n","\n","import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrame\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs_exp.pkl'\n","\n","# Read pickle and sample 10 records\n","df_logs_exp = pd.read_pickle(logs_pickle)\n","sample_df = df_logs_exp.sample(n=10, random_state=42)  # random_state for reproducibility\n","\n","# Save sample to CSV in same directory\n","sample_df.to_csv(data_dir / 'logs_sample.csv', index=False)\n","\n","print(\"10 record sample saved to logs_sample.csv\")"],"metadata":{"id":"dYYJCdYmLQNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fix field name and datatype inconsistencies\n","import pandas as pd\n","from pathlib import Path\n","\n","def normalize_timestamp(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Convert to datetime and truncate to seconds\n","   df['_time'] = pd.to_datetime(df['_time']).dt.floor('s')\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Timestamps normalized and saved\")\n","\n","def normalize_account_ids(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID fields to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","   df['account_id'] = pd.to_numeric(df['account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Account IDs normalized and saved\")\n","\n","def normalize_prop_size_id(df_path: Path) -> None:\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID field to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Property size ID normalized and saved\")\n","\n","if __name__ == \"__main__\":\n","   # Define base directory\n","   data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","   # Define pickle files\n","   logs_pickle = data_dir / 'df_logs.pkl'\n","   prop_pickle = data_dir / 'df_prop.pkl'\n","   prop_size_pickle = data_dir / 'df_prop_size.pkl'\n","\n","   # Run normalizations\n","   normalize_timestamp(logs_pickle)\n","   normalize_account_ids(prop_pickle)\n","   normalize_prop_size_id(prop_size_pickle)"],"metadata":{"id":"vSw2NT1WCaE7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lLvvK85MCce"},"outputs":[],"source":["# Check join logic\n","# Does df_logs.property_ud join with df_prop.id ?\n","# Sample value 3436570000094511023\n","# Found 20 property_ids in logs that don't exist in properties table\n","\n","import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrames\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs.pkl'\n","prop_pickle = data_dir / 'df_prop.pkl'\n","\n","df_logs = pd.read_pickle(logs_pickle)\n","df_prop = pd.read_pickle(prop_pickle)\n","\n","# Check unique values in each column\n","n_log_props = df_logs['property_id'].nunique()\n","n_prop_ids = df_prop['id'].nunique()\n","\n","print(f\"Unique property_ids in logs: {n_log_props}\")\n","print(f\"Unique ids in properties: {n_prop_ids}\")\n","\n","# Check which property_ids in logs don't exist in properties\n","missing_props = set(df_logs['property_id'].unique()) - set(df_prop['id'].unique())\n","\n","if missing_props:\n","    print(f\"\\nFound {len(missing_props)} property_ids in logs that don't exist in properties table\")\n","    print(\"Sample of missing ids:\", list(missing_props)[:5])\n","else:\n","    print(\"\\nAll property_ids in logs exist in properties table\")\n","\n","# Check actual join\n","merged_df = df_logs.merge(df_prop, left_on='property_id', right_on='id', how='left')\n","n_unmatched = merged_df['id'].isna().sum()\n","\n","print(f\"\\nUnmatched rows after join: {n_unmatched} ({(n_unmatched/len(df_logs))*100:.2f}% of logs)\")\n"]},{"cell_type":"code","source":["# Sample missing data\n","\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","   with open(path_to_credentials, 'r') as file:\n","       for line in file:\n","           if '=' in line:\n","               key, value = line.split('=', 1)\n","               os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","   'host': os.getenv('DB_HOST'),\n","   'dbname': os.getenv('DB_NAME'),\n","   'user': os.getenv('DB_USER'),\n","   'password': os.getenv('DB_PASSWORD'),\n","   'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch full rows where property_type is NULL\n","query = \"SELECT * FROM properties WHERE property_type IS NULL;\"\n","\n","cursor.execute(query)\n","columns = [desc[0] for desc in cursor.description]\n","missing_prop_types = pd.DataFrame(cursor.fetchall(), columns=columns)\n","\n","print(missing_prop_types.to_string())\n","\n","# Close connection\n","cursor.close()\n","connection.close()"],"metadata":{"id":"CrRG6PPLADrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check join logic\n","# Does df_logs.property_id join with df_prop_size.id\n","\n","# Check join keys\n","print(\"Unique property_id in df_logs:\", df_logs['property_id'].nunique())\n","print(\"Unique ids in df_prop_size:\", df_prop_size['id'].nunique())\n","\n","# Check overlap between keys\n","common_keys = set(df_logs['property_id']) & set(df_prop_size['id'])\n","print(\"\\nNumber of common property IDs:\", len(common_keys))\n","\n","# Percentage of matching keys\n","logs_match_percentage = len(common_keys) / df_logs['property_id'].nunique() * 100\n","prop_size_match_percentage = len(common_keys) / df_prop_size['id'].nunique() * 100\n","\n","print(f\"\\nPercentage of property_id in logs that match prop_size: {logs_match_percentage:.2f}%\")\n","print(f\"Percentage of ids in prop_size that match logs: {prop_size_match_percentage:.2f}%\")\n","\n","# Sample of matching and non-matching keys\n","print(\"\\nSample of matching property IDs (first 10):\")\n","print(list(common_keys)[:10])\n","\n","# Check for any non-matching keys\n","non_matching_in_logs = set(df_logs['property_id']) - set(df_prop_size['id'])\n","non_matching_in_prop_size = set(df_prop_size['id']) - set(df_logs['property_id'])\n","\n","print(\"\\nNumber of property_ids in logs not in prop_size:\", len(non_matching_in_logs))\n","print(\"\\nSample of non-matching property_ids in logs (first 10):\")\n","print(list(non_matching_in_logs)[:10])\n","\n","# Perform a left join to see unmatched rows\n","merged_df = df_logs.merge(df_prop_size, left_on='property_id', right_on='id', how='left', indicator=True)\n","print(\"\\nMerge result:\")\n","print(merged_df['_merge'].value_counts())"],"metadata":{"id":"d_WEqx1z_v78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check join logic\n","# Check join between df_prop.property_type and df_prop_type.id\n","\n","print(\"Unique property_type in df_prop:\", df_prop['property_type'].nunique())\n","print(\"Unique ids in df_prop_type:\", df_prop_type['id'].nunique())\n","\n","# Check overlap between keys\n","common_keys = set(df_prop['property_type']) & set(df_prop_type['id'])\n","print(\"\\nNumber of common property types:\", len(common_keys))\n","\n","# Percentage of matching keys\n","prop_match_percentage = len(common_keys) / df_prop['property_type'].nunique() * 100\n","prop_type_match_percentage = len(common_keys) / df_prop_type['id'].nunique() * 100\n","\n","print(f\"\\nPercentage of property_type in prop that match prop_type: {prop_match_percentage:.2f}%\")\n","print(f\"Percentage of ids in prop_type that match prop: {prop_type_match_percentage:.2f}%\")\n","\n","# Perform a left join to see unmatched rows\n","merged_df = df_prop.merge(df_prop_type, left_on='property_type', right_on='id', how='left', indicator=True)\n","print(\"\\nMerge result:\")\n","print(merged_df['_merge'].value_counts())\n","\n","# Check unmatched property types\n","unmatched = merged_df[merged_df['_merge'] == 'left_only']\n","print(\"\\nUnmatched property types:\")\n","print(unmatched['property_type'].unique())"],"metadata":{"id":"4Auysgoa7GaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decorate log data with property metadata\n","\n","df_logs_exp = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","df_logs_metadata = df_logs_exp.copy()\n","\n","# Left join to append property_type from df_prop\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop[['id', 'property_type']],\n","   left_on='property_id',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Drop the redundant id column\n","df_logs_metadata = df_logs_metadata.drop(columns=['id'])\n","\n","# Left join to append name from df_prop_type\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop_type[['id', 'name']],\n","   left_on='property_type',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Drop the redundant id column\n","df_logs_metadata = df_logs_metadata.drop(columns=['id'])\n","\n","# Left join to append Parking Space Count from df_prop_size\n","df_logs_metadata = df_logs_metadata.merge(\n","   df_prop_size[['id', 'Parking Space Count']],\n","   left_on='property_id',\n","   right_on='id',\n","   how='left'\n",")\n","\n","# Save the data\n","df_logs_metadata.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_metadata.pkl')\n"],"metadata":{"id":"IdD-cbAXnyCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data clean-up\n","\n","import pandas as pd\n","import pytz\n","\n","def is_valid_timestamp(timestamp):\n","    try:\n","        pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%fZ')\n","        return True\n","    except:\n","        try:\n","            pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%SZ')\n","            return True\n","        except:\n","            try:\n","                pd.to_datetime(timestamp, format='%Y-%m-%dT%H:%M:%S.%f%z')\n","                return True\n","            except:\n","                return False\n","\n","def remove_invalid_timestamps(df):\n","    return df[df['timestamp'].apply(is_valid_timestamp)]\n","\n","def truncate_timestamp(df):\n","    df['timestamp'] = df['timestamp'].str[:16]\n","    return df\n","\n","def convert_to_pacific_time(df):\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_convert('US/Pacific')\n","    return df\n","\n","def process_timestamps(df):\n","    df = remove_invalid_timestamps(df)\n","    df = truncate_timestamp(df)\n","    df = convert_to_pacific_time(df)\n","    return df\n","\n","def clean_and_process_data(df):\n","    # Drop unnecessary columns\n","    df = df.drop(columns=['id', 'property_type'])\n","\n","    # Reorder and select columns\n","    df = df[['property_id', 'name', 'Parking Space Count', 'transaction_id', 'timestamp', 'user_id', 'unit', 'value']]\n","\n","    # Rename columns\n","    df = df.rename(columns={'name': 'property_type', 'Parking Space Count': 'property_size'})\n","\n","    # Cast numeric formats\n","    df['property_size'] = pd.to_numeric(df['property_size'], errors='coerce').fillna(0).astype(int)\n","    df['value'] = pd.to_numeric(df['value'], errors='coerce').fillna(0).astype(int)\n","\n","    # Process timestamps\n","    df = process_timestamps(df)\n","\n","    return df\n","\n","# Main execution\n","def main():\n","    # Load the data\n","    df_logs_metadata = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_metadata.pkl')\n","\n","    # Clean and process the data\n","    df_logs_metadata_processed = clean_and_process_data(df_logs_metadata)\n","\n","    # Save the processed data\n","    df_logs_metadata_processed.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_metadata.pkl')\n","\n","    # Verify the processed data\n","    print(\"Processed DataFrame Info:\")\n","    print(df_logs_metadata_processed.info())\n","    print(\"\\nFirst few rows of processed data:\")\n","    print(df_logs_metadata_processed.head())\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"MznnCS7TnyFE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNc4o_n59G_G"},"source":["## Data Exploration"]},{"cell_type":"code","source":["df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl') # Event data from Splunk\n","df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl') # Property metadata from AWS\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl') # Property size data from SalesForce\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl') # Property metadata from AWS\n","df_logs_metadata = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_metadata.pkl') # Decorated log data\n"],"metadata":{"id":"dy2BR71T2uSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_logs_metadata.info()"],"metadata":{"id":"mzkrtvxn1ZO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Does each transaction_id have a unique timestamp ?\n","\n","import pandas as pd\n","\n","# Create a new column with the combination of transaction_id and timestamp\n","df_logs_metadata['transaction_timestamp_pair'] = df_logs_metadata['transaction_id'].astype(str) + '_' + df_logs_metadata['timestamp'].astype(str)\n","\n","# Count the number of occurrences of each pair\n","pair_counts = df_logs_metadata['transaction_timestamp_pair'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = pair_counts[pair_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"transaction_id and timestamp pairs are unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate transaction_id and timestamp pairs.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique pairs: {len(pair_counts)}\")\n","print(f\"Are all pairs unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_pairs = duplicates.index[:5]  # Get the first 5 duplicate pairs\n","    for pair in duplicate_pairs:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'] == pair][['transaction_id', 'timestamp', 'property_id', 'user_id', 'value', 'unit']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate pairs:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","    print(f\"Number of unique units in duplicates: {duplicate_df['unit'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())"],"metadata":{"id":"IDqQkBB95XRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Is the combination of transaction_id, timestamp and unit unique ?\n","import pandas as pd\n","\n","# Assuming df_logs_metadata is your existing DataFrame\n","\n","# Create a new column with the combination of transaction_id, timestamp, and unit\n","df_logs_metadata['transaction_timestamp_unit_combo'] = (\n","    df_logs_metadata['transaction_id'].astype(str) + '_' +\n","    df_logs_metadata['timestamp'].astype(str) + '_' +\n","    df_logs_metadata['unit']\n",")\n","\n","# Count the number of occurrences of each combination\n","combo_counts = df_logs_metadata['transaction_timestamp_unit_combo'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = combo_counts[combo_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"The combination of transaction_id, timestamp, and unit is unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate combinations of transaction_id, timestamp, and unit.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique combinations: {len(combo_counts)}\")\n","print(f\"Are all combinations unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_combos = duplicates.index[:5]  # Get the first 5 duplicate combinations\n","    for combo in duplicate_combos:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_unit_combo'] == combo][['transaction_id', 'timestamp', 'unit', 'property_id', 'user_id', 'value']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_unit_combo'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate combinations:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())\n","\n","\n","\n","\n","# Example of a \"duplicate\" transaction_id: 1938384688\n","result = df_logs_metadata[df_logs_metadata['transaction_id'] == 1938384688]\n","print(result)\n","\n","\n","\n","\n"],"metadata":{"id":"Te6OtkQz6eSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of a \"duplicate\" transaction_id: 1938384688\n","result = df_logs_metadata[df_logs_metadata['transaction_id'] == 1938384688]\n","print(result)\n","\n"],"metadata":{"id":"KTD987pt7Ugu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How often does transaction_id have multiple user_ids ?\n","# Only 1 example was found.\n","\n","# Count unique user_ids for each transaction_id\n","user_id_counts = df_logs_metadata.groupby('transaction_id')['user_id'].nunique()\n","\n","# Count unique property_ids for each transaction_id\n","property_id_counts = df_logs_metadata.groupby('transaction_id')['property_id'].nunique()\n","\n","# Calculate results\n","total_transactions = len(df_logs_metadata['transaction_id'].unique())\n","transactions_with_multiple_users = (user_id_counts > 1).sum()\n","transactions_with_multiple_properties = (property_id_counts > 1).sum()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions with multiple user_ids: {transactions_with_multiple_users}\")\n","print(f\"Percentage of transactions with multiple user_ids: {transactions_with_multiple_users/total_transactions*100:.2f}%\")\n","print(f\"Transactions with multiple property_ids: {transactions_with_multiple_properties}\")\n","print(f\"Percentage of transactions with multiple property_ids: {transactions_with_multiple_properties/total_transactions*100:.2f}%\")\n","\n","# If you want to see examples of transactions with multiple user_ids or property_ids:\n","print(\"\\nExamples of transactions with multiple user_ids:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(user_id_counts[user_id_counts > 1].index)].groupby('transaction_id')[['user_id', 'property_id']].head())\n","\n","print(\"\\nExamples of transactions with multiple property_ids:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(property_id_counts[property_id_counts > 1].index)].groupby('transaction_id')[['user_id', 'property_id']].head())"],"metadata":{"id":"btpv4cC37UmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How often does transaction_id have multiple year or month values ?\n","# Looks like its concentrated at the begining or ending of a month\n","\n","# Extract year and month from the timestamp\n","df_logs_metadata['year'] = df_logs_metadata['timestamp'].dt.year\n","df_logs_metadata['month'] = df_logs_metadata['timestamp'].dt.month\n","\n","# Count unique years for each transaction_id\n","year_counts = df_logs_metadata.groupby('transaction_id')['year'].nunique()\n","\n","# Count unique months for each transaction_id\n","month_counts = df_logs_metadata.groupby('transaction_id')['month'].nunique()\n","\n","# Calculate results\n","total_transactions = len(df_logs_metadata['transaction_id'].unique())\n","transactions_with_multiple_years = (year_counts > 1).sum()\n","transactions_with_multiple_months = (month_counts > 1).sum()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions spanning multiple years: {transactions_with_multiple_years}\")\n","print(f\"Percentage of transactions spanning multiple years: {transactions_with_multiple_years/total_transactions*100:.2f}%\")\n","print(f\"Transactions spanning multiple months: {transactions_with_multiple_months}\")\n","print(f\"Percentage of transactions spanning multiple months: {transactions_with_multiple_months/total_transactions*100:.2f}%\")\n","\n","# If you want to see examples of transactions spanning multiple years or months:\n","print(\"\\nExamples of transactions spanning multiple years:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(year_counts[year_counts > 1].index)].groupby('transaction_id')[['timestamp', 'year', 'month']].head())\n","\n","print(\"\\nExamples of transactions spanning multiple months:\")\n","print(df_logs_metadata[df_logs_metadata['transaction_id'].isin(month_counts[month_counts > 1].index)].groupby('transaction_id')[['timestamp', 'year', 'month']].head())"],"metadata":{"id":"-1a3e54F7Uoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How often does transaction_id have multiple A or W non-0 values  ?\n","\n","# Filter for non-zero 'A' and 'W' values\n","df_non_zero = df_logs_metadata[(df_logs_metadata['value'] != 0) &\n","                               (df_logs_metadata['unit'].isin(['A', 'W']))]\n","\n","# Group by transaction_id and unit, then count non-zero values\n","value_counts = df_non_zero.groupby(['transaction_id', 'unit'])['value'].count().unstack(fill_value=0)\n","\n","# Count transactions with multiple non-zero 'A' values\n","multiple_A = (value_counts['A'] > 1).sum()\n","\n","# Count transactions with multiple non-zero 'W' values\n","multiple_W = (value_counts['W'] > 1).sum()\n","\n","# Total unique transactions\n","total_transactions = df_logs_metadata['transaction_id'].nunique()\n","\n","print(f\"Total unique transactions: {total_transactions}\")\n","print(f\"Transactions with multiple non-zero 'A' values: {multiple_A}\")\n","print(f\"Percentage of transactions with multiple non-zero 'A' values: {multiple_A/total_transactions*100:.2f}%\")\n","print(f\"Transactions with multiple non-zero 'W' values: {multiple_W}\")\n","print(f\"Percentage of transactions with multiple non-zero 'W' values: {multiple_W/total_transactions*100:.2f}%\")\n","\n","# Examples of transactions with multiple non-zero 'A' or 'W' values\n","print(\"\\nExamples of transactions with multiple non-zero 'A' values:\")\n","print(df_non_zero[df_non_zero['transaction_id'].isin(value_counts[value_counts['A'] > 1].index) &\n","                  (df_non_zero['unit'] == 'A')].groupby('transaction_id').head())\n","\n","print(\"\\nExamples of transactions with multiple non-zero 'W' values:\")\n","print(df_non_zero[df_non_zero['transaction_id'].isin(value_counts[value_counts['W'] > 1].index) &\n","                  (df_non_zero['unit'] == 'W')].groupby('transaction_id').head())\n","\n","\n"],"metadata":{"id":"pWQlq-do7Uq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"maoVHBYB91fB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ja2XI3Dy91ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2_jWYUOP91me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YwqSEDfD7UwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcZxp8ni89AZ"},"outputs":[],"source":["# Check uniqueness of transaction_id across different column combinations\n","def check_uniqueness(df, columns):\n","    unique_combinations = df.groupby(columns)['transaction_id'].nunique()\n","    print(f\"\\nUniqueness check for {columns}:\")\n","    print(\"Number of unique transaction_ids per group:\", unique_combinations.max())\n","\n","    if unique_combinations.max() > 1:\n","        print(\"\\nGroups with multiple transaction_ids:\")\n","        print(unique_combinations[unique_combinations > 1])\n","\n","# Check various column combinations\n","check_columns = [\n","    ['user_id', 'timestamp', 'property_id', 'value', 'unit'],\n","    ['user_id', 'property_id', 'value', 'unit'],\n","    ['timestamp', 'property_id', 'value', 'unit'],\n","    ['user_id', 'timestamp', 'value', 'unit']\n","]\n","\n","for cols in check_columns:\n","    check_uniqueness(df_logs_metadata, cols)\n","\n","# Additional overall statistics\n","print(\"\\nOverall unique counts:\")\n","print(\"Total rows:\", len(df_logs_metadata))\n","print(\"Unique transaction_ids:\", df_logs_metadata['transaction_id'].nunique())\n","print(\"Unique combinations:\",\n","    df_logs_metadata.groupby(['user_id', 'timestamp', 'property_id', 'value', 'unit'])['transaction_id'].nunique().max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeuNqAdJ59_z"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming df_logs_metadata is your existing DataFrame\n","\n","# Create a new column with the combination of transaction_id and timestamp\n","df_logs_metadata['transaction_timestamp_pair'] = df_logs_metadata['transaction_id'].astype(str) + '_' + df_logs_metadata['timestamp'].astype(str)\n","\n","# Count the number of occurrences of each pair\n","pair_counts = df_logs_metadata['transaction_timestamp_pair'].value_counts()\n","\n","# Check if there are any duplicates\n","duplicates = pair_counts[pair_counts > 1]\n","\n","if duplicates.empty:\n","    print(\"transaction_id and timestamp pairs are unique.\")\n","    is_unique = True\n","else:\n","    print(f\"Found {len(duplicates)} duplicate transaction_id and timestamp pairs.\")\n","    print(\"Examples of duplicates:\")\n","    print(duplicates.head())\n","    is_unique = False\n","\n","print(f\"\\nTotal rows: {len(df_logs_metadata)}\")\n","print(f\"Unique pairs: {len(pair_counts)}\")\n","print(f\"Are all pairs unique? {is_unique}\")\n","\n","# If you want to see the actual duplicate rows:\n","if not is_unique:\n","    print(\"\\nExample of duplicate rows:\")\n","    duplicate_pairs = duplicates.index[:5]  # Get the first 5 duplicate pairs\n","    for pair in duplicate_pairs:\n","        print(df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'] == pair][['transaction_id', 'timestamp', 'property_id', 'user_id', 'value', 'unit']])\n","        print()\n","\n","# Additional analysis on duplicates if they exist\n","if not is_unique:\n","    duplicate_df = df_logs_metadata[df_logs_metadata['transaction_timestamp_pair'].isin(duplicates.index)]\n","\n","    print(\"\\nAnalysis of duplicate pairs:\")\n","    print(f\"Number of unique property_ids in duplicates: {duplicate_df['property_id'].nunique()}\")\n","    print(f\"Number of unique user_ids in duplicates: {duplicate_df['user_id'].nunique()}\")\n","    print(f\"Number of unique values in duplicates: {duplicate_df['value'].nunique()}\")\n","    print(f\"Number of unique units in duplicates: {duplicate_df['unit'].nunique()}\")\n","\n","    print(\"\\nMost common property_types in duplicates:\")\n","    print(duplicate_df['property_type'].value_counts().head())import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Histogram of timestamps per transaction_id\n","timestamps_per_transaction = df_logs_enriched.groupby('transaction_id')['timestamp'].nunique()\n","\n","plt.figure(figsize=(12, 5))\n","\n","# Left plot - log scale\n","plt.subplot(1, 2, 1)\n","plt.hist(timestamps_per_transaction, bins=np.logspace(np.log10(1), np.log10(timestamps_per_transaction.max()), 50))\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.title('Unique Timestamps per Transaction ID (Log Scale)')\n","plt.xlabel('Number of Unique Timestamps')\n","plt.ylabel('Count of Transaction IDs')\n","plt.grid(True)\n","\n","# Right plot - properties per transaction\n","properties_per_transaction = df_logs_enriched.groupby('transaction_id')['property_id'].nunique()\n","plt.subplot(1, 2, 2)\n","plt.hist(properties_per_transaction, bins=50)\n","plt.title('Unique Properties per Transaction ID')\n","plt.xlabel('Number of Unique Properties')\n","plt.ylabel('Count of Transaction IDs')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTimestamps per transaction summary:\")\n","print(timestamps_per_transaction.describe())\n","print(\"\\nProperties per transaction summary:\")\n","print(properties_per_transaction.describe())\n","\n","# Print additional context\n","print(\"\\nTotal number of unique transactions:\", len(timestamps_per_transaction))\n","print(\"Total number of unique properties:\", df_logs_enriched['property_id'].nunique())\n","print(\"Total number of timestamps:\", df_logs_enriched['timestamp'].nunique())"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","source":["import pandas as pd\n","import pytz\n","import numpy as np\n","from datetime import datetime\n","\n","def convert_to_pst_components(df):\n","    \"\"\"Convert timestamp to PST and extract components.\"\"\"\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')\n","    df['timestamp'] = df['timestamp'].dt.tz_convert('US/Pacific')\n","\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","    df['minute'] = df['timestamp'].dt.minute\n","\n","    return df\n","\n","def add_day_info(df):\n","    \"\"\"Add day of week and weekend indicator.\"\"\"\n","    df['day_of_week'] = df['timestamp'].dt.dayofweek + 1\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)\n","    return df\n","\n","def get_nearest_holiday(df):\n","    \"\"\"Calculate proximity to major US holidays.\"\"\"\n","    major_holidays = {\n","        '2024-01-01': \"New Year's Day\",\n","        '2024-01-15': \"Martin Luther King Jr. Day\",\n","        '2024-02-19': \"Presidents Day\",\n","        '2024-05-27': \"Memorial Day\",\n","        '2024-07-04': \"Independence Day\",\n","        '2024-09-02': \"Labor Day\",\n","        '2024-11-28': \"Thanksgiving\",\n","        '2024-12-25': \"Christmas\",\n","        '2025-01-01': \"New Year's Day\",\n","        '2025-01-20': \"Martin Luther King Jr. Day\",\n","        '2025-02-17': \"Presidents Day\",\n","        '2025-05-26': \"Memorial Day\",\n","        '2025-07-04': \"Independence Day\",\n","        '2025-09-01': \"Labor Day\",\n","        '2025-11-27': \"Thanksgiving\",\n","        '2025-12-25': \"Christmas\"\n","    }\n","\n","    holiday_dates = pd.to_datetime(list(major_holidays.keys())).sort_values()\n","    holiday_dates_array = holiday_dates.values\n","    dates_array = pd.to_datetime(df['timestamp'].dt.date.unique()).values\n","    holiday_lookup = {}\n","\n","    for date in dates_array:\n","        days_diff = np.abs((holiday_dates_array - date).astype('timedelta64[D]').astype(int))\n","        closest_idx = np.argmin(days_diff)\n","        closest_date = holiday_dates[closest_idx]\n","\n","        holiday_lookup[pd.Timestamp(date).date()] = {\n","            'days_to_nearest_holiday': days_diff[closest_idx],\n","            'nearest_holiday_date': closest_date,\n","            'nearest_holiday_name': major_holidays[closest_date.strftime('%Y-%m-%d')]\n","        }\n","\n","    # Create and merge holiday information\n","    df['date'] = df['timestamp'].dt.date\n","    result = pd.DataFrame.from_dict(holiday_lookup, orient='index')\n","    result.index = pd.to_datetime(result.index).date\n","    df = df.merge(result, left_on='date', right_index=True)\n","    df = df.drop('date', axis=1)\n","\n","    return df\n","\n","def process_timestamps(input_path, output_path):\n","    \"\"\"Main function to process all timestamp-related features.\"\"\"\n","    df = pd.read_pickle(input_path)\n","    df = convert_to_pst_components(df)\n","    df = add_day_info(df)\n","    df = get_nearest_holiday(df)\n","    df.to_pickle(output_path)\n","    return df\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","\n","    df_processed = process_timestamps(input_path, output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"th5Y2zwzXBm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Add user and usage data\n","import pandas as pd\n","\n","def add_unique_user_counts(df, group_cols, user_col):\n","    \"\"\"Count unique users per group.\"\"\"\n","    user_counts = df.groupby(group_cols)[user_col].nunique().reset_index()\n","    user_counts.rename(columns={user_col: 'unique_user_count'}, inplace=True)\n","    df = df.merge(user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols):\n","    \"\"\"Add sums of values for each unit type by group.\"\"\"\n","    # Calculate sums for each unit type\n","    sums = df.groupby([*group_cols, 'unit_encoded'])['value'].sum().reset_index()\n","\n","    # Pivot to create separate columns for A and W\n","    sums = sums.pivot(\n","        index=group_cols,\n","        columns='unit_encoded',\n","        values='value'\n","    ).reset_index()\n","\n","    # Rename columns\n","    sums.rename(\n","        columns={\n","            0: 'sum_value_A',   # Amps were encoded as 0\n","            1: 'sum_value_Wh'   # Watts were encoded as 1\n","        },\n","        inplace=True\n","    )\n","\n","    return df.merge(sums, on=group_cols, how='left')\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl'\n","\n","    # Read data\n","    df = pd.read_pickle(input_path)\n","\n","    # Extract datetime components\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","\n","    # Define grouping columns after datetime components are created\n","    group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","\n","    # Add user and usage metrics\n","    df = add_unique_user_counts(df, group_cols, 'user_id')\n","    df = add_usage_sums(df, group_cols)\n","\n","    # Save results\n","    df.to_pickle(output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"5d2mRKE6U2VA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')"],"metadata":{"id":"l3w1u1r0nADm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"kT15u3G6nWmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWLnoqvaKEB6"},"outputs":[],"source":["# Halt\n","sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for analysis"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric, handling errors\n","columns_to_analyze = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Create a clean dataframe for analysis\n","df_clean = df[columns_to_analyze].copy()\n","\n","# Convert each column to numeric, handling errors\n","for col in columns_to_analyze:\n","    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n","\n","    # Print info about conversion\n","    print(f\"\\nColumn: {col}\")\n","    print(f\"Null values after conversion: {df_clean[col].isnull().sum()}\")\n","    print(f\"Sample unique values: {df_clean[col].dropna().sample(5).tolist()}\")\n","\n","# Calculate correlations for cleaned numeric columns\n","correlations = df_clean.corr()\n","\n","# Create correlation heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","# Basic summary statistics\n","print(\"\\nSummary Statistics:\")\n","print(df_clean.describe())\n","\n","# Check for patterns across categorical variables\n","print(\"\\nMean energy consumption (sum_value_Wh) by:\")\n","print(\"\\nDay of Week:\")\n","print(df_clean.groupby('day_of_week')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nHour of Day:\")\n","print(df_clean.groupby('hour')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nMonth:\")\n","print(df_clean.groupby('month')['sum_value_Wh'].mean().sort_values(ascending=False))"],"metadata":{"id":"UsCbfrlAE1-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load and clean data\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric\n","df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Group by hour and calculate various statistics\n","hourly_stats = df.groupby('hour').agg({\n","    'sum_value_Wh': ['mean', 'median', 'std', 'count'],\n","    'unique_user_count': 'mean'\n","}).round(2)\n","\n","# Create a figure with two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n","\n","# Plot 1: Energy consumption pattern\n","ax1.plot(hourly_stats[('sum_value_Wh', 'mean')], marker='o')\n","ax1.fill_between(range(24),\n","                 hourly_stats[('sum_value_Wh', 'mean')] - hourly_stats[('sum_value_Wh', 'std')],\n","                 hourly_stats[('sum_value_Wh', 'mean')] + hourly_stats[('sum_value_Wh', 'std')],\n","                 alpha=0.2)\n","ax1.set_title('24-Hour Energy Consumption Pattern')\n","ax1.set_xlabel('Hour of Day')\n","ax1.set_ylabel('Average Energy Consumption (Wh)')\n","ax1.grid(True)\n","\n","# Plot 2: Users vs Energy\n","ax2.scatter(hourly_stats[('unique_user_count', 'mean')],\n","           hourly_stats[('sum_value_Wh', 'mean')],\n","           alpha=0.6)\n","# Add hour labels to each point\n","for i in range(24):\n","    ax2.annotate(str(i),\n","                (hourly_stats[('unique_user_count', 'mean')][i],\n","                 hourly_stats[('sum_value_Wh', 'mean')][i]))\n","ax2.set_title('Users vs Energy Consumption by Hour')\n","ax2.set_xlabel('Average Number of Users')\n","ax2.set_ylabel('Average Energy Consumption (Wh)')\n","ax2.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print detailed statistics for peak hours\n","peak_hours = hourly_stats.nlargest(5, ('sum_value_Wh', 'mean'))\n","print(\"\\nPeak Hours Analysis:\")\n","print(peak_hours)\n","\n","# Calculate relative increase during peak vs off-peak\n","off_peak_mean = hourly_stats.loc[0:6, ('sum_value_Wh', 'mean')].mean()\n","peak_mean = hourly_stats.loc[16:19, ('sum_value_Wh', 'mean')].mean()\n","increase_factor = peak_mean / off_peak_mean\n","\n","print(f\"\\nPeak vs Off-peak Analysis:\")\n","print(f\"Average off-peak consumption (midnight-6am): {off_peak_mean:.2f} Wh\")\n","print(f\"Average peak consumption (4pm-7pm): {peak_mean:.2f} Wh\")\n","print(f\"Peak hours consume {increase_factor:.1f}x more energy than off-peak hours\")"],"metadata":{"id":"kuoFSz6pGHoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert sum_value_Wh to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","\n","# Create a single property type column for easier plotting\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Create box plot\n","plt.figure(figsize=(15, 8))\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df)\n","plt.xticks(rotation=45, ha='right')\n","plt.title('Energy Consumption Distribution by Property Type')\n","plt.xlabel('Property Type')\n","plt.ylabel('Energy Consumption (Wh)')\n","plt.tight_layout()\n","plt.show()\n","\n","# Print basic stats for each property type\n","print(\"\\nBasic statistics by property type:\")\n","stats = df.groupby('property_type')['sum_value_Wh'].describe()\n","print(stats)\n","\n","# Count number of observations for each property type\n","print(\"\\nNumber of observations per property type:\")\n","counts = df['property_type'].value_counts()\n","print(counts)"],"metadata":{"id":"2-BGTCfAGHwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Create a single property type column\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Calculate energy per user\n","df['energy_per_user'] = df['sum_value_Wh'] / df['unique_user_count']\n","\n","# Create two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n","\n","# Plot 1: Total Energy\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df, ax=ax1)\n","ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n","ax1.set_title('Total Energy Consumption by Property Type')\n","ax1.set_xlabel('Property Type')\n","ax1.set_ylabel('Energy Consumption (Wh)')\n","\n","# Plot 2: Energy per User\n","sns.boxplot(x='property_type', y='energy_per_user', data=df, ax=ax2)\n","ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n","ax2.set_title('Energy Consumption per User by Property Type')\n","ax2.set_xlabel('Property Type')\n","ax2.set_ylabel('Energy Consumption per User (Wh/user)')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print statistics\n","print(\"\\nStatistics by property type:\")\n","stats = df.groupby('property_type').agg({\n","    'sum_value_Wh': ['count', 'mean'],\n","    'unique_user_count': 'mean',\n","    'energy_per_user': 'mean'\n","}).round(2)\n","\n","print(stats)"],"metadata":{"id":"ZLVLTbCcGH2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First, let's look at the raw data before any processing\n","print(\"Original data counts by property type:\")\n","for col in prop_type_cols:\n","    print(f\"\\n{col}:\")\n","    print(f\"Number of 1s: {df[col].sum()}\")\n","    print(f\"Number of non-null values: {df[col].count()}\")\n","\n","# Let's also check for nulls in key columns\n","print(\"\\nNull values in key columns:\")\n","print(df[['sum_value_Wh', 'unique_user_count']].isnull().sum())\n","\n","# Let's look at the data before any type conversion\n","print(\"\\nSample of raw data before conversion:\")\n","sample_data = df[['property_type', 'sum_value_Wh', 'unique_user_count']].head(10)\n","print(sample_data)\n","\n","# Check data types of key columns\n","print(\"\\nData types of columns:\")\n","print(df.dtypes)"],"metadata":{"id":"Xt5KFkm2GH92"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","def clean_numeric_string(value):\n","    \"\"\"Clean numeric string by taking the first valid number.\"\"\"\n","    if pd.isna(value):\n","        return 0\n","    # Convert to string if not already\n","    value = str(value)\n","    # Find first number (integer or decimal)\n","    parts = value.split('.')\n","    if not parts:\n","        return 0\n","    try:\n","        # Take first valid number\n","        return float(parts[0])\n","    except ValueError:\n","        return 0\n","\n","# Clean and convert sum_value columns\n","df['sum_value_A'] = df['sum_value_A'].apply(clean_numeric_string).astype(int)\n","df['sum_value_Wh'] = df['sum_value_Wh'].apply(clean_numeric_string).astype(int)\n","df['unique_user_count'] = df['unique_user_count'].astype(int)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy\n","X.replace([np.inf, -np.inf], 0, inplace=True)\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)"]},{"cell_type":"code","source":["# Drop suspect features and run VIF again\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Drop suspect features\n","df = df.drop(['day_weekend', 'year'], axis=1)\n","\n","# Define numerical columns\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[numerical_columns].copy()\n","for column in numerical_columns:\n","    # Convert to numeric, coerce any non-numeric values to NaN\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","\n","    # Fill NaN values with the median of the column\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Sort VIF values in descending order\n","vif_data = vif_data.sort_values('VIF', ascending=False)\n","\n","# Display the VIF values\n","print(\"\\nVariance Inflation Factors:\")\n","print(vif_data)"],"metadata":{"id":"oMvFjeGOqEtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Define the dependent variable (y) - in this case let's use sum_value_Wh as our target\n","y = \"sum_value_Wh\"\n","\n","# Define independent variables (features to compare groups)\n","independent_vars = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\"\n","    #\"sum_value_A\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[independent_vars].copy()\n","for column in independent_vars:\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Convert y to numeric\n","df[y] = pd.to_numeric(df[y], errors='coerce')\n","df[y] = df[y].fillna(df[y].median())\n","\n","# Function to run one-way ANOVA for each independent variable against y\n","def run_anova_with_target(df, independent_vars, y):\n","    results = []\n","\n","    for var in independent_vars:\n","        # Create groups based on the independent variable\n","        groups = []\n","        # Create 5 groups using quantiles for continuous variables\n","        df['group'] = pd.qcut(df[var], q=5, labels=['G1', 'G2', 'G3', 'G4', 'G5'])\n","\n","        # Get the y values for each group\n","        for group in df['group'].unique():\n","            groups.append(df[df['group'] == group][y].values)\n","\n","        # Perform one-way ANOVA\n","        f_stat, p_val = stats.f_oneway(*groups)\n","\n","        results.append({\n","            'Independent Variable': var,\n","            'F-statistic': f_stat,\n","            'p-value': p_val\n","        })\n","\n","    return pd.DataFrame(results)\n","\n","# Run ANOVA\n","anova_results = run_anova_with_target(df, independent_vars, y)\n","\n","# Sort results by p-value\n","anova_results_sorted = anova_results.sort_values('p-value')\n","\n","# Display results\n","pd.set_option('display.float_format', lambda x: '{:.10f}'.format(x) if x < 0.0001 else '{:.4f}'.format(x))\n","print(f\"\\nOne-way ANOVA Results (dependent variable: {y}):\")\n","print(anova_results_sorted)\n","\n","# Add significance indicators\n","anova_results_sorted['Significance'] = ['***' if p < 0.001\n","                                      else '**' if p < 0.01\n","                                      else '*' if p < 0.05\n","                                      else 'ns' for p in anova_results_sorted['p-value']]\n","\n","print(\"\\nSignificance levels:\")\n","print(\"***: p < 0.001\")\n","print(\"**: p < 0.01\")\n","print(\"*: p < 0.05\")\n","print(\"ns: not significant\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"xvFZX6JKKyHa"},"source":["####AWS Tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUKjVft2kxIk"},"outputs":[],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"code","source":["# AWS Extract of occp_sessions\n","\n","import pandas as pd\n","import glob\n","import os\n","\n","def combine_large_csv_files(directory_path, output_filename, chunksize=100):\n","    # Get all CSV files in the directory\n","    all_files = glob.glob(os.path.join(directory_path, \"*.csv\"))\n","\n","    # Write header from first file\n","    first_chunk = pd.read_csv(all_files[0], nrows=0)\n","    first_chunk.to_csv(output_filename, index=False)\n","\n","    # Process each file\n","    for i, filename in enumerate(all_files):\n","        print(f\"Processing file {i+1} of {len(all_files)}: {filename}\")\n","\n","        # Process file in chunks\n","        for chunk in pd.read_csv(filename, chunksize=chunksize):\n","            # Append chunk to output file without headers\n","            chunk.to_csv(output_filename,\n","                        mode='a',\n","                        header=False,\n","                        index=False)\n","\n","    print(f\"Successfully combined {len(all_files)} files into {output_filename}\")\n","\n","# Usage\n","directory = \"/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/\"  # Replace with your directory path\n","output_file = \"/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/Q4_2024.csv\"    # Name for your output file"],"metadata":{"id":"BUxZmV44Li01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set Headers:\n","\n","df_aws.columns = ['qrcode','connector', 'serial_num', 'org_id', 'property_id', 'station_id', 'transaction_id', 'metered_type', 'timestamp', 'metered_value']\n","\n","\n","# Get the minimum (earliest) datetime\n","min_datetime = df_aws['timestamp'].min()\n","\n","# Get the maximum (latest) datetime\n","max_datetime = df_aws['timestamp'].max()\n","\n","print(\"Earliest datetime:\", min_datetime)\n","print(\"Latest datetime:\", max_datetime)"],"metadata":{"id":"QJRNe_QRPXmC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS\n","Scratch that. Queries are impacting production data. Need to stick with splunk logs...."]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyMueGGzyu86alYE0OZZMY++"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}