{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"markdown","metadata":{"id":"f4X4QT-W860Y"},"source":["Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13749,"status":"ok","timestamp":1736472236053,"user":{"displayName":"David E.","userId":"12500566977266345478"},"user_tz":480},"id":"mhwSzFEWit8p","outputId":"f632f874-f259-475a-8c33-59765f5462fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fz_Nyx0M2KOU","outputId":"f6566bb1-4f67-4a7f-bde8-cc9ce3b70980"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"]}],"source":["# Packages and methods\n","\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import os\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import matplotlib.pyplot as p\n","\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2458,"status":"ok","timestamp":1736446695968,"user":{"displayName":"David E.","userId":"12500566977266345478"},"user_tz":480},"id":"wvDdY57l4fxE","outputId":"cd7171a6-c1ed-499a-810f-25233a20d310"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned token starts with: ghp_\n","Fetching GitHub token...\n","Token format check - starts with: ghp_\n","Token successfully retrieved.\n","Connected to repository: davidelgas/DataSciencePortfolio\n","Using commit message: Updated notebook from Colab\n","Notebook content read from /content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\n","Target file path in repo: OCCP/OCCP.ipynb\n","Checking if file exists at OCCP/OCCP.ipynb...\n","File updated successfully in branch 'main'.\n"]}],"source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function with your parameters\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest data"]},{"cell_type":"code","source":["# import logs\n","# These are from Splunk logs and are a 1% sample due to size\n","\n","import pandas as pd\n","import numpy as np\n","import json\n","\n","def load_file(file_path):\n","    \"\"\"Load a single CSV file.\"\"\"\n","    return pd.read_csv(file_path)\n","\n","def concatenate_files(file_paths):\n","    \"\"\"Load and combine multiple CSV files.\"\"\"\n","    dfs = []\n","    for file_path in file_paths:\n","        df = load_file(file_path)\n","        if not df.empty:\n","            dfs.append(df)\n","\n","    return pd.concat(dfs, ignore_index=True)\n","\n","def expand_json(df, json_column):\n","    \"\"\"Expand JSON column into separate columns.\"\"\"\n","    parsed = df[json_column].apply(lambda x: json.loads(x) if pd.notna(x) else {})\n","\n","    expanded = pd.DataFrame()\n","    expanded['property_id'] = df['property_id']\n","    expanded['user_id'] = df['user_id']\n","    expanded['timestamp'] = parsed.apply(lambda x: x.get('meterValue', [{}])[0].get('timestamp'))\n","\n","    sampled_values = parsed.apply(lambda x: x.get('meterValue', [{}])[0].get('sampledValue', [{}]))\n","    expanded['value'] = sampled_values.apply(lambda x: x[0].get('value') if x else None)\n","    expanded['unit'] = sampled_values.apply(lambda x: x[0].get('unit') if x else None)\n","\n","    return expanded\n","\n","def clean_ids(df):\n","    \"\"\"Remove rows with invalid property_ids.\"\"\"\n","    return df[df['property_id'].notna()].reset_index(drop=True)\n","\n","def save_df(df, filepath):\n","    \"\"\"Save DataFrame.\"\"\"\n","    df.to_pickle(filepath + '.pkl')\n","\n","def process_logs(file_paths, output_path):\n","    \"\"\"Complete workflow to process log files.\"\"\"\n","    # Concatenate files\n","    df_combined = concatenate_files(file_paths)\n","\n","    # Process JSON and clean data\n","    df_expanded = expand_json(df_combined, 'cleaned_message')\n","    df_logs = clean_ids(df_expanded)\n","\n","    # Save processed data\n","    save_df(df_logs, output_path)\n","    return df_logs\n","\n","if __name__ == \"__main__\":\n","    file_paths = [\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv'\n","    ]\n","\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs'\n","\n","    # Run complete workflow\n","    df_logs = process_logs(file_paths, output_path)\n",""],"metadata":{"id":"suHAcKhHVfV8","executionInfo":{"status":"ok","timestamp":1736452211864,"user_tz":480,"elapsed":71375,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')\n","df_logs.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKtLXiLBdmpY","executionInfo":{"status":"ok","timestamp":1736466740163,"user_tz":480,"elapsed":2133,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"c62adcb2-ab25-4d04-d6c4-2740e7238ff1"},"execution_count":185,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1319124 entries, 0 to 1319123\n","Data columns (total 5 columns):\n"," #   Column       Non-Null Count    Dtype \n","---  ------       --------------    ----- \n"," 0   property_id  1319124 non-null  object\n"," 1   user_id      1268308 non-null  object\n"," 2   timestamp    1319124 non-null  object\n"," 3   value        1319124 non-null  object\n"," 4   unit         1319124 non-null  object\n","dtypes: object(5)\n","memory usage: 50.3+ MB\n"]}]},{"cell_type":"code","source":["# import property table\n","import pandas as pd\n","\n","def process_properties(file_path, output_path):\n","    \"\"\"Process properties file workflow.\"\"\"\n","    # Load CSV\n","    df_prop = pd.read_csv(file_path)\n","\n","    # Clean IDs\n","    df_prop = df_prop[df_prop['id'].notna()].reset_index(drop=True)\n","\n","    # Rename id column\n","    df_prop = df_prop.rename(columns={'id': 'property_id'})\n","\n","    # Save processed data\n","    df_prop.to_pickle(output_path_3 + '.pkl')\n","\n","    return df_lookup\n","\n","\n","if __name__ == \"__main__\":\n","    input_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","    output_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop'\n","\n","    # Run workflow\n","    df_prop = process_properties(input_path_3, output_path_3)\n"],"metadata":{"id":"Jni86zvuc4ym","executionInfo":{"status":"ok","timestamp":1736466505597,"user_tz":480,"elapsed":106,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl')\n","df_prop.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"W_CkxBg2cyq8","executionInfo":{"status":"error","timestamp":1736472215945,"user_tz":480,"elapsed":255,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"4e9c7218-bd08-4a3c-a539-d00bd040ae7a"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-463d287906e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_prop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_prop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","source":["# Can I join df_log and df_prop on property_id\n","# Merge the DataFrames on property_id\n","df_merged = pd.merge(df_logs, df_prop,\n","                    left_on='property_id',\n","                    right_on='property_id',\n","                    how='left',\n","                    indicator=True)\n","\n","# Count matches and non-matches\n","matches = df_merged[df_merged['_merge'] == 'both']\n","non_matches = df_merged[df_merged['_merge'] == 'left_only']\n","\n","print(\"Total rows in df_log:\", len(df_logs))\n","print(\"Rows that matched:\", len(matches))\n","print(\"Rows that did not match:\", len(non_matches))\n","print(\"\\nPercentage matched: {:.2f}%\".format(len(matches) / len(df_logs) * 100))\n","\n","# Non-matching property_id values from df_logs\n","unmatched_property_ids_logs = non_matches['property_id'].unique()\n","print(\"Unique unmatched property_id values in df_logs:\")\n","print(unmatched_property_ids_logs)\n","print(\"Number of unique unmatched property_id values:\", len(unmatched_property_ids_logs))\n","\n","# Optional: Save unmatched property_ids to CSV\n","pd.DataFrame(unmatched_property_ids_logs, columns=['property_id']).to_csv(\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/unmatched_property_ids.csv',\n","    index=False\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Vsh92JSeVnY","executionInfo":{"status":"ok","timestamp":1736466869168,"user_tz":480,"elapsed":4773,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"b273a5a2-dd11-4346-af82-67b628ed6df1"},"execution_count":189,"outputs":[{"output_type":"stream","name":"stdout","text":["Total rows in df_log: 1319124\n","Rows that matched: 1318319\n","Rows that did not match: 805\n","\n","Percentage matched: 99.94%\n","Unique unmatched property_id values in df_logs:\n","['9c1bc7da-9235-4b6a-92ed-51bbdad719db'\n"," 'b5da81dd-e999-4e23-a8d0-466e92b34576\\na7dc9a96-91f3-40ea-aed2-582f3099fb03'\n"," '4d8f5977-bc0c-4bed-a2b6-690d8d0f93b4\\n8471ffd3-bfaf-44a7-9135-dca30b64b73a'\n"," '54e7058c-0a1f-4d5a-8c64-31b0e948ace4\\n348c41a5-baaa-4275-b8c5-4d9e6ba8f97d'\n"," '2360d17a-1103-4f67-9419-0b016df70fe9\\n060c8da3-e20e-4b49-89e2-f0bc228bc9c9'\n"," '8bbdcff9-06b0-44cf-8c9f-a9ddce51a317\\n24d7e3f1-9d63-457c-9f0c-a50b0bc08ebc'\n"," '24d7e3f1-9d63-457c-9f0c-a50b0bc08ebc\\ne95523e6-3470-4a60-b586-ee715cd9f34b'\n"," '7044cd75-52ab-4044-aeaa-e0fdb02bc4d8\\n38b28445-1cdb-467c-b5be-628b2902d03a'\n"," '8f411e48-b09c-4834-88e9-4b7f59e86130\\nc4ab1149-c4a9-46ab-8bfd-3b471b70d6d9'\n"," '53e9cab2-aed2-4c35-ab39-e8375825e6fb\\n4cb8af8c-e0ed-41d9-8c08-6e66c4125cdd'\n"," 'e95523e6-3470-4a60-b586-ee715cd9f34b\\n7dc180ff-efbe-4c31-8d05-f452ec7db3b2'\n"," '6d5e9f52-8dcd-4e58-a139-5bfcd3149d53\\n69de08c2-2c9f-43a8-9c03-d2fc05d03b2b'\n"," '500c202d-143e-4e3d-854c-77fefb209253\\n406ec42d-df0b-401d-a0a2-5e21824cb3d0'\n"," 'f14f0f6a-1ec0-4136-8bf4-90b96ec0f604\\n0b372bad-6c5f-4ff7-ab2f-5f9b380d5e6f'\n"," 'a8475020-74e2-40ca-8307-2b0adead38ab\\n724598d1-dab6-4ade-9f53-539023e34787']\n","Number of unique unmatched property_id values: 15\n"]}]},{"cell_type":"code","source":["\n","\n","# Here are a couple property_id values that are in df_logs but not in df_prop\n","9c1bc7da-9235-4b6a-92ed-51bbdad719db\n","b5da81dd-e999-4e23-a8d0-466e92b34576 a7dc9a96-91f3-40ea-aed2-582f3099fb03\n","4d8f5977-bc0c-4bed-a2b6-690d8d0f93b4 8471ffd3-bfaf-44a7-9135-dca30b64b73a\n","\n","\n","#Check\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [properties]\n","]\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n","\n","\n","\n"],"metadata":{"id":"RrnwdU-zhuGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9VBlLIIreRpf","executionInfo":{"status":"ok","timestamp":1736466610417,"user_tz":480,"elapsed":2,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":181,"outputs":[]},{"cell_type":"code","source":["# Load property_type metadata\n","# This is from AWS\n","import pandas as pd\n","\n","def process_property_types(file_path, output_path):\n","    \"\"\"Process property types file workflow.\"\"\"\n","    # Load CSV\n","    df_prop_type = pd.read_csv(file_path)\n","\n","    # Clean IDs\n","    df_prop_type = df_prop_type[df_prop_type['id'].notna()].reset_index(drop=True)\n","\n","    # Rename columns\n","    df_prop_type = df_prop_type.rename(columns={\n","        'id': 'property_id',\n","        'name': 'prop_type'\n","    })\n","\n","    # Save processed data\n","    df_prop_type.to_pickle(output_path_1 + '.pkl')\n","\n","    return df_prop_type\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    input_path_1 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","    output_path_1 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type'\n","\n","    # Run workflow\n","    df_prop_type = process_property_types(input_path_1, output_path_1)"],"metadata":{"id":"9QzJ-mTrc4lD","executionInfo":{"status":"ok","timestamp":1736452082766,"user_tz":480,"elapsed":135,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# Ingest prop size data\n","# This is from Salesforce\n","\n","import pandas as pd\n","\n","def clean_record_id(record_id):\n","   \"\"\"\n","   Remove 'zcrm_' prefix from Record Id\n","   \"\"\"\n","   return str(record_id).replace('zcrm_', '') if pd.notna(record_id) else record_id\n","\n","def process_property_sizes(file_path, output_path):\n","   \"\"\"Process property size file workflow.\"\"\"\n","   # Load CSV with explicit encoding\n","   df_prop_size = pd.read_csv(file_path, encoding='latin-1')\n","\n","   # Rename columns\n","   df_prop_size = df_prop_size.rename(columns={\n","       'Record Id': 'Record_id_lg',\n","       'Record Id (Managed Account)': 'Record_id_js'\n","   })\n","\n","   # Clean IDs by removing 'zcrm_' prefix directly in the existing column\n","   df_prop_size['Record_id_js'] = df_prop_size['Record_id_js'].apply(clean_record_id)\n","\n","   # Clean IDs\n","   df_prop_size = df_prop_size[df_prop_size['Record_id_js'].notna()].reset_index(drop=True)\n","\n","   # Cast id to object type\n","   df_prop_size = df_prop_size.astype({'Record_id_js': 'object'})\n","\n","   # Save processed data\n","   df_prop_size.to_pickle(output_path + '.pkl')\n","\n","   return df_prop_size\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","   input_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/All_Viable_Accounts_JS.csv'\n","   output_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size'\n","\n","   # Run workflow\n","   df_prop_size = process_property_sizes(input_path_2, output_path_2)"],"metadata":{"id":"0IjOthMDc4uL","executionInfo":{"status":"ok","timestamp":1736466070987,"user_tz":480,"elapsed":531,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl')\n","df_prop_size.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"1NKWhvDTcocf","executionInfo":{"status":"ok","timestamp":1736466152660,"user_tz":480,"elapsed":256,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"f2d36b0f-e6ac-460c-e1a4-48a63c00f8a6"},"execution_count":173,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google/colab/_dataframe_summarizer.py:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  cast_date_col = pd.to_datetime(column, errors=\"coerce\")\n"]},{"output_type":"execute_result","data":{"text/plain":["               Record_id_lg         Record_id_js     ID Number  \\\n","0  zcrm_3436570000137786029  3436570000137658197  3.440000e+18   \n","1  zcrm_3436570000081274491  3436570000088955132  3.440000e+18   \n","2  zcrm_3436570000076671363  3436570000121879092  3.440000e+18   \n","3  zcrm_3436570000062920079  3436570000072666096  3.440000e+18   \n","4  zcrm_3436570000062906661  3436570000105470072  3.440000e+18   \n","\n","                                        Project Name  \\\n","0        665 Butte Ave, Big Bear Lake, CA 92315 - EV   \n","1  18111 Nordhoff Street, Northridge CA 91330 - C...   \n","2  520 Media Pl. Sacramento, CA 95815 - SMUD 5 ch...   \n","3       6545 N 19th Avenue Phoenix AZ 85015 - SRP EV   \n","4  340 Hauser Blvd Los Angeles CA 90036 (Palazzo ...   \n","\n","                                Managed Account Name  \\\n","0        665 Butte Ave, Big Bear Lake, CA 92315 - EV   \n","1  18111 Nordhoff Street, Northridge CA 91330 - C...   \n","2  520 Media Pl. Sacramento, CA 95815 - SMUD 5 ch...   \n","3       6545 N 19th Avenue Phoenix AZ 85015 - SRP EV   \n","4  346 Hauser Blvd, Los Angeles, CA 90036 (Palazz...   \n","\n","   Total Parking Space Count  Chargers Allowed  Chargers Allowed Number  \\\n","0                        4.0                 1                      1.0   \n","1                      200.0                 8                      8.0   \n","2                      200.0                 5                      5.0   \n","3                      136.0                20                     20.0   \n","4                      800.0                40                      8.0   \n","\n","  Activated Date                           Project Stage 2  \\\n","0            NaN  0. Missing Documents /or/ Site Not Ready   \n","1            NaN  0. Missing Documents /or/ Site Not Ready   \n","2            NaN  0. Missing Documents /or/ Site Not Ready   \n","3            NaN  0. Missing Documents /or/ Site Not Ready   \n","4            NaN  0. Missing Documents /or/ Site Not Ready   \n","\n","  Primary Funding Sources Secondary Funding Source               Core Parent  \n","0                    Cash                      NaN       E & J Worldwide LLC  \n","1                    Cash                      NaN                       NaN  \n","2             SMUD Rebate                      NaN       Redwood Residential  \n","3              SRP Rebate                      NaN  Macroreal Commercial Inc  \n","4            LADWP Rebate                     Cash           AIR Communities  "],"text/html":["\n","  <div id=\"df-4bd459f7-d7ba-470a-8adb-ec3b8f0c8c47\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Record_id_lg</th>\n","      <th>Record_id_js</th>\n","      <th>ID Number</th>\n","      <th>Project Name</th>\n","      <th>Managed Account Name</th>\n","      <th>Total Parking Space Count</th>\n","      <th>Chargers Allowed</th>\n","      <th>Chargers Allowed Number</th>\n","      <th>Activated Date</th>\n","      <th>Project Stage 2</th>\n","      <th>Primary Funding Sources</th>\n","      <th>Secondary Funding Source</th>\n","      <th>Core Parent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>zcrm_3436570000137786029</td>\n","      <td>3436570000137658197</td>\n","      <td>3.440000e+18</td>\n","      <td>665 Butte Ave, Big Bear Lake, CA 92315 - EV</td>\n","      <td>665 Butte Ave, Big Bear Lake, CA 92315 - EV</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>0. Missing Documents /or/ Site Not Ready</td>\n","      <td>Cash</td>\n","      <td>NaN</td>\n","      <td>E &amp; J Worldwide LLC</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>zcrm_3436570000081274491</td>\n","      <td>3436570000088955132</td>\n","      <td>3.440000e+18</td>\n","      <td>18111 Nordhoff Street, Northridge CA 91330 - C...</td>\n","      <td>18111 Nordhoff Street, Northridge CA 91330 - C...</td>\n","      <td>200.0</td>\n","      <td>8</td>\n","      <td>8.0</td>\n","      <td>NaN</td>\n","      <td>0. Missing Documents /or/ Site Not Ready</td>\n","      <td>Cash</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>zcrm_3436570000076671363</td>\n","      <td>3436570000121879092</td>\n","      <td>3.440000e+18</td>\n","      <td>520 Media Pl. Sacramento, CA 95815 - SMUD 5 ch...</td>\n","      <td>520 Media Pl. Sacramento, CA 95815 - SMUD 5 ch...</td>\n","      <td>200.0</td>\n","      <td>5</td>\n","      <td>5.0</td>\n","      <td>NaN</td>\n","      <td>0. Missing Documents /or/ Site Not Ready</td>\n","      <td>SMUD Rebate</td>\n","      <td>NaN</td>\n","      <td>Redwood Residential</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>zcrm_3436570000062920079</td>\n","      <td>3436570000072666096</td>\n","      <td>3.440000e+18</td>\n","      <td>6545 N 19th Avenue Phoenix AZ 85015 - SRP EV</td>\n","      <td>6545 N 19th Avenue Phoenix AZ 85015 - SRP EV</td>\n","      <td>136.0</td>\n","      <td>20</td>\n","      <td>20.0</td>\n","      <td>NaN</td>\n","      <td>0. Missing Documents /or/ Site Not Ready</td>\n","      <td>SRP Rebate</td>\n","      <td>NaN</td>\n","      <td>Macroreal Commercial Inc</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>zcrm_3436570000062906661</td>\n","      <td>3436570000105470072</td>\n","      <td>3.440000e+18</td>\n","      <td>340 Hauser Blvd Los Angeles CA 90036 (Palazzo ...</td>\n","      <td>346 Hauser Blvd, Los Angeles, CA 90036 (Palazz...</td>\n","      <td>800.0</td>\n","      <td>40</td>\n","      <td>8.0</td>\n","      <td>NaN</td>\n","      <td>0. Missing Documents /or/ Site Not Ready</td>\n","      <td>LADWP Rebate</td>\n","      <td>Cash</td>\n","      <td>AIR Communities</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bd459f7-d7ba-470a-8adb-ec3b8f0c8c47')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4bd459f7-d7ba-470a-8adb-ec3b8f0c8c47 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4bd459f7-d7ba-470a-8adb-ec3b8f0c8c47');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4ef1ef79-4178-4816-a1c6-7d38b092e042\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ef1ef79-4178-4816-a1c6-7d38b092e042')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4ef1ef79-4178-4816-a1c6-7d38b092e042 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_prop_size","summary":"{\n  \"name\": \"df_prop_size\",\n  \"rows\": 1249,\n  \"fields\": [\n    {\n      \"column\": \"Record_id_lg\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1126,\n        \"samples\": [\n          \"zcrm_3436570000098094266\",\n          \"zcrm_3436570000027422001\",\n          \"zcrm_3436570000010204020\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Record_id_js\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1249,\n        \"samples\": [\n          \"3436570000032458003\",\n          \"3436570000094642049\",\n          \"3436570000034109609\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ID Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 3.44e+18,\n        \"max\": 3.44e+18,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.44e+18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1123,\n        \"samples\": [\n          \"9500 W Maule Ave, Las Vegas NV, 89148-EV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Managed Account Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1243,\n        \"samples\": [\n          \"22100 Erwin St, Woodland Hills CA 91367 - Reserve at Warner Center - EV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Parking Space Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2619.779957097167,\n        \"min\": 4.0,\n        \"max\": 90057.0,\n        \"num_unique_values\": 241,\n        \"samples\": [\n          23.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chargers Allowed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 80,\n        \"min\": 0,\n        \"max\": 800,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          60\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chargers Allowed Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.82606627776096,\n        \"min\": 0.0,\n        \"max\": 300.0,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          26.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Activated Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-11-05 00:00:00\",\n        \"max\": \"2025-01-09 00:00:00\",\n        \"num_unique_values\": 463,\n        \"samples\": [\n          \"26-Aug-22\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Stage 2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"0. Missing Documents /or/ Site Not Ready\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Primary Funding Sources\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Cash\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Secondary Funding Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"SCE CSMR Rebate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Core Parent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 273,\n        \"samples\": [\n          \"TCRE Corp (Total Commercial Real Estate, Inc.)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":173}]},{"cell_type":"code","source":["# Here are the dfs Ill be working with\n","\n","df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl')\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl')\n","df_lookup = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_lookup.pkl')\n","df_logs_parking = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_parking.pkl')\n","\n"],"metadata":{"id":"K0sBeZzEtE5O","executionInfo":{"status":"ok","timestamp":1736465176101,"user_tz":480,"elapsed":2386,"user":{"displayName":"David E.","userId":"12500566977266345478"}}},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":["## Join Logic\n","df_log.property_id = df_lookup.property_id\n","\n","df_lookup.property_id != df_prop_type.property_id\n","\n","df_prop_type.name is the type of property to extract\n","\n","\n","df_prop_size.Record_id_js == df_lookup.managed_account_id\n","\n"],"metadata":{"id":"Cv7ZaYg4xBws"}},{"cell_type":"code","source":["df_lookup.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jDdifzaW--z","executionInfo":{"status":"ok","timestamp":1736465587771,"user_tz":480,"elapsed":166,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"888c441b-613b-4099-8f52-f67f02f30153"},"execution_count":169,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 797 entries, 0 to 796\n","Data columns (total 26 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   property_id           797 non-null    object \n"," 1   organization_id       797 non-null    object \n"," 2   name                  797 non-null    object \n"," 3   phone                 725 non-null    object \n"," 4   contact               751 non-null    object \n"," 5   longitude             791 non-null    float64\n"," 6   latitude              791 non-null    float64\n"," 7   watts_soft_limit      797 non-null    int64  \n"," 8   property_type         786 non-null    object \n"," 9   note                  245 non-null    object \n"," 10  utility_provider      791 non-null    object \n"," 11  gateway_type          681 non-null    object \n"," 12  carrier_name          598 non-null    object \n"," 13  address_1             794 non-null    object \n"," 14  address_2             10 non-null     object \n"," 15  city                  794 non-null    object \n"," 16  state                 794 non-null    object \n"," 17  zip                   794 non-null    object \n"," 18  email                 344 non-null    object \n"," 19  has_editable_penalty  797 non-null    bool   \n"," 20  managed_account_id    786 non-null    object \n"," 21  created_at            797 non-null    object \n"," 22  updated_at            797 non-null    object \n"," 23  reboot_cron_schedule  38 non-null     object \n"," 24  account_id            761 non-null    float64\n"," 25  hidden                797 non-null    bool   \n","dtypes: bool(2), float64(3), int64(1), object(20)\n","memory usage: 151.1+ KB\n"]}]},{"cell_type":"code","source":["western_jet_record = df_lookup[df_lookup['name'] == 'Western Jet']\n","\n","print(western_jet_record)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PhYfvDXW_A7","executionInfo":{"status":"ok","timestamp":1736465589548,"user_tz":480,"elapsed":133,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"6d3a4073-a803-4032-9c7f-c089e5d5f0fd"},"execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["                             property_id  \\\n","32  907a7e25-d890-4953-97ea-9ee9b994bc21   \n","\n","                         organization_id         name phone contact  \\\n","32  cea56dbe-bde9-4675-92b8-975687b8d3ed  Western Jet   NaN     NaN   \n","\n","     longitude   latitude  watts_soft_limit  \\\n","32 -118.485946  34.209651              1000   \n","\n","                           property_type note  ... state    zip email  \\\n","32  b94d385c-cd17-4a0f-83ec-a2368baa8aba  NaN  ...    CA  91406   NaN   \n","\n","   has_editable_penalty   managed_account_id  \\\n","32                False  3436570000034423029   \n","\n","                          created_at                        updated_at  \\\n","32  2024-03-06 05:23:08.827026+00:00  2024-03-22 23:40:14.196047+00:00   \n","\n","   reboot_cron_schedule    account_id  hidden  \n","32                  NaN  3.436570e+18   False  \n","\n","[1 rows x 26 columns]\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv')\n","western_jet_record = df[df['name'] == 'Western Jet']\n","print(western_jet_record)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUkJdpsiW_DX","executionInfo":{"status":"ok","timestamp":1736465522354,"user_tz":480,"elapsed":313,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"6a6f165b-7f18-4f95-c714-750bbda267a7"},"execution_count":168,"outputs":[{"output_type":"stream","name":"stdout","text":["                                      id  \\\n","32  907a7e25-d890-4953-97ea-9ee9b994bc21   \n","\n","                         organization_id         name phone contact  \\\n","32  cea56dbe-bde9-4675-92b8-975687b8d3ed  Western Jet   NaN     NaN   \n","\n","     longitude   latitude  watts_soft_limit  \\\n","32 -118.485946  34.209651              1000   \n","\n","                           property_type note  ... state    zip email  \\\n","32  b94d385c-cd17-4a0f-83ec-a2368baa8aba  NaN  ...    CA  91406   NaN   \n","\n","   has_editable_penalty   managed_account_id  \\\n","32                False  3436570000034423029   \n","\n","                          created_at                        updated_at  \\\n","32  2024-03-06 05:23:08.827026+00:00  2024-03-22 23:40:14.196047+00:00   \n","\n","   reboot_cron_schedule    account_id  hidden  \n","32                  NaN  3.436570e+18   False  \n","\n","[1 rows x 26 columns]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"drcP8SuPaCBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["western_jet_record = df[df['name'] == 'Western Jet']\n","\n","print(western_jet_record)\n","\n","3436570000034420000\n","3436570000034423029"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TBV2u3HjW_Fq","executionInfo":{"status":"ok","timestamp":1736465254038,"user_tz":480,"elapsed":105,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"b0c3d407-59ff-46be-e4c5-c5340a369fd5"},"execution_count":167,"outputs":[{"output_type":"stream","name":"stdout","text":["                                      id  \\\n","32  907a7e25-d890-4953-97ea-9ee9b994bc21   \n","\n","                         organization_id         name phone contact  \\\n","32  cea56dbe-bde9-4675-92b8-975687b8d3ed  Western Jet   NaN     NaN   \n","\n","     longitude   latitude  watts_soft_limit  \\\n","32 -118.485946  34.209651              1000   \n","\n","                           property_type note  ... state    zip email  \\\n","32  b94d385c-cd17-4a0f-83ec-a2368baa8aba  NaN  ...    CA  91406   NaN   \n","\n","   has_editable_penalty   managed_account_id  \\\n","32                False  3436570000034423029   \n","\n","                          created_at                        updated_at  \\\n","32  2024-03-06 05:23:08.827026+00:00  2024-03-22 23:40:14.196047+00:00   \n","\n","   reboot_cron_schedule    account_id  hidden  \n","32                  NaN  3.436570e+18   False  \n","\n","[1 rows x 26 columns]\n"]}]},{"cell_type":"code","source":["# Get unique managed_account_id values from df_lookup\n","lookup_managed_account_ids = set(df_lookup.managed_account_id.dropna())\n","prop_size_record_ids = set(df_prop_size.Record_id_js)\n","\n","# Find matching values\n","matching_values = lookup_managed_account_ids.intersection(prop_size_record_ids)\n","\n","print(\"Total unique values in df_lookup.managed_account_id:\", len(lookup_managed_account_ids))\n","print(\"Total unique values in df_prop_size.Record_id_js:\", len(prop_size_record_ids))\n","print(\"Number of matching values:\", len(matching_values))\n","print(\"Number of non-matching values in df_lookup.managed_account_id:\",\n","      len(lookup_managed_account_ids) - len(matching_values))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23rtn42VxPXx","executionInfo":{"status":"ok","timestamp":1736462784335,"user_tz":480,"elapsed":146,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"d7ab3519-a440-4480-a563-03e317149af2"},"execution_count":154,"outputs":[{"output_type":"stream","name":"stdout","text":["Total unique values in df_lookup.managed_account_id: 755\n","Total unique values in df_prop_size.Record_id_js: 1249\n","Number of matching values: 643\n","Number of non-matching values in df_lookup.managed_account_id: 112\n"]}]},{"cell_type":"code","source":["# Get sets of values that are failing the join\n","# These records were sent to JS to analyze\n","lookup_managed_account_ids = set(df_lookup.managed_account_id.dropna())\n","prop_size_record_ids = set(df_prop_size.Record_id_js)\n","\n","# Find values in lookup that are not in prop_size\n","non_matching_ids = lookup_managed_account_ids - prop_size_record_ids\n","\n","# Get the full records from df_lookup for these non-matching IDs\n","non_matching_records = df_lookup[df_lookup.managed_account_id.isin(non_matching_ids)]\n","\n","# Save to CSV\n","non_matching_records.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/non_matching_managed_account_ids.csv', index=False)\n","\n","print(\"Number of non-matching managed_account_id values:\", len(non_matching_ids))\n","print(\"CSV saved to: /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/non_matching_managed_account_ids.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Im5wbdTW89SR","executionInfo":{"status":"ok","timestamp":1736462957527,"user_tz":480,"elapsed":121,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"7dfe3004-6028-4fdf-e563-784111e61e74"},"execution_count":157,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of non-matching managed_account_id values: 112\n","CSV saved to: /content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/non_matching_managed_account_ids.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHAuNx_tKCa1"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# UDF to clean the 'user_id' column\n","def clean_user_id(df, column_name=\"user_id\"):\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'timestamp' column\n","def clean_timestamp(df, column_name=\"timestamp\"):\n","    df.loc[:, column_name] = pd.to_datetime(df[column_name], errors='coerce')  # Convert to datetime\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'property_id' column\n","def clean_property_id(df, column_name=\"property_id\"):\n","    df_cleaned = df.dropna(subset=[column_name])\n","    return df_cleaned\n","\n","# UDF to clean the 'value_0' column\n","def clean_value_0(df, column_name=\"value_0\"):\n","    df[column_name] = df[column_name].fillna(0).astype(int)\n","    return df\n","\n","# UDF to clean the 'value_1' column\n","def clean_value_1(df, column_name=\"value_1\"):\n","    df[column_name] = df[column_name].fillna(0).astype(int)\n","    return df\n","\n","# UDF to remove rows where both unit_0 and unit_1 are null\n","def remove_null_units(df):\n","    return df.dropna(subset=['unit_0', 'unit_1'], how='all')\n","\n","# UDF to rename 'value_0' to 'Watthrs' and 'value_1' to 'Amps'\n","def rename_values(df):\n","    df = df.rename(columns={'value_0': 'Watthrs', 'value_1': 'Amps'})\n","    return df\n","\n","# UDF to drop 'unit_0' and 'unit_1' columns\n","def drop_units(df):\n","    df = df.drop(columns=['unit_0', 'unit_1'])\n","    return df\n","\n","# UDF to set data types\n","def set_data_types(df):\n","    dtype_mapping = {\n","        \"property_id\": \"object\",\n","        \"user_id\": \"object\",\n","        \"timestamp\": \"datetime64[ns]\",\n","        \"Watthrs\": \"int64\",  # updated to reflect the renamed column\n","        \"Amps\": \"int64\",     # updated to reflect the renamed column\n","    }\n","\n","    for column, dtype in dtype_mapping.items():\n","        try:\n","            if dtype == \"datetime64[ns]\":\n","                df.loc[:, column] = pd.to_datetime(df[column], errors='coerce')\n","                invalid_rows = df[column].isna().sum()\n","                if invalid_rows > 0:\n","                    print(f\"Warning: {invalid_rows} invalid timestamps found in '{column}' and coerced to NaT.\")\n","            else:\n","                df.loc[:, column] = df[column].astype(dtype)\n","        except KeyError:\n","            print(f\"Column '{column}' not found in DataFrame.\")\n","        except Exception as e:\n","            print(f\"Error converting column '{column}' to type '{dtype}': {e}\")\n","    return df\n","\n","# Function to compact the data and combine Amps and Watthrs in the same row\n","def compact_data(df):\n","    # Group by the relevant columns and aggregate to ensure Amps and Watthrs are in the same row\n","    df_compacted = df.groupby(['property_id', 'user_id', 'timestamp', 'property_type'], as_index=False).agg(\n","        Watthrs=('Watthrs', 'max'),  # Take the maximum (non-zero) value for Watthrs\n","        Amps=('Amps', 'max')         # Take the maximum (non-zero) value for Amps\n","    )\n","\n","    print(f\"After compacting data, data shape: {df_compacted.shape}\")\n","    return df_compacted\n","\n","# Generalized function to clean a DataFrame\n","def clean_data_with_udfs_and_dtypes(df, cleaning_functions):\n","    for func, col in cleaning_functions:\n","        print(f\"Applying cleaning rule: {func.__name__} on column: {col}\")\n","        if col:\n","            df = func(df, column_name=col)\n","        else:\n","            df = func(df)  # Call function without column_name argument\n","    print(\"Setting data types...\")\n","    df = set_data_types(df)\n","    return df\n","\n","# File paths\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties.csv'\n","]\n","\n","# Define cleaning rules\n","cleaning_rules = [\n","    (clean_user_id, \"user_id\"),\n","    (clean_timestamp, \"timestamp\"),\n","    (clean_property_id, \"property_id\"),\n","    (clean_value_0, \"value_0\"),\n","    (clean_value_1, \"value_1\"),\n","    (remove_null_units, None),\n","    (rename_values, None),  # Renaming values columns\n","    (drop_units, None),     # Dropping unit columns\n","]\n","\n","# Process each file\n","for file_path in file_paths:\n","    print(f\"Processing file: {file_path}\")\n","    try:\n","        df = pd.read_csv(file_path)\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","        continue  # Skip to the next file if this one isn't found\n","\n","    df_cleaned = clean_data_with_udfs_and_dtypes(df.copy(), cleaning_rules)\n","\n","    # Compact the data to ensure Watthrs and Amps are in the same row\n","    df_compacted = compact_data(df_cleaned)\n","\n","    print(\"\\nFinal DataFrame Info:\")\n","    print(df_compacted.info())\n","\n","    # Save the compacted DataFrame\n","    output_path = file_path.replace(\".csv\", \"_cleaned_compacted.csv\")\n","    df_compacted.to_csv(output_path, index=False)\n","\n","    print(f\"Compacted file saved to: {output_path}\")\n","    print(f\"Rows before cleaning: {len(df)}, Rows after cleaning: {len(df_compacted)}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk6sjC-mPN3W"},"outputs":[],"source":["# Data check\n","\n","import pandas as pd\n","import numpy as np\n","\n","# Function to count NaN and infinite values in the DataFrame\n","def count_nan_inf(df):\n","    # Count NaN values\n","    nan_count = df.isna().sum().sum()\n","\n","    # Count infinite values (positive and negative infinity)\n","    inf_count = (df == np.inf).sum().sum() + (df == -np.inf).sum().sum()\n","\n","    print(f\"NaN values: {nan_count}\")\n","    print(f\"Inf values: {inf_count}\")\n","\n","    # Optionally: Display count of NaN and Inf values per column\n","    print(\"\\nNaN and Inf values per column:\")\n","    print(df.isna().sum())  # Count of NaN per column\n","    print(\"\\nInfinite values per column:\")\n","    print((df == np.inf).sum() + (df == -np.inf).sum())  # Count of Inf per column\n","\n","    return nan_count, inf_count\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv'\n","df = pd.read_csv(file_path)\n","\n","# Run the check for NaN and Inf values\n","count_nan_inf(df)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R075jkxkSD2"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHW_OWXZbl-3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Utility Functions\n","def add_day_info(df, timestamp_col='timestamp'):\n","    df['day_of_week'] = df[timestamp_col].dt.dayofweek + 1  # Convert 0-6 (Monday-Sunday) to 1-7 (Sunday-Saturday)\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)  # Weekend: Saturday (6) and Sunday (7)\n","    return df\n","\n","def calculate_days_to_nearest_holiday(df, date_col, holiday_dates):\n","    df[date_col] = pd.to_datetime(df[date_col])\n","\n","    if df[date_col].dt.tz is not None:\n","        holiday_dates = [\n","            holiday if holiday.tz is not None else holiday.tz_localize('UTC')\n","            for holiday in holiday_dates\n","        ]\n","    else:\n","        holiday_dates = [\n","            holiday.tz_convert(None) if holiday.tz is not None else holiday\n","            for holiday in holiday_dates\n","        ]\n","\n","    df['days_to_nearest_holiday'] = df[date_col].apply(\n","        lambda x: min(abs((x - holiday).days) for holiday in holiday_dates)\n","    )\n","    return df\n","\n","def add_datetime_components(df, timestamp_col='timestamp'):\n","    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')  # Coerce invalid datetime to NaT\n","\n","    # Drop rows where the timestamp is NaT after coercion\n","    df = df.dropna(subset=[timestamp_col])\n","\n","    # Extract the datetime components\n","    df['year'] = df[timestamp_col].dt.year\n","    df['month'] = df[timestamp_col].dt.month\n","    df['day'] = df[timestamp_col].dt.day\n","    df['hour'] = df[timestamp_col].dt.hour\n","\n","    return df\n","\n","def encode_month_column(df, month_col='month'):\n","    month_mapping = {'September': 9, 'October': 10, 'November': 11}\n","    df['month_encoded'] = df[month_col].map(month_mapping).fillna(df[month_col]).astype(int)\n","    return df\n","\n","def add_unique_user_counts(df, group_cols, user_col='user_id'):\n","    unique_user_counts = (\n","        df.groupby(group_cols)[user_col]\n","        .nunique()\n","        .reset_index()\n","        .rename(columns={user_col: 'unique_user_count'})\n","    )\n","    df = df.merge(unique_user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols, value_cols):\n","    sums = df.groupby(group_cols)[value_cols].sum().reset_index()\n","    sums.rename(\n","        columns={\n","            value_cols[0]: 'hour_sum_value_Wh',\n","            value_cols[1]: 'hour_sum_value_A'\n","        },\n","        inplace=True\n","    )\n","    df = df.merge(sums, on=group_cols, how='left')\n","    return df\n","\n","# New function to encode 'property_type' column using label encoding\n","def encode_property_type(df):\n","    label_encoder = LabelEncoder()\n","    df['property_type'] = label_encoder.fit_transform(df['property_type'])\n","    return df\n","\n","def engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates=None):\n","    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce', utc=True)\n","    df = add_day_info(df, timestamp_col)\n","    if holiday_dates:\n","        df = calculate_days_to_nearest_holiday(df, timestamp_col, holiday_dates)\n","    df = add_datetime_components(df, timestamp_col)\n","    df = add_unique_user_counts(df, group_cols, user_col)\n","    df = add_usage_sums(df, group_cols, value_cols)\n","\n","    # Encode 'property_type' using label encoding\n","    df = encode_property_type(df)\n","\n","    return df\n","\n","def process_files(file_paths, output_dir, group_cols, value_cols, timestamp_col='timestamp', user_col='user_id', holiday_dates=None):\n","    processed_dfs = []\n","    for file_path in file_paths:\n","        file_name = os.path.splitext(os.path.basename(file_path))[0]\n","        output_file = os.path.join(output_dir, f\"{file_name}_eng_features.csv\")\n","        print(f\"Processing file: {file_path}\")\n","        df = pd.read_csv(file_path)\n","\n","        # Engineer data and handle NaT in timestamp column by dropping those rows\n","        processed_df = engineer_data(df, timestamp_col, user_col, group_cols, value_cols, holiday_dates)\n","\n","        # Optionally, drop any rows with NaT in the timestamp column before saving\n","        processed_df = processed_df.dropna(subset=[timestamp_col])\n","\n","        processed_df.to_csv(output_file, index=False)\n","        print(f\"Processed file saved to: {output_file}\")\n","        processed_dfs.append(processed_df)\n","\n","    return processed_dfs\n","\n","# Usage\n","file_paths = ['/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted.csv']\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/'\n","group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","value_cols = ['Watthrs', 'Amps']  # Updated to reflect the compacted columns\n","holiday_dates = [\n","    pd.Timestamp(\"2024-09-04\", tz='UTC'),\n","    pd.Timestamp(\"2024-10-09\", tz='UTC'),\n","    pd.Timestamp(\"2024-11-23\", tz='UTC')\n","]\n","\n","# Process files\n","processed_dfs = process_files(\n","    file_paths=file_paths,\n","    output_dir=output_dir,\n","    group_cols=group_cols,\n","    value_cols=value_cols,\n","    timestamp_col='timestamp',\n","    user_col='user_id',\n","    holiday_dates=holiday_dates\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuS1cyCcPuAo"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","\n","# Function to count NaN and infinite values in the DataFrame\n","def count_nan_inf(df):\n","    # Count NaN values\n","    nan_count = df.isna().sum().sum()\n","\n","    # Count infinite values (positive and negative infinity)\n","    inf_count = (df == np.inf).sum().sum() + (df == -np.inf).sum().sum()\n","\n","    print(f\"NaN values: {nan_count}\")\n","    print(f\"Inf values: {inf_count}\")\n","\n","    # Optionally: Display count of NaN and Inf values per column\n","    print(\"\\nNaN and Inf values per column:\")\n","    print(df.isna().sum())  # Count of NaN per column\n","    print(\"\\nInfinite values per column:\")\n","    print((df == np.inf).sum() + (df == -np.inf).sum())  # Count of Inf per column\n","\n","    return nan_count, inf_count\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"hour_sum_value_A\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy (e.g., median, mean)\n","X.replace([np.inf, -np.inf], 0, inplace=True)  # Replace inf values with 0\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bgs5LYhzdls4"},"outputs":[],"source":["# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for regression/ANOVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import statsmodels.api as sm\n","\n","# Load your dataset (replace with the correct file path)\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/logs_with_properties_cleaned_compacted_eng_features.csv'\n","df = pd.read_csv(file_path)\n","\n","# Define the fields to drop\n","fields_to_drop = ['property_id', 'user_id', 'timestamp', 'hour_sum_value_A','Watthrs','Amps']\n","\n","# Drop non-numeric columns from the dataframe\n","X = df.drop(columns=fields_to_drop)  # All columns except the dependent variable\n","y = df['hour_sum_value_Wh']  # Dependent variable\n","\n","# Check types of X and y\n","print(\"X dtypes:\\n\", X.dtypes)\n","print(\"y dtype:\", y.dtypes)\n","\n","# Add a constant to the model (intercept)\n","X = sm.add_constant(X)\n","\n","# Fit the regression model\n","model = sm.OLS(y, X).fit()\n","\n","# Print the summary of the regression model\n","print(model.summary())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpSYEsmq3VUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umUOsBSn3VXA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-e5kpAZ3VZS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F20AGhU3Vbq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1F9VbEO3VhO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"J85SXxSCIwdY"},"source":["## Extract from Eddie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAlXiQuUIu76"},"outputs":[],"source":["file_path_a = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/2024-08-01.csv'\n","\n","df_big = pd.read_csv(file_path_a)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J89LPzgmdzuZ"},"source":["### Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMZPkMirgm9l"},"outputs":[],"source":["df_big.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOkPWChKg5k8"},"outputs":[],"source":["header = [\n","    \"qrcode\",  # Column 0\n","    \"connector\",  # Column 1\n","    \"serial_num\",  # Column 2\n","    \"org_id\",  # Column 3\n","    \"property_id\",  # Column 4\n","    \"station_id\",  # Column 5\n","    \"transaction_id\",  # Column 6\n","    \"metered_type\",  # Column 7\n","    \"timestamp\",  # Column 8\n","    \"metered_value\"   # Column 9\n","]\n","\n","df_big.columns = header"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gcxojux1h63w"},"outputs":[],"source":["df_big['Timestamp'] = pd.to_datetime(df_big['Timestamp'])\n"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"c6ZMTHHnZg9V"},"source":["### Tables I can access"]},{"cell_type":"code","execution_count":190,"metadata":{"id":"NUKjVft2kxIk","executionInfo":{"status":"ok","timestamp":1736467216028,"user_tz":480,"elapsed":871,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66dd8e1a-8f25-456b-c82c-fd2698921049"},"outputs":[{"output_type":"stream","name":"stdout","text":["properties\n","adjustment\n","group_discount\n","group_discount_properties\n","payment\n","roles\n","station_credit_program\n","subscription\n","user_discount_properties\n","pos_device\n","users\n","location\n","station_history\n","vehicle\n","router\n","stripe_payment_intent\n","cluster_name\n","rfid_user\n","station_logs\n","global_setting\n","api_token\n","station_model\n","awsdms_ddl_audit\n","user_access\n","adr\n","audit\n","pricing\n","stations\n","credit_program\n","errors\n","gateway\n","gateway_ip_lease\n","refresh_token\n","net_device_ip_lease\n","ocpp_sub_session\n","property_types\n","maintenance_window\n","transaction\n","user_device\n","accounts\n","address\n","clusters\n","connectors\n","flyway_schema_history\n","net_devices\n","ocpp_session\n","organizations\n","panels\n","Connection closed.\n"]}],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Xz1nKBmBM2nA"},"source":["###Create a table for all property info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7enpOKwkM2Gu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup_2.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names to lowercase and strip whitespace\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the left join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='left',  # Use 'left' join to keep all rows from properties and add property_type name where available\n","    left_on='property_type',  # Assuming 'property_type' is the column in properties.csv\n","    right_on='id',  # Assuming 'id' is the column in property_types.csv\n","    suffixes=('_property', '_type')\n",")\n","\n","# Keep all columns from properties and just add the 'name_type' column as 'property_type'\n","property_lookup['property_type'] = property_lookup['name_type']\n","\n","# Drop the 'name_type' column, since we already added it as 'property_type'\n","property_lookup = property_lookup.drop(columns=['name_type'])\n","\n","# Rename 'id_property' column to 'property_id'\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE633fu_M2Iz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrQU1kppM2LR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOTj1nOYM2N5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaoKGEHJw0uL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import logging\n","from itertools import combinations\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Path to the directory containing the CSV files\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n","\n","# List of tables (as per your previous code)\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Function to load CSV files into DataFrames\n","def load_dataframes(tables):\n","    dataframes = {}\n","    for table in tables:\n","        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","            dataframes[table] = df\n","            logging.info(f\"Loaded data for table: {table}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading data for table {table}: {e}\")\n","    return dataframes\n","\n","# Function to find strict join matches\n","def find_strict_joins(df1, df2, table1_name, table2_name):\n","    strict_joins = []\n","    # Iterate over all column pairs\n","    for col1 in df1.columns:\n","        for col2 in df2.columns:\n","            if df1[col1].dtype == df2[col2].dtype:\n","                # Perform the join\n","                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n","                # Check if all rows in df1 are in the joined DataFrame\n","                if len(joined_df) == len(df1):\n","                    strict_joins.append((col1, col2))\n","                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n","    return strict_joins\n","\n","# Main function to perform the strict join analysis\n","def analyze_strict_joins(tables):\n","    dataframes = load_dataframes(tables)\n","    results = {}\n","    table_pairs = combinations(tables, 2)\n","\n","    for table1, table2 in table_pairs:\n","        df1 = dataframes.get(table1)\n","        df2 = dataframes.get(table2)\n","\n","        if df1 is not None and df2 is not None:\n","            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n","            joins = find_strict_joins(df1, df2, table1, table2)\n","            if joins:\n","                results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n","\n","    return results\n","\n","# Run the strict join analysis\n","strict_join_results = analyze_strict_joins(tables)\n","\n","# Print the results\n","for table_pair, joins in strict_join_results.items():\n","    print(f\"\\nStrict joins for {table_pair}:\")\n","    for col1, col2 in joins:\n","        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n","\n","if not strict_join_results:\n","    print(\"No strict joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqQ_iKw5dn0M"},"outputs":[],"source":["import os\n","import logging\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection string for SQLAlchemy\n","connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n","engine = create_engine(connection_string)\n","\n","# Function to fetch column names for a table\n","def get_columns(table_name):\n","    try:\n","        query = f\"\"\"\n","        SELECT column_name, data_type\n","        FROM information_schema.columns\n","        WHERE table_name = '{table_name}';\n","        \"\"\"\n","        with engine.connect() as connection:\n","            df = pd.read_sql_query(query, connection)\n","        return df[['column_name', 'data_type']].to_dict('records')\n","    except Exception as e:\n","        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n","        return []\n","\n","# Function to test join logic between two tables\n","def test_joins(table1, table2, attempts=3):\n","    columns_table1 = get_columns(table1)\n","    columns_table2 = get_columns(table2)\n","    successful_joins = []\n","\n","    for col1 in columns_table1:\n","        for col2 in columns_table2:\n","            # Only test joins on matching data types\n","            if col1['data_type'] == col2['data_type']:\n","                success_count = 0\n","                for _ in range(attempts):  # Attempt the join multiple times\n","                    query = f\"\"\"\n","                    SELECT *\n","                    FROM {table1} t1\n","                    INNER JOIN {table2} t2\n","                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n","                    LIMIT 1;  -- Test with one row at a time\n","                    \"\"\"\n","                    try:\n","                        with engine.connect() as connection:\n","                            df = pd.read_sql_query(query, connection)\n","                            if not df.empty:\n","                                success_count += 1\n","                    except Exception as e:\n","                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n","\n","                if success_count == attempts:  # Only count as successful if all attempts work\n","                    successful_joins.append((col1['column_name'], col2['column_name']))\n","                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n","\n","    return successful_joins\n","\n","# Cross-check join fields for all table pairs\n","tables = [\n","    \"users\", \"ocpp_sub_session\"\n","]\n","\n","results = {}\n","\n","for i, table1 in enumerate(tables):\n","    for table2 in tables[i+1:]:\n","        logging.info(f\"Testing joins between {table1} and {table2}\")\n","        joins = test_joins(table1, table2)\n","        if joins:\n","            results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.info(f\"No join found between {table1} and {table2}\")\n","\n","# Print results\n","for table_pair, joins in results.items():\n","    print(f\"Successful joins for {table_pair}: {joins}\")\n","\n","if not results:\n","    print(\"No successful joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU41V4jQdn2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMXMJghgdn5H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuwvrH2ndn7Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VIJZs8fdn9k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kH2jY7TdoAK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ex8BAejxDoB"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Example dataframe (assuming df['message'] contains the raw strings)\n","# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n","def clean_message(msg):\n","    try:\n","        # Remove the prefix\n","        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n","\n","        # Attempt to load the cleaned message as JSON\n","        return json.loads(msg_cleaned)\n","    except (json.JSONDecodeError, TypeError):\n","        # If the message cannot be decoded as JSON, return None or handle as needed\n","        return None\n","\n","# Apply the function to the 'message' column\n","df['message'] = df['message'].apply(clean_message)\n","\n","# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n","df = df[df['message'].notna()]\n","\n","# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n","flattened_rows = []\n","\n","for idx, row in df.iterrows():\n","    message = row['message']  # Now this is a valid JSON object\n","\n","    # Extract top-level fields\n","    connector_id = message.get('connectorId')\n","    transaction_id = message.get('transactionId')\n","\n","    # Keep the 'meterValue' field as is (as a list of dicts)\n","    meter_value = message.get('meterValue', [])\n","\n","    # Add a row to the flattened list, including the nested 'meterValue' list\n","    flattened_rows.append({\n","        '_time': row['time'],  # Retain the original timestamp from the dataframe\n","        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","        'connectorId': connector_id,\n","        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n","    })\n","\n","# Step 2: Create a new DataFrame from the flattened rows\n","flattened_df = pd.DataFrame(flattened_rows)\n","\n","# Display the resulting DataFrame\n","print(flattened_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbk_sGLxZ_Z"},"outputs":[],"source":["import pandas as pd\n","\n","# Set pandas options to display the full content of any column (e.g., 'meterValue')\n","pd.set_option('display.max_colwidth', None)\n","\n","# Now, display the full content of the 'meterValue' column for the first 5 rows\n","print(flattened_df['meterValue'].head(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhRPDO5zJH0"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to hold the expanded rows\n","expanded_rows = []\n","\n","# Iterate over each row in the dataframe\n","for idx, row in flattened_df.iterrows():\n","    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n","\n","    # For each meter value entry (there should be one timestamp and a list of measurements)\n","    for meter in meter_values:\n","        timestamp = meter['timestamp']  # Extract the timestamp\n","\n","        # Initialize values for each measurement type\n","        watt_hours_value = None  # WattHours\n","        amps_value = None        # Amps (Current)\n","        voltage_value = None     # Voltage (Volts)\n","\n","        # Iterate over the sampledValue list (which contains the three measurements)\n","        for sample in meter['sampledValue']:\n","            # Check the 'unit' to assign the value to the correct column\n","            if sample['unit'] == 'Wh':  # WattHours\n","                watt_hours_value = sample['value']\n","            elif sample['unit'] == 'A':  # Amps (Current)\n","                amps_value = sample['value']\n","            elif sample['unit'] == 'V':  # Volts (Voltage)\n","                voltage_value = sample['value']\n","\n","        # Append the expanded row with the extracted values\n","        expanded_rows.append({\n","            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n","            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","            'connectorId': row['connectorId'],  # Connector ID\n","            'timestamp': timestamp,  # Timestamp from the meter value\n","            'WattHours': watt_hours_value,  # Renamed to WattHours\n","            'Amps': amps_value,  # Keep Amps as the column name\n","            'Voltage': voltage_value  # Value for Voltage (V)\n","        })\n","\n","# Create a new DataFrame from the expanded rows\n","expanded_df = pd.DataFrame(expanded_rows)\n","\n","# Convert the numeric columns to appropriate types (float)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Display the resulting DataFrame\n","print(expanded_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcgRo8Dz1Nm"},"outputs":[],"source":["# Ensure all columns are numeric (in case there are any string values left)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Classify values as 0 or > 0 for each of the measurements\n","expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n","\n","# Set up the plot\n","plt.figure(figsize=(18, 6))\n","\n","# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n","plt.subplot(1, 3, 1)\n","sns.countplot(data=expanded_df, x='WattHours_Class')\n","plt.title('Count of Rows with WattHours: 0 vs > 0')\n","plt.xlabel('WattHours Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 2)\n","sns.countplot(data=expanded_df, x='Amps_Class')\n","plt.title('Count of Rows with Amps: 0 vs > 0')\n","plt.xlabel('Amps Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 3)\n","sns.countplot(data=expanded_df, x='Voltage_Class')\n","plt.title('Count of Rows with Voltage: 0 vs > 0')\n","plt.xlabel('Voltage Class')\n","plt.ylabel('Count')\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsRrQLIz1QQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure that the '_time' column is in datetime format\n","expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n","\n","# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Drop rows where any of the values are missing\n","expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create the figure and axes for the plots\n","plt.figure(figsize=(18, 6))\n","\n","# Plot Amps over time\n","plt.subplot(1, 3, 1)\n","plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n","plt.title('Amps over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Amps')\n","plt.xticks(rotation=45)\n","\n","# Plot WattHours over time\n","plt.subplot(1, 3, 2)\n","plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n","plt.title('WattHours over Time')\n","plt.xlabel('Time')\n","plt.ylabel('WattHours')\n","plt.xticks(rotation=45)\n","\n","# Plot Voltage over time\n","plt.subplot(1, 3, 3)\n","plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n","plt.title('Voltage over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Voltage')\n","plt.xticks(rotation=45)\n","\n","# Adjust layout to avoid overlap of labels\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi2OjXK_2hXK"},"outputs":[],"source":["expanded_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TbILosnz1Sh"},"outputs":[],"source":["# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n","descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n","\n","# Display the statistics\n","print(descriptive_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zZUwehE16FW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure with 3 subplots (1 row, 3 columns)\n","plt.figure(figsize=(18, 6))\n","\n","# Plot for Amps\n","plt.subplot(1, 3, 1)\n","sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n","plt.title('Boxplot of Amps')\n","plt.ylabel('Amps')\n","\n","# Plot for WattHours\n","plt.subplot(1, 3, 2)\n","sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n","plt.title('Boxplot of WattHours')\n","plt.ylabel('WattHours')\n","\n","# Plot for Voltage\n","plt.subplot(1, 3, 3)\n","sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n","plt.title('Boxplot of Voltage')\n","plt.ylabel('Voltage')\n","\n","# Adjust layout to avoid overlap\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBnl60S2LIP"},"outputs":[],"source":["# count of propertyIDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9r14vD2LK6"},"outputs":[],"source":["#Count of cluster IDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F01IOcSb2LNZ"},"outputs":[],"source":["# counts of peropertyID and clusterIDimport os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'cluster_id' values for each 'property_id'\n","query = f\"\"\"\n","SELECT property_id, COUNT(DISTINCT cluster_id)\n","FROM {table}\n","GROUP BY property_id\n",";\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    property_id, cluster_count = row\n","    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values for each 'cluster_id'\n","query = f\"\"\"\n","SELECT cluster_id, COUNT(DISTINCT property_id)\n","FROM {table}\n","GROUP BY cluster_id;\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    cluster_id, property_count = row\n","    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n"]},{"cell_type":"markdown","metadata":{"id":"gJCn043ops5i"},"source":["OCPP_SessionID has a userID and TransactionID\n","I need to map to the cluster and property\n","\n","Stations has propertyID and cluster_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkQRHSF2pYLF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRERnF5HpYNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZNFFc8pYQY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pbxg6jVhpYS3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wI1hDsHpYVx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiA1AO1z2LSG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnNNDMY2LUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpS8FyTqEWgq"},"outputs":[],"source":["# Clean message field and port to a df\n","import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n","valid_df = df[df['parsed_message'].notnull()]\n","\n","# Flatten all the JSON objects and store them in a new DataFrame\n","flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n","new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n","\n","# Write the DataFrame to CSV with new naming convention\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n","new_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnkEtU3uDKj"},"outputs":[],"source":["new_df.info\n","new_df.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDqqE7Swt6vj"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df= new_df\n","\n","# Assuming your DataFrame is named df\n","# Step 1: Convert 'time' to datetime\n","df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n","\n","# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n","df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n","\n","# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n","df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n","\n","# Step 4: Plot the time series\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n","plt.xlabel('Time')\n","plt.ylabel('Meter Value')\n","plt.title('Meter Value Over Time')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlT8kevDasAZ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['message'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcwXooB_tXnf"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming new_df is already defined and contains the necessary columns\n","\n","# List of columns to keep\n","columns_to_keep = [\n","    #'time',\n","    'user_id',\n","    #'station_id',\n","    'property_id',\n","    'connectorId',\n","    'meterValue_0_timestamp',\n","    'meterValue_0_sampledValue_1_value',\n","    'meterValue_0_sampledValue_1_context',\n","    'meterValue_0_sampledValue_1_format',\n","    'meterValue_0_sampledValue_1_measurand',\n","    'meterValue_0_sampledValue_1_phase',\n","    'meterValue_0_sampledValue_1_location',\n","    'meterValue_0_sampledValue_1_unit'\n","]\n","\n","# Create new_df_2 with only the selected columns\n","new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","\n","# Convert 'time' to datetime\n","new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n","\n","# Check for any NaT values that may have resulted from the conversion\n","if new_df_2['meterValue_0_timestamp'].isnull().any():\n","    print(\"Some values could not be converted to datetime.\")\n","\n","# Extract day and hour using .loc to avoid warnings\n","new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n","new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLthqnU8u2eM"},"outputs":[],"source":["new_df_2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMxEqA8Qbkxz"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = new_df_2\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N29p7Kq_xWuu"},"outputs":[],"source":["# Assuming new_df_2 is the df\n","\n","unique_values = new_df_2['user_id'].unique()\n","\n","# To display the unique values\n","print(unique_values)\n","\n","\n","# Assuming new_df_2 is your DataFrame\n","unique_count = new_df_2['user_id'].nunique()\n","\n","# To display the count of unique user_id values\n","print(f\"Number of unique user_id values: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEjr917eu9Rb"},"outputs":[],"source":["import pandas as pd\n","\n","\n","new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n","\n","max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n","\n","result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n","\n","print(result_df)\n","result_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnNlof0taJFO"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of meterValue_0_timestamp')\n","plt.title('Count of meterValue_0_timestamp per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"842I60p_-VjA"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming result_df is your DataFrame\n","\n","# Print column names to verify\n","print(\"Column names in DataFrame:\")\n","print(result_df.columns.tolist())\n","\n","# Specify the user_id you're interested in\n","specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n","\n","# Filter the DataFrame for the specific user_id\n","filtered_df = result_df[result_df['user_id'] == specific_user_id]\n","\n","# Check if there are any rows for the specified user_id\n","if not filtered_df.empty:\n","    # Check for the timestamp column again\n","    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n","    value_col = 'meterValue_0_sampledValue_1_value'\n","\n","    # Ensure the column names are correct\n","    print(\"Filtered DataFrame columns:\")\n","    print(filtered_df.columns.tolist())\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n","    plt.title(f'Meter Values for User ID: {specific_user_id}')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Meter Value')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n","    plt.grid()\n","    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","    plt.show()\n","else:\n","    print(f\"No data found for user_id: {specific_user_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp5ImngUusix"},"outputs":[],"source":["\n","# Write the DataFrame to CSV\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n","result_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ZZ-b6HuslW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3cWsdlTusnX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TvKiocBsdvU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A0Tx9v-sdxw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKXZgFB5sd0F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBdB1TPsd2a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDJEMK-rsd4_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyMtBhiqi4Vl7r0zltp2fudw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}