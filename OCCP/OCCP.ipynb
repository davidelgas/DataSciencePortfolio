{"cells":[{"cell_type":"markdown","metadata":{"id":"3KBInAINFc_Y"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"sVqn4_9rFPYU"},"source":["This project will explore the OCCP data. Open Charge Point Protocol (OCPP) is an open standard communication protocol for Electric Vehicle (EV) charging stations. It defines interactions between EV charging stations and a central system, helping to facilitate security, transactions, diagnostics, and more.\n","\n","This dataset if from OCCP v1.6"]},{"cell_type":"markdown","metadata":{"id":"XznKesLDcf0o"},"source":["## Charging System Diagram\n","Organization < Property < Location < Cluster < Station < UserID\n","\n","A cluster is a grouping of chargers/stations. This for convenience/load balancing\n","\n","Each circuit can have multiple clusters.\n","\n","Each cluster has its own breaker\n"]},{"cell_type":"markdown","metadata":{"id":"ylcs9vE6TRgG"},"source":["## Prepare Enviornment"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mhwSzFEWit8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737256203080,"user_tz":480,"elapsed":40053,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"ce7722c4-f75e-4004-f786-a13ec5336528"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Access to Google Drive\n","# This seems to propagate credentials better from its own cell\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz_Nyx0M2KOU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"75a07650-9899-4441-aec7-0df51525dc4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyGithub\n","  Downloading PyGithub-2.5.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pynacl>=1.4.0 (from PyGithub)\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.15)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n","Downloading PyGithub-2.5.0-py3-none-any.whl (375 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.9/375.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Packages and methods\n","\n","!pip install PyGithub\n","from github import Github\n","import os\n","import datetime\n","from google.colab import userdata\n","\n","!pip install pandas pyxlsb\n","import pandas as pd\n","\n","import numpy as np\n","\n","import sys\n","import logging\n","import psycopg2\n","\n","!pip install SQLAlchemy psycopg2-binary\n","import seaborn as sns\n","import matplotlib.pyplot as p\n","\n","import json\n","\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","import matplotlib.pyplot as plt\n","\n","from datetime import timedelta\n","import holidays\n","\n","!pip install statsmodels\n","import statsmodels.api as sm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvDdY57l4fxE"},"outputs":[],"source":["# Update github\n","\n","def colab_to_github(notebook_path, github_repo, folder_path=None, commit_message=None, branch=\"main\"):\n","   try:\n","       print(\"Fetching GitHub token...\")\n","       token = os.getenv('GITHUB_TOKEN')\n","       if not token:\n","           raise ValueError(\"GitHub token is missing or invalid. Ensure it is set as an environment variable.\")\n","\n","       # Add debug logging (only showing first few chars for security)\n","       print(f\"Token format check - starts with: {token[:4]}\")\n","\n","       print(\"Token successfully retrieved.\")\n","       g = Github(token)\n","       repo = g.get_repo(github_repo)\n","       print(f\"Connected to repository: {github_repo}\")\n","\n","       if not commit_message:\n","           commit_message = f\"Auto-commit from Colab: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n","       print(f\"Using commit message: {commit_message}\")\n","\n","       with open(notebook_path, 'r') as file:\n","           notebook_content = file.read()\n","       print(f\"Notebook content read from {notebook_path}\")\n","\n","       filename = os.path.basename(notebook_path)\n","       # Construct the full file path including the folder if specified\n","       file_path = f\"{folder_path}/{filename}\" if folder_path else filename\n","       print(f\"Target file path in repo: {file_path}\")\n","\n","       try:\n","           print(f\"Checking if file exists at {file_path}...\")\n","           existing_file = repo.get_contents(file_path, ref=branch)\n","           repo.update_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               sha=existing_file.sha,\n","               branch=branch\n","           )\n","           print(f\"File updated successfully in branch '{branch}'.\")\n","       except Exception:\n","           print(f\"File does not exist at {file_path}. Attempting to create...\")\n","           repo.create_file(\n","               path=file_path,\n","               message=commit_message,\n","               content=notebook_content,\n","               branch=branch\n","           )\n","           print(f\"File created successfully in branch '{branch}'.\")\n","\n","   except Exception as e:\n","       print(f\"Error occurred: {e}\")\n","\n","raw_token = userdata.get('GITHUB_TOKEN')\n","cleaned_token = raw_token.replace('token ', '').strip()\n","print(f\"Cleaned token starts with: {cleaned_token[:4]}\")\n","\n","os.environ['GITHUB_TOKEN'] = cleaned_token\n","\n","# Call the function with your parameters\n","notebook_path = \"/content/drive/MyDrive/Colab Notebooks/OCCP.ipynb\"\n","github_repo = \"davidelgas/DataSciencePortfolio\"  # This is the correct repository path\n","folder_path = \"OCCP\"  # This specifies the directory within the repository\n","commit_message = \"Updated notebook from Colab\"\n","\n","colab_to_github(notebook_path, github_repo, folder_path, commit_message)"]},{"cell_type":"markdown","metadata":{"id":"My2ExD4GMgls"},"source":["## Ingest data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suHAcKhHVfV8"},"outputs":[],"source":["# import log data\n","\n","import pandas as pd\n","import numpy as np\n","\n","def load_file(file_path):\n","    \"\"\"Load a single CSV file.\"\"\"\n","    return pd.read_csv(file_path)\n","\n","def concatenate_files(file_paths):\n","    \"\"\"Load and combine multiple CSV files.\"\"\"\n","    dfs = []\n","    for file_path in file_paths:\n","        df = load_file(file_path)\n","        if not df.empty:\n","            dfs.append(df)\n","\n","    return pd.concat(dfs, ignore_index=True)\n","\n","if __name__ == \"__main__\":\n","    file_paths = [\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/sept_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/oct_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/nov_100_sample.csv',\n","        '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/dec_100_sample.csv'\n","    ]\n","\n","    # Concatenate all files\n","    df_logs = concatenate_files(file_paths)\n","\n","    # Save the combined raw data\n","    df_logs.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgAP-CX4LKV4"},"outputs":[],"source":["#Import property tables from AWS\n","\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","    with open(path_to_credentials, 'r') as file:\n","        for line in file:\n","            if '=' in line:\n","                key, value = line.split('=', 1)\n","                os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch properties table\n","cursor.execute(\"SELECT * FROM properties;\")\n","df_prop = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.csv', index=False)\n","df_prop.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl')\n","\n","# Fetch property_types table\n","cursor.execute(\"SELECT * FROM property_types;\")\n","df_prop_type = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n","df_prop_type.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.csv', index=False)\n","df_prop_type.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl')\n","\n","# Close connection\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IjOthMDc4uL","executionInfo":{"status":"ok","timestamp":1737247582960,"user_tz":480,"elapsed":217,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"464c4bd6-98f8-4372-c973-2e89459e73c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Data types after conversion:\n","account_id             int64\n","managed_account_id     int64\n","Parking Space Count    int64\n","dtype: object\n"]}],"source":["#Import property table from SalesForce\n","\n","import pandas as pd\n","from pathlib import Path\n","import os\n","\n","def process_property_sizes(input_file: Path) -> pd.DataFrame:\n","    # Define fields to cast as int\n","    int_fields = [\n","        'account_id',\n","        'managed_account_id',\n","        'Parking Space Count'\n","        # Add any other fields that should be int\n","    ]\n","\n","    # Load CSV with explicit encoding\n","    df_prop_size = pd.read_csv(input_file, encoding='latin-1')\n","\n","    # Cast specified fields to int, handling any errors\n","    for field in int_fields:\n","        if field in df_prop_size.columns:\n","            df_prop_size[field] = pd.to_numeric(df_prop_size[field], errors='coerce').fillna(0).astype(int)\n","\n","    # Save pickle to same directory as input file\n","    prop_size_pickle_path = input_file.parent / 'df_prop_size.pkl'\n","    df_prop_size.to_pickle(prop_size_pickle_path)\n","\n","    # Print info about the conversions\n","    print(\"\\nData types after conversion:\")\n","    print(df_prop_size[int_fields].dtypes)\n","\n","    return df_prop_size\n","\n","if __name__ == \"__main__\":\n","    # Define base directory\n","    data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","    # Input path\n","    prop_size_input = data_dir / 'Properties Table jan2025.csv'\n","\n","    # Run workflow\n","    df_prop_size = process_property_sizes(prop_size_input)"]},{"cell_type":"code","source":["df_prop_size.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrxgUsRtAlhC","executionInfo":{"status":"ok","timestamp":1737247539862,"user_tz":480,"elapsed":147,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"5e3c3fac-7c04-4bbe-8ca5-9b5e72bed781"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 784 entries, 0 to 783\n","Data columns (total 31 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   id                    784 non-null    object \n"," 1   organization_id       784 non-null    object \n"," 2   name                  784 non-null    object \n"," 3   phone                 723 non-null    object \n"," 4   contact               751 non-null    object \n"," 5   longitude             783 non-null    float64\n"," 6   latitude              783 non-null    float64\n"," 7   watts_soft_limit      784 non-null    int64  \n"," 8   property_type         783 non-null    object \n"," 9   note                  245 non-null    object \n"," 10  utility_provider      782 non-null    object \n"," 11  gateway_type          682 non-null    object \n"," 12  carrier_name          600 non-null    object \n"," 13  address_1             784 non-null    object \n"," 14  address_2             8 non-null      object \n"," 15  city                  783 non-null    object \n"," 16  state                 784 non-null    object \n"," 17  zip                   784 non-null    int64  \n"," 18  email                 349 non-null    object \n"," 19  has_editable_penalty  784 non-null    bool   \n"," 20  managed_account_id    783 non-null    float64\n"," 21  created_at            784 non-null    object \n"," 22  updated_at            784 non-null    object \n"," 23  reboot_cron_schedule  35 non-null     object \n"," 24  account_id            775 non-null    float64\n"," 25  hidden                784 non-null    bool   \n"," 26  decommissioned_at     1 non-null      object \n"," 27  exclusive_domain_1    0 non-null      float64\n"," 28  exclusive_domain_2    0 non-null      float64\n"," 29  Unnamed: 29           0 non-null      float64\n"," 30  Parking Space Count   777 non-null    float64\n","dtypes: bool(2), float64(8), int64(2), object(19)\n","memory usage: 179.3+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"hN-XTvd8X3eG"},"source":["## Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0sBeZzEtE5O"},"outputs":[],"source":["# Here are the dfs Ill be working with\n","\n","df_logs = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs.pkl') # Event data from Splunk\n","df_prop = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop.pkl') # Property metadata from AWS\n","df_prop_size = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_size.pkl') # Property size data from SalesForce\n","df_prop_type = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type.pkl') # Property metadata from AWS"]},{"cell_type":"markdown","metadata":{"id":"mtmzDPK5HSAw"},"source":["###This is what the JSON field looks like\n","\n","\n","\n","{\"connectorId\":1,\"transactionId\":1417592169,\"meterValue\":[{\"timestamp\":\"2025-01-14T13:27:37.145Z\",\"sampledValue\":[{\"value\":\"31323855.0\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Energy.Active.Import.Register\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"Wh\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"240.57\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Voltage\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"V\"},{\"value\":\"28\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Temperature\",\"phase\":null,\"location\":\"Body\",\"unit\":\"Celsius\"},{\"value\":\"6.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Current.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"A\"},{\"value\":\"1440.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Offered\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"},{\"value\":\"0.00\",\"context\":\"Sample.Periodic\",\"format\":\"Raw\",\"measurand\":\"Power.Active.Import\",\"phase\":\"L1\",\"location\":\"Outlet\",\"unit\":\"W\"}]}]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdW03n8B_At0"},"outputs":[],"source":["# Unpack the JSON field in the log file\n","import pandas as pd\n","import json\n","\n","def expand_message_json(df):\n","    rows = []\n","\n","    for idx, row in df.iterrows():\n","        # Parse the JSON message\n","        message = json.loads(row['cleaned_message']) if pd.notna(row['cleaned_message']) else {}\n","\n","        # Get transactionId from the message\n","        transaction_id = message.get('transactionId')\n","\n","        # Extract meter values\n","        meter_values = message.get('meterValue', [])\n","        for meter in meter_values:\n","            timestamp = meter.get('timestamp')\n","            sampled_values = meter.get('sampledValue', [])\n","\n","            # Filter for only A and W units\n","            for sample in sampled_values:\n","                unit = sample.get('unit')\n","                if unit in ['A', 'W']:\n","                    rows.append({\n","                        'property_id': row['property_id'],\n","                        'user_id': row['user_id'],\n","                        'transaction_id': transaction_id,  # Fixed variable name here\n","                        'timestamp': timestamp,\n","                        'value': sample.get('value'),\n","                        'unit': unit\n","                    })\n","\n","    return pd.DataFrame(rows)\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","# Expand the JSON column\n","df_logs_exp = expand_message_json(df_logs)\n","\n","# Save the expanded data\n","df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n"]},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrame\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs_exp.pkl'\n","\n","# Read pickle and sample 10 records\n","df_logs_exp = pd.read_pickle(logs_pickle)\n","sample_df = df_logs_exp.sample(n=10, random_state=42)  # random_state for reproducibility\n","\n","# Save sample to CSV in same directory\n","sample_df.to_csv(data_dir / 'logs_sample.csv', index=False)\n","\n","print(\"10 record sample saved to logs_sample.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYYJCdYmLQNG","executionInfo":{"status":"ok","timestamp":1737250362796,"user_tz":480,"elapsed":1673,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"f30cc651-c962-4deb-c35f-b132a151cd85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10 record sample saved to logs_sample.csv\n"]}]},{"cell_type":"code","source":["# Address feld name and datatype inconsidtencies\n","import pandas as pd\n","from pathlib import Path\n","\n","def normalize_timestamp(df_path: Path) -> None:\n","   \"\"\"\n","   Convert timestamp column to datetime and truncate to seconds.\n","   \"\"\"\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Convert to datetime and truncate to seconds\n","   df['_time'] = pd.to_datetime(df['_time']).dt.floor('s')\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Timestamps normalized and saved\")\n","\n","def normalize_account_ids(df_path: Path) -> None:\n","   \"\"\"\n","   Convert account ID fields to integers and save back to pickle.\n","\n","   Args:\n","       df_path: Path to the pickle file\n","   \"\"\"\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID fields to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","   df['account_id'] = pd.to_numeric(df['account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Account IDs normalized and saved\")\n","\n","def normalize_prop_size_id(df_path: Path) -> None:\n","   \"\"\"\n","   Convert managed_account_id field to integer and save back to pickle.\n","\n","   Args:\n","       df_path: Path to the pickle file\n","   \"\"\"\n","   # Load DataFrame\n","   df = pd.read_pickle(df_path)\n","\n","   # Cast ID field to int\n","   df['managed_account_id'] = pd.to_numeric(df['managed_account_id'], errors='coerce').fillna(0).astype(int)\n","\n","   # Save back to pickle\n","   df.to_pickle(df_path)\n","\n","   print(\"Property size ID normalized and saved\")\n","\n","if __name__ == \"__main__\":\n","   # Define base directory\n","   data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","\n","   # Define pickle files\n","   logs_pickle = data_dir / 'df_logs.pkl'\n","   prop_pickle = data_dir / 'df_prop.pkl'\n","   prop_size_pickle = data_dir / 'df_prop_size.pkl'\n","\n","   # Run normalizations\n","   normalize_timestamp(logs_pickle)\n","   normalize_account_ids(prop_pickle)\n","   normalize_prop_size_id(prop_size_pickle)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSw2NT1WCaE7","executionInfo":{"status":"ok","timestamp":1737249114348,"user_tz":480,"elapsed":28457,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"f91ceb5a-c75b-427d-f015-1b405b21dd5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Timestamps normalized and saved\n","Account IDs normalized and saved\n","Property size ID normalized and saved\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5lLvvK85MCce","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737256117151,"user_tz":480,"elapsed":562,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"ddebcce9-c137-4a04-9ef6-dac55c465a22"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Unique property_ids in logs: 700\n","Unique ids in properties: 807\n","\n","Found 20 property_ids in logs that don't exist in properties table\n","Sample of missing ids: ['54e7058c-0a1f-4d5a-8c64-31b0e948ace4\\n348c41a5-baaa-4275-b8c5-4d9e6ba8f97d', '6d5e9f52-8dcd-4e58-a139-5bfcd3149d53\\n69de08c2-2c9f-43a8-9c03-d2fc05d03b2b', '8f411e48-b09c-4834-88e9-4b7f59e86130\\nc4ab1149-c4a9-46ab-8bfd-3b471b70d6d9', 'a8475020-74e2-40ca-8307-2b0adead38ab\\n724598d1-dab6-4ade-9f53-539023e34787', 'e95523e6-3470-4a60-b586-ee715cd9f34b\\n7dc180ff-efbe-4c31-8d05-f452ec7db3b2']\n","\n","Unmatched rows after join: 8054 (0.45% of logs)\n"]}],"source":["# Check join logic\n","# Does df_logs.property_ud join with df_prop.id\n","# Sample value 3436570000094511023\n","\n","import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrames\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","logs_pickle = data_dir / 'df_logs.pkl'\n","prop_pickle = data_dir / 'df_prop.pkl'\n","\n","df_logs = pd.read_pickle(logs_pickle)\n","df_prop = pd.read_pickle(prop_pickle)\n","\n","# Check unique values in each column\n","n_log_props = df_logs['property_id'].nunique()\n","n_prop_ids = df_prop['id'].nunique()\n","\n","print(f\"Unique property_ids in logs: {n_log_props}\")\n","print(f\"Unique ids in properties: {n_prop_ids}\")\n","\n","# Check which property_ids in logs don't exist in properties\n","missing_props = set(df_logs['property_id'].unique()) - set(df_prop['id'].unique())\n","\n","if missing_props:\n","    print(f\"\\nFound {len(missing_props)} property_ids in logs that don't exist in properties table\")\n","    print(\"Sample of missing ids:\", list(missing_props)[:5])\n","else:\n","    print(\"\\nAll property_ids in logs exist in properties table\")\n","\n","# Check actual join\n","merged_df = df_logs.merge(df_prop, left_on='property_id', right_on='id', how='left')\n","n_unmatched = merged_df['id'].isna().sum()\n","\n","print(f\"\\nUnmatched rows after join: {n_unmatched} ({(n_unmatched/len(df_logs))*100:.2f}% of logs)\")\n"]},{"cell_type":"code","source":["# Missing data to be dealt with\n","import psycopg2\n","import pandas as pd\n","import os\n","\n","# Load credentials\n","def load_credentials(path_to_credentials):\n","   with open(path_to_credentials, 'r') as file:\n","       for line in file:\n","           if '=' in line:\n","               key, value = line.split('=', 1)\n","               os.environ[key.strip()] = value.strip()\n","\n","# Connection parameters\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","connection_params = {\n","   'host': os.getenv('DB_HOST'),\n","   'dbname': os.getenv('DB_NAME'),\n","   'user': os.getenv('DB_USER'),\n","   'password': os.getenv('DB_PASSWORD'),\n","   'port': os.getenv('DB_PORT')\n","}\n","\n","# Connect and fetch data\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Fetch full rows where property_type is NULL\n","query = \"SELECT * FROM properties WHERE property_type IS NULL;\"\n","\n","cursor.execute(query)\n","columns = [desc[0] for desc in cursor.description]\n","missing_prop_types = pd.DataFrame(cursor.fetchall(), columns=columns)\n","\n","print(missing_prop_types.to_string())\n","\n","# Close connection\n","cursor.close()\n","connection.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrRG6PPLADrN","executionInfo":{"status":"ok","timestamp":1737251767313,"user_tz":480,"elapsed":1644,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"4e008c4f-fb90-48da-ff5c-7555310dd15d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                     id                       organization_id                                             name       phone contact            longitude            latitude  watts_soft_limit property_type  note   utility_provider gateway_type carrier_name               address_1  address_2         city state    zip email  has_editable_penalty   managed_account_id                       created_at                       updated_at reboot_cron_schedule account_id  hidden decommissioned_at exclusive_domain_1 exclusive_domain_2\n","0  522f3cfd-1a6a-4929-be9e-793642a7d1cd  e4c8d63e-b801-4d72-830a-0a5a8aad8097                    TEST - NewPropertyTestPricing                                     None                None               100          None                    LADWP         None         None                                                                                    False                      2024-10-02 04:45:34.289552+00:00 2025-01-08 21:12:29.442263+00:00                 None              False              None               None               None\n","1  f574df18-7928-4ea6-9845-09e814784053  a26a3318-8779-43ff-a8ca-ef5f694fc58f                   TEST - Chargie Test Property 2                                        0                   0               100          None                      SCE         None         None                                                                                    False                      2025-01-09 05:37:43.763938+00:00 2025-01-09 05:37:43.763938+00:00                 None               True              None               None               None\n","2  510a495d-a6e4-4518-892b-7b16674340d4  a26a3318-8779-43ff-a8ca-ef5f694fc58f                      TEST - Chargie Test Penalty  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:04:06.328713+00:00            * * * * *       None   False              None               None               None\n","3  047de1d3-0bd2-40fe-b92f-5155448ea171  69512d85-02da-42bc-b87b-85bd68df1783                          TEST - Calabasas Office                              -118.653976           34.155115           1000000          None  None                SCE         None         None  5038 Parkway Calabasas  Suite 101    Calabasas    CA  91302                       False  3436570000048718466 2024-03-06 05:23:08.827026+00:00 2024-06-12 15:30:40.209580+00:00                 None       None    True              None               None               None\n","4  50b8b410-ec8a-4656-a95c-659ef9aec749  a26a3318-8779-43ff-a8ca-ef5f694fc58f                     TEST - Chargie Test Property                               -118.52757            34.23555              1000          None        ROSEVILLEELECTRIC  CRADLEPOINT          ATT       3947 Landmark St.             Culver City    CA  90232                       False                    0 2023-05-03 05:01:09.099967+00:00 2024-09-10 23:04:55.447822+00:00                 None       None   False              None               None               None\n","5  49dead0e-f152-42ce-a47b-e2289bd1010f  a26a3318-8779-43ff-a8ca-ef5f694fc58f  TEST - Chargie Test No Transactions No Stations  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:03:34.470639+00:00            * * * * *       None   False              None               None               None\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrames\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","prop_pickle = data_dir / 'df_prop.pkl'\n","prop_types_pickle = data_dir / 'df_prop_type.pkl'\n","\n","df_prop = pd.read_pickle(prop_pickle)\n","df_prop_type = pd.read_pickle(prop_types_pickle)\n","\n","# Check for None values\n","print(\"None values in df_prop:\")\n","print(df_prop[df_prop['property_type'].isna()])\n","\n","print(\"\\nNone values in df_prop_type:\")\n","print(df_prop_type[df_prop_type['id'].isna()])\n","\n","# Count of None values\n","print(\"\\nNumber of None values in df_prop property_type:\", df_prop['property_type'].isna().sum())\n","print(\"Number of None values in df_prop_type id:\", df_prop_type['id'].isna().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"929u3DqvADtb","executionInfo":{"status":"ok","timestamp":1737251513974,"user_tz":480,"elapsed":142,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"efbba34b-cf60-458d-da86-84ef9bbdcc7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None values in df_prop:\n","                                       id  \\\n","12   522f3cfd-1a6a-4929-be9e-793642a7d1cd   \n","46   f574df18-7928-4ea6-9845-09e814784053   \n","147  510a495d-a6e4-4518-892b-7b16674340d4   \n","168  047de1d3-0bd2-40fe-b92f-5155448ea171   \n","203  50b8b410-ec8a-4656-a95c-659ef9aec749   \n","230  49dead0e-f152-42ce-a47b-e2289bd1010f   \n","\n","                          organization_id  \\\n","12   e4c8d63e-b801-4d72-830a-0a5a8aad8097   \n","46   a26a3318-8779-43ff-a8ca-ef5f694fc58f   \n","147  a26a3318-8779-43ff-a8ca-ef5f694fc58f   \n","168  69512d85-02da-42bc-b87b-85bd68df1783   \n","203  a26a3318-8779-43ff-a8ca-ef5f694fc58f   \n","230  a26a3318-8779-43ff-a8ca-ef5f694fc58f   \n","\n","                                                name       phone contact  \\\n","12                     TEST - NewPropertyTestPricing                       \n","46                    TEST - Chargie Test Property 2                       \n","147                      TEST - Chargie Test Penalty  6261234567           \n","168                          TEST - Calabasas Office                       \n","203                     TEST - Chargie Test Property                       \n","230  TEST - Chargie Test No Transactions No Stations  6261234567           \n","\n","               longitude            latitude  watts_soft_limit property_type  \\\n","12                  None                None               100          None   \n","46                     0                   0               100          None   \n","147  -118.38719434785678  34.026757491823616              1000          None   \n","168          -118.653976           34.155115           1000000          None   \n","203           -118.52757            34.23555              1000          None   \n","230  -118.38719434785678  34.026757491823616              1000          None   \n","\n","     note  ... has_editable_penalty   managed_account_id  \\\n","12         ...                False                    0   \n","46         ...                False                    0   \n","147        ...                 True            123456789   \n","168  None  ...                False  3436570000048718336   \n","203        ...                False                    0   \n","230        ...                 True            123456789   \n","\n","                          created_at                       updated_at  \\\n","12  2024-10-02 04:45:34.289552+00:00 2025-01-08 21:12:29.442263+00:00   \n","46  2025-01-09 05:37:43.763938+00:00 2025-01-09 05:37:43.763938+00:00   \n","147 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:04:06.328713+00:00   \n","168 2024-03-06 05:23:08.827026+00:00 2024-06-12 15:30:40.209580+00:00   \n","203 2023-05-03 05:01:09.099967+00:00 2024-09-10 23:04:55.447822+00:00   \n","230 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:03:34.470639+00:00   \n","\n","    reboot_cron_schedule account_id hidden decommissioned_at  \\\n","12                  None          0  False               NaT   \n","46                  None          0   True               NaT   \n","147            * * * * *          0  False               NaT   \n","168                 None          0   True               NaT   \n","203                 None          0  False               NaT   \n","230            * * * * *          0  False               NaT   \n","\n","    exclusive_domain_1  exclusive_domain_2  \n","12                None                None  \n","46                None                None  \n","147               None                None  \n","168               None                None  \n","203               None                None  \n","230               None                None  \n","\n","[6 rows x 29 columns]\n","\n","None values in df_prop_type:\n","Empty DataFrame\n","Columns: [id, name, description]\n","Index: []\n","\n","Number of None values in df_prop property_type: 6\n","Number of None values in df_prop_type id: 0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","\n","# Load the DataFrames\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP')\n","prop_pickle = data_dir / 'df_prop.pkl'\n","\n","df_prop = pd.read_pickle(prop_pickle)\n","\n","# Filter rows where property_type is None\n","none_prop_rows = df_prop[df_prop['property_type'].isna()]\n","\n","# Display all columns for these rows\n","print(none_prop_rows.to_string())\n","\n","# Or if that's too much, you can be more selective about columns\n","print(\"\\nSelected columns for rows with None property_type:\")\n","columns_to_show = [\n","    'id', 'name', 'organization_id', 'longitude', 'latitude',\n","    'created_at', 'updated_at', 'watts_soft_limit'\n","]\n","print(none_prop_rows[columns_to_show].to_string())\n","\n","# Basic statistics about these rows\n","print(\"\\nNumber of rows with None property_type:\", len(none_prop_rows))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTZY9jfuQwwP","executionInfo":{"status":"ok","timestamp":1737251622054,"user_tz":480,"elapsed":208,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"7e6dd2e0-3dcc-4b3f-8576-25b8fc588e33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                       id                       organization_id                                             name       phone contact            longitude            latitude  watts_soft_limit property_type  note   utility_provider gateway_type carrier_name               address_1  address_2         city state    zip email  has_editable_penalty   managed_account_id                       created_at                       updated_at reboot_cron_schedule  account_id  hidden decommissioned_at exclusive_domain_1 exclusive_domain_2\n","12   522f3cfd-1a6a-4929-be9e-793642a7d1cd  e4c8d63e-b801-4d72-830a-0a5a8aad8097                    TEST - NewPropertyTestPricing                                     None                None               100          None                    LADWP         None         None                                                                                    False                    0 2024-10-02 04:45:34.289552+00:00 2025-01-08 21:12:29.442263+00:00                 None           0   False               NaT               None               None\n","46   f574df18-7928-4ea6-9845-09e814784053  a26a3318-8779-43ff-a8ca-ef5f694fc58f                   TEST - Chargie Test Property 2                                        0                   0               100          None                      SCE         None         None                                                                                    False                    0 2025-01-09 05:37:43.763938+00:00 2025-01-09 05:37:43.763938+00:00                 None           0    True               NaT               None               None\n","147  510a495d-a6e4-4518-892b-7b16674340d4  a26a3318-8779-43ff-a8ca-ef5f694fc58f                      TEST - Chargie Test Penalty  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:04:06.328713+00:00            * * * * *           0   False               NaT               None               None\n","168  047de1d3-0bd2-40fe-b92f-5155448ea171  69512d85-02da-42bc-b87b-85bd68df1783                          TEST - Calabasas Office                              -118.653976           34.155115           1000000          None  None                SCE         None         None  5038 Parkway Calabasas  Suite 101    Calabasas    CA  91302                       False  3436570000048718336 2024-03-06 05:23:08.827026+00:00 2024-06-12 15:30:40.209580+00:00                 None           0    True               NaT               None               None\n","203  50b8b410-ec8a-4656-a95c-659ef9aec749  a26a3318-8779-43ff-a8ca-ef5f694fc58f                     TEST - Chargie Test Property                               -118.52757            34.23555              1000          None        ROSEVILLEELECTRIC  CRADLEPOINT          ATT       3947 Landmark St.             Culver City    CA  90232                       False                    0 2023-05-03 05:01:09.099967+00:00 2024-09-10 23:04:55.447822+00:00                 None           0   False               NaT               None               None\n","230  49dead0e-f152-42ce-a47b-e2289bd1010f  a26a3318-8779-43ff-a8ca-ef5f694fc58f  TEST - Chargie Test No Transactions No Stations  6261234567          -118.38719434785678  34.026757491823616              1000          None                      SCE    EVOCHARGE          ATT       3947 Landmark St.             Culver City    CA  90232                        True            123456789 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:03:34.470639+00:00            * * * * *           0   False               NaT               None               None\n","\n","Selected columns for rows with None property_type:\n","                                       id                                             name                       organization_id            longitude            latitude                       created_at                       updated_at  watts_soft_limit\n","12   522f3cfd-1a6a-4929-be9e-793642a7d1cd                    TEST - NewPropertyTestPricing  e4c8d63e-b801-4d72-830a-0a5a8aad8097                 None                None 2024-10-02 04:45:34.289552+00:00 2025-01-08 21:12:29.442263+00:00               100\n","46   f574df18-7928-4ea6-9845-09e814784053                   TEST - Chargie Test Property 2  a26a3318-8779-43ff-a8ca-ef5f694fc58f                    0                   0 2025-01-09 05:37:43.763938+00:00 2025-01-09 05:37:43.763938+00:00               100\n","147  510a495d-a6e4-4518-892b-7b16674340d4                      TEST - Chargie Test Penalty  a26a3318-8779-43ff-a8ca-ef5f694fc58f  -118.38719434785678  34.026757491823616 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:04:06.328713+00:00              1000\n","168  047de1d3-0bd2-40fe-b92f-5155448ea171                          TEST - Calabasas Office  69512d85-02da-42bc-b87b-85bd68df1783          -118.653976           34.155115 2024-03-06 05:23:08.827026+00:00 2024-06-12 15:30:40.209580+00:00           1000000\n","203  50b8b410-ec8a-4656-a95c-659ef9aec749                     TEST - Chargie Test Property  a26a3318-8779-43ff-a8ca-ef5f694fc58f           -118.52757            34.23555 2023-05-03 05:01:09.099967+00:00 2024-09-10 23:04:55.447822+00:00              1000\n","230  49dead0e-f152-42ce-a47b-e2289bd1010f  TEST - Chargie Test No Transactions No Stations  a26a3318-8779-43ff-a8ca-ef5f694fc58f  -118.38719434785678  34.026757491823616 2023-07-12 21:31:34.050843+00:00 2024-09-10 23:03:34.470639+00:00              1000\n","\n","Number of rows with None property_type: 6\n"]}]},{"cell_type":"code","source":["\n","\n","\n","# Check join logic\n","# Does df_logs.property_id join with df_prop.property_id\n","\n","# Check overlap between the two columns\n","total_log_ids = len(df_logs['property_id'].unique())\n","total_type_ids = len(df_prop['property_id'].unique())\n","\n","# Check for matches\n","common_ids = set(df_logs['property_id']) & set(df_prop['property_id'])\n","total_matches = len(common_ids)\n","\n","print(f\"Unique values in df_logs.property_id: {total_log_ids:,}\")\n","print(f\"Unique values in df_prop_type.property_id: {total_type_ids:,}\")\n","print(f\"Number of matching IDs: {total_matches:,}\")\n","\n","# Calculate percentages\n","match_pct_logs = (total_matches / total_log_ids) * 100\n","match_pct_type = (total_matches / total_type_ids) * 100\n","\n","print(f\"\\nPercentage of df_logs.property_id with matches: {match_pct_logs:.1f}%\")\n","print(f\"Percentage of df_prop_type.property_id with matches: {match_pct_type:.1f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"d_WEqx1z_v78","executionInfo":{"status":"error","timestamp":1737249385063,"user_tz":480,"elapsed":134,"user":{"displayName":"David E.","userId":"12500566977266345478"}},"outputId":"244b96b8-57b5-43bb-a534-0a49088a26e8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'property_id'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'property_id'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6c98d1dc79e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Check overlap between the two columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_log_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'property_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtotal_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'property_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Check for matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'property_id'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4Auysgoa7GaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PL-sgedz7Ge_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"75tm6Ar97GhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cTRkot1i7Gjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1ImAff137Gl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y_WuHiph7GoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GUrTOKDG7Gqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jni86zvuc4ym"},"outputs":[],"source":["# import property data\n","import pandas as pd\n","\n","def process_properties(file_path, output_path):\n","    # Load CSV\n","    df_prop = pd.read_csv(file_path)\n","\n","    # Clean IDs\n","    df_prop = df_prop[df_prop['id'].notna()].reset_index(drop=True)\n","\n","    # Rename id column\n","    df_prop = df_prop.rename(columns={'id': 'property_id'})\n","\n","    # Save processed data\n","    df_prop.to_pickle(output_path_3 + '.pkl')\n","\n","    return df_prop\n","\n","if __name__ == \"__main__\":\n","    input_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","    output_path_3 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop'\n","\n","    # Run workflow\n","    df_prop = process_properties(input_path_3, output_path_3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7gGYKeQIlYm"},"outputs":[],"source":["# import prop type data\n","# This is from AWS\n","\n","import pandas as pd\n","\n","def clean_record_id(record_id):\n","   \"\"\"\n","   Remove 'zcrm_' prefix from Record Id\n","   \"\"\"\n","   return str(record_id).replace('zcrm_', '') if pd.notna(record_id) else record_id\n","\n","def process_property_types(file_path, output_path):\n","   # Load CSV with explicit encoding\n","   df_types = pd.read_csv(file_path, encoding='latin-1')\n","\n","   # Rename columns\n"," #  df_prop_size = df_prop_size.rename(columns={\n","  #     'Record Id': 'Record_id_lg',\n"," #      'Record Id (Managed Account)': 'Record_id_js'\n","  # })\n","\n","  # # Clean IDs by removing 'zcrm_' prefix directly in the existing column\n"," #  df_prop_size['Record_id_js'] = df_prop_size['Record_id_js'].apply(clean_record_id)\n","\n","  # # Clean IDs\n","  # df_prop_size = df_prop_size[df_prop_size['Record_id_js'].notna()].reset_index(drop=True)\n","\n","   # Cast id to object type\n","  # df_prop_size = df_prop_size.astype({'Record_id_js': 'object'})\n","\n","   # Save processed data\n","   df_types.to_pickle(output_path + '.pkl')\n","\n","   return df_types\n","\n","if __name__ == \"__main__\":\n","   input_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","   output_path_2 = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_prop_type'\n","\n","   # Run workflow\n","   df_types = process_property_types(input_path_2, output_path_2)"]},{"cell_type":"code","source":[],"metadata":{"id":"95GD5vSH7TAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSOQ6IB3OzuL"},"outputs":[],"source":["# Normalize column naming\n","\n","df_prop_type = df_prop_type.rename(columns={'id': 'property_id'})\n","df_prop_type = df_prop_type.rename(columns={'name': 'prop_type'})\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kazsa0EcHcT"},"outputs":[],"source":["# Update log df with property metadata\n","\n","# Load the logs data from pickle\n","df_logs_exp = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","\n","def create_property_metadata(df_prop):\n","    # Select only the required columns\n","    df_metadata = df_prop[['property_id', 'property_type', 'managed_account_id']]\n","    return df_metadata\n","\n","def append_property_type_name(df_metadata, df_prop_type):\n","    df_metadata = df_metadata.merge(\n","        df_prop_type[['property_id', 'prop_type']],\n","        left_on='property_type',\n","        right_on='property_id',\n","        how='left'\n","    )\n","\n","    # Drop redundant property_id column and rename property_id_x\n","    df_metadata = df_metadata.drop(columns=['property_id_y'])\n","    df_metadata = df_metadata.rename(columns={'property_id_x': 'property_id'})\n","\n","    return df_metadata\n","\n","def append_parking_spaces(df_metadata, df_prop_size):\n","    df_metadata = df_metadata.merge(\n","        df_prop_size[['Record_id_js', 'Total Parking Space Count']],\n","        left_on='managed_account_id',\n","        right_on='Record_id_js',\n","        how='left'\n","    )\n","\n","    # Drop redundant join key and rename parking spaces column\n","    df_metadata = df_metadata.drop(columns=['Record_id_js'])\n","    df_metadata = df_metadata.rename(columns={'Total Parking Space Count': 'parking_spaces'})\n","\n","    return df_metadata\n","\n","def append_metadata_to_logs(df_logs, df_prop_metadata):\n","    \"\"\"\n","    Append property metadata to logs using property_id as join key.\n","    Left join preserves all logs records.\n","    \"\"\"\n","    df_logs_enriched = df_logs.merge(\n","        df_prop_metadata,\n","        on='property_id',\n","        how='left'\n","    )\n","\n","    return df_logs_enriched\n","\n","def process_and_save_enriched_logs(df_prop, df_prop_type, df_prop_size, df_logs, output_path):\n","    \"\"\"Process metadata and logs workflow and save to pickle.\"\"\"\n","\n","    # Ingest df\n","    df_logs_exp.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_exp.pkl')\n","\n","    # Create metadata table\n","    df_prop_metadata = create_property_metadata(df_prop)\n","\n","    # Append property type name\n","    df_prop_metadata = append_property_type_name(df_prop_metadata, df_prop_type)\n","\n","    # Append parking spaces\n","    df_prop_metadata = append_parking_spaces(df_prop_metadata, df_prop_size)\n","\n","    # Append metadata to logs\n","    df_logs_enriched = append_metadata_to_logs(df_logs, df_prop_metadata)\n","\n","    # Save enriched logs to pickle\n","    df_logs_enriched.to_pickle(output_path + '.pkl')\n","\n","    return df_logs_enriched\n","\n","if __name__ == \"__main__\":\n","    # Define output path\n","    enriched_logs_output = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched'\n","\n","    # Run workflow and save enriched logs\n","    df_logs_enriched = process_and_save_enriched_logs(\n","        df_prop,\n","        df_prop_type,\n","        df_prop_size,\n","        df_logs_exp,\n","        enriched_logs_output\n","    )\n","\n","    # Print final shapes to verify\n","    print(\"Original logs shape:\", df_logs.shape)\n","    print(\"Enriched logs shape:\", df_logs_enriched.shape)\n","    print(\"\\nNew columns added from metadata:\", sorted(set(df_logs_enriched.columns) - set(df_logs.columns)))\n","\n","df_logs_enriched.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xTeoi_jSVTw"},"outputs":[],"source":["# Remove rows with Null data and convert timestamp to UTC\n","import numpy as np\n","\n","df_logs_enriched = df_logs_enriched.replace([np.inf, -np.inf], np.nan)\n","df_logs_enriched = df_logs_enriched.dropna()\n","df_logs_enriched = df_logs_enriched[['property_id','prop_type','parking_spaces','user_id','transaction_id','timestamp','unit','value']]\n","\n","# Convert timestamp to UTC\n","df_logs_enriched['timestamp'] = pd.to_datetime(df_logs_enriched['timestamp'], format='ISO8601', utc=True)\n","\n","# Save the cleaned dataframe\n","df_logs_enriched.to_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl')\n","\n","# Flatten, encode\n","# One-hot encode 'prop_type' with 1/0 values\n","df_logs_enriched = pd.get_dummies(df_logs_enriched, columns=['prop_type'], prefix='prop_type', dtype=int)\n","\n","# Binary encode unit (0 for 'A', 1 for 'W') - being explicit about handling NaN\n","df_logs_enriched['unit_encoded'] = df_logs_enriched['unit'].map({'A': 0, 'W': 1}).fillna(0).astype(int)\n","df_logs_enriched = df_logs_enriched.drop('unit', axis=1)\n","\n","\n","# Save processed data\n","output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched'\n","\n","df_logs_enriched.to_pickle(output_path + '.pkl')"]},{"cell_type":"markdown","metadata":{"id":"UNc4o_n59G_G"},"source":["## Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5fnXoeF70NJ"},"outputs":[],"source":["# Check unique timestamp formats\n","print(\"Sample of unique timestamp formats:\")\n","print(df_logs_enriched['timestamp'].drop_duplicates().head(10))\n","\n","# Group by transaction_id and get min/max timestamps\n","time_spans = df_logs_enriched.groupby('transaction_id').agg({\n","   'timestamp': ['min', 'max']\n","}).reset_index()\n","\n","# Calculate duration for each transaction\n","time_spans['duration'] = time_spans['timestamp']['max'] - time_spans['timestamp']['min']\n","\n","# Calculate different time spans\n","spans = {\n","   'different_days': time_spans['duration'] >= pd.Timedelta(days=1),\n","   'different_hours': time_spans['duration'] >= pd.Timedelta(hours=1),\n","   'different_minutes': time_spans['duration'] >= pd.Timedelta(minutes=1),\n","   'different_seconds': time_spans['duration'] >= pd.Timedelta(seconds=1),\n","   'different_milliseconds': time_spans['duration'] >= pd.Timedelta(milliseconds=1)\n","}\n","\n","# Count transactions in each category\n","for span_type, mask in spans.items():\n","   count = mask.sum()\n","   percent = (count / len(time_spans)) * 100\n","   print(f\"Transactions spanning {span_type}: {count:,} ({percent:.2f}%)\")\n","\n","print(\"\\nDuration statistics:\")\n","print(time_spans['duration'].describe())\n","\n","# Additional context\n","print(\"\\nTotal transactions analyzed:\", len(time_spans))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcZxp8ni89AZ"},"outputs":[],"source":["# Compare usae data across property types\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# First ensure timestamps are datetime\n","df_logs_enriched['timestamp'] = pd.to_datetime(df_logs_enriched['timestamp'], format='ISO8601')\n","\n","# Create time_spans from df_logs_enriched\n","time_spans = df_logs_enriched.groupby('transaction_id').agg({\n","    'timestamp': ['min', 'max'],\n","    'property_id': 'first'  # keep property_id for merging\n","}).reset_index()\n","\n","time_spans.columns = ['transaction_id', 'min_time', 'max_time', 'property_id']\n","time_spans['duration'] = time_spans['max_time'] - time_spans['min_time']\n","time_spans['duration_hours'] = time_spans['duration'].dt.total_seconds() / 3600\n","\n","# Merge with property types from df_logs_enriched\n","prop_type_cols = [col for col in df_logs_enriched.columns if col.startswith('prop_type_')]\n","durations_with_type = time_spans.merge(\n","    df_logs_enriched[['property_id'] + prop_type_cols].drop_duplicates(),\n","    on='property_id'\n",")\n","\n","# Create subplot for each property type\n","n_types = len(prop_type_cols)\n","n_cols = 3\n","n_rows = (n_types + n_cols - 1) // n_cols\n","\n","plt.figure(figsize=(15, 4*n_rows))\n","\n","for idx, prop_type in enumerate(prop_type_cols, 1):\n","    plt.subplot(n_rows, n_cols, idx)\n","\n","    # Get durations for this property type\n","    type_durations = durations_with_type[durations_with_type[prop_type] == 1]['duration_hours']\n","    type_durations = type_durations[type_durations > 0]  # Filter positive durations\n","\n","    if len(type_durations) > 0:  # Only plot if we have data\n","        plt.hist(type_durations,\n","                bins=np.logspace(np.log10(type_durations.min()),\n","                               np.log10(type_durations.max()),\n","                               50))\n","        plt.xscale('log')\n","        plt.yscale('log')\n","\n","        plt.title(f'{prop_type.replace(\"prop_type_\", \"\")} (n={len(type_durations):,})')\n","        plt.xlabel('Duration (hours) - Log Scale')\n","        plt.ylabel('Number of Sessions - Log Scale')\n","        plt.grid(True)\n","\n","        # Add statistics\n","        plt.text(0.02, 0.95,\n","                f'Mean: {type_durations.mean():.1f}h\\n'\n","                f'Median: {type_durations.median():.1f}h',\n","                transform=plt.gca().transAxes,\n","                bbox=dict(facecolor='white', alpha=0.8))\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary statistics by property type\n","print(\"\\nSummary statistics by property type:\")\n","for prop_type in prop_type_cols:\n","    type_durations = durations_with_type[durations_with_type[prop_type] == 1]['duration_hours']\n","    type_durations = type_durations[type_durations > 0]\n","    if len(type_durations) > 0:\n","        print(f\"\\n{prop_type.replace('prop_type_', '')}:\")\n","        print(type_durations.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeuNqAdJ59_z"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Histogram of timestamps per transaction_id\n","timestamps_per_transaction = df_logs_enriched.groupby('transaction_id')['timestamp'].nunique()\n","\n","plt.figure(figsize=(12, 5))\n","\n","# Left plot - log scale\n","plt.subplot(1, 2, 1)\n","plt.hist(timestamps_per_transaction, bins=np.logspace(np.log10(1), np.log10(timestamps_per_transaction.max()), 50))\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.title('Unique Timestamps per Transaction ID (Log Scale)')\n","plt.xlabel('Number of Unique Timestamps')\n","plt.ylabel('Count of Transaction IDs')\n","plt.grid(True)\n","\n","# Right plot - properties per transaction\n","properties_per_transaction = df_logs_enriched.groupby('transaction_id')['property_id'].nunique()\n","plt.subplot(1, 2, 2)\n","plt.hist(properties_per_transaction, bins=50)\n","plt.title('Unique Properties per Transaction ID')\n","plt.xlabel('Number of Unique Properties')\n","plt.ylabel('Count of Transaction IDs')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nTimestamps per transaction summary:\")\n","print(timestamps_per_transaction.describe())\n","print(\"\\nProperties per transaction summary:\")\n","print(properties_per_transaction.describe())\n","\n","# Print additional context\n","print(\"\\nTotal number of unique transactions:\", len(timestamps_per_transaction))\n","print(\"Total number of unique properties:\", df_logs_enriched['property_id'].nunique())\n","print(\"Total number of timestamps:\", df_logs_enriched['timestamp'].nunique())"]},{"cell_type":"markdown","metadata":{"id":"uGWTmO7jkzys"},"source":["## Engineer Features"]},{"cell_type":"code","source":["import pandas as pd\n","import pytz\n","import numpy as np\n","from datetime import datetime\n","\n","def convert_to_pst_components(df):\n","    \"\"\"Convert timestamp to PST and extract components.\"\"\"\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')\n","    df['timestamp'] = df['timestamp'].dt.tz_convert('US/Pacific')\n","\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","    df['minute'] = df['timestamp'].dt.minute\n","\n","    return df\n","\n","def add_day_info(df):\n","    \"\"\"Add day of week and weekend indicator.\"\"\"\n","    df['day_of_week'] = df['timestamp'].dt.dayofweek + 1\n","    df['day_weekend'] = (df['day_of_week'] >= 6).astype(int)\n","    return df\n","\n","def get_nearest_holiday(df):\n","    \"\"\"Calculate proximity to major US holidays.\"\"\"\n","    major_holidays = {\n","        '2024-01-01': \"New Year's Day\",\n","        '2024-01-15': \"Martin Luther King Jr. Day\",\n","        '2024-02-19': \"Presidents Day\",\n","        '2024-05-27': \"Memorial Day\",\n","        '2024-07-04': \"Independence Day\",\n","        '2024-09-02': \"Labor Day\",\n","        '2024-11-28': \"Thanksgiving\",\n","        '2024-12-25': \"Christmas\",\n","        '2025-01-01': \"New Year's Day\",\n","        '2025-01-20': \"Martin Luther King Jr. Day\",\n","        '2025-02-17': \"Presidents Day\",\n","        '2025-05-26': \"Memorial Day\",\n","        '2025-07-04': \"Independence Day\",\n","        '2025-09-01': \"Labor Day\",\n","        '2025-11-27': \"Thanksgiving\",\n","        '2025-12-25': \"Christmas\"\n","    }\n","\n","    holiday_dates = pd.to_datetime(list(major_holidays.keys())).sort_values()\n","    holiday_dates_array = holiday_dates.values\n","    dates_array = pd.to_datetime(df['timestamp'].dt.date.unique()).values\n","    holiday_lookup = {}\n","\n","    for date in dates_array:\n","        days_diff = np.abs((holiday_dates_array - date).astype('timedelta64[D]').astype(int))\n","        closest_idx = np.argmin(days_diff)\n","        closest_date = holiday_dates[closest_idx]\n","\n","        holiday_lookup[pd.Timestamp(date).date()] = {\n","            'days_to_nearest_holiday': days_diff[closest_idx],\n","            'nearest_holiday_date': closest_date,\n","            'nearest_holiday_name': major_holidays[closest_date.strftime('%Y-%m-%d')]\n","        }\n","\n","    # Create and merge holiday information\n","    df['date'] = df['timestamp'].dt.date\n","    result = pd.DataFrame.from_dict(holiday_lookup, orient='index')\n","    result.index = pd.to_datetime(result.index).date\n","    df = df.merge(result, left_on='date', right_index=True)\n","    df = df.drop('date', axis=1)\n","\n","    return df\n","\n","def process_timestamps(input_path, output_path):\n","    \"\"\"Main function to process all timestamp-related features.\"\"\"\n","    df = pd.read_pickle(input_path)\n","    df = convert_to_pst_components(df)\n","    df = add_day_info(df)\n","    df = get_nearest_holiday(df)\n","    df.to_pickle(output_path)\n","    return df\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","\n","    df_processed = process_timestamps(input_path, output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"th5Y2zwzXBm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Add user and usage data\n","import pandas as pd\n","\n","def add_unique_user_counts(df, group_cols, user_col):\n","    \"\"\"Count unique users per group.\"\"\"\n","    user_counts = df.groupby(group_cols)[user_col].nunique().reset_index()\n","    user_counts.rename(columns={user_col: 'unique_user_count'}, inplace=True)\n","    df = df.merge(user_counts, on=group_cols, how='left')\n","    return df\n","\n","def add_usage_sums(df, group_cols):\n","    \"\"\"Add sums of values for each unit type by group.\"\"\"\n","    # Calculate sums for each unit type\n","    sums = df.groupby([*group_cols, 'unit_encoded'])['value'].sum().reset_index()\n","\n","    # Pivot to create separate columns for A and W\n","    sums = sums.pivot(\n","        index=group_cols,\n","        columns='unit_encoded',\n","        values='value'\n","    ).reset_index()\n","\n","    # Rename columns\n","    sums.rename(\n","        columns={\n","            0: 'sum_value_A',   # Amps were encoded as 0\n","            1: 'sum_value_Wh'   # Watts were encoded as 1\n","        },\n","        inplace=True\n","    )\n","\n","    return df.merge(sums, on=group_cols, how='left')\n","\n","if __name__ == \"__main__\":\n","    input_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_holidays.pkl'\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl'\n","\n","    # Read data\n","    df = pd.read_pickle(input_path)\n","\n","    # Extract datetime components\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    df['year'] = df['timestamp'].dt.year\n","    df['month'] = df['timestamp'].dt.month\n","    df['day'] = df['timestamp'].dt.day\n","    df['hour'] = df['timestamp'].dt.hour\n","\n","    # Define grouping columns after datetime components are created\n","    group_cols = ['property_id', 'year', 'month', 'day', 'hour']\n","\n","    # Add user and usage metrics\n","    df = add_unique_user_counts(df, group_cols, 'user_id')\n","    df = add_usage_sums(df, group_cols)\n","\n","    # Save results\n","    df.to_pickle(output_path)\n","    print(f\"Processing complete. File saved to: {output_path}\")"],"metadata":{"id":"5d2mRKE6U2VA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')"],"metadata":{"id":"l3w1u1r0nADm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"kT15u3G6nWmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWLnoqvaKEB6"},"outputs":[],"source":["# Halt\n","sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"69Y399-MUhvx"},"source":["## Check for colinearity"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric, handling errors\n","columns_to_analyze = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Create a clean dataframe for analysis\n","df_clean = df[columns_to_analyze].copy()\n","\n","# Convert each column to numeric, handling errors\n","for col in columns_to_analyze:\n","    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n","\n","    # Print info about conversion\n","    print(f\"\\nColumn: {col}\")\n","    print(f\"Null values after conversion: {df_clean[col].isnull().sum()}\")\n","    print(f\"Sample unique values: {df_clean[col].dropna().sample(5).tolist()}\")\n","\n","# Calculate correlations for cleaned numeric columns\n","correlations = df_clean.corr()\n","\n","# Create correlation heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","# Basic summary statistics\n","print(\"\\nSummary Statistics:\")\n","print(df_clean.describe())\n","\n","# Check for patterns across categorical variables\n","print(\"\\nMean energy consumption (sum_value_Wh) by:\")\n","print(\"\\nDay of Week:\")\n","print(df_clean.groupby('day_of_week')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nHour of Day:\")\n","print(df_clean.groupby('hour')['sum_value_Wh'].mean().sort_values(ascending=False))\n","\n","print(\"\\nMonth:\")\n","print(df_clean.groupby('month')['sum_value_Wh'].mean().sort_values(ascending=False))"],"metadata":{"id":"UsCbfrlAE1-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load and clean data\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert columns to numeric\n","df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Group by hour and calculate various statistics\n","hourly_stats = df.groupby('hour').agg({\n","    'sum_value_Wh': ['mean', 'median', 'std', 'count'],\n","    'unique_user_count': 'mean'\n","}).round(2)\n","\n","# Create a figure with two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n","\n","# Plot 1: Energy consumption pattern\n","ax1.plot(hourly_stats[('sum_value_Wh', 'mean')], marker='o')\n","ax1.fill_between(range(24),\n","                 hourly_stats[('sum_value_Wh', 'mean')] - hourly_stats[('sum_value_Wh', 'std')],\n","                 hourly_stats[('sum_value_Wh', 'mean')] + hourly_stats[('sum_value_Wh', 'std')],\n","                 alpha=0.2)\n","ax1.set_title('24-Hour Energy Consumption Pattern')\n","ax1.set_xlabel('Hour of Day')\n","ax1.set_ylabel('Average Energy Consumption (Wh)')\n","ax1.grid(True)\n","\n","# Plot 2: Users vs Energy\n","ax2.scatter(hourly_stats[('unique_user_count', 'mean')],\n","           hourly_stats[('sum_value_Wh', 'mean')],\n","           alpha=0.6)\n","# Add hour labels to each point\n","for i in range(24):\n","    ax2.annotate(str(i),\n","                (hourly_stats[('unique_user_count', 'mean')][i],\n","                 hourly_stats[('sum_value_Wh', 'mean')][i]))\n","ax2.set_title('Users vs Energy Consumption by Hour')\n","ax2.set_xlabel('Average Number of Users')\n","ax2.set_ylabel('Average Energy Consumption (Wh)')\n","ax2.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print detailed statistics for peak hours\n","peak_hours = hourly_stats.nlargest(5, ('sum_value_Wh', 'mean'))\n","print(\"\\nPeak Hours Analysis:\")\n","print(peak_hours)\n","\n","# Calculate relative increase during peak vs off-peak\n","off_peak_mean = hourly_stats.loc[0:6, ('sum_value_Wh', 'mean')].mean()\n","peak_mean = hourly_stats.loc[16:19, ('sum_value_Wh', 'mean')].mean()\n","increase_factor = peak_mean / off_peak_mean\n","\n","print(f\"\\nPeak vs Off-peak Analysis:\")\n","print(f\"Average off-peak consumption (midnight-6am): {off_peak_mean:.2f} Wh\")\n","print(f\"Average peak consumption (4pm-7pm): {peak_mean:.2f} Wh\")\n","print(f\"Peak hours consume {increase_factor:.1f}x more energy than off-peak hours\")"],"metadata":{"id":"kuoFSz6pGHoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert sum_value_Wh to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","\n","# Create a single property type column for easier plotting\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Create box plot\n","plt.figure(figsize=(15, 8))\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df)\n","plt.xticks(rotation=45, ha='right')\n","plt.title('Energy Consumption Distribution by Property Type')\n","plt.xlabel('Property Type')\n","plt.ylabel('Energy Consumption (Wh)')\n","plt.tight_layout()\n","plt.show()\n","\n","# Print basic stats for each property type\n","print(\"\\nBasic statistics by property type:\")\n","stats = df.groupby('property_type')['sum_value_Wh'].describe()\n","print(stats)\n","\n","# Count number of observations for each property type\n","print(\"\\nNumber of observations per property type:\")\n","counts = df['property_type'].value_counts()\n","print(counts)"],"metadata":{"id":"2-BGTCfAGHwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Convert to numeric\n","df['sum_value_Wh'] = pd.to_numeric(df['sum_value_Wh'], errors='coerce')\n","df['unique_user_count'] = pd.to_numeric(df['unique_user_count'], errors='coerce')\n","\n","# Create a single property type column\n","prop_type_cols = [col for col in df.columns if col.startswith('prop_type_')]\n","df['property_type'] = np.nan\n","\n","for col in prop_type_cols:\n","    mask = df[col] == 1\n","    df.loc[mask, 'property_type'] = col.replace('prop_type_', '')\n","\n","# Calculate energy per user\n","df['energy_per_user'] = df['sum_value_Wh'] / df['unique_user_count']\n","\n","# Create two subplots\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n","\n","# Plot 1: Total Energy\n","sns.boxplot(x='property_type', y='sum_value_Wh', data=df, ax=ax1)\n","ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n","ax1.set_title('Total Energy Consumption by Property Type')\n","ax1.set_xlabel('Property Type')\n","ax1.set_ylabel('Energy Consumption (Wh)')\n","\n","# Plot 2: Energy per User\n","sns.boxplot(x='property_type', y='energy_per_user', data=df, ax=ax2)\n","ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n","ax2.set_title('Energy Consumption per User by Property Type')\n","ax2.set_xlabel('Property Type')\n","ax2.set_ylabel('Energy Consumption per User (Wh/user)')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print statistics\n","print(\"\\nStatistics by property type:\")\n","stats = df.groupby('property_type').agg({\n","    'sum_value_Wh': ['count', 'mean'],\n","    'unique_user_count': 'mean',\n","    'energy_per_user': 'mean'\n","}).round(2)\n","\n","print(stats)"],"metadata":{"id":"ZLVLTbCcGH2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First, let's look at the raw data before any processing\n","print(\"Original data counts by property type:\")\n","for col in prop_type_cols:\n","    print(f\"\\n{col}:\")\n","    print(f\"Number of 1s: {df[col].sum()}\")\n","    print(f\"Number of non-null values: {df[col].count()}\")\n","\n","# Let's also check for nulls in key columns\n","print(\"\\nNull values in key columns:\")\n","print(df[['sum_value_Wh', 'unique_user_count']].isnull().sum())\n","\n","# Let's look at the data before any type conversion\n","print(\"\\nSample of raw data before conversion:\")\n","sample_data = df[['property_type', 'sum_value_Wh', 'unique_user_count']].head(10)\n","print(sample_data)\n","\n","# Check data types of key columns\n","print(\"\\nData types of columns:\")\n","print(df.dtypes)"],"metadata":{"id":"Xt5KFkm2GH92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8ZOVtYORGH_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZHlAIGuNGICQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THQrGWLwUQrW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Load your dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","def clean_numeric_string(value):\n","    \"\"\"Clean numeric string by taking the first valid number.\"\"\"\n","    if pd.isna(value):\n","        return 0\n","    # Convert to string if not already\n","    value = str(value)\n","    # Find first number (integer or decimal)\n","    parts = value.split('.')\n","    if not parts:\n","        return 0\n","    try:\n","        # Take first valid number\n","        return float(parts[0])\n","    except ValueError:\n","        return 0\n","\n","# Clean and convert sum_value columns\n","df['sum_value_A'] = df['sum_value_A'].apply(clean_numeric_string).astype(int)\n","df['sum_value_Wh'] = df['sum_value_Wh'].apply(clean_numeric_string).astype(int)\n","df['unique_user_count'] = df['unique_user_count'].astype(int)\n","\n","# Select only numerical features for VIF calculation\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"day_weekend\",\n","    \"days_to_nearest_holiday\",\n","    \"year\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Prepare the DataFrame for VIF calculation\n","X = df[numerical_columns].copy()\n","\n","# Check for NaN and inf values\n","print(f\"NaN values before VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values before VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Handle NaN and inf values\n","X = X.fillna(0)  # Replace NaN values with 0 or other strategy\n","X.replace([np.inf, -np.inf], 0, inplace=True)\n","\n","# Check again after handling NaN and inf values\n","print(f\"NaN values after VIF calculation: {X.isna().sum().sum()}\")\n","print(f\"Inf values after VIF calculation: {((X == np.inf) | (X == -np.inf)).sum().sum()}\")\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Display the VIF values\n","print(vif_data)"]},{"cell_type":"code","source":["# Drop suspect features and run VIF again\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Drop suspect features\n","df = df.drop(['day_weekend', 'year'], axis=1)\n","\n","# Define numerical columns\n","numerical_columns = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\",\n","    \"sum_value_A\",\n","    \"sum_value_Wh\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[numerical_columns].copy()\n","for column in numerical_columns:\n","    # Convert to numeric, coerce any non-numeric values to NaN\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","\n","    # Fill NaN values with the median of the column\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Add a constant column for intercept\n","X['intercept'] = 1\n","\n","# Calculate VIF for each feature\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","# Drop the constant column after VIF calculation\n","vif_data = vif_data[vif_data[\"Feature\"] != \"intercept\"]\n","\n","# Sort VIF values in descending order\n","vif_data = vif_data.sort_values('VIF', ascending=False)\n","\n","# Display the VIF values\n","print(\"\\nVariance Inflation Factors:\")\n","print(vif_data)"],"metadata":{"id":"oMvFjeGOqEtc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3XDub4aqdrX"},"source":["# Prep df for regression/ANOVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTFhCBuk3VRU"},"outputs":[],"source":["import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/df_logs_enriched_with_usage.pkl')\n","\n","# Define the dependent variable (y) - in this case let's use sum_value_Wh as our target\n","y = \"sum_value_Wh\"\n","\n","# Define independent variables (features to compare groups)\n","independent_vars = [\n","    \"day_of_week\",\n","    \"days_to_nearest_holiday\",\n","    \"month\",\n","    \"day\",\n","    \"hour\",\n","    \"unique_user_count\"\n","    #\"sum_value_A\"\n","]\n","\n","# Convert columns to numeric type and handle any non-numeric values\n","X = df[independent_vars].copy()\n","for column in independent_vars:\n","    X[column] = pd.to_numeric(X[column], errors='coerce')\n","    X[column] = X[column].fillna(X[column].median())\n","\n","# Convert y to numeric\n","df[y] = pd.to_numeric(df[y], errors='coerce')\n","df[y] = df[y].fillna(df[y].median())\n","\n","# Function to run one-way ANOVA for each independent variable against y\n","def run_anova_with_target(df, independent_vars, y):\n","    results = []\n","\n","    for var in independent_vars:\n","        # Create groups based on the independent variable\n","        groups = []\n","        # Create 5 groups using quantiles for continuous variables\n","        df['group'] = pd.qcut(df[var], q=5, labels=['G1', 'G2', 'G3', 'G4', 'G5'])\n","\n","        # Get the y values for each group\n","        for group in df['group'].unique():\n","            groups.append(df[df['group'] == group][y].values)\n","\n","        # Perform one-way ANOVA\n","        f_stat, p_val = stats.f_oneway(*groups)\n","\n","        results.append({\n","            'Independent Variable': var,\n","            'F-statistic': f_stat,\n","            'p-value': p_val\n","        })\n","\n","    return pd.DataFrame(results)\n","\n","# Run ANOVA\n","anova_results = run_anova_with_target(df, independent_vars, y)\n","\n","# Sort results by p-value\n","anova_results_sorted = anova_results.sort_values('p-value')\n","\n","# Display results\n","pd.set_option('display.float_format', lambda x: '{:.10f}'.format(x) if x < 0.0001 else '{:.4f}'.format(x))\n","print(f\"\\nOne-way ANOVA Results (dependent variable: {y}):\")\n","print(anova_results_sorted)\n","\n","# Add significance indicators\n","anova_results_sorted['Significance'] = ['***' if p < 0.001\n","                                      else '**' if p < 0.01\n","                                      else '*' if p < 0.05\n","                                      else 'ns' for p in anova_results_sorted['p-value']]\n","\n","print(\"\\nSignificance levels:\")\n","print(\"***: p < 0.001\")\n","print(\"**: p < 0.01\")\n","print(\"*: p < 0.05\")\n","print(\"ns: not significant\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpSYEsmq3VUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umUOsBSn3VXA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-e5kpAZ3VZS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F20AGhU3Vbq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1F9VbEO3VhO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkEIez9-o-Wr"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_sampled['unique_user_count'], df_sampled['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_sampled['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_sampled['property_id'] = df_sampled['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_sampled.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_sampled).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJU6o-8Jo-ZB"},"outputs":[],"source":["# Calculate the IQR for the column with potential outliers\n","Q1 = df_sampled['hour_sum_value_A'].quantile(0.25)\n","Q3 = df_sampled['hour_sum_value_A'].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Filter out the outliers\n","df_filtered = df_sampled[(df_sampled['hour_sum_value_A'] >= lower_bound) & (df_sampled['hour_sum_value_A'] <= upper_bound)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uft9sM-JFrIP"},"outputs":[],"source":["## Create a property lookup\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table};\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"location\",\n","    \"properties\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/{table}_table_extract.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avvahCDuo-bP"},"outputs":[],"source":["import numpy as np\n","# Fit a regression model\n","X = sm.add_constant(df_sampled['unique_user_count'])  # Add intercept\n","model = sm.OLS(df_sampled['hour_sum_value_A'], X).fit()\n","\n","# Predict values for regression line\n","predictions = model.predict(X)\n","\n","# Plot scatter with regression line\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df_filtered['unique_user_count'], df_filtered['hour_sum_value_A'], alpha=0.6, label='Data Points')\n","plt.plot(df_filtered['unique_user_count'], predictions, color='red', label='Regression Line')\n","plt.title('Regression Plot: Unique User Count vs Hour Sum Value 0')\n","plt.xlabel('Unique User Count')\n","plt.ylabel('Hour hour_sum_value_A')\n","plt.legend()\n","plt.show()\n","\n","\n","# Ensure property_id is treated as a categorical variable\n","df_filtered['property_id'] = df_filtered['property_id'].astype('category')\n","\n","# Prepare the formula for ANOVA\n","independent_vars = ['unique_user_count', 'property_id'] + [col for col in df_filtered.columns if col.startswith('day_')]\n","formula = 'hour_sum_value_A ~ ' + ' + '.join(independent_vars)\n","\n","# Fit the model\n","model = ols(formula, data=df_filtered).fit()\n","\n","# Perform ANOVA\n","anova_results = sm.stats.anova_lm(model, typ=2)\n","\n","# Display the ANOVA results\n","print(anova_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EAhWIUhxi0B"},"outputs":[],"source":["# Decorate data with engineered values\n","\n","from datetime import datetime\n","import pytz\n","\n","# Function to convert to PST and extract datetime\n","def convert_to_pst_as_datetime(timestamp):\n","    # Parse the UTC timestamp\n","    utc_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n","    # Set timezone to UTC\n","    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n","    # Convert to PST\n","    pst_time = utc_time.astimezone(pytz.timezone('US/Pacific'))\n","    # Truncate to day, month, year, and hour (zero minutes and seconds)\n","    return pst_time.replace(minute=0, second=0, microsecond=0)\n","\n","# Apply the function to convert timestamp\n","df_a_s_o['time_sample'] = df_a_s_o['timestamp'].apply(convert_to_pst_as_datetime)\n","\n","# Add a column for day of the week (0 = Monday, 6 = Sunday)\n","df_a_s_o['day_of_week'] = df_a_s_o['time_sample'].dt.dayofweek\n","\n","# Add a column for hour of the day (24hr format)\n","df_a_s_o['hour_of_day'] = df_a_s_o['time_sample'].dt.hour\n","\n","# Add a column for ISO week number\n","df_a_s_o['week_number'] = df_a_s_o['time_sample'].dt.isocalendar().week\n","\n","# Add in count of unique users\n","df_a_s_o['unique_user_count'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['user_id']\n","    .transform('nunique')\n",")\n","\n","# Add in sum of unit_a\n","df_a_s_o['sum_of_unit_a'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_a']\n","    .transform('sum')\n",")\n","\n","# Add in sum of watt_h\n","df_a_s_o['sum_of_unit_wh'] = (\n","    df_a_s_o\n","    .groupby(['week_number', 'day_of_week', 'hour_of_day'])['unit_wh']\n","    .transform('sum')\n",")\n","\n","# Print the updated DataFrame\n","print(df_a_s_o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6HV07FXtTr"},"outputs":[],"source":["# Data Check\n","print(df_a_s_o['week_number'].unique())\n","\n","\n","# Calculate the overall count of unique user IDs\n","unique_user_count = df_a_s_o['user_id'].nunique()\n","\n","# Calculate the sum of unit_a\n","sum_of_unit_a = df_a_s_o['unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = df_a_s_o['unit_wh'].sum()\n","\n","# Print the results\n","print(f\"Unique User Count: {unique_user_count}\")\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUNjyr1Nxi4u"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vaGBSdjg0_I"},"outputs":[],"source":["# Reduce the DataFrame to unique rows based on the specified columns\n","reduced_df = df_a_s_o.drop_duplicates(\n","    subset=['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']\n",")\n","\n","# Keep only the specified columns\n","reduced_df = reduced_df[['day_of_week', 'hour_of_day', 'week_number', 'unique_user_count', 'sum_of_unit_a', 'sum_of_unit_wh']]\n","\n","# Display the resulting DataFrame\n","print(reduced_df.info())\n","print(reduced_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBxpWJMKg4z7"},"outputs":[],"source":["\n","# Calculate the sum of unit_a\n","sum_of_unit_a = reduced_df['sum_of_unit_a'].sum()\n","\n","# Calculate the sum of watt_h\n","sum_of_unit_wh = reduced_df['sum_of_unit_wh'].sum()\n","\n","# Print the results\n","\n","print(f\"Sum of unit_a: {sum_of_unit_a}\")\n","print(f\"Sum of unit_wh: {sum_of_unit_wh}\")\n","\n","# Unique User Count: 1028\n","# Sum of unit_a: 84714332.39000002\n","# Sum of unit_wh: 57182938816884.78\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7oaFCsfLjT1"},"outputs":[],"source":["# Write a local file to take a look\n","\n","df_a_s_o.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/df_a_s_o.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bM80beG-xi9j"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.regplot(x='unique_user_count', y='sum_of_unit_wh', data=df_a_s_o, scatter_kws={'alpha': 0.3}, line_kws={'color': 'red'})\n","plt.xlabel('User unique_user_count Count')\n","plt.ylabel('Total Unit WH')\n","plt.title('Regression Plot: User ID Count vs. Total Unit WH')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F__BqafGHzpU"},"outputs":[],"source":["df_a_s_o.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIVH6Ob5LlXb"},"outputs":[],"source":["# Data is week 32 through week 44 (12)\n","# So below, there is no week lag1 value for week 32 because it is the first\n","\n","# Identify the peak total_unit_wh for each week\n","peak_weekly_data = df.loc[df.groupby('week_number')['sum_of_unit_wh'].idxmax()]\n","\n","# Sort by week number to ensure correct lagging\n","peak_weekly_data = peak_weekly_data.sort_values('week_number')\n","\n","# Add only lag_1 features\n","peak_weekly_data['lag_1_day_of_week'] = peak_weekly_data['day_of_week'].shift(1)\n","peak_weekly_data['lag_1_hour'] = peak_weekly_data['hour_of_day'].shift(1)\n","\n","# Drop rows with insufficient lag (week 1)\n","peak_weekly_data = peak_weekly_data.dropna()\n","\n","# Retain only relevant columns\n","peak_weekly_data = peak_weekly_data[['week_number', 'day_of_week', 'hour_of_day', 'lag_1_day_of_week', 'lag_1_hour']]\n","\n","print(\"Updated DataFrame:\")\n","print(peak_weekly_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoEouHpMLlce"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Features (lagged day of week and hour) and target (day of week)\n","X = peak_weekly_data[['lag_1_day_of_week', 'lag_1_hour']]\n","y = peak_weekly_data['day_of_week']  # Target: Day of the week\n","\n","# Train-test split (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Random Forest classifier\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Day of Week Prediction Accuracy:\", accuracy)\n","\n","# Display true vs predicted values\n","results = pd.DataFrame({'True Day': y_test, 'Predicted Day': y_pred})\n","print(\"\\nTrue vs Predicted Days of the Week:\")\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozIzbbVKLlew"},"outputs":[],"source":["\n","\n","# Feature importance for day_of_week classification\n","clf_importances = clf.feature_importances_\n","plt.barh(X.columns, clf_importances)\n","plt.title(\"Feature Importance for Day of Week Prediction\")\n","plt.show()\n","\n","# Feature importance for hour regression\n","reg_importances = reg.feature_importances_\n","plt.barh(X.columns, reg_importances)\n","plt.title(\"Feature Importance for Hour Prediction\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"J89LPzgmdzuZ"},"source":["### Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"uz7Y_6f1MZM0"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"xvFZX6JKKyHa"},"source":["####AWS Tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUKjVft2kxIk"},"outputs":[],"source":["\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Execute a query to fetch all table names\n","    query = \"\"\"\n","    SELECT table_name\n","    FROM information_schema.tables\n","    WHERE table_schema = 'public';\n","    \"\"\"\n","\n","    cursor.execute(query)\n","    tables = cursor.fetchall()\n","\n","    # Print the table names\n","    for table in tables:\n","        print(table[0])\n","\n","except Exception as error:\n","    print(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        print(\"Connection closed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwifoFzdUDwh"},"outputs":[],"source":["# This creates a table of field names and sample values\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","\n","# \"station_logs\" is the big one. WOuld have the same fields/data as MeterValues data in Splunk.\n","\n","\n","\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Query to fetch the first few rows from the current table\n","        query = f\"SELECT * FROM {table} LIMIT 10;\"\n","        cursor.execute(query)\n","\n","        # Fetch the rows\n","        rows = cursor.fetchall()\n","        # Fetch the column headers\n","        column_names = [desc[0] for desc in cursor.description]\n","\n","        # Create a DataFrame from the fetched data\n","        df = pd.DataFrame(rows, columns=column_names)\n","\n","        # Prepare the transposed DataFrame\n","        transposed_data = {\n","            'Header': column_names,\n","            'Data Type': [df[col].dtype.name for col in column_names],  # Get the data type\n","            'Example': [df[col].iloc[0] if not df[col].empty else None for col in column_names]  # Example from the first row\n","        }\n","\n","        df_transposed = pd.DataFrame(transposed_data)\n","\n","        # Write the DataFrame to CSV\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_fields.csv'\n","        df_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btMrMBD0jHEN"},"outputs":[],"source":["# This creates a table of sample records\n","\n","import os\n","import logging\n","import psycopg2\n","import pandas as pd\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection parameters from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')  # Fetch the port from environment variables\n","}\n","\n","# Function to fetch non-transposed data\n","def fetch_non_transposed_data(cursor, table):\n","    query = f\"SELECT * FROM {table} LIMIT 10;\"\n","    cursor.execute(query)\n","    rows = cursor.fetchall()\n","    column_names = [desc[0] for desc in cursor.description]\n","\n","    # Create a DataFrame from the fetched data\n","    df = pd.DataFrame(rows, columns=column_names)\n","    return df\n","\n","# List of tables to process\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Connect to the PostgreSQL database\n","try:\n","    connection = psycopg2.connect(**connection_params)\n","    cursor = connection.cursor()\n","\n","    # Loop through each table name\n","    for table in tables:\n","        logging.info(f\"Processing table: {table}\")\n","\n","        # Fetch non-transposed data\n","        df_non_transposed = fetch_non_transposed_data(cursor, table)\n","\n","        # Write the DataFrame to CSV with new naming convention\n","        output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/{table}_example_data.csv'\n","        df_non_transposed.to_csv(output_csv_path, index=False)\n","        logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","except Exception as error:\n","    logging.error(f\"Error connecting to the database: {error}\")\n","\n","finally:\n","    if 'connection' in locals() and connection:\n","        cursor.close()\n","        connection.close()\n","        logging.info(\"Connection closed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Xz1nKBmBM2nA"},"source":["###Create a table for all property info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7enpOKwkM2Gu"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# File paths\n","properties_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/properties.csv'\n","property_types_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_types.csv'\n","output_file = '/content/drive/MyDrive/Colab Notebooks/Data_sets/OCCP/property_lookup_2.csv'\n","\n","# Load and verify files\n","if not os.path.exists(properties_file):\n","    raise FileNotFoundError(f\"File not found: {properties_file}\")\n","if not os.path.exists(property_types_file):\n","    raise FileNotFoundError(f\"File not found: {property_types_file}\")\n","\n","properties = pd.read_csv(properties_file)\n","property_types = pd.read_csv(property_types_file)\n","\n","# Normalize column names to lowercase and strip whitespace\n","properties.columns = properties.columns.str.strip().str.lower()\n","property_types.columns = property_types.columns.str.strip().str.lower()\n","\n","# Perform the left join with suffixes\n","property_lookup = properties.merge(\n","    property_types,\n","    how='left',  # Use 'left' join to keep all rows from properties and add property_type name where available\n","    left_on='property_type',  # Assuming 'property_type' is the column in properties.csv\n","    right_on='id',  # Assuming 'id' is the column in property_types.csv\n","    suffixes=('_property', '_type')\n",")\n","\n","# Keep all columns from properties and just add the 'name_type' column as 'property_type'\n","property_lookup['property_type'] = property_lookup['name_type']\n","\n","# Drop the 'name_type' column, since we already added it as 'property_type'\n","property_lookup = property_lookup.drop(columns=['name_type'])\n","\n","# Rename 'id_property' column to 'property_id'\n","property_lookup = property_lookup.rename(columns={'id_property': 'property_id'})\n","\n","# Save the resulting DataFrame to CSV\n","property_lookup.to_csv(output_file, index=False)\n","print(f\"Property lookup table saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE633fu_M2Iz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrQU1kppM2LR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOTj1nOYM2N5"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gVJVYYv8s_tV"},"source":["# Now I need to build the correct table directly from RS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaoKGEHJw0uL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import logging\n","from itertools import combinations\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Path to the directory containing the CSV files\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/'\n","\n","# List of tables (as per your previous code)\n","tables = [\n","    \"group_discount_properties\", \"adjustment\", \"payment\", \"group_discount\",\n","    \"roles\", \"pos_device\", \"location\", \"station_credit_program\", \"station_history\",\n","    \"subscription\", \"user_discount_properties\", \"users\", \"router\", \"vehicle\",\n","    \"properties\", \"rfid_user\", \"stripe_payment_intent\", \"adr\", \"audit\",\n","    \"cluster_name\", \"global_setting\", \"station_logs\", \"station_model\",\n","    \"awsdms_ddl_audit\", \"user_access\", \"pricing\", \"stations\", \"gateway\",\n","    \"gateway_ip_lease\", \"errors\", \"credit_program\", \"maintenance_window\",\n","    \"refresh_token\", \"net_device_ip_lease\", \"ocpp_sub_session\", \"property_types\",\n","    \"user_device\", \"transaction\", \"address\", \"accounts\", \"net_devices\",\n","    \"organizations\", \"ocpp_session\", \"panels\", \"flyway_schema_history\",\n","    \"connectors\", \"clusters\"\n","]\n","\n","# Function to load CSV files into DataFrames\n","def load_dataframes(tables):\n","    dataframes = {}\n","    for table in tables:\n","        csv_path = os.path.join(data_dir, f\"{table}_example_data.csv\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","            dataframes[table] = df\n","            logging.info(f\"Loaded data for table: {table}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading data for table {table}: {e}\")\n","    return dataframes\n","\n","# Function to find strict join matches\n","def find_strict_joins(df1, df2, table1_name, table2_name):\n","    strict_joins = []\n","    # Iterate over all column pairs\n","    for col1 in df1.columns:\n","        for col2 in df2.columns:\n","            if df1[col1].dtype == df2[col2].dtype:\n","                # Perform the join\n","                joined_df = pd.merge(df1, df2, left_on=col1, right_on=col2, how='inner')\n","                # Check if all rows in df1 are in the joined DataFrame\n","                if len(joined_df) == len(df1):\n","                    strict_joins.append((col1, col2))\n","                    logging.info(f\"Strict join success: {table1_name}.{col1} <-> {table2_name}.{col2}\")\n","    return strict_joins\n","\n","# Main function to perform the strict join analysis\n","def analyze_strict_joins(tables):\n","    dataframes = load_dataframes(tables)\n","    results = {}\n","    table_pairs = combinations(tables, 2)\n","\n","    for table1, table2 in table_pairs:\n","        df1 = dataframes.get(table1)\n","        df2 = dataframes.get(table2)\n","\n","        if df1 is not None and df2 is not None:\n","            logging.info(f\"Analyzing strict joins between {table1} and {table2}\")\n","            joins = find_strict_joins(df1, df2, table1, table2)\n","            if joins:\n","                results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.warning(f\"Data for {table1} or {table2} is missing. Skipping.\")\n","\n","    return results\n","\n","# Run the strict join analysis\n","strict_join_results = analyze_strict_joins(tables)\n","\n","# Print the results\n","for table_pair, joins in strict_join_results.items():\n","    print(f\"\\nStrict joins for {table_pair}:\")\n","    for col1, col2 in joins:\n","        print(f\"Columns: {table_pair.split(' <-> ')[0]}.{col1} <-> {table_pair.split(' <-> ')[1]}.{col2}\")\n","\n","if not strict_join_results:\n","    print(\"No strict joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqQ_iKw5dn0M"},"outputs":[],"source":["import os\n","import logging\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Load credentials from file\n","def load_credentials(path_to_credentials):\n","    try:\n","        with open(path_to_credentials, 'r') as file:\n","            for line_num, line in enumerate(file, start=1):\n","                line = line.strip()\n","                if line and '=' in line:\n","                    key, value = line.split('=', 1)  # Split only on the first '='\n","                    os.environ[key.strip()] = value.strip()\n","                else:\n","                    logging.warning(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n","        logging.info(\"Credentials loaded successfully.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading credentials: {str(e)}\")\n","\n","# Call the function to load credentials\n","path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/aws_credentials.txt'\n","load_credentials(path_to_credentials)\n","\n","# Create connection string for SQLAlchemy\n","connection_string = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n","engine = create_engine(connection_string)\n","\n","# Function to fetch column names for a table\n","def get_columns(table_name):\n","    try:\n","        query = f\"\"\"\n","        SELECT column_name, data_type\n","        FROM information_schema.columns\n","        WHERE table_name = '{table_name}';\n","        \"\"\"\n","        with engine.connect() as connection:\n","            df = pd.read_sql_query(query, connection)\n","        return df[['column_name', 'data_type']].to_dict('records')\n","    except Exception as e:\n","        logging.error(f\"Error fetching columns for table {table_name}: {e}\")\n","        return []\n","\n","# Function to test join logic between two tables\n","def test_joins(table1, table2, attempts=3):\n","    columns_table1 = get_columns(table1)\n","    columns_table2 = get_columns(table2)\n","    successful_joins = []\n","\n","    for col1 in columns_table1:\n","        for col2 in columns_table2:\n","            # Only test joins on matching data types\n","            if col1['data_type'] == col2['data_type']:\n","                success_count = 0\n","                for _ in range(attempts):  # Attempt the join multiple times\n","                    query = f\"\"\"\n","                    SELECT *\n","                    FROM {table1} t1\n","                    INNER JOIN {table2} t2\n","                    ON t1.{col1['column_name']} = t2.{col2['column_name']}\n","                    LIMIT 1;  -- Test with one row at a time\n","                    \"\"\"\n","                    try:\n","                        with engine.connect() as connection:\n","                            df = pd.read_sql_query(query, connection)\n","                            if not df.empty:\n","                                success_count += 1\n","                    except Exception as e:\n","                        logging.debug(f\"Join failed for {table1}.{col1['column_name']} = {table2}.{col2['column_name']}: {e}\")\n","\n","                if success_count == attempts:  # Only count as successful if all attempts work\n","                    successful_joins.append((col1['column_name'], col2['column_name']))\n","                    logging.info(f\"Successful join: {table1}.{col1['column_name']} = {table2}.{col2['column_name']}\")\n","\n","    return successful_joins\n","\n","# Cross-check join fields for all table pairs\n","tables = [\n","    \"users\", \"ocpp_sub_session\"\n","]\n","\n","results = {}\n","\n","for i, table1 in enumerate(tables):\n","    for table2 in tables[i+1:]:\n","        logging.info(f\"Testing joins between {table1} and {table2}\")\n","        joins = test_joins(table1, table2)\n","        if joins:\n","            results[f\"{table1} <-> {table2}\"] = joins\n","        else:\n","            logging.info(f\"No join found between {table1} and {table2}\")\n","\n","# Print results\n","for table_pair, joins in results.items():\n","    print(f\"Successful joins for {table_pair}: {joins}\")\n","\n","if not results:\n","    print(\"No successful joins found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU41V4jQdn2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMXMJghgdn5H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuwvrH2ndn7Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VIJZs8fdn9k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kH2jY7TdoAK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ex8BAejxDoB"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Example dataframe (assuming df['message'] contains the raw strings)\n","# Clean the 'message' column by removing the prefix 'OCPP : MeterValues '\n","def clean_message(msg):\n","    try:\n","        # Remove the prefix\n","        msg_cleaned = msg.lstrip('OCPP : MeterValues ')\n","\n","        # Attempt to load the cleaned message as JSON\n","        return json.loads(msg_cleaned)\n","    except (json.JSONDecodeError, TypeError):\n","        # If the message cannot be decoded as JSON, return None or handle as needed\n","        return None\n","\n","# Apply the function to the 'message' column\n","df['message'] = df['message'].apply(clean_message)\n","\n","# Filter out rows where the 'message' column is None (indicating a JSON parse failure)\n","df = df[df['message'].notna()]\n","\n","# Step 1: Extract top-level fields and keep 'meterValue' as is (as a list of dicts)\n","flattened_rows = []\n","\n","for idx, row in df.iterrows():\n","    message = row['message']  # Now this is a valid JSON object\n","\n","    # Extract top-level fields\n","    connector_id = message.get('connectorId')\n","    transaction_id = message.get('transactionId')\n","\n","    # Keep the 'meterValue' field as is (as a list of dicts)\n","    meter_value = message.get('meterValue', [])\n","\n","    # Add a row to the flattened list, including the nested 'meterValue' list\n","    flattened_rows.append({\n","        '_time': row['time'],  # Retain the original timestamp from the dataframe\n","        'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","        'connectorId': connector_id,\n","        'meterValue': meter_value  # The entire 'meterValue' field, as it is (list of dictionaries)\n","    })\n","\n","# Step 2: Create a new DataFrame from the flattened rows\n","flattened_df = pd.DataFrame(flattened_rows)\n","\n","# Display the resulting DataFrame\n","print(flattened_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbk_sGLxZ_Z"},"outputs":[],"source":["import pandas as pd\n","\n","# Set pandas options to display the full content of any column (e.g., 'meterValue')\n","pd.set_option('display.max_colwidth', None)\n","\n","# Now, display the full content of the 'meterValue' column for the first 5 rows\n","print(flattened_df['meterValue'].head(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhRPDO5zJH0"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to hold the expanded rows\n","expanded_rows = []\n","\n","# Iterate over each row in the dataframe\n","for idx, row in flattened_df.iterrows():\n","    meter_values = row['meterValue']  # This is the list of meter readings (list of dicts)\n","\n","    # For each meter value entry (there should be one timestamp and a list of measurements)\n","    for meter in meter_values:\n","        timestamp = meter['timestamp']  # Extract the timestamp\n","\n","        # Initialize values for each measurement type\n","        watt_hours_value = None  # WattHours\n","        amps_value = None        # Amps (Current)\n","        voltage_value = None     # Voltage (Volts)\n","\n","        # Iterate over the sampledValue list (which contains the three measurements)\n","        for sample in meter['sampledValue']:\n","            # Check the 'unit' to assign the value to the correct column\n","            if sample['unit'] == 'Wh':  # WattHours\n","                watt_hours_value = sample['value']\n","            elif sample['unit'] == 'A':  # Amps (Current)\n","                amps_value = sample['value']\n","            elif sample['unit'] == 'V':  # Volts (Voltage)\n","                voltage_value = sample['value']\n","\n","        # Append the expanded row with the extracted values\n","        expanded_rows.append({\n","            '_time': row['_time'],  # Retain the original timestamp from the dataframe\n","            'user_id': row['user_id'],  # Assuming 'user_id' is part of the original dataframe\n","            'connectorId': row['connectorId'],  # Connector ID\n","            'timestamp': timestamp,  # Timestamp from the meter value\n","            'WattHours': watt_hours_value,  # Renamed to WattHours\n","            'Amps': amps_value,  # Keep Amps as the column name\n","            'Voltage': voltage_value  # Value for Voltage (V)\n","        })\n","\n","# Create a new DataFrame from the expanded rows\n","expanded_df = pd.DataFrame(expanded_rows)\n","\n","# Convert the numeric columns to appropriate types (float)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Display the resulting DataFrame\n","print(expanded_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcgRo8Dz1Nm"},"outputs":[],"source":["# Ensure all columns are numeric (in case there are any string values left)\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Classify values as 0 or > 0 for each of the measurements\n","expanded_df['WattHours_Class'] = expanded_df['WattHours'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Amps_Class'] = expanded_df['Amps'].apply(lambda x: '0' if x == 0 else '>0')\n","expanded_df['Voltage_Class'] = expanded_df['Voltage'].apply(lambda x: '0' if x == 0 else '>0')\n","\n","# Set up the plot\n","plt.figure(figsize=(18, 6))\n","\n","# Plot the count of each class for 'WattHours', 'Amps', and 'Voltage'\n","plt.subplot(1, 3, 1)\n","sns.countplot(data=expanded_df, x='WattHours_Class')\n","plt.title('Count of Rows with WattHours: 0 vs > 0')\n","plt.xlabel('WattHours Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 2)\n","sns.countplot(data=expanded_df, x='Amps_Class')\n","plt.title('Count of Rows with Amps: 0 vs > 0')\n","plt.xlabel('Amps Class')\n","plt.ylabel('Count')\n","\n","plt.subplot(1, 3, 3)\n","sns.countplot(data=expanded_df, x='Voltage_Class')\n","plt.title('Count of Rows with Voltage: 0 vs > 0')\n","plt.xlabel('Voltage Class')\n","plt.ylabel('Count')\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsRrQLIz1QQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Ensure that the '_time' column is in datetime format\n","expanded_df['timestamp'] = pd.to_datetime(expanded_df['timestamp'], errors='coerce')\n","\n","# Convert 'Amps', 'WattHours', and 'Voltage' to numeric (handling any errors)\n","expanded_df['Amps'] = pd.to_numeric(expanded_df['Amps'], errors='coerce')\n","expanded_df['WattHours'] = pd.to_numeric(expanded_df['WattHours'], errors='coerce')\n","expanded_df['Voltage'] = pd.to_numeric(expanded_df['Voltage'], errors='coerce')\n","\n","# Drop rows where any of the values are missing\n","expanded_df = expanded_df.dropna(subset=['_time', 'Amps', 'WattHours', 'Voltage'])\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create the figure and axes for the plots\n","plt.figure(figsize=(18, 6))\n","\n","# Plot Amps over time\n","plt.subplot(1, 3, 1)\n","plt.plot(expanded_df['timestamp'], expanded_df['Amps'], label='Amps', color='b', alpha=0.7)\n","plt.title('Amps over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Amps')\n","plt.xticks(rotation=45)\n","\n","# Plot WattHours over time\n","plt.subplot(1, 3, 2)\n","plt.plot(expanded_df['timestamp'], expanded_df['WattHours'], label='WattHours', color='g', alpha=0.7)\n","plt.title('WattHours over Time')\n","plt.xlabel('Time')\n","plt.ylabel('WattHours')\n","plt.xticks(rotation=45)\n","\n","# Plot Voltage over time\n","plt.subplot(1, 3, 3)\n","plt.plot(expanded_df['timestamp'], expanded_df['Voltage'], label='Voltage', color='r', alpha=0.7)\n","plt.title('Voltage over Time')\n","plt.xlabel('Time')\n","plt.ylabel('Voltage')\n","plt.xticks(rotation=45)\n","\n","# Adjust layout to avoid overlap of labels\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi2OjXK_2hXK"},"outputs":[],"source":["expanded_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TbILosnz1Sh"},"outputs":[],"source":["# Run descriptive statistics on 'Amps', 'WattHours', and 'Voltage'\n","descriptive_stats = expanded_df[['Amps', 'WattHours', 'Voltage']].describe()\n","\n","# Display the statistics\n","print(descriptive_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zZUwehE16FW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the style for the plots\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure with 3 subplots (1 row, 3 columns)\n","plt.figure(figsize=(18, 6))\n","\n","# Plot for Amps\n","plt.subplot(1, 3, 1)\n","sns.boxplot(data=expanded_df['Amps'], color='skyblue')\n","plt.title('Boxplot of Amps')\n","plt.ylabel('Amps')\n","\n","# Plot for WattHours\n","plt.subplot(1, 3, 2)\n","sns.boxplot(data=expanded_df['WattHours'], color='lightgreen')\n","plt.title('Boxplot of WattHours')\n","plt.ylabel('WattHours')\n","\n","# Plot for Voltage\n","plt.subplot(1, 3, 3)\n","sns.boxplot(data=expanded_df['Voltage'], color='lightcoral')\n","plt.title('Boxplot of Voltage')\n","plt.ylabel('Voltage')\n","\n","# Adjust layout to avoid overlap\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcBnl60S2LIP"},"outputs":[],"source":["# count of propertyIDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT property_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'property_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9r14vD2LK6"},"outputs":[],"source":["#Count of cluster IDs\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values\n","query = f\"SELECT COUNT(DISTINCT cluster_id) FROM {table};\"\n","cursor.execute(query)\n","\n","# Fetch the result\n","result = cursor.fetchone()\n","\n","# Extract and print the count\n","unique_property_id_count = result[0] if result else 0\n","print(f\"Number of unique 'cluster_id' values: {unique_property_id_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F01IOcSb2LNZ"},"outputs":[],"source":["# counts of peropertyID and clusterIDimport os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'cluster_id' values for each 'property_id'\n","query = f\"\"\"\n","SELECT property_id, COUNT(DISTINCT cluster_id)\n","FROM {table}\n","GROUP BY property_id\n",";\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    property_id, cluster_count = row\n","    print(f\"Property ID: {property_id}, Unique Cluster ID Count: {cluster_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n","\n","import os\n","import psycopg2\n","\n","# Load credentials from environment variables\n","connection_params = {\n","    'host': os.getenv('DB_HOST'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'port': os.getenv('DB_PORT')\n","}\n","\n","# Table to process (the 'stations' table)\n","table = \"stations\"\n","\n","# Connect to the PostgreSQL database and run the query\n","connection = psycopg2.connect(**connection_params)\n","cursor = connection.cursor()\n","\n","# Query to count unique 'property_id' values for each 'cluster_id'\n","query = f\"\"\"\n","SELECT cluster_id, COUNT(DISTINCT property_id)\n","FROM {table}\n","GROUP BY cluster_id;\n","\"\"\"\n","cursor.execute(query)\n","\n","# Fetch all the results\n","results = cursor.fetchall()\n","\n","# Print the results\n","for row in results:\n","    cluster_id, property_count = row\n","    print(f\"Cluster ID: {cluster_id}, Unique Property ID Count: {property_count}\")\n","\n","# Clean up\n","cursor.close()\n","connection.close()\n"]},{"cell_type":"markdown","metadata":{"id":"gJCn043ops5i"},"source":["OCPP_SessionID has a userID and TransactionID\n","I need to map to the cluster and property\n","\n","Stations has propertyID and cluster_id\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkQRHSF2pYLF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRERnF5HpYNr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzZNFFc8pYQY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pbxg6jVhpYS3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wI1hDsHpYVx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiA1AO1z2LSG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmnNNDMY2LUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpS8FyTqEWgq"},"outputs":[],"source":["# Clean message field and port to a df\n","import json\n","import pandas as pd\n","\n","# Function to clean up the 'message' field by removing the prefix and parsing JSON\n","def clean_and_parse_message(message):\n","    try:\n","        # Strip the non-JSON prefix before the first '{'\n","        cleaned_message = message[message.find('{'):]\n","        # Parse the cleaned JSON string\n","        return json.loads(cleaned_message)\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON in message: {message}\\nError: {e}\")\n","        return None\n","\n","# Function to flatten nested JSON\n","def flatten_json(y):\n","    out = {}\n","\n","    def flatten(x, name=''):\n","        if isinstance(x, dict):\n","            for a in x:\n","                flatten(x[a], name + a + '_')\n","        elif isinstance(x, list):\n","            i = 0\n","            for a in x:\n","                flatten(a, name + str(i) + '_')\n","                i += 1\n","        else:\n","            out[name[:-1]] = x\n","\n","    flatten(y)\n","    return out\n","\n","# Apply the cleaning and parsing function to all rows in the 'message' field\n","df['parsed_message'] = df['message'].apply(clean_and_parse_message)\n","\n","# Drop rows where parsing failed (invalid JSON) or was not cleaned properly\n","valid_df = df[df['parsed_message'].notnull()]\n","\n","# Flatten all the JSON objects and store them in a new DataFrame\n","flattened_data = valid_df['parsed_message'].apply(flatten_json).apply(pd.Series)\n","\n","# Combine the flattened JSON fields with the original DataFrame (excluding the original 'message' and 'parsed_message' fields)\n","new_df = pd.concat([valid_df.drop(columns=['message', 'parsed_message']), flattened_data], axis=1)\n","\n","# Write the DataFrame to CSV with new naming convention\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/new_df.csv'\n","new_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PnkEtU3uDKj"},"outputs":[],"source":["new_df.info\n","new_df.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDqqE7Swt6vj"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df= new_df\n","\n","# Assuming your DataFrame is named df\n","# Step 1: Convert 'time' to datetime\n","df['time'] = pd.to_datetime(df['time'], errors='coerce')  # errors='coerce' will turn invalid parsing to NaT\n","\n","# Step 2: Convert 'meterValue_0_sampledValue_0_value' to numeric\n","df['meterValue_0_timestamp'] = pd.to_numeric(df['meterValue_0_timestamp'], errors='coerce')\n","\n","# Step 3: Drop any rows with NaT or NaN values (optional, depending on your needs)\n","df = df.dropna(subset=['time', 'meterValue_0_sampledValue_0_value'])\n","\n","# Step 4: Plot the time series\n","plt.figure(figsize=(10, 6))\n","plt.plot(df['meterValue_0_timestamp'], df['meterValue_0_sampledValue_0_value'], label='Meter Value', color='b')\n","plt.xlabel('Time')\n","plt.ylabel('Meter Value')\n","plt.title('Meter Value Over Time')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlT8kevDasAZ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['message'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcwXooB_tXnf"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming new_df is already defined and contains the necessary columns\n","\n","# List of columns to keep\n","columns_to_keep = [\n","    #'time',\n","    'user_id',\n","    #'station_id',\n","    'property_id',\n","    'connectorId',\n","    'meterValue_0_timestamp',\n","    'meterValue_0_sampledValue_1_value',\n","    'meterValue_0_sampledValue_1_context',\n","    'meterValue_0_sampledValue_1_format',\n","    'meterValue_0_sampledValue_1_measurand',\n","    'meterValue_0_sampledValue_1_phase',\n","    'meterValue_0_sampledValue_1_location',\n","    'meterValue_0_sampledValue_1_unit'\n","]\n","\n","# Create new_df_2 with only the selected columns\n","new_df_2 = new_df[columns_to_keep].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","\n","# Convert 'time' to datetime\n","new_df_2['meterValue_0_timestamp'] = pd.to_datetime(new_df_2['meterValue_0_timestamp'], errors='coerce')\n","\n","# Check for any NaT values that may have resulted from the conversion\n","if new_df_2['meterValue_0_timestamp'].isnull().any():\n","    print(\"Some values could not be converted to datetime.\")\n","\n","# Extract day and hour using .loc to avoid warnings\n","new_df_2.loc[:, 'meterValue_0_day'] = new_df_2['meterValue_0_timestamp'].dt.date\n","new_df_2.loc[:, 'meterValue_0_hour'] = new_df_2['meterValue_0_timestamp'].dt.hour\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLthqnU8u2eM"},"outputs":[],"source":["new_df_2.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMxEqA8Qbkxz"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = new_df_2\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of message')\n","plt.title('Count of message per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N29p7Kq_xWuu"},"outputs":[],"source":["# Assuming new_df_2 is the df\n","\n","unique_values = new_df_2['user_id'].unique()\n","\n","# To display the unique values\n","print(unique_values)\n","\n","\n","# Assuming new_df_2 is your DataFrame\n","unique_count = new_df_2['user_id'].nunique()\n","\n","# To display the count of unique user_id values\n","print(f\"Number of unique user_id values: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEjr917eu9Rb"},"outputs":[],"source":["import pandas as pd\n","\n","\n","new_df_2['meterValue_0_sampledValue_1_value'] = pd.to_numeric(new_df_2['meterValue_0_sampledValue_1_value'], errors='coerce')\n","\n","max_values = new_df_2.loc[new_df_2.groupby(['user_id', 'meterValue_0_day'])['meterValue_0_sampledValue_1_value'].idxmax()]\n","\n","result_df = max_values[['user_id', 'meterValue_0_day', 'meterValue_0_sampledValue_1_value', 'meterValue_0_timestamp']]\n","\n","print(result_df)\n","result_df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnNlof0taJFO"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming df is your DataFrame\n","# Group by user_id and count the occurrences of meterValue_0_timestamp\n","hist_data = df.groupby('user_id')['meterValue_0_timestamp'].count().reset_index()\n","\n","# Rename the columns for clarity\n","hist_data.columns = ['user_id', 'count']\n","\n","# Sort the data by user_id for better visualization\n","hist_data = hist_data.sort_values('user_id')\n","\n","# Plotting the normal line plot\n","plt.figure(figsize=(10, 6))\n","plt.plot(hist_data['user_id'], hist_data['count'], marker='o', linestyle='-', color='skyblue')\n","plt.xlabel('User ID')\n","plt.ylabel('Count of meterValue_0_timestamp')\n","plt.title('Count of meterValue_0_timestamp per User ID')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.grid()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"842I60p_-VjA"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Assuming result_df is your DataFrame\n","\n","# Print column names to verify\n","print(\"Column names in DataFrame:\")\n","print(result_df.columns.tolist())\n","\n","# Specify the user_id you're interested in\n","specific_user_id = '013f0335-da69-4fdd-b378-b6a9a8cfc8a8'  # replace with the actual user_id\n","\n","# Filter the DataFrame for the specific user_id\n","filtered_df = result_df[result_df['user_id'] == specific_user_id]\n","\n","# Check if there are any rows for the specified user_id\n","if not filtered_df.empty:\n","    # Check for the timestamp column again\n","    timestamp_col = 'meterValue_0_timestamp'  # Update if necessary\n","    value_col = 'meterValue_0_sampledValue_1_value'\n","\n","    # Ensure the column names are correct\n","    print(\"Filtered DataFrame columns:\")\n","    print(filtered_df.columns.tolist())\n","\n","    # Plotting\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(filtered_df[timestamp_col], filtered_df[value_col], marker='o')\n","    plt.title(f'Meter Values for User ID: {specific_user_id}')\n","    plt.xlabel('Timestamp')\n","    plt.ylabel('Meter Value')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n","    plt.grid()\n","    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","    plt.show()\n","else:\n","    print(f\"No data found for user_id: {specific_user_id}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp5ImngUusix"},"outputs":[],"source":["\n","# Write the DataFrame to CSV\n","output_csv_path = f'/content/drive/MyDrive/Colab Notebooks/Data_sets/Chargie/result_df_exported.csv'\n","result_df.to_csv(output_csv_path, index=False)\n","logging.info(f\"Data written to {output_csv_path} successfully.\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ZZ-b6HuslW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3cWsdlTusnX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TvKiocBsdvU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A0Tx9v-sdxw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKXZgFB5sd0F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBdB1TPsd2a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDJEMK-rsd4_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b5uC-F76-aAQ75cQ-luVy0hXajNBJSFN","timestamp":1733340556580},{"file_id":"16uU93i_V5dD_ek6YdIVMzJ9oDkhWpDn1","timestamp":1731541149049}],"mount_file_id":"11y38iI97BbjLgUt8QX8sx0DYgkr60xWp","authorship_tag":"ABX9TyO97VgkHvN2oZUkM3MrEFX5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}