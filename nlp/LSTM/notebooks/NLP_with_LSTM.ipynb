{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/nlp/LSTM/notebooks/NLP_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACjQwCY2Pej"
      },
      "source": [
        "# Project Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylDvc8SLcDw"
      },
      "source": [
        "**Define Project Goals and Contraints:**\n",
        "\n",
        "This will be instrumental in selecting specific architecture and data processing strategies.\n",
        "\n",
        "\n",
        "**Data Cleaning and Preprocessing:**\n",
        "\n",
        "Load your data into a Pandas DataFrame.\n",
        "Perform basic cleaning: remove duplicates, handle missing values.\n",
        "Normalize text: convert to lowercase, remove punctuation, and special characters.\n",
        "\n",
        "**Text Preprocessing for BPE:**\n",
        "\n",
        "Apply BPE tokenization to your corpus. This involves learning the BPE vocab from your dataset and then applying it to both questions and answers to tokenize them.\n",
        "\n",
        "**Splitting the Dataset:**\n",
        "\n",
        "Split your data into training, validation, and test sets. A common split ratio is 80% training, 10% validation, and 10% test.\n",
        "\n",
        "**Converting Text to Sequences:**\n",
        "\n",
        "Convert your tokenized text into sequences of integers using the BPE vocabulary. This step transforms the textual data into a format that can be fed into the LSTM model.\n",
        "\n",
        "**Padding Sequences:**\n",
        "\n",
        "Since LSTM models require inputs of the same length, use padding to ensure all sequences in a batch have the same length.\n",
        "\n",
        "**Designing the LSTM Model:**\n",
        "\n",
        "Build your LSTM model architecture using TensorFlow/Keras. The model should include an Embedding layer, one or more LSTM layers, and a Dense output layer.\n",
        "\n",
        "**Compiling the Model:**\n",
        "\n",
        "Compile the model with an appropriate optimizer (e.g., Adam), loss function (e.g., sparse_categorical_crossentropy for classification tasks), and metrics (e.g., accuracy).\n",
        "\n",
        "**Training the Model:**\n",
        "\n",
        "Train the model on your training set while also validating its performance on the validation set. Use model checkpoints and early stopping to prevent overfitting.\n",
        "\n",
        "**Evaluating the Model:**\n",
        "\n",
        "After training, evaluate the model's performance on the test set to get a sense of its generalization ability.\n",
        "\n",
        "**Model Deployment:**\n",
        "\n",
        "Deploy the model into a production environment. This could be a simple web application or a REST API that takes in a question and returns the predicted answer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals and Constraints\n",
        "\n",
        "**Goal**\n",
        "\n",
        "The goal is to create a \"virtual mechanic\" to help owners maintain older cars that have a dwindling set of experts available to turn to.\n",
        "\n",
        "\n",
        "**Task Type:**\n",
        "\n",
        "The project aims to build a generative language model that will accept written unstructured questions in English from users and provide the user with targeted written answers in English. The model will use sequence prediction and text generation. The model will not use classification, image recognition, or sentiment analysis.\n",
        "\n",
        "**Data Characteristics:**\n",
        "\n",
        "The training corpus for the data will be User Generated Content scraped from a domain-specific online forum. The corpus will generally be unstructured with a very limited set of metadata.\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "Performance of the project will be scored on accuracy and speed of responses.\n",
        "\n",
        "**Resource Constraints:**\n",
        "\n",
        "The project will be built in Python utilizing limited CPU compute resources from Google Colab.\n",
        "\n",
        "**Existing Tools or Frameworks:**\n",
        "\n",
        "The corpus will be stored in Snowflake database.\n",
        "\n",
        "**Scalability and Adaptability:**\n",
        "\n",
        "There is no need to support additional user languages. However, when available, the corpus will be supplemented with additional written unstructured text.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bn5lG1xQvCSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Creation\n",
        "\n",
        "The corpus used was assembled using Beautiful Soup to scrape a pubic forum specific to the BMW E9 (www.e9coupe.com). This active forum has been exsitence since 2003. The data was compiled and stored in a Snowflake database for multiple NLP projects, including LDA, GRU and LSTM. Furture ideas include supplementing the forum text with an existing users guide specific to this model."
      ],
      "metadata": {
        "id": "htWB29T9ikMo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQU98RCj42U"
      },
      "source": [
        "##Language Model Architectures\n",
        "\n",
        "### Recurrent Neural Networks (RNNs):\n",
        "\n",
        "**Pros:**\n",
        "1. *Sequential Processing:* RNNs process sequential data efficiently, making them suitable for tasks like text generation where the order of input elements matters.\n",
        "2. *Memory:* RNNs have a form of memory that allows them to remember past information while processing current inputs.\n",
        "3. *Interpretability:* Due to their sequential nature, RNNs are often more interpretable compared to more complex architectures like Transformers.\n",
        "4. *Ease of Development:* RNNs have been around for longer and have a simpler architecture compared to Transformers, making them easier to develop and understand for beginners.\n",
        "5. *CPU Needs:* RNNs can be trained and run on CPU instances, although training large models or processing large datasets may benefit from GPU acceleration.\n",
        "\n",
        "**Cons:**\n",
        "1. *Vanishing/Exploding Gradient:* RNNs can suffer from vanishing or exploding gradient problems, especially when dealing with long sequences, which can lead to difficulties in learning long-term dependencies.\n",
        "2. *Limited Context:* Traditional RNNs have a limited memory span, making them less effective at capturing long-range dependencies in data.\n",
        "3. *Computationally Inefficient:* Training RNNs can be computationally expensive, especially when dealing with large datasets and long sequences.\n",
        "\n",
        "### Transformer Architectures:\n",
        "\n",
        "**Pros:**\n",
        "1. *Parallelization:* Transformers allow for highly parallelized computation, leading to faster training and inference compared to sequential models like RNNs.\n",
        "2. *Long-Range Dependencies:* Transformers can capture long-range dependencies in data more effectively than traditional RNNs, making them well-suited for tasks requiring global context, such as machine translation and text generation.\n",
        "3. *Attention Mechanism:* Transformers use attention mechanisms to weigh the importance of different input elements, allowing them to focus on relevant information and ignore irrelevant parts of the input sequence.\n",
        "4. *Ease of Development:* While more complex than RNNs, Transformers have a modular architecture that can be easier to develop and experiment with compared to traditional recurrent architectures.\n",
        "\n",
        "**Cons:**\n",
        "1. *Complexity:* Transformers have a more complex architecture compared to RNNs, which can make them harder to understand, implement, and interpret.\n",
        "2. *Data Requirements:* Transformers require large amounts of data to train effectively, especially for tasks with complex patterns and dependencies.\n",
        "3. *Resource Intensive:* Training large transformer models requires significant computational resources, including powerful GPUs or TPUs, making them less accessible for smaller-scale projects or individuals with limited resources.\n",
        "\n",
        "### Hybrid Model (Combining RNNs and Transformers):\n",
        "\n",
        "**Pros:**\n",
        "1. *Combine Strengths:* A hybrid model can potentially combine the strengths of both RNNs and Transformers, leveraging the sequential processing capabilities of RNNs with the long-range dependency handling of Transformers.\n",
        "2. *Flexibility:* A hybrid approach offers flexibility in model design, allowing researchers and practitioners to tailor the architecture to specific task requirements and data characteristics.\n",
        "\n",
        "**Cons:**\n",
        "1. *Complexity:* Developing and training a hybrid model can be more complex compared to using either RNNs or Transformers alone, as it requires integration of different architectural components and potentially more sophisticated training procedures.\n",
        "2. *Resource Intensive:* Depending on the specific architecture and scale, training a hybrid model may require significant computational resources, similar to Transformers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization Strategies\n",
        "\n",
        "#### Word-Level Tokenization:\n",
        "\n",
        "**Description:**\n",
        "Word-level tokenization splits the text into individual words, treating each word as a token.\n",
        "\n",
        "**Libraries:**\n",
        "1. NLTK (Natural Language Toolkit): Provides tokenization tools for various NLP tasks, including word-level tokenization.\n",
        "2. spaCy: Another popular NLP library that offers word-level tokenization along with other NLP functionalities.\n",
        "\n",
        "**Pros:**\n",
        "1. Preserves semantic meaning of individual words.\n",
        "2. Intuitive representation of text for language modeling tasks.\n",
        "\n",
        "**Cons:**\n",
        "1. May struggle with out-of-vocabulary words, especially in domain-specific or informal language.\n",
        "2. Increases vocabulary size, potentially leading to higher memory usage.\n",
        "\n",
        "**Suitability:**\n",
        "Word-level tokenization may be suitable for this project as it preserves the semantic meaning of individual words, which can be important for generating coherent responses to user questions.\n",
        "\n",
        "#### Character-Level Tokenization:\n",
        "\n",
        "**Description:**\n",
        "Character-level tokenization treats each character in the text as a separate token.\n",
        "\n",
        "**Libraries:**\n",
        "1. TensorFlow Text: Part of the TensorFlow ecosystem, TensorFlow Text provides utilities for various text processing tasks, including character-level tokenization.\n",
        "2. Keras: With its text preprocessing module, Keras offers character-level tokenization capabilities.\n",
        "\n",
        "**Pros:**\n",
        "1. Captures fine-grained details in the text, useful for handling misspellings or morphologically complex words.\n",
        "2. Helps in handling out-of-vocabulary terms effectively.\n",
        "\n",
        "**Cons:**\n",
        "1. Can be computationally expensive due to larger token vocabulary.\n",
        "2. May not capture higher-level semantic meaning as effectively as word-level tokenization.\n",
        "\n",
        "**Suitability:**\n",
        "Character-level tokenization might not be the best choice for this project, as it may not capture the semantic meaning of words effectively. However, it could be useful for capturing fine-grained details in the text if necessary.\n",
        "\n",
        "#### Byte Pair Encoding (BPE):\n",
        "\n",
        "**Description:**\n",
        "Byte Pair Encoding (BPE) tokenization iteratively merges the most frequent pairs of tokens to build a vocabulary of subword units.\n",
        "\n",
        "**Libraries:**\n",
        "1. Hugging Face Transformers: Provides tokenization functionalities, including BPE, along with pre-trained language models for various NLP tasks.\n",
        "2. Tokenizers: A Python library specifically designed for fast and customizable tokenization, including BPE tokenization.\n",
        "\n",
        "**Pros:**\n",
        "1. Handles rare or out-of-vocabulary terms effectively.\n",
        "2. Offers a good balance between accuracy and efficiency.\n",
        "\n",
        "**Cons:**\n",
        "1. Requires additional pre-processing steps compared to traditional tokenization methods.\n",
        "2. Increases complexity of tokenization process, potentially impacting speed.\n",
        "\n",
        "**Suitability:**\n",
        "BPE tokenization could be a good choice for this project as it effectively handles rare or out-of-vocabulary terms, which may be present in the user-generated content scraped from online forums. It also offers good balance between accuracy and speed, which aligns with the project's performance metrics and resource constraints.\n"
      ],
      "metadata": {
        "id": "22tVZoiex1yS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_YEpMzr3b5S"
      },
      "source": [
        "## Summarization Strategies\n",
        "Very difficult to find a winning strategy here that can accomidate both long and short length text blocks.\n",
        "\n",
        "**Extractive Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Good with Raw Text: Extractive methods can work directly with raw, unstructured text, as they mainly focus on selecting key sentences or phrases without needing deep linguistic processing.\n",
        "Straightforward Implementation: These methods do not require complex preprocessing like tokenization or lemmatization, simplifying their implementation.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Limited Depth in Understanding: While they can handle raw text, they may not fully capture the nuanced meaning, especially when the text contains complex structures or unorthodox language use.\n",
        "Less Effective with Poorly Structured Text: In cases where the text is poorly structured or highly informal, extractive summarization might struggle to identify the main points effectively.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**Abstractive Summarization** (like sshleifer/distilbart-cnn-12-6)\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Advanced Processing Capabilities: Abstractive models, especially those based on transformer architectures, are designed to handle and interpret raw text, capturing deeper linguistic and contextual nuances.\n",
        "Higher Tolerance for Unstructured Text: These models can manage unstructured or informal text by understanding and then rephrasing it in a more coherent and structured summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Dependence on Preprocessing for Optimal Performance: While they can process raw text, the quality of the output can be significantly improved with proper tokenization and lemmatization, especially for complex texts.\n",
        "Potential Overhead: Requires more computational resources to process and understand raw text, which might be more efficiently handled with some level of preprocessing.\n",
        "<br>\n",
        "<br>\n",
        "**Hybrid Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Flexibility in Text Processing: Combining extractive and abstractive methods allows for handling both raw and preprocessed text, adapting to the text's structure and complexity.\n",
        "Balanced Approach: Can leverage the strengths of extractive methods in handling raw text for identifying key points, while using abstractive techniques for generating a coherent summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Complex Preprocessing Requirements: The need to integrate both extractive and abstractive approaches may necessitate more sophisticated preprocessing strategies to optimize performance.\n",
        "Potential for Processing Inefficiencies: The combined approach might lead to redundancies or inefficiencies in processing, especially if the text is either too raw or overly preprocessed.\n",
        "<br>\n",
        "<br>\n",
        "After attempting sshleifer/distilbart-cnn-12-6 I found it had a character limit (1024) that is too restrictive for my needs. However, T5 has no limits and is what Ill be trying."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeecHyOQt6lU",
        "outputId": "4b3ad9fc-8cc4-401b-bf40-bf4905ddd4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect source data\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "\n",
        "# And use them in your Snowflake connection (adjust as necessary for your specific case):\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Select source data\n",
        "query = \"\"\"\n",
        "SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "# Load your data into a Pandas DataFrame.\n",
        "e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "# Close the cursor and the connection\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9ap8HP26MQ2",
        "outputId": "6d491f24-8d73-483c-8b79-c876f0efac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.5)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.0.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.10.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.13.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Collecting platformdirs<4.0.0,>=2.6.0 (from snowflake-connector-python)\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.12.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n",
            "Installing collected packages: asn1crypto, tomlkit, platformdirs, snowflake-connector-python\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.2.0\n",
            "    Uninstalling platformdirs-4.2.0:\n",
            "      Successfully uninstalled platformdirs-4.2.0\n",
            "Successfully installed asn1crypto-1.5.1 platformdirs-3.11.0 snowflake-connector-python-3.7.1 tomlkit-0.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n"
      ],
      "metadata": {
        "id": "p3zcZueUn6gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This revised code removes tokenization from NLTK and focuses on cleaning.\n",
        "# BPM tokenizatin is handled later on.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_corpus(df):\n",
        "    cleaned_titles = []\n",
        "    cleaned_posts = []\n",
        "\n",
        "    for title in df['THREAD_TITLE']:\n",
        "        if isinstance(title, str):\n",
        "            cleaned_titles.append(clean_text(title))\n",
        "        else:\n",
        "            cleaned_titles.append('')  # If the title is not a string, append an empty string\n",
        "\n",
        "    df['THREAD_TITLE_CLEAN'] = cleaned_titles\n",
        "\n",
        "    for post in df['THREAD_ALL_POSTS']:\n",
        "        if isinstance(post, str):\n",
        "            cleaned_posts.append(clean_text(post))\n",
        "        else:\n",
        "            cleaned_posts.append('')  # If the post is not a string, append an empty string\n",
        "\n",
        "    df['THREAD_POSTS_CLEAN'] = cleaned_posts\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "lstm = clean_corpus(e9_forum_corpus.copy())\n",
        "\n",
        "\n",
        "# Drop unecessary columns\n",
        "lstm.drop(columns=['THREAD_TITLE'], inplace=True)\n",
        "lstm.drop(columns=['THREAD_ALL_POSTS'], inplace=True)\n",
        "lstm.drop(columns=['THREAD_FIRST_POST'], inplace=True)\n",
        "\n",
        "\n",
        "# Rename columns to describe their roles\n",
        "lstm.rename(columns={\"THREAD_TITLE_CLEAN\": \"question\", \"THREAD_POSTS_CLEAN\": \"answer\"}, inplace=True)"
      ],
      "metadata": {
        "id": "L7EBIm935wr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Dataset"
      ],
      "metadata": {
        "id": "G7e1PAHpoCuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data, validation, and test sets.Common split ratios are 80% training, 10% validation, and 10% test.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data\n",
        "# train_df, temp_df = train_test_split(lstm, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Decreasing the training size to speed up\n",
        "train_df, temp_df = train_test_split(lstm, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "# Further splitting the temporary dataset into validation and test datasets (50% validation, 50% test from the temp dataset)\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "3wtCScWPoFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training Set Size: {len(train_df)}\")\n",
        "print(f\"Validation Set Size: {len(validation_df)}\")\n",
        "print(f\"Test Set Size: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l0GAXShUKgy",
        "outputId": "045c295d-5ffb-4c42-cd9c-064f5f3235db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Size: 993\n",
            "Validation Set Size: 4472\n",
            "Test Set Size: 4472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "I-e2jffh-9rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing for BPE:\n",
        "!pip install tokenizers\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Concatenate questions and answers into a list of texts\n",
        "texts = lstm['question'].tolist() + lstm['answer'].tolist()\n",
        "\n",
        "# Save the texts to a file\n",
        "with open(\"text_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in texts:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train(files=\"text_data.txt\", vocab_size=30_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "# Save the trained tokenizer\n",
        "tokenizer.save_model(\".\", \"bpe_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msr9aBeTUKeb",
        "outputId": "88ae75a3-e9e0-409a-e21d-d51767b6df9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./bpe_tokenizer-vocab.json', './bpe_tokenizer-merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequencing"
      ],
      "metadata": {
        "id": "w4SLE2cYq81-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Assuming your tokenizer files are saved in the current directory\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./bpe_tokenizer-vocab.json\",\n",
        "    \"./bpe_tokenizer-merges.txt\",\n",
        ")\n",
        "\n",
        "def tokenize_and_encode(df, tokenizer):\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        question = tokenizer.encode(row['question']).ids\n",
        "        answer = tokenizer.encode(row['answer']).ids\n",
        "\n",
        "        questions.append(question)\n",
        "        answers.append(answer)\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "# Apply the function to each of your DataFrames\n",
        "train_questions, train_answers = tokenize_and_encode(train_df, tokenizer)\n",
        "validation_questions, validation_answers = tokenize_and_encode(validation_df, tokenizer)\n",
        "test_questions, test_answers = tokenize_and_encode(test_df, tokenizer)\n"
      ],
      "metadata": {
        "id": "DihsH_wFq4FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "XH2JtWsRrLPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set maximum sequence length\n",
        "sequence_length = 100\n",
        "\n",
        "# Calculate maximum sequence length\n",
        "#sequence_length = max(max(len(seq) for seq in train_questions + train_answers),\n",
        "#                 max(len(seq) for seq in validation_questions + validation_answers),\n",
        "#                 max(len(seq) for seq in test_questions + test_answers))\n",
        "\n",
        "# Use 'max_length' for padding/truncating\n",
        "train_answers_padded = pad_sequences(train_answers, maxlen=sequence_length, padding='post', truncating='post')\n",
        "train_questions_padded = pad_sequences(train_questions, maxlen=sequence_length, padding='post', truncating='post')\n",
        "\n",
        "validation_questions_padded = pad_sequences(validation_questions, maxlen=sequence_length, padding='post', truncating='post')\n",
        "validation_answers_padded = pad_sequences(validation_answers, maxlen=sequence_length, padding='post', truncating='post')\n",
        "test_questions_padded = pad_sequences(test_questions, maxlen=sequence_length, padding='post', truncating='post')\n",
        "test_answers_padded = pad_sequences(test_answers, maxlen=sequence_length, padding='post', truncating='post')\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "Ov-8wdaeq4Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Designing the Model\n",
        "\n",
        "Your LSTM model architecture looks well-constructed for a sequence processing task, with a bidirectional LSTM layer to capture patterns from both directions of your input sequences, enhancing the model's understanding of the context. The embedding layer is essential for representing your tokenized words as dense vectors of fixed size, and the dense layers, including a dropout layer for regularization, form the decision-making part of the network. However, there are a few considerations and potential adjustments depending on the specifics of your task:\n",
        "\n",
        "**Model Architecture Considerations**\n",
        "\n",
        "- **Output Layer**: The current model ends with a single dense layer with one unit (`Dense(1)`) and no activation function specified. This setup is typical for binary classification tasks. If your task is to generate text (like in a virtual mechanic answering questions), you might need a different setup. For text generation, the output layer often has as many units as the size of the vocabulary and uses a softmax activation function to produce a probability distribution over the vocabulary for each output token.\n",
        "\n",
        "- **Embedding Layer Input Dimension**: Ensure the `input_dim` parameter of the Embedding layer matches the size of your vocabulary. This parameter seems to be set implicitly to a large value (7680000), likely intended as the `output_dim` parameter for the embedding vectors. The `input_dim` should be the size of your BPE vocabulary plus one (for padding).\n",
        "\n",
        "- **Sequence Length**: The `input_length` parameter in the Embedding layer is set to `None`, allowing for variable-length sequences. This is fine as long as all sequences are padded to the same length before training. Ensure that the sequence length matches the `maxlen` used during padding.\n",
        "\n",
        "**Potential Adjustments for a Text Generation Task**\n",
        "\n",
        "If your goal is to generate text (answers) given questions, consider the following adjustments:\n",
        "\n",
        "dense_1 = Dense(vocab_size, activation='softmax')\n",
        "Loss Function: For a model generating text, use sparse_categorical_crossentropy as your loss function, which is suitable for classification problems with multiple classes, where the targets are integers.\n",
        "\n",
        "Sequence-to-Sequence Model: Depending on your exact requirements, a sequence-to-sequence model architecture might be more appropriate. This involves using one LSTM for encoding the input sequence and another for generating the output sequence, potentially with attention mechanisms for better context capture.\n",
        "\n",
        "Example Correction for Text Generation\n",
        "If you're aiming for a model that generates text, here's a slight adjustment to your architecture for clarity:\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "\n",
        "vocab_size = 30000  # Example vocabulary size, adjust based on your actual vocabulary\n",
        "max_length = 100  # Adjust based on your padded sequence length\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_length),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(vocab_size, activation='softmax')  # Adjusted for text generation\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "**Output Layer Adjustment**\n",
        "\n",
        "Change the last Dense layer to have a size matching your vocabulary and add a softmax activation function. For example, if your vocabulary size is vocab_size, the adjustment would be:\n",
        "\n",
        "\n",
        "Ensure you adjust vocab_size and max_length to match your dataset's specifics. This setup is more aligned with a model that generates text, predicting the probability distribution of the next word in a sequence given the context.\n",
        "\n",
        "If your project has different objectives or if there are any other aspects you'd like to discuss or clarify, feel free to share more details!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HRSRTgNGwDsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling the Model"
      ],
      "metadata": {
        "id": "fx37f6oXwBLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
        "\n",
        "vocab_size = 30000  # Example vocabulary size, adjust based on your actual vocabulary\n",
        "max_length = 100  # Adjust based on your padded sequence length\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_length),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),  # Ensure LSTM returns sequences\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    TimeDistributed(Dense(vocab_size, activation='softmax'))  # Use TimeDistributed here\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIzlqB1jq4Mh",
        "outputId": "28e028f4-9bf8-4e87-b39e-5fb721a02b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 256)          7680000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 100, 256)          394240    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100, 128)          32896     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 128)          0         \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 100, 30000)        3870000   \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11977136 (45.69 MB)\n",
            "Trainable params: 11977136 (45.69 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "#### Pre-training Checklist:\n",
        "\n",
        "**1. Review Model Architecture**\n",
        "\n",
        "- Confirm Layer Configurations: Make sure each layer is configured as intended for your task. For a sequence generation model like yours, using Bidirectional(LSTM()) with return_sequences=True and a TimeDistributed(Dense()) layer is appropriate.\n",
        "- Output Layer Compatibility: The final TimeDistributed(Dense(vocab_size, activation='softmax')) layer should match your vocabulary size, ensuring the model can predict each token in the sequence.\n",
        "\n",
        "**2. Verify Data Preprocessing**\n",
        "\n",
        "- Tokenization and Encoding: Ensure your questions and answers have been correctly tokenized and encoded to integer sequences. This usually involves using a tokenizer that fits your dataset.\n",
        "- Padding: Verify that both input (questions) and output (answers) sequences are padded to the correct max_length. All sequences should have the same length to ensure consistent model input and output shapes.\n",
        "\n",
        "**3. Ensure Correct Data Split**\n",
        "\n",
        "- Training, Validation, and Test Sets: Confirm you have split your data into appropriate sets. Typically, you'd want a training set for model training, a validation set for tuning, and a test set for final evaluation.\n",
        "- Balance and Representativeness: Check that each data split is representative of the overall dataset to avoid bias.\n",
        "\n",
        "**4. Check Compilation Settings**\n",
        "\n",
        "- Loss Function: For a sequence generation task, sparse_categorical_crossentropy is suitable when your labels are integer-encoded (not one-hot encoded). Ensure this aligns with how your target data is prepared.\n",
        "- Optimizer and Metrics: Validate that you've chosen an optimizer and metrics that align with your model's goals. adam and accuracy are common choices, but ensure they fit your specific task.\n",
        "\n",
        "**5. Model Summary Review**\n",
        "\n",
        "- Use model.summary() to review your model's architecture. Confirm the number of parameters and the output shape at each layer align with your expectations.\n",
        "\n",
        "**6. Small Scale Test Run**\n",
        "\n",
        "- Consider doing a small-scale test run of your model training with a subset of your data. This can help identify potential issues early without the need for a full training cycle.\n",
        "\n",
        "**7. Hardware and Runtime Environment**\n",
        "\n",
        "- GPU Availability: Ensure you have access to a suitable GPU for training if your dataset and model are large. Training on a CPU can be significantly slower.\n",
        "- Memory Constraints: Monitor memory usage during the test run to ensure your environment has sufficient resources to handle the full training process.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nLZwgJJljKzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Check Input Data Shape\n",
        "First, confirm the shape of your padded questions (train_questions_padded) to ensure they match the expected input shape for the model. Given your model architecture, the input shape should be (None, 100) for max_length of 100."
      ],
      "metadata": {
        "id": "2rYmIR99vQ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train_questions_padded:\", train_questions_padded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsDARUHEvLrP",
        "outputId": "dad6f86b-c209-49a2-e67e-066fe29f4388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_questions_padded: (993, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Verify Target Data Shape\n",
        "If your model is designed for sequence generation with answers as targets, check the shape of your target data (train_answers_padded). For sequence-to-sequence models, the target data typically should have the same sequence length as the input data."
      ],
      "metadata": {
        "id": "UK_dmZgdvcCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train_answers_padded:\", train_answers_padded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dJJvwpbjJ6o",
        "outputId": "79aa9e28-2fa8-4b7f-d381-228b3e718ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_answers_padded: (993, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Confirm Matching Dimensions for Input and Target\n",
        "The sequence length of train_questions_padded and train_answers_padded should match the max_length specified in your model (100 in this case). Ensure both have the shape (num_samples, 100)."
      ],
      "metadata": {
        "id": "Ae3CzrX8vkGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_questions_padded.shape[1] == 100 and train_answers_padded.shape[1] == 100:\n",
        "    print(\"Input and target data are correctly shaped.\")\n",
        "else:\n",
        "    print(\"Mismatch in input or target data shapes detected.\")\n",
        "\n",
        "print(\"Shape of train_questions_padded:\", train_questions_padded.shape)\n",
        "print(\"Shape of train_answers_padded:\", train_answers_padded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_467xA7vo1z",
        "outputId": "472fca2b-92cc-4682-cd07-91d82987b1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input and target data are correctly shaped.\n",
            "Shape of train_questions_padded: (993, 100)\n",
            "Shape of train_answers_padded: (993, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Check: Model's Expected Input Shape\n",
        "To ensure your model's first layer is configured to accept the shape of your input data, you can also verify the model's expected input shape:"
      ],
      "metadata": {
        "id": "mq2FVGN7vs89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input_shape = model.layers[0].input_shape\n",
        "print(\"Model's expected input shape:\", model_input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp6b_YsqvwZs",
        "outputId": "40a64fcf-e3cb-4ff6-9a2e-461aa6965836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's expected input shape: (None, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limited data, batches and epochs to expedite learnings.\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Model definition (repeating for clarity)\n",
        "vocab_size = 30000  # Adjust based on your actual vocabulary size\n",
        "max_length = 100    # Sequence length that you have chosen for padding\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_length),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly=False  # Set to False for more efficient training, now that we're done debugging\n",
        ")\n",
        "\n",
        "# Actual training with corrected padded data\n",
        "history = model.fit(\n",
        "    train_questions_padded,\n",
        "    train_answers_padded,\n",
        "    batch_size=1,                     # Adjust as per your computational resource\n",
        "    epochs=3,                         # Set a suitable number of epochs\n",
        "    validation_split=0.1               # Use a portion of the data for validation\n",
        ")\n",
        "\n",
        "# Optionally, save your trained model\n",
        "model.save('path_to_save_your_model/my_model.h5')"
      ],
      "metadata": {
        "id": "5vBWWe7lwmoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b54e945-3150-472d-ae7e-b0fa8489a5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "893/893 [==============================] - 445s 489ms/step - loss: 7.0681 - accuracy: 0.1365 - val_loss: 6.7512 - val_accuracy: 0.1418\n",
            "Epoch 2/3\n",
            "893/893 [==============================] - 435s 487ms/step - loss: 6.7034 - accuracy: 0.1413 - val_loss: 6.8019 - val_accuracy: 0.1427\n",
            "Epoch 3/3\n",
            "893/893 [==============================] - 375s 420ms/step - loss: 6.6074 - accuracy: 0.1418 - val_loss: 6.7729 - val_accuracy: 0.1431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code cell will stop execution of subsequent cells\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass  # This will prevent the traceback from being shown\n",
        "\n",
        "raise StopExecution(\"Execution stopped by user\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9yT_ZMGCPzJ",
        "outputId": "e01c784d-c600-4d84-f6a8-138faedc22b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopExecution",
          "evalue": "Execution stopped by user",
          "traceback": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Results"
      ],
      "metadata": {
        "id": "vhP3JUujCEgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial results: Batch: 64, Epochs: 10\n",
        "# This is taking ~10 to 15 minutes per Epoch\n",
        "\n",
        "\n",
        "Epoch 1/10\n",
        "84/84 [==============================] - 1036s 12s/step - loss: 7.0193 - accuracy: 0.1689 - val_loss: 6.2323 - val_accuracy: 0.1764\n",
        "Epoch 2/10\n",
        "84/84 [==============================] - 999s 12s/step - loss: 6.2303 - accuracy: 0.1787 - val_loss: 6.1813 - val_accuracy: 0.1783\n",
        "Epoch 3/10\n",
        "84/84 [==============================] - 1005s 12s/step - loss: 6.1779 - accuracy: 0.1794 - val_loss: 6.1575 - val_accuracy: 0.1787\n",
        "Epoch 4/10\n",
        "84/84 [==============================] - 1002s 12s/step - loss: 6.1540 - accuracy: 0.1797 - val_loss: 6.1482 - val_accuracy: 0.1789\n",
        "Epoch 5/10\n",
        "84/84 [==============================] - 998s 12s/step - loss: 6.1337 - accuracy: 0.1801 - val_loss: 6.1177 - val_accuracy: 0.1793\n",
        "Epoch 6/10\n",
        "84/84 [==============================] - 958s 11s/step - loss: 6.0721 - accuracy: 0.1835 - val_loss: 6.1016 - val_accuracy: 0.1829\n",
        "Epoch 7/10\n",
        "84/84 [==============================] - 994s 12s/step - loss: 5.9832 - accuracy: 0.1921 - val_loss: 6.1179 - val_accuracy: 0.1815\n",
        "Epoch 8/10\n",
        "84/84 [==============================] - 993s 12s/step - loss: 5.9335 - accuracy: 0.1984 - val_loss: 6.1405 - val_accuracy: 0.1766\n",
        "Epoch 9/10\n",
        "84/84 [==============================] - 993s 12s/step - loss: 5.8925 - accuracy: 0.2027 - val_loss: 6.2001 - val_accuracy: 0.1674\n",
        "Epoch 10/10\n",
        "84/84 [==============================] - ETA: 0s - loss: 5.8618 - accuracy: 0.2062"
      ],
      "metadata": {
        "id": "vsXWw5KxsRBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parking Lot"
      ],
      "metadata": {
        "id": "QCMefv29CJzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Troubleshooting"
      ],
      "metadata": {
        "id": "wfT_my2qL2PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Troubleshooting\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
        "\n",
        "tf.keras.backend.clear_session()  # Clearing the session to reset any leftover state\n",
        "\n",
        "# Re-define your model here\n",
        "vocab_size = 30000  # Vocabulary size\n",
        "max_length = 100  # Adjust based on padded sequence length\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_length),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Attempt to train the model again with the dummy data\n",
        "history = model.fit(train_questions_padded, dummy_targets,\n",
        "                    batch_size=64,\n",
        "                    epochs=1,\n",
        "                    validation_split=0.1)"
      ],
      "metadata": {
        "id": "_-CO7qYo2umH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_questions_padded, test_answers_padded, batch_size=64)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "e3FaqoxZ7ifX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a small subset for a quick test\n",
        "subset_train_questions_padded = train_questions_padded[:10]\n",
        "subset_train_answers_padded = train_answers_padded[:10]\n",
        "\n",
        "# Perform a quick training cycle\n",
        "history = model.fit(subset_train_questions_padded, subset_train_answers_padded,\n",
        "                    batch_size=2,  # Small batch size for quick feedback\n",
        "                    epochs=1,  # Single epoch to minimize wait time\n",
        "                    verbose=2)  # Less verbose output\n"
      ],
      "metadata": {
        "id": "uS2umi5r1ohN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of your target data\n",
        "print(\"Shape of train_answers_padded:\", train_answers_padded.shape)\n",
        "\n",
        "# If train_answers_padded is not already 1D, you might need to adjust how you're preparing this data.\n",
        "# For a simple check and reshape (this is hypothetical and depends on your specific data preparation):\n",
        "if len(train_answers_padded.shape) > 1:\n",
        "    # Hypothetically flattening the target data if it's not in the expected 1D shape\n",
        "    # Note: This is just illustrative; you'll likely need a different approach based on your data\n",
        "    train_answers_padded = train_answers_padded.reshape(-1)\n",
        "    print(\"New shape of train_answers_padded:\", train_answers_padded.shape)\n"
      ],
      "metadata": {
        "id": "EXyp6zlq0F0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Assuming your model is compiled and ready for training\n",
        "history = model.fit(train_questions_padded, train_answers_padded,\n",
        "          batch_size=64, # Adjust based on your dataset size and memory constraints\n",
        "          epochs=100, # Set a large number and rely on EarlyStopping to halt training\n",
        "          validation_data=(validation_questions_padded, validation_answers_padded),\n",
        "          callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "                     ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)])\n"
      ],
      "metadata": {
        "id": "iPrJLZ19q4O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure y_train and y_test are of shape (batch_size, sequence_length)\n",
        "# where each element is an integer class label\n",
        "\n",
        "# Example: If y_train is currently one-hot encoded or improperly shaped, adjust it\n",
        "# This step assumes y_train needs to be converted or reshaped\n",
        "\n",
        "# If y_train is already in the correct shape, adjust the following line accordingly or skip\n",
        "\n",
        "# Check the current shape of y_train and y_test\n",
        "print(\"Before reshape, y_train shape:\", y_train.shape)\n",
        "print(\"Before reshape, y_test shape:\", y_test.shape)\n",
        "\n",
        "# Reshape or adjust y_train and y_test here based on the specific issue identified\n",
        "# This is a placeholder step - replace it with the actual adjustment needed for your data\n",
        "\n",
        "# Example: If y_train needs to be reshaped or converted, add that code here\n",
        "# Since I don't have the exact format of your y_train, I can't provide specific code without more details\n",
        "\n",
        "# After adjustment, if needed:\n",
        "print(\"After reshape, y_train shape:\", y_train.shape)\n",
        "print(\"After reshape, y_test shape:\", y_test.shape)\n",
        "\n",
        "# Continue with model training as before\n"
      ],
      "metadata": {
        "id": "9ormEAGZbncJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input data shape:\", X.shape)\n",
        "print(\"Number of batches:\", len(X) // 32)"
      ],
      "metadata": {
        "id": "kHqAvXXqio7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataframe shape:\", df_q_a.shape)\n",
        "print(df_q_a.head())"
      ],
      "metadata": {
        "id": "TghNey2JjSPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)  # Only the most common 10,000 words\n",
        "tokenizer.fit_on_texts(df_q_a['question'])\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "questions_seq = tokenizer.texts_to_sequences(df_q_a['question'])\n",
        "answers_seq = tokenizer.texts_to_sequences(df_q_a['answer'])\n",
        "\n",
        "# Pad the sequences so they are all the same length\n",
        "max_length = max(max(len(seq) for seq in questions_seq), max(len(seq) for seq in answers_seq))\n",
        "\n",
        "questions_padded = pad_sequences(questions_seq, maxlen=max_length)\n",
        "answers_padded = pad_sequences(answers_seq, maxlen=max_length)\n",
        "\n",
        "print(\"Padded questions shape:\", questions_padded.shape)\n",
        "print(\"Padded answers shape:\", answers_padded.shape)\n"
      ],
      "metadata": {
        "id": "lhC1_zyvjSRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace None with empty strings\n",
        "df_q_a['question'].fillna('', inplace=True)\n",
        "df_q_a['answer'].fillna('', inplace=True)\n",
        "\n",
        "# Assuming you have already instantiated and fit a tokenizer\n",
        "# Convert text to sequences of integers\n",
        "questions_seq = tokenizer.texts_to_sequences(df_q_a['question'])\n",
        "answers_seq = tokenizer.texts_to_sequences(df_q_a['answer'])\n",
        "\n",
        "# Pad the sequences so they are all the same length\n",
        "max_length = max(max(len(seq) for seq in questions_seq), max(len(seq) for seq in answers_seq))\n",
        "\n",
        "questions_padded = pad_sequences(questions_seq, maxlen=max_length)\n",
        "answers_padded = pad_sequences(answers_seq, maxlen=max_length)\n",
        "\n",
        "print(\"Padded questions shape:\", questions_padded.shape)\n",
        "print(\"Padded answers shape:\", answers_padded.shape)\n"
      ],
      "metadata": {
        "id": "1br2HuHpjjgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"Output y shape:\", y.shape)\n",
        "\n",
        "num_samples = X.shape[0]\n",
        "batch_size = 32\n",
        "expected_num_batches = np.ceil(num_samples / batch_size)\n",
        "\n",
        "print(\"Expected number of batches:\", expected_num_batches)\n"
      ],
      "metadata": {
        "id": "IxHYJnn4kGQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sequences of integers using the correct method for ByteLevelBPETokenizer\n",
        "questions_seq = [tokenizer.encode(question).ids for question in df_q_a['question']]\n",
        "answers_seq = [tokenizer.encode(answer).ids for answer in df_q_a['answer']]\n",
        "\n",
        "# For demonstration, let's just print the first few sequences to confirm they're correctly encoded\n",
        "print(\"First question sequence:\", questions_seq[0])\n",
        "print(\"First answer sequence:\", answers_seq[0])"
      ],
      "metadata": {
        "id": "nJzlI-IIlKva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Load the tokenized DataFrame\n",
        "df_q_a = pd.read_csv('/content/drive/MyDrive/lstm/df_q_a_tokenized.csv')\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df_q_a.dropna(inplace=True)\n",
        "\n",
        "# Convert tokenized sequences to numpy arrays\n",
        "X = np.array(eval(df_q_a['question_tokens'].values[0]))\n",
        "y = np.array(eval(df_q_a['answer_tokens'].values[0]))\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure there are no None values in the arrays\n",
        "if X_train is not None and X_test is not None and y_train is not None and y_test is not None:\n",
        "    # Define LSTM model\n",
        "    max_sequence_length = 100  # Update with your max sequence length\n",
        "    vocab_size = np.max(X_train) + 1  # Get the maximum token index\n",
        "    embedding_dim = 128  # Example value, adjust as needed\n",
        "\n",
        "    lstm_model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "        LSTM(units=64),\n",
        "        Dense(max_sequence_length, activation='softmax')  # Adjust output shape and activation function as needed\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Reshape input data for LSTM\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "    X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "    # Train the model\n",
        "    history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Optionally, you can plot training history to visualize model performance over epochs\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training accuracy\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"One or more numpy arrays contain None values. Please check your data.\")\n"
      ],
      "metadata": {
        "id": "WPQt21Jl-J76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code cell will raise an exception to stop execution of subsequent cells\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass  # This will prevent the traceback from being shown\n",
        "\n",
        "raise StopExecution(\"Execution stopped by user\")"
      ],
      "metadata": {
        "id": "vlBbN3lXERWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, y_val))  # Adjust epochs, batch_size, validation_data as needed\n",
        "\n",
        "# Optionally, you can plot training history to visualize model performance over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NclQD1rKDiOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfLt04ay4WEM"
      },
      "outputs": [],
      "source": [
        "# Hybrid Model leveraging GPT\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "lines = E9_FORUM_CORPUS['THREAD_ALL_POST'].tolist()  # This is only 100 rows\n",
        "\n",
        "# Batch tokenization\n",
        "#tokens = tokenizer(lines, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "tokens = tokenizer(lines, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\") #reducing the compute\n",
        "input_ids = tokens['input_ids']\n",
        "attention_masks = tokens['attention_mask']\n",
        "\n",
        "# Check that I have enough samples to split\n",
        "if input_ids.size(0) > 1:\n",
        "    # Use a smaller portion of the data for quicker experiments (10% of the original data)\n",
        "    _, small_train_inputs, _, small_train_masks = train_test_split(input_ids, attention_masks, test_size=0.9, random_state=42) #reducing the compute\n",
        "    _, small_val_inputs, _, small_val_masks = train_test_split(input_ids, attention_masks, test_size=0.9, random_state=42) #reducing the compute\n",
        "else:\n",
        "    # Not enough data to split, so we use what we have for both training and validation\n",
        "    small_train_inputs = input_ids\n",
        "    small_train_masks = attention_masks\n",
        "    small_val_inputs = input_ids\n",
        "    small_val_masks = attention_masks\n",
        "\n",
        "# Define the TextDataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_ids, masks):\n",
        "        self.input_ids = input_ids\n",
        "        self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.masks[idx]}\n",
        "\n",
        "# Create datasets with the smaller subset\n",
        "small_train_dataset = TextDataset(small_train_inputs, small_train_masks)\n",
        "small_val_dataset = TextDataset(small_val_inputs, small_val_masks)\n",
        "\n",
        "# Update DataLoaders with the smaller dataset\n",
        "train_loader = DataLoader(small_train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(small_val_dataset, batch_size=1)\n",
        "\n",
        "# Set up the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Experimenting with values to find steady state\n",
        "epochs = 5\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone().detach()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    training_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = input_ids.clone().detach()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    validation_losses.append(avg_val_loss)\n",
        "    print(f'Epoch {epoch}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, epochs+1), training_losses, 'bo-', label='Training loss')\n",
        "plt.plot(range(1, epochs+1), validation_losses, 'ro-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PgN-BksuWou"
      },
      "outputs": [],
      "source": [
        "# This is a summarization of the initial thread post\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Initialize the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "def summarize_text(df):\n",
        "\n",
        "    thread_first_post_summary = []  # Initialize the list to hold summaries\n",
        "    for thread_id, text in zip(df['thread_id'], df['thread_first_post']): # ensures pairing\n",
        "\n",
        "        # Prefixing the input text with \"summarize: \" as T5 expects\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        summary_ids = model.generate(inputs, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated ids to get the summary text\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Append the tuple containing thread_id and summary to the list\n",
        "        thread_first_post_summary.append((thread_id, summary))\n",
        "\n",
        "    return thread_first_post_summary\n",
        "\n",
        "# Fetch first post content and convert to DataFrame\n",
        "data = summarize_text(df_threads)\n",
        "df_short_desc = pd.DataFrame(data, columns=['thread_id', 'thread_first_post_summary'])\n",
        "\n",
        "# Merge the summarized_df with df_threads on the 'thread_id' column\n",
        "df_threads = pd.merge(df_threads, df_short_desc, on='thread_id', how='left')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a-w-75fdero"
      },
      "outputs": [],
      "source": [
        "# Gather keywords from the initial thread post\n",
        "# Keyword extraction with BERT\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract keywords using BERT\n",
        "def bert_extract_keywords(text, tokenizer, model, top_n=5):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Compute word importance by summing up the embeddings\n",
        "    word_importance = torch.sum(embeddings, dim=1)\n",
        "\n",
        "    # Get the indices of the top n important words\n",
        "    top_n_indices = word_importance.argsort(descending=True)[:top_n]\n",
        "\n",
        "    # Filter out indices that are out of range of input_ids\n",
        "    top_n_indices = [idx for idx in top_n_indices if idx < len(input_ids)]\n",
        "\n",
        "    # Decode the top n words\n",
        "    keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_n_indices]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# df_threads['keywords'] = bert_extract_keywords(df_threads['summary'],tokenizer, model)\n",
        "df_threads['thread_first_post_keywords'] = df_threads['thread_first_post_summary'].apply(lambda x: bert_extract_keywords(x, tokenizer, model))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWwiLTr0mKHO"
      },
      "outputs": [],
      "source": [
        "# Process threads and fetch all post data'\n",
        "\n",
        "# As written this will fetch all the posts on the first page, which is 20\n",
        "# This might need to be updated to iterate through all page values (1 through n)\n",
        "\n",
        "\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')  # Correct class name as example\n",
        "        for article in articles:\n",
        "            post_id = article.get('id', 'N/A')\n",
        "            numeric_post_id = re.findall(r'\\d+', post_id)[0] if re.findall(r'\\d+', post_id) else 'N/A'\n",
        "\n",
        "            if numeric_post_id not in processed_posts:\n",
        "                processed_posts.add(numeric_post_id)\n",
        "                content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "                #timestamp = article.find('time', class_='u-dt').get_text(strip=True) if article.find('time', class_='u-dt') else 'N/A'\n",
        "                #post_number_element = article.find('ul', class_='message-attribution-opposite').find('li').find_next_sibling('li')\n",
        "                #post_number = post_number_element.get_text(strip=True) if post_number_element else 'N/A'\n",
        "                #post_number = post_number.lstrip('#') if post_number != 'N/A' else post_number\n",
        "\n",
        "                post_data.append({\n",
        "                    'thread_id': row['thread_id'],  # Corrected to use row's data\n",
        "                    'post_id': numeric_post_id,\n",
        "                    #'post_number': post_number,\n",
        "                    'post_raw': content\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(post_data, columns=['thread_id', 'post_id','post_raw'])\n",
        "\n",
        "# Fetch thread URLs and titles, and store in a DataFrame\n",
        "df_posts = fetch_and_parse_thread(df_threads)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_posts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqMhFejIYcvR"
      },
      "outputs": [],
      "source": [
        "# This is a summarization of all posts for a given thread\n",
        "\n",
        "# Count the number of unique threads to ensure I dont drop any\n",
        "print(df_posts['thread_id'].nunique())\n",
        "\n",
        "aggregated_posts = df_posts.groupby(['thread_id'])['post_raw'].apply(lambda x: ' '.join(x)).reset_index(name='post_concat')\n",
        "\n",
        "df_threads = df_threads.merge(aggregated_posts, on=['thread_id'], how='left')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTlqkHV0NgmK"
      },
      "outputs": [],
      "source": [
        "# This is a summarization of all posts for a given thread\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Initialize the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "def summarize_text(df):\n",
        "    sum_text = []  # Initialize the list to hold summaries\n",
        "    for text in df['post_concat']:\n",
        "        # Ensure the text is a string and not empty\n",
        "        #if not isinstance(text, str) or not text.strip():\n",
        "        #    sum_text.append(\"\")  # Append an empty string for non-valid entries\n",
        "        #   continue\n",
        "\n",
        "        # Prefixing the input text with \"summarize: \" as T5 expects\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated ids to get the summary text\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        sum_text.append(summary)\n",
        "\n",
        "    return sum_text\n",
        "\n",
        "df_threads['post_summary'] = summarize_text(df_threads)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "def summarize_text(texts, batch_size=4):\n",
        "    sum_texts = []  # Initialize the list to hold summaries\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        input_encodings = tokenizer.batch_encode_plus(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "        # Generating summaries in batches\n",
        "        summary_ids = model.generate(input_encodings['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        for summary_id in summary_ids:\n",
        "            summary = tokenizer.decode(summary_id, skip_special_tokens=True)\n",
        "            sum_texts.append(summary)\n",
        "\n",
        "    return sum_texts\n",
        "\n",
        "# Example usage\n",
        "df_threads['post_summary'] = summarize_text(df_threads['post_concat'].tolist())"
      ],
      "metadata": {
        "id": "HDzgGEBKgPLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update to the above\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Initialize the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "def summarize_text(df):\n",
        "    sum_text = []  # Initialize the list to hold summaries\n",
        "    for text in df['post_concat']:\n",
        "        # Ensure the text is a string and not empty\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            sum_text.append(\"\")  # Append an empty string for non-valid entries\n",
        "            continue\n",
        "\n",
        "        # Prefixing the input text with \"summarize: \" as T5 expects\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated ids to get the summary text\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        sum_text.append(summary)\n",
        "\n",
        "    return sum_text\n",
        "\n",
        "# Assuming df_threads is your DataFrame and it has a column named 'post_concat'\n",
        "df_threads['post_summary'] = summarize_text(df_threads)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(df_threads.head())\n"
      ],
      "metadata": {
        "id": "7hXQiHlLRofI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWGj_PDvEb40"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract keywords using BERT\n",
        "def bert_extract_keywords(text, tokenizer, model, top_n=5):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Compute word importance by summing up the embeddings\n",
        "    word_importance = torch.sum(embeddings, dim=1)\n",
        "\n",
        "    # Get the indices of the top n important words\n",
        "    top_n_indices = word_importance.argsort(descending=True)[:top_n]\n",
        "\n",
        "    # Filter out indices that are out of range of input_ids\n",
        "    top_n_indices = [idx for idx in top_n_indices if idx < len(input_ids)]\n",
        "\n",
        "    # Decode the top n words\n",
        "    keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_n_indices]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "df_threads['post_keywords'] = df_threads['post_summary'].apply(lambda x: bert_extract_keywords(x, tokenizer, model))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t83z6mu7cvm"
      },
      "outputs": [],
      "source": [
        "# Export and save result\n",
        "df_threads.to_csv('/content/drive/MyDrive/e9/nlp/df_threads.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}