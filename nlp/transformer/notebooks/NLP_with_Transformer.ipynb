{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiopuKtPS7x6ggYTLW8i1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/nlp/transformer/notebooks/NLP_with_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Workflow"
      ],
      "metadata": {
        "id": "75JbMfbHdUiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Project Goals and Contraints:**\n",
        "\n",
        "This will be instrumental in selecting specific architecture and data processing strategies.\n",
        "\n",
        "\n",
        "**Data Cleaning and Preprocessing:**\n",
        "\n",
        "Load your data into a Pandas DataFrame.\n",
        "Perform basic cleaning: remove duplicates, handle missing values.\n",
        "Normalize text: convert to lowercase, remove punctuation, and special characters.\n",
        "\n",
        "**Text Preprocessing for BPE:**\n",
        "\n",
        "Apply BPE tokenization to your corpus. This involves learning the BPE vocab from your dataset and then applying it to both questions and answers to tokenize them.\n",
        "\n",
        "**Splitting the Dataset:**\n",
        "\n",
        "Split your data into training, validation, and test sets. A common split ratio is 80% training, 10% validation, and 10% test.\n",
        "\n",
        "**Converting Text to Sequences:**\n",
        "\n",
        "Convert your tokenized text into sequences of integers using the BPE vocabulary. This step transforms the textual data into a format that can be fed into the LSTM model.\n",
        "\n",
        "**Padding Sequences:**\n",
        "\n",
        "Since LSTM models require inputs of the same length, use padding to ensure all sequences in a batch have the same length.\n",
        "\n",
        "**Designing the LSTM Model:**\n",
        "\n",
        "Build your LSTM model architecture using TensorFlow/Keras. The model should include an Embedding layer, one or more LSTM layers, and a Dense output layer.\n",
        "\n",
        "**Compiling the Model:**\n",
        "\n",
        "Compile the model with an appropriate optimizer (e.g., Adam), loss function (e.g., sparse_categorical_crossentropy for classification tasks), and metrics (e.g., accuracy).\n",
        "\n",
        "**Training the Model:**\n",
        "\n",
        "Train the model on your training set while also validating its performance on the validation set. Use model checkpoints and early stopping to prevent overfitting.\n",
        "\n",
        "**Evaluating the Model:**\n",
        "\n",
        "After training, evaluate the model's performance on the test set to get a sense of its generalization ability.\n",
        "\n",
        "**Model Deployment:**\n",
        "\n",
        "Deploy the model into a production environment. This could be a simple web application or a REST API that takes in a question and returns the predicted answer."
      ],
      "metadata": {
        "id": "I53mkFzBdVtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals and Constraints\n",
        "\n",
        "**Goal**\n",
        "\n",
        "The goal is to create a \"virtual mechanic\" to help owners maintain older cars that have a dwindling set of experts available to turn to.\n",
        "\n",
        "\n",
        "**Task Type:**\n",
        "\n",
        "The project aims to build a generative language model that will accept written unstructured questions in English from users and provide the user with targeted written answers in English. The model will use sequence prediction and text generation. The model will not use classification, image recognition, or sentiment analysis.\n",
        "\n",
        "**Data Characteristics:**\n",
        "\n",
        "The training corpus for the data will be User Generated Content scraped from a domain-specific online forum. The corpus will generally be unstructured with a very limited set of metadata.\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "Performance of the project will be scored on accuracy and speed of responses.\n",
        "\n",
        "**Resource Constraints:**\n",
        "\n",
        "The project will be built in Python utilizing limited CPU compute resources from Google Colab.\n",
        "\n",
        "**Existing Tools or Frameworks:**\n",
        "\n",
        "The corpus will be stored in Snowflake database.\n",
        "\n",
        "**Scalability and Adaptability:**\n",
        "\n",
        "There is no need to support additional user languages. However, when available, the corpus will be supplemented with additional written unstructured text.\n"
      ],
      "metadata": {
        "id": "yj9fAsQmdhVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Creation\n",
        "\n",
        "The corpus used was assembled using Beautiful Soup to scrape a pubic forum specific to the BMW E9 (www.e9coupe.com). This active forum has been exsitence since 2003. The data was compiled and stored in a Snowflake database for multiple NLP projects, including LDA, GRU and LSTM. Furture ideas include supplementing the forum text with an existing users guide specific to this model."
      ],
      "metadata": {
        "id": "y3tNUl3yfwv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Language Model Architectures\n",
        "\n",
        "### Recurrent Neural Networks (RNNs):\n",
        "\n",
        "**Pros:**\n",
        "1. *Sequential Processing:* RNNs process sequential data efficiently, making them suitable for tasks like text generation where the order of input elements matters.\n",
        "2. *Memory:* RNNs have a form of memory that allows them to remember past information while processing current inputs.\n",
        "3. *Interpretability:* Due to their sequential nature, RNNs are often more interpretable compared to more complex architectures like Transformers.\n",
        "4. *Ease of Development:* RNNs have been around for longer and have a simpler architecture compared to Transformers, making them easier to develop and understand for beginners.\n",
        "5. *CPU Needs:* RNNs can be trained and run on CPU instances, although training large models or processing large datasets may benefit from GPU acceleration.\n",
        "\n",
        "**Cons:**\n",
        "1. *Vanishing/Exploding Gradient:* RNNs can suffer from vanishing or exploding gradient problems, especially when dealing with long sequences, which can lead to difficulties in learning long-term dependencies.\n",
        "2. *Limited Context:* Traditional RNNs have a limited memory span, making them less effective at capturing long-range dependencies in data.\n",
        "3. *Computationally Inefficient:* Training RNNs can be computationally expensive, especially when dealing with large datasets and long sequences.\n",
        "\n",
        "### Transformer Architectures:\n",
        "\n",
        "**Pros:**\n",
        "1. *Parallelization:* Transformers allow for highly parallelized computation, leading to faster training and inference compared to sequential models like RNNs.\n",
        "2. *Long-Range Dependencies:* Transformers can capture long-range dependencies in data more effectively than traditional RNNs, making them well-suited for tasks requiring global context, such as machine translation and text generation.\n",
        "3. *Attention Mechanism:* Transformers use attention mechanisms to weigh the importance of different input elements, allowing them to focus on relevant information and ignore irrelevant parts of the input sequence.\n",
        "4. *Ease of Development:* While more complex than RNNs, Transformers have a modular architecture that can be easier to develop and experiment with compared to traditional recurrent architectures.\n",
        "\n",
        "**Cons:**\n",
        "1. *Complexity:* Transformers have a more complex architecture compared to RNNs, which can make them harder to understand, implement, and interpret.\n",
        "2. *Data Requirements:* Transformers require large amounts of data to train effectively, especially for tasks with complex patterns and dependencies.\n",
        "3. *Resource Intensive:* Training large transformer models requires significant computational resources, including powerful GPUs or TPUs, making them less accessible for smaller-scale projects or individuals with limited resources.\n",
        "\n",
        "### Hybrid Model (Combining RNNs and Transformers):\n",
        "\n",
        "**Pros:**\n",
        "1. *Combine Strengths:* A hybrid model can potentially combine the strengths of both RNNs and Transformers, leveraging the sequential processing capabilities of RNNs with the long-range dependency handling of Transformers.\n",
        "2. *Flexibility:* A hybrid approach offers flexibility in model design, allowing researchers and practitioners to tailor the architecture to specific task requirements and data characteristics.\n",
        "\n",
        "**Cons:**\n",
        "1. *Complexity:* Developing and training a hybrid model can be more complex compared to using either RNNs or Transformers alone, as it requires integration of different architectural components and potentially more sophisticated training procedures.\n",
        "2. *Resource Intensive:* Depending on the specific architecture and scale, training a hybrid model may require significant computational resources, similar to Transformers.\n"
      ],
      "metadata": {
        "id": "JsvPl3QPd_ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization Strategies\n",
        "\n",
        "#### Word-Level Tokenization:\n",
        "\n",
        "**Description:**\n",
        "Word-level tokenization splits the text into individual words, treating each word as a token.\n",
        "\n",
        "**Libraries:**\n",
        "1. NLTK (Natural Language Toolkit): Provides tokenization tools for various NLP tasks, including word-level tokenization.\n",
        "2. spaCy: Another popular NLP library that offers word-level tokenization along with other NLP functionalities.\n",
        "\n",
        "**Pros:**\n",
        "1. Preserves semantic meaning of individual words.\n",
        "2. Intuitive representation of text for language modeling tasks.\n",
        "\n",
        "**Cons:**\n",
        "1. May struggle with out-of-vocabulary words, especially in domain-specific or informal language.\n",
        "2. Increases vocabulary size, potentially leading to higher memory usage.\n",
        "\n",
        "**Suitability:**\n",
        "Word-level tokenization may be suitable for this project as it preserves the semantic meaning of individual words, which can be important for generating coherent responses to user questions.\n",
        "\n",
        "#### Character-Level Tokenization:\n",
        "\n",
        "**Description:**\n",
        "Character-level tokenization treats each character in the text as a separate token.\n",
        "\n",
        "**Libraries:**\n",
        "1. TensorFlow Text: Part of the TensorFlow ecosystem, TensorFlow Text provides utilities for various text processing tasks, including character-level tokenization.\n",
        "2. Keras: With its text preprocessing module, Keras offers character-level tokenization capabilities.\n",
        "\n",
        "**Pros:**\n",
        "1. Captures fine-grained details in the text, useful for handling misspellings or morphologically complex words.\n",
        "2. Helps in handling out-of-vocabulary terms effectively.\n",
        "\n",
        "**Cons:**\n",
        "1. Can be computationally expensive due to larger token vocabulary.\n",
        "2. May not capture higher-level semantic meaning as effectively as word-level tokenization.\n",
        "\n",
        "**Suitability:**\n",
        "Character-level tokenization might not be the best choice for this project, as it may not capture the semantic meaning of words effectively. However, it could be useful for capturing fine-grained details in the text if necessary.\n",
        "\n",
        "#### Byte Pair Encoding (BPE):\n",
        "\n",
        "**Description:**\n",
        "Byte Pair Encoding (BPE) tokenization iteratively merges the most frequent pairs of tokens to build a vocabulary of subword units.\n",
        "\n",
        "**Libraries:**\n",
        "1. Hugging Face Transformers: Provides tokenization functionalities, including BPE, along with pre-trained language models for various NLP tasks.\n",
        "2. Tokenizers: A Python library specifically designed for fast and customizable tokenization, including BPE tokenization.\n",
        "\n",
        "**Pros:**\n",
        "1. Handles rare or out-of-vocabulary terms effectively.\n",
        "2. Offers a good balance between accuracy and efficiency.\n",
        "\n",
        "**Cons:**\n",
        "1. Requires additional pre-processing steps compared to traditional tokenization methods.\n",
        "2. Increases complexity of tokenization process, potentially impacting speed.\n",
        "\n",
        "**Suitability:**\n",
        "BPE tokenization could be a good choice for this project as it effectively handles rare or out-of-vocabulary terms, which may be present in the user-generated content scraped from online forums. It also offers good balance between accuracy and speed, which aligns with the project's performance metrics and resource constraints.\n"
      ],
      "metadata": {
        "id": "cdwKiRyleAqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization Strategies\n",
        "Very difficult to find a winning strategy here that can accomidate both long and short length text blocks.\n",
        "\n",
        "**Extractive Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Good with Raw Text: Extractive methods can work directly with raw, unstructured text, as they mainly focus on selecting key sentences or phrases without needing deep linguistic processing.\n",
        "Straightforward Implementation: These methods do not require complex preprocessing like tokenization or lemmatization, simplifying their implementation.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Limited Depth in Understanding: While they can handle raw text, they may not fully capture the nuanced meaning, especially when the text contains complex structures or unorthodox language use.\n",
        "Less Effective with Poorly Structured Text: In cases where the text is poorly structured or highly informal, extractive summarization might struggle to identify the main points effectively.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**Abstractive Summarization** (like sshleifer/distilbart-cnn-12-6)\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Advanced Processing Capabilities: Abstractive models, especially those based on transformer architectures, are designed to handle and interpret raw text, capturing deeper linguistic and contextual nuances.\n",
        "Higher Tolerance for Unstructured Text: These models can manage unstructured or informal text by understanding and then rephrasing it in a more coherent and structured summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Dependence on Preprocessing for Optimal Performance: While they can process raw text, the quality of the output can be significantly improved with proper tokenization and lemmatization, especially for complex texts.\n",
        "Potential Overhead: Requires more computational resources to process and understand raw text, which might be more efficiently handled with some level of preprocessing.\n",
        "<br>\n",
        "<br>\n",
        "**Hybrid Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Flexibility in Text Processing: Combining extractive and abstractive methods allows for handling both raw and preprocessed text, adapting to the text's structure and complexity.\n",
        "Balanced Approach: Can leverage the strengths of extractive methods in handling raw text for identifying key points, while using abstractive techniques for generating a coherent summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Complex Preprocessing Requirements: The need to integrate both extractive and abstractive approaches may necessitate more sophisticated preprocessing strategies to optimize performance.\n",
        "Potential for Processing Inefficiencies: The combined approach might lead to redundancies or inefficiencies in processing, especially if the text is either too raw or overly preprocessed.\n",
        "<br>\n",
        "<br>\n",
        "After attempting sshleifer/distilbart-cnn-12-6 I found it had a character limit (1024) that is too restrictive for my needs. However, T5 has no limits and is what Ill be trying."
      ],
      "metadata": {
        "id": "TtjKY0SUj-bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Designing the Model\n",
        "\n",
        "This is a project to learn how to build a model using transformers.In the context of artificial intelligence, a transformer is a type of deep learning model that has revolutionized the field of natural language processing (NLP) and beyond. Introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017, transformers are designed to handle sequential data, like text, in a more efficient and effective way compared to previous models like recurrent neural networks (RNNs) and long short-term memory networks (LSTMs).\n",
        "\n",
        "Transformers are characterized by their use of self-attention mechanisms, which allow them to weigh the importance of different parts of the input data differently. This is particularly useful in understanding the context and relationships between words in a sentence or elements in a sequence, without being constrained by the sequence's order or proximity.\n",
        "\n",
        "Key features and advantages of transformers include:\n",
        "\n",
        "**Parallelization:**\n",
        "\n",
        "Unlike RNNs and LSTMs, which process data sequentially, transformers can process entire sequences of data in parallel. This significantly reduces training times and allows for more efficient computation.\n",
        "\n",
        "**Scalability:**\n",
        "\n",
        "Transformers can be scaled up with more layers and parameters to handle larger datasets and more complex tasks, making them highly effective for a wide range of applications.\n",
        "\n",
        "**Flexibility:**\n",
        "\n",
        "They can be adapted for a variety of tasks beyond text processing, including image recognition, audio processing, and even tasks in other domains like genomics.\n",
        "\n",
        "**Self-Attention:**\n",
        "\n",
        "The self-attention mechanism allows transformers to consider the entire context of a sequence when processing each element, leading to a better understanding of the data.\n",
        "\n",
        "**Transfer Learning:**\n",
        "\n",
        "Models based on transformers, such as BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pretrained Transformer), and others, have shown remarkable performance in transfer learning, where a model trained on one task can be adapted for another related task with minimal additional training. Transformers are now a foundational element in the development of state-of-the-art AI systems for natural language understanding, text generation, translation, and more, pushing the boundaries of what's possible in AI research and applications."
      ],
      "metadata": {
        "id": "mIp-U04ReSdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "#### Pre-training Checklist:\n",
        "\n",
        "**1. Review Model Architecture**\n",
        "\n",
        "- Confirm Layer Configurations: Make sure each layer is configured as intended for your task. For a sequence generation model like yours, using Bidirectional(LSTM()) with return_sequences=True and a TimeDistributed(Dense()) layer is appropriate.\n",
        "- Output Layer Compatibility: The final TimeDistributed(Dense(vocab_size, activation='softmax')) layer should match your vocabulary size, ensuring the model can predict each token in the sequence.\n",
        "\n",
        "**2. Verify Data Preprocessing**\n",
        "\n",
        "- Tokenization and Encoding: Ensure your questions and answers have been correctly tokenized and encoded to integer sequences. This usually involves using a tokenizer that fits your dataset.\n",
        "- Padding: Verify that both input (questions) and output (answers) sequences are padded to the correct max_length. All sequences should have the same length to ensure consistent model input and output shapes.\n",
        "\n",
        "**3. Ensure Correct Data Split**\n",
        "\n",
        "- Training, Validation, and Test Sets: Confirm you have split your data into appropriate sets. Typically, you'd want a training set for model training, a validation set for tuning, and a test set for final evaluation.\n",
        "- Balance and Representativeness: Check that each data split is representative of the overall dataset to avoid bias.\n",
        "\n",
        "**4. Check Compilation Settings**\n",
        "\n",
        "- Loss Function: For a sequence generation task, sparse_categorical_crossentropy is suitable when your labels are integer-encoded (not one-hot encoded). Ensure this aligns with how your target data is prepared.\n",
        "- Optimizer and Metrics: Validate that you've chosen an optimizer and metrics that align with your model's goals. adam and accuracy are common choices, but ensure they fit your specific task.\n",
        "\n",
        "**5. Model Summary Review**\n",
        "\n",
        "- Use model.summary() to review your model's architecture. Confirm the number of parameters and the output shape at each layer align with your expectations.\n",
        "\n",
        "**6. Small Scale Test Run**\n",
        "\n",
        "- Consider doing a small-scale test run of your model training with a subset of your data. This can help identify potential issues early without the need for a full training cycle.\n",
        "\n",
        "**7. Hardware and Runtime Environment**\n",
        "\n",
        "- GPU Availability: Ensure you have access to a suitable GPU for training if your dataset and model are large. Training on a CPU can be significantly slower.\n",
        "- Memory Constraints: Monitor memory usage during the test run to ensure your environment has sufficient resources to handle the full training process.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BUlaEHhKfi9i"
      }
    }
  ]
}