{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAVQFX2kIWSSwx9eijq14r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/NLP_corpus_creation%20/noteboooks/NLP_Corpus_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozT9arDzlfUA"
      },
      "source": [
        " Data Collection and Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fWO8xyGhyy0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "# Data Collection\n",
        "import os\n",
        "\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "\n",
        "!pip3 install requests\n",
        "import requests\n",
        "\n",
        "!pip3 install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "\n",
        "# Data Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip3 install numpy\n",
        "import numpy as np\n",
        "\n",
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n"
      ],
      "metadata": {
        "id": "phiTC3nry3T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection\n"
      ],
      "metadata": {
        "id": "t_ACGDp-y8w-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "\n",
        "### Web Scraping\n",
        "- **Tools:** Beautiful Soup, online SaaS products\n",
        "    - **Pros:**\n",
        "        - **Direct Access to Targeted Data:** Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "        - **Efficiency in Data Collection:** Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.\n",
        "    - **Cons:**\n",
        "        - **Potential for Incomplete Data:** May miss embedded content or dynamically loaded data, depending on the websiteâ€™s structure.\n",
        "        - **Ethical and Legal Considerations:** Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "        - **Very Platform Dependent:** Forum specific solutions result in forum specific data schemas that must be reverse engineered to for successful text extraction.\n",
        "\n",
        "### Forum-specific APIs\n",
        "- **Tools:** Python (`requests` library for API calls and `json` library for handling responses)\n",
        "    - **Pros:**\n",
        "        - **Structured and Reliable Data Retrieval:** APIs provide structured data, making it easier to process and integrate into your project.\n",
        "        - **Efficient and Direct Access:** Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "        - **Compliance and Ethical Data Use:** Utilizing APIs respects the forum's data use policies and ensures access is in line with user agreements.\n",
        "    - **Cons:**\n",
        "        - **Rate Limiting:** APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "        - **API Changes:** Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "        - **Access Restrictions:** Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Beautiful Soup to create my corpus.**\n"
      ],
      "metadata": {
        "id": "egcgaDGAzB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the list of thread_ids to scrape and parse\n",
        "# There are currently approximately 15k threads\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_thread_ids.csv'\n",
        "\n",
        "# Set the number of incremental thread_ids to process\n",
        "threads = 500\n",
        "\n",
        "# Check if the file exists and has content. If it does, update last_thread_id\n",
        "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "    e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "    last_thread_id = e9_forum_thread_ids['thread_id'].iloc[-1]\n",
        "    last_thread_id = int(last_thread_id)  # Convert to integer\n",
        "\n",
        "else:\n",
        "    last_thread_id = 0\n",
        "\n",
        "# Function to create URLs from the thread_ids\n",
        "def create_urls(threads, last_thread_id):\n",
        "    urls = []\n",
        "    for thread_id in range(last_thread_id + 1, last_thread_id + threads + 1):\n",
        "        urls.append({'thread_id': thread_id})\n",
        "    return urls\n",
        "\n",
        "urls = create_urls(threads, last_thread_id)\n",
        "\n",
        "last_thread_id_processed = urls[-1]['thread_id']\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "e9_forum_thread_ids = pd.DataFrame(urls)\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "e9_forum_thread_ids.to_csv(file_path, mode='a', header=['thread_id'], index=False)\n",
        "\n",
        "print(\"Starting with thread_id \" + str(last_thread_id))\n",
        "print(\"Processing additional \" + str(threads) + \" threads\")\n",
        "print(\"Ending with thread_id \" + str(last_thread_id_processed))"
      ],
      "metadata": {
        "id": "FZ5yYeiWzFzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the URL and title for each thread\n",
        "\n",
        "pages = 1\n",
        "\n",
        "def fetch_thread_data(df, pages=1):\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fetch thread URLs and title\n",
        "e9_forum_threads = fetch_thread_data(e9_forum_thread_ids)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_threads.csv'\n",
        "\n",
        "header = ['thread_id', 'thread_title', 'thread_url']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads.to_csv(file_path, mode='a', header=header, index=False)"
      ],
      "metadata": {
        "id": "sJGl0qJRzHk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the first post in the thread creation\n",
        "# I may use this as part of the question portion of the RAG\n",
        "\n",
        "def fetch_first_post_content(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        if first_post:\n",
        "            post_content = first_post.get_text(strip=True)\n",
        "        else:\n",
        "            post_content = \"No content found\"  # Handle case where no post content is found\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_first_post': post_content})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Fetch first post content\n",
        "data = fetch_first_post_content(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_threads_decorated = pd.DataFrame(data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_threads_decorated.csv'\n",
        "\n",
        "header = not os.path.exists(file_path)\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads_decorated.to_csv(file_path, mode='a', header=header, index=False)"
      ],
      "metadata": {
        "id": "UMzKa9dfzKoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all posts associated with each thread\n",
        "\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            # Extracting post timestamp instead of post ID\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "            post_data.append({\n",
        "                'thread_id': row['thread_id'],\n",
        "                'post_timestamp': post_timestamp,\n",
        "                'post_raw': content\n",
        "            })\n",
        "\n",
        "    return post_data\n",
        "\n",
        "# Fetch all thread post content\n",
        "post_data = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_posts = pd.DataFrame(post_data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = ('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_posts.csv')\n",
        "\n",
        "header = ['thread_id', 'post_timestamp','post_raw']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_posts.to_csv(file_path, mode='a', header=header, index=False)"
      ],
      "metadata": {
        "id": "V3Hc3ujNzMm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the corpus by aggregating all posts into one column\n",
        "# and merging with the threads df\n",
        "\n",
        "# Group by THREAD_ID and concatenate the POST_RAW values\n",
        "aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Rename the column to indicate that it contains concatenated post content\n",
        "aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "# Cast 'thread_id' column to int64 in both DataFrames\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype('int64')\n",
        "aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "# Merge the two DataFrames\n",
        "e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_corpus.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus.csv', index=False)"
      ],
      "metadata": {
        "id": "ZUDiJ_gTzO9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the db and schema in Snowfake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create a database for the corpus and load the tables\n",
        "try:\n",
        "    # Create a new database\n",
        "    cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "\n",
        "    # Use the new database\n",
        "    cur.execute(\"USE DATABASE e9_corpus\")\n",
        "\n",
        "    # Create a new schema\n",
        "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "\n",
        "    print(\"Database and schema created successfully.\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "cur.close()\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "8F5kWA9szREI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to Snowflake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Check if the table exists\n",
        "try:\n",
        "    cur.execute(\"SELECT 1 FROM e9_corpus.e9_corpus_schema.e9_forum_corpus LIMIT 1\")\n",
        "    table_exists = True\n",
        "except snowflake.connector.errors.ProgrammingError:\n",
        "    table_exists = False\n",
        "\n",
        "# If the table does not exist, create it\n",
        "if not table_exists:\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            thread_id NUMBER(38,0),\n",
        "            thread_title VARCHAR(16777216),\n",
        "            thread_first_post VARCHAR(16777216),\n",
        "            thread_all_posts VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# Insert data into e9_forum_corpus table\n",
        "for index, row in e9_forum_corpus.iterrows():\n",
        "\n",
        "    row = row.where(pd.notnull(row), None)\n",
        "\n",
        "    # Prepare the INSERT command with placeholders for the values\n",
        "    insert_command = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (thread_id, thread_title, thread_first_post, thread_all_posts)\n",
        "    VALUES\n",
        "    (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the row values as parameters to safely insert the data\n",
        "    cur.execute(insert_command, (row['thread_id'], row['thread_title'], row['thread_first_post'], row['thread_all_posts']))\n",
        "    conn.commit()\n",
        "\n",
        "print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "h5X-iiE4zTCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm dataset in Snowflake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Select source data\n",
        "query = \"\"\"\n",
        "SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "# Load data into a df.\n",
        "e9_forum_corpus = cur.fetch_pandas_all()\n",
        "e9_forum_corpus.info()"
      ],
      "metadata": {
        "id": "yUsS-xY5zUyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}