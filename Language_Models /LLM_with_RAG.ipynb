{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/Language_Models%20/LLM_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLMB-P30ER2F"
      },
      "source": [
        "# Project Objective and Limitations\n",
        "\n",
        "## i. Project Overview\n",
        "The advent of modern automobile manufacturing has led to increased technical complexity, often resulting in mechanics opting to replace parts rather than diagnose and fix issues. This approach, while convenient for contemporary vehicles, poses a significant challenge for classic cars built 30 to 40 years ago, where replacement parts are scarce or non-existent.\n",
        "\n",
        "To address this problem, this project aims to leverage Generative AI to create a \"virtual mechanic.\" By building a corpus gathered from a classic car forum, this tool will be capable of understanding unstructured questions and providing relevant answers. This solution aims to assist classic car enthusiasts and mechanics by offering expert guidance, thereby preserving the heritage and functionality of vintage automobiles.\n",
        "\n",
        "## ii. Objectives\n",
        "The primary objective of this project is the development of a Natural Language Processing (NLP) model as part of a portfolio of AI projects that can be showcased to potential employers. This will include an outline of the necessary workflow with a comparison and selection of architectures, libraries, and methods.\n",
        "\n",
        "## iii. Use Case\n",
        "With this code, a user will be able to ask questions in plain, unstructured English and receive answers that are driven from previous similar questions. Users will see these answers in plain English. I will have control over the extent to which the answers are sourced from the supplemental corpus versus the pre-trained model.\n",
        "\n",
        "## iv. Limitations and Challenges\n",
        "To address budget constraints, a combination of open source and free resources will be used. Python will be the primary programming language. Google Colab will be used for the notebook with compute resources limited to CPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-jOHmOOCkEvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d44501-1ad5-48cb-8ee6-0a985d5a63eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Access to Google Drive\n",
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "NCSnoYOsUFmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8867c6f4-d951-47e1-fe3d-b00d67fdda26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: tensorflow-text==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.10.0) (0.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.10.0) (2.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "# Step Prepare Enviornment\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Suppress DeprecationWarnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# transformers\n",
        "try:\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "except ImportError:\n",
        "    !pip install transformers\n",
        "    from transformers import (\n",
        "        BertTokenizer, BertModel, pipeline, AutoTokenizer, DistilBertModel,\n",
        "        T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel,\n",
        "        GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSeq2SeqLM\n",
        "    )\n",
        "\n",
        "# gensim\n",
        "try:\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "except ImportError:\n",
        "    !pip install gensim\n",
        "    from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "# sumy\n",
        "try:\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "except ImportError:\n",
        "    !pip install sumy\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "\n",
        "# datasets\n",
        "#try:\n",
        "#    from datasets import load_metric\n",
        "#except ImportError:\n",
        "#    !pip install datasets\n",
        "#    from datasets import load_metric\n",
        "\n",
        "# evaluate\n",
        "#try:\n",
        "#    import evaluate\n",
        "#except ImportError:\n",
        "#    !pip install evaluate\n",
        "#    import evaluate\n",
        "\n",
        "# pyspellchecker\n",
        "try:\n",
        "    from spellchecker import SpellChecker\n",
        "except ImportError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install pyspellchecker\n",
        "    from spellchecker import SpellChecker\n",
        "\n",
        "# faiss\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    !pip install faiss-cpu\n",
        "    import faiss\n",
        "\n",
        "# snowflake.connector\n",
        "try:\n",
        "    import snowflake.connector\n",
        "except ImportError:\n",
        "    !pip install snowflake-connector-python\n",
        "    import snowflake.connector\n",
        "\n",
        "# pandas\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    !pip install pandas\n",
        "    import pandas as pd\n",
        "\n",
        "# requests\n",
        "try:\n",
        "    import requests\n",
        "except ImportError:\n",
        "    !pip install requests\n",
        "    import requests\n",
        "\n",
        "# BeautifulSoup\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "except ImportError:\n",
        "    !pip install beautifulsoup4\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "# nltk\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "except ImportError:\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# re\n",
        "try:\n",
        "    import re\n",
        "except ImportError:\n",
        "    !pip install re\n",
        "    import re\n",
        "\n",
        "# langdetect\n",
        "try:\n",
        "    from langdetect import detect\n",
        "except ImportError:\n",
        "    !pip install langdetect\n",
        "    from langdetect import detect\n",
        "\n",
        "# torch\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    !pip install torch\n",
        "    import torch\n",
        "\n",
        "# numpy\n",
        "try:\n",
        "    import numpy as np\n",
        "except ImportError:\n",
        "    !pip install numpy\n",
        "    import numpy as np\n",
        "\n",
        "# pyLDAvis\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError:\n",
        "    !pip install pyLDAvis\n",
        "    import pyLDAvis\n",
        "\n",
        "# pickle\n",
        "try:\n",
        "    import pickle\n",
        "except ImportError:\n",
        "    !pip install pickle\n",
        "    import pickle\n",
        "\n",
        "# sklearn\n",
        "try:\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except ImportError:\n",
        "    !pip install scikit-learn\n",
        "    from sklearn.decomposition import LatentDirichletAllocation\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Other necessary imports\n",
        "import time\n",
        "import itertools\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "\n",
        "# Install specific compatible versions of TensorFlow and TensorFlow Text\n",
        "!pip install tensorflow==2.10.0 tensorflow-text==2.10.0\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOUiNzICFDj8"
      },
      "source": [
        "# 1 Architectures and Frameworks\n",
        "\n",
        "This document provides an overview of various architectures, models, and tools used in natural language processing tasks. Understanding the strengths and weaknesses of different approaches is crucial for designing effective NLP systems tailored to my specific use case and requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VkM_EQvV6sw"
      },
      "source": [
        "\n",
        "## 1.1 NLP Architectures\n",
        "\n",
        "### 1.1.1 Traditional Models\n",
        "\n",
        "**Solution:** Bag-of-Words (BoW)  \n",
        "**Description:** Represents text data as a collection of unique words and their frequencies.  \n",
        "**Example:** TfidfVectorizer  \n",
        "**Pros:**  \n",
        "- Simple and efficient representation.\n",
        "- Works well for tasks like sentiment analysis and document classification.\n",
        "\n",
        "**Cons:**  \n",
        "- Ignores word order and context.\n",
        "- Doesn't capture semantic meanings well.  \n",
        "\n",
        "**Solution:** N-gram Model  \n",
        "**Description:** Represents text data as a sequence of N consecutive words (N-grams).  \n",
        "**Examples:** Bigram, Trigram  \n",
        "**Pros:**  \n",
        "- Captures some local word order and context.\n",
        "- Simple and easy to implement.\n",
        "\n",
        "**Cons:**  \n",
        "- Limited in capturing long-range dependencies.\n",
        "- Can become computationally expensive with larger N values.\n",
        "\n",
        "**Solution:** Rule-Based Models  \n",
        "**Description:** Uses a set of manually crafted linguistic rules to process text data.  \n",
        "**Examples:** Regular Expressions, SpaCy Rule-Based Matching  \n",
        "**Pros:**  \n",
        "- High precision for well-defined tasks.\n",
        "- Transparent and interpretable.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires extensive domain knowledge and manual effort.\n",
        "- Not scalable for large or diverse datasets.\n",
        "\n",
        "### 1.1.2 Statistical NLP Models\n",
        "\n",
        "**Solution:** Hidden Markov Models (HMM)  \n",
        "**Description:** Sequential text models based on hidden state transitions.  \n",
        "**Example:** hmmlearn  \n",
        "**Pros:**  \n",
        "- Captures sequential dependencies effectively.\n",
        "- Suitable for tasks like part-of-speech tagging and named entity recognition.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- May struggle with capturing complex semantic relationships.\n",
        "\n",
        "**Solution:** Conditional Random Fields (CRF)  \n",
        "**Description:** Sequence labeling models.  \n",
        "**Example:** sklearn-crfsuite  \n",
        "**Pros:**  \n",
        "- Effective for sequential labeling tasks.\n",
        "- Incorporates feature dependencies between adjacent labels.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires labeled sequential data for training.\n",
        "- Less effective for capturing long-range dependencies.\n",
        "\n",
        "**Solution:** Support Vector Machines (SVM)  \n",
        "**Description:** A supervised learning model used for classification and regression analysis.  \n",
        "**Example:** scikit-learn  \n",
        "**Pros:**  \n",
        "- Effective in high-dimensional spaces.\n",
        "- Versatile with different kernel functions for flexibility in decision boundaries.  \n",
        "\n",
        "**Cons:**  \n",
        "- Memory-intensive for large datasets.\n",
        "- May require careful selection of kernel functions and tuning parameters.\n",
        "\n",
        "### 1.1.3 Deep Learning Models\n",
        "\n",
        "**Solution:** Word Embeddings  \n",
        "**Description:** Represent words as dense vectors in a continuous vector space.  \n",
        "**Examples:** Word2Vec, GloVe  \n",
        "**Pros:**  \n",
        "- Captures semantic meanings and relationships between words.\n",
        "- Provides dense vector representations suitable for downstream tasks.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of data for training.\n",
        "- Struggles with out-of-vocabulary words.\n",
        "\n",
        "**Solution:** Recurrent Neural Networks (RNN)  \n",
        "**Description:** Neural networks that process sequences by iterating through elements.  \n",
        "**Examples:** Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU)  \n",
        "**Pros:**  \n",
        "- Effective for capturing sequential dependencies in data.\n",
        "- Suitable for tasks like language modeling and machine translation.\n",
        "\n",
        "**Cons:**  \n",
        "- Vulnerable to vanishing and exploding gradient problems.\n",
        "- Computationally expensive to train.\n",
        "\n",
        "**Solution:** Convolutional Neural Networks (CNN) for Text  \n",
        "**Description:** Application of convolution operations to capture local dependencies in text.  \n",
        "**Examples:** TextCNN, KimCNN  \n",
        "**Pros:**  \n",
        "- Effective for tasks like sentence classification and text categorization.\n",
        "- Captures local patterns and relationships in text.  \n",
        "\n",
        "**Cons:**  \n",
        "- May not capture long-range dependencies as effectively as other solutions.\n",
        "- Requires careful tuning of convolutional filters and pooling strategies.\n",
        "\n",
        "### 1.1.4 Transformers\n",
        "\n",
        "**Solution:** Transformer Models  \n",
        "**Description:** Neural network architecture based entirely on self-attention mechanisms.  \n",
        "**Examples:** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer)  \n",
        "**Pros:**  \n",
        "- Captures long-range dependencies effectively.\n",
        "- Parallelizable training process.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires large amounts of computational resources.\n",
        "- Limited interpretability compared to traditional models.\n",
        "\n",
        "**Solution:** Pre-trained Models  \n",
        "**Description:** Models pre-trained on large corpora and fine-tuned for specific tasks.  \n",
        "**Examples:** BERT, GPT, T5  \n",
        "**Pros:**  \n",
        "- Leverage large amounts of unlabeled data for pre-training.\n",
        "- Achieve state-of-the-art performance on various NLP tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Resource-intensive pre-training process.\n",
        "- May require substantial computational resources for fine-tuning.\n",
        "\n",
        "**Solution:** Attention Mechanisms  \n",
        "**Description:** Mechanisms that enable models to focus on specific parts of the input.  \n",
        "**Examples:** Self-Attention, Multi-Head Attention  \n",
        "**Pros:**  \n",
        "- Improves the ability to capture dependencies and relationships within the data.\n",
        "- Enhances performance in various machine translation and text summarization.\n",
        "\n",
        "**Cons:**  \n",
        "- Can be computationally intensive.\n",
        "- Complexity increases with the number of attention heads and layers.\n",
        "\n",
        "### 1.1.5 Additional Models and Techniques\n",
        "\n",
        "**Solution:** Retriever-Generator Models  \n",
        "**Description:** Models combine retrieval and generation components for text generation tasks.  \n",
        "**Examples:** RAG  \n",
        "**Pros:**  \n",
        "- Incorporates both structured and unstructured information for generation.\n",
        "- Produces more diverse and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires efficient retrieval mechanisms.\n",
        "- Increased complexity in model architecture.\n",
        "\n",
        "**Solution:** Knowledge-Enhanced Retrieval-Augmented Generation (KERAG)  \n",
        "**Description:** A variant of RAG that incorporates knowledge graphs.  \n",
        "**Examples:** Graph-BERT  \n",
        "**Pros:**  \n",
        "- Integrates structured knowledge for improved understanding and generation.\n",
        "- Enables more coherent and contextually relevant responses.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires high-quality and curated knowledge graphs.\n",
        "- Increased computational complexity compared to standard RAG.\n",
        "\n",
        "**Solution:** Elastic Search  \n",
        "**Description:** Distributed search and analytics engine for indexing and searching big data.  \n",
        "**Examples:** Elasticsearch, Apache Solr  \n",
        "**Pros:**  \n",
        "- Scalable and distributed architecture.\n",
        "- Supports full-text search and complex query structures.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires infrastructure for deployment and maintenance.\n",
        "- Indexing and search performance may degrade with large datasets.\n",
        "\n",
        "## Architecture Options Score Card\n",
        "\n",
        "| Model/Architecture  | Key Strength                   | CPU Compatibility | Ease of Use | Performance & Accuracy | Scalability | Integration | Total |\n",
        "|---------------------|-------------------------------|-------------------|-------------|------------------------|-------------|-------------|-------|\n",
        "| RAG                 | Context Understanding          | 1                 | 1           | 2                      | 2           | 2           | 8     |\n",
        "| BoW                 | Simplicity                     | 2                 | 2           | 0                      | 2           | 2           | 8     |\n",
        "| N-gram Model        | Local Context                  | 2                 | 2           | 1                      | 1           | 2           | 8     |\n",
        "| Pre-trained Models  | Accuracy                       | 1                 | 1           | 1                      | 2           | 2           | 7     |\n",
        "| Word Embeddings     | Semantic Understanding         | 1                 | 1           | 2                      | 1           | 2           | 7     |\n",
        "| Elastic Search      | Scalability                    | 2                 | 1           | 1                      | 2           | 1           | 7     |\n",
        "| CNN                 | Local Pattern Recognition      | 1                 | 1           | 2                      | 1           | 1           | 6     |\n",
        "| Rule Based          | High Precision                 | 2                 | 2           | 0                      | 1           | 1           | 6     |\n",
        "| Transformer Models  | State-of-the-Art               | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| KERAG               | Knowledge Integration          | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| HMM                 | Sequence Modeling              | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| CRF                 | Sequential Labeling            | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| SVM                 | Versatile                      | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Sequence Models     | Order Preservation             | 1                 | 1           | 1                      | 1           | 1           | 5     |\n",
        "| Attention Mechanisms| Focus on Specific Parts        | 0                 | 0           | 2                      | 1           | 2           | 5     |\n",
        "| RNN                 | Sequential Dependencies        | 0                 | 1           | 2                      | 0           | 1           | 4     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The score card was a valuable tool to reduce options down to those most appropriate for this project. A final audit is as follows:\n",
        "\n",
        "### RAG (Retriever-Augmented Generation):\n",
        "- **Strengths:** Excellent at context understanding and contextually relevant responses.\n",
        "- **Weaknesses:** Computationally intensive and may require more resources.\n",
        "- **Suitability:** High, if you need detailed, context-aware answers and have the necessary computational resources.\n",
        "\n",
        "### BoW (Bag-of-Words):\n",
        "- **Strengths:** Simple and efficient, easy to implement, and works well for basic tasks.\n",
        "- **Weaknesses:** Ignores word order and context, may not capture semantic meaning well.\n",
        "- **Suitability:** Moderate, for straightforward tasks where simplicity and efficiency are prioritized over contextual understanding.\n",
        "\n",
        "### N-gram Model:\n",
        "- **Strengths:** Captures some local word order and context, relatively simple to implement.\n",
        "- **Weaknesses:** Limited in capturing long-range dependencies, can become computationally expensive with larger N values.\n",
        "- **Suitability:** Moderate, for tasks where local context is important, but computational efficiency is still needed.\n",
        "\n",
        "## Conclusion\n",
        "The RAG model is most appropriate for this effort given its heavy use of domain specific information (that may be missing from a stand-alone pre-trained model). Its contextual accuracy has a higher weighted value for this use case than Ease of Use. Having said that, it is a computationally heavy architecture, and it is unclear if free cloud CPU resources will be sufficient.\n",
        "\n",
        "## Example of RAG Model Implementation\n",
        "1. **Query:** \"What are the benefits of using a RAG model?\"\n",
        "2. **Retriever:**\n",
        "   - Searches a corpus for relevant documents or passages related to \"benefits of using a RAG model\".\n",
        "   - Retrieves top-k documents or passages that discuss the advantages of RAG models.\n",
        "3. **Generator:**\n",
        "   - Takes the retrieved documents and generates a response: \"A RAG model combines the strengths of information retrieval and generative modeling. It retrieves relevant documents to provide context and generates accurate and contextually appropriate responses. This makes it highly effective for tasks requiring detailed and specific information.\"\n",
        "\n",
        "For this project, the Retriever will be the corpus scraped from the online forum processed with Word Embeddings, and the Generator will be from a pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpRVNUJFDm-"
      },
      "source": [
        "## 1.2 Generator Options\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** GPT (GPT-2)  \n",
        "**Description:** Generative Pre-trained Transformer for generating text.  \n",
        "**Pros:**  \n",
        "- Highly capable of generating coherent and contextually relevant text.\n",
        "- Free to access and use.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources for fine-tuning.\n",
        "- GPT-2 is less powerful than newer models.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Claude (Claude 3)  \n",
        "**Description:** AI assistant designed for safety and ethical considerations.  \n",
        "**Pros:**  \n",
        "- Enhanced safety features and focus on ethical AI use.\n",
        "- Designed for robust handling of varying text lengths.\n",
        "\n",
        "**Cons:**  \n",
        "- Primarily available for research access, which may limit commercial use.\n",
        "- Conditional access, potentially limiting deployment flexibility.\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** DistilBART (sshleifer/distilbart-cnn-12-6)  \n",
        "**Description:** A distilled version of BART optimized for efficiency.  \n",
        "**Pros:**  \n",
        "- Optimized for CPU usage, making it efficient for resource usage.\n",
        "- Open-source and free, allowing for flexible use and customization.  \n",
        "\n",
        "**Cons:**  \n",
        "- Less powerful than the full BART model due to distillation.\n",
        "- May require additional integration efforts compared to more commercial models.\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** T5 (t5-small)  \n",
        "**Description:** Text-to-Text Transfer Transformer for various NLP tasks.  \n",
        "**Pros:**  \n",
        "- Highly flexible and powerful for a wide range of text-to-text tasks.\n",
        "- Open-source and free to use and fine-tune.\n",
        "\n",
        "**Cons:**  \n",
        "- May require additional preprocessing steps for certain tasks.\n",
        "- The small version is less powerful compared to larger T5 models.\n",
        "\n",
        "**Vendor:** Amazon  \n",
        "**Package:** AWS Comprehend  \n",
        "**Description:** Managed NLP service for text analysis and insights.  \n",
        "**Pros:**  \n",
        "- Fully managed service, reducing the burden of infrastructure management.\n",
        "- Tight integration with AWS ecosystem, offering scalability and ease of use.\n",
        "  \n",
        "**Cons:**  \n",
        "- API-dependent, limiting control over the underlying models.\n",
        "- Cloud performance may not be as optimized as specific CPU performance.\n",
        "\n",
        "## Generator Option Score Card\n",
        "\n",
        "| Vendor    | Package       | Key Strength               | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------|----------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | DistilBART    | CPU Efficiency             | 2                 | 2           | 1                      | 2                         | 2           | 9     |\n",
        "| Google    | T5-small      | Flexibility                | 2                 | 1           | 2                      | 2                         | 1           | 8     |\n",
        "| Amazon    | Comprehend    | Managed Service            | 1                 | 2           | 1                      | 2                         | 1           | 7     |\n",
        "| OpenAI    | GPT-2         | Coherent Text Generation   | 0                 | 2           | 2                      | 2                         | 0           | 6     |\n",
        "| Anthropic | Claude 3      | Ethical AI                 | 1                 | 1           | 2                      | 1                         | 1           | 6     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLhXj52bwtE"
      },
      "source": [
        "\n",
        "## 1.3 Frameworks and Tools\n",
        "\n",
        "**Vendor:** Google  \n",
        "**Package:** TensorFlow  \n",
        "**Description:** Open-source ML framework for building and deploying models.  \n",
        "**Pros:**  \n",
        "- Comprehensive ecosystem with deep learning support.\n",
        "- Scalable on both CPUs and GPUs.  \n",
        "\n",
        "**Cons:**  \n",
        "- Steeper learning curve than some other frameworks.\n",
        "- Limited support for dynamic computation graphs.\n",
        "\n",
        "**Vendor:** Meta  \n",
        "**Package:** PyTorch  \n",
        "**Description:** Open-source deep learning framework by Meta AI Research.  \n",
        "**Pros:**  \n",
        "- Pythonic and intuitive interface for model development.\n",
        "- Dynamic computation graph for easier debugging and experimentation.  \n",
        "\n",
        "**Cons:**  \n",
        "- Less optimized for production deployment than TensorFlow.\n",
        "- Limited built-in support for distributed training.\n",
        "\n",
        "**Vendor:** AWS  \n",
        "**Package:** Amazon Bedrock  \n",
        "**Description:** Fully managed service for building, deploying, and scaling ML models.  \n",
        "**Pros:**  \n",
        "- Integrated support for various ML frameworks.\n",
        "- Scalable infrastructure with extensive AWS services integration.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires AWS-specific knowledge for optimal use.\n",
        "- Potentially high costs for extensive usage.\n",
        "\n",
        "**Vendor:** OpenAI  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "**Vendor:** Anthropic  \n",
        "**Package:** Hugging Face Transformers  \n",
        "**Description:** Open-source library providing pre-trained models and tools for NLP tasks.  \n",
        "**Pros:**  \n",
        "- Easy access to a wide range of pre-trained models.\n",
        "- Supports integration with both TensorFlow and PyTorch.\n",
        "\n",
        "**Cons:**  \n",
        "- Requires knowledge of underlying frameworks for customization.\n",
        "- Performance dependent on the selected model and hardware.\n",
        "\n",
        "## Framework Score Card\n",
        "\n",
        "| Vendor    | Package             | Key Strength                  | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|---------------------|-------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | PyTorch             | Pythonic Interface            | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Google    | TensorFlow          | Comprehensive Ecosystem       | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| AWS       | Bedrock             | Fully Managed Service         | 2                 | 1           | 2                      | 2                         | 2           | 9     |\n",
        "| OpenAI    | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Anthropic | Hugging Face        | Wide Range of Pre-trained Models | 2               | 2           | 2                      | 2                         | 1           | 9     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yafxx82Qbq17"
      },
      "source": [
        "\n",
        "## 1.4 Embedding\n",
        "\n",
        "**Solution:** Universal Sentence Encoder  \n",
        "**Provider:** Google  \n",
        "**Libraries:** TensorFlow Hub  \n",
        "**Pros:**  \n",
        "- Captures sentence-level embeddings, enhancing text understanding.\n",
        "- Efficient and easy to integrate with TensorFlow models.  \n",
        "**Cons:**  \n",
        "- May not capture fine-grained word-level nuances.\n",
        "- Performance can vary depending on the complexity of the sentences.\n",
        "\n",
        "**Solution:** FastText  \n",
        "**Provider:** Meta  \n",
        "**Libraries:** Gensim, TensorFlow, PyTorch  \n",
        "**Pros:**  \n",
        "- Handles out-of-vocabulary words as bags of character n-grams.\n",
        "- Captures subword information, enhancing the representation of rare words.  \n",
        "**Cons:**  \n",
        "- Increases computational complexity due to subword representations.\n",
        "- Larger model size compared to Word2Vec and GloVe.\n",
        "\n",
        "**Solution:** Amazon SageMaker Embeddings  \n",
        "**Provider:** AWS  \n",
        "**Libraries:** Amazon SageMaker  \n",
        "**Pros:**  \n",
        "- Provides pre-built models for embeddings, simplifying deployment.\n",
        "- Integrates seamlessly with other AWS services for scalability.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires familiarity with the AWS ecosystem.\n",
        "- Costs can increase with extensive usage.\n",
        "\n",
        "**Solution:** GPT-3 Embeddings  \n",
        "**Provider:** OpenAI  \n",
        "**Libraries:** OpenAI API  \n",
        "**Pros:**  \n",
        "- Generates high-quality, contextually relevant text embeddings.\n",
        "- Handles long-range dependencies and contextual information.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires significant computational resources.\n",
        "- Access may require API usage and associated costs.\n",
        "\n",
        "**Solution:** Claude Embeddings  \n",
        "**Provider:** Anthropic  \n",
        "**Libraries:** Anthropic API  \n",
        "**Pros:**  \n",
        "- Offers state-of-the-art embeddings with a focus on safety and ethics.\n",
        "- Handles context and nuances effectively for complex tasks.  \n",
        "\n",
        "**Cons:**  \n",
        "- Primarily available for research access, limiting commercial use.\n",
        "- Access may require API usage and associated costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewprjkrLbicq"
      },
      "source": [
        "## 1.5 Tokenization\n",
        "\n",
        "Tokenization is a crucial preprocessing step in NLP, segmenting text into manageable units for further analysis or model training. The choice of tokenization strategy affects both the complexity of the model and its ability to understand the text.\n",
        "\n",
        "**Solution:** Word-level Tokenization  \n",
        "**Libraries:** NLTK, spaCy, TensorFlow/Keras Tokenizers, BPE, Hugging Face Tokenizers  \n",
        "**Pros:**  \n",
        "- Preserves word integrity and meaning, crucial for comprehension tasks.\n",
        "- Subword tokenization methods like BPE can efficiently handle unknown words.\n",
        "\n",
        "**Cons:**  \n",
        "- Can result in a large vocabulary, increasing memory and processing needs.\n",
        "- May overlook nuances in character-level variations.\n",
        "\n",
        "**Solution:** Character-level Tokenization  \n",
        "**Libraries:** Supported by deep learning frameworks like TensorFlow and Keras  \n",
        "**Pros:**  \n",
        "- Captures morphological nuances at the character level, aiding rich languages.\n",
        "- Simplifies vocabulary to unique characters, reducing model complexity.  \n",
        "\n",
        "**Cons:**  \n",
        "- Leads to longer input sequences, increasing computational costs.\n",
        "- Loses direct access to semantic information in words or phrases.\n",
        "\n",
        "**Solution:** Subword Tokenization  \n",
        "**Libraries:** A blend of word-level and character-level tokenization methods  \n",
        "**Pros:**  \n",
        "- Balances vocabulary size and semantic information preservation.\n",
        "- Handles rare or unknown words by breaking them into recognizable subwords.  \n",
        "\n",
        "**Cons:**  \n",
        "- Requires preprocessing to establish a subword vocabulary, adding complexity.\n",
        "- Generated subwords may lack standalone meaning, complicating interpretation.\n",
        "\n",
        "**Solution:** Model-Specific Tokenization  \n",
        "**Libraries:** Hugging Face's transformers library provides access to pre-built tokenizers  \n",
        "**Pros:**  \n",
        "- Ensures tokenization consistency with the model's original training data.\n",
        "- Reduces the need for extra preprocessing steps and custom tokenization.  \n",
        "\n",
        "**Cons:**  \n",
        "- Limited flexibility to change tokenization beyond the model's method.\n",
        "- May not be efficient for tasks outside the model's specific design.\n",
        "\n",
        "Tokenization and embedding must be considered together because tokenization directly impacts the quality of embedding. The choice of tokenization method determines how text is segmented, which in turn affects how embeddings capture context and meaning. Inconsistent tokenization can lead to poor embeddings and reduced model performance. Properly aligned tokenization and embedding processes ensure that the text's structure and semantics are preserved, enhancing overall model effectiveness.\n",
        "\n",
        "## Tokenization and Embedding Score Card\n",
        "\n",
        "| Vendor    | Embedder                    | Tokenizer                      | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-----------------------------|--------------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | Universal Sentence Encoder  | TensorFlow Text                | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Meta      | FastText                    | Gensim Tokenizer or NLTK       | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| OpenAI    | GPT-2 Embeddings            | GPT-2 Tokenizer                | 2                 | 2           | 2                      | 2                         | 1           | 9     |\n",
        "| Amazon    | SageMaker Embeddings        | SageMaker’s preprocessing tools| 1                 | 2           | 2                      | 2                         | 1           | 8     |\n",
        "| Anthropic | Claude Embeddings           | Built-in tokenization          | 1                 | 2           | 2                      | 2                         | 0           | 7     |\n",
        "\n",
        "**0: Does not meet 1: Partially meets 2: Fully meets**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvnMsI-bcPS"
      },
      "source": [
        "\n",
        "## 1.6 Solution Leader Board\n",
        "\n",
        "| Vendor    | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Google    | 6                 | 4           | 6                      | 6                         | 5           | 27    |\n",
        "| Meta      | 6                 | 6           | 5                      | 6                         | 4           | 27    |\n",
        "| Amazon    | 4                 | 5           | 5                      | 6                         | 4           | 24    |\n",
        "| OpenAI    | 4                 | 6           | 6                      | 6                         | 2           | 24    |\n",
        "| Anthropic | 4                 | 5           | 6                      | 5                         | 2           | 22    |\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The results are interesting and paint a clearer picture of how the strengths of each option compare. While close, Google seems to have an edge on Performance and Accuracy but suffers a bit on Ease of Use and Scalability. Having said that, the use of Google would support my efforts to gain Google Cloud Certification—a highly desirable skill in the job market. With that in mind, I’ll be moving forward with a Google dominant stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBhF-lbTb6Q7"
      },
      "source": [
        "# 2 Develop Corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWbtT5GoFDpf"
      },
      "source": [
        "## 2.1 Data Ethics\n",
        "The data collected here is a collection of posts from widely available public forum. However, should this project move into public distribution, additional steps will be necessary to ensure PII is obfuscated or removed. In addition, this document shall serve as full disclosure of the project's goals and data gathering process.\n",
        "\n",
        "### Data Collection\n",
        "The project leverages user-generated content from a domain-specific online forum as the training corpus. This data is largely unstructured, with minimal metadata available. The following tools were considered to gather the source text for the corpus:\n",
        "\n",
        "#### Web Scraping\n",
        "**Tools:** Beautiful Soup, online SaaS products  \n",
        "**Pros:**  \n",
        "- Direct Access to Targeted Data: Enables precise extraction of user-generated content from specific sections or threads within the forum.\n",
        "- Efficiency in Data Collection: Automated scripts can gather large volumes of data in a short amount of time, making it suitable for assembling significant datasets for NLP.  \n",
        "\n",
        "**Cons:**  \n",
        "- Potential for Incomplete Data: May miss embedded content or dynamically loaded data, depending on the website’s structure.\n",
        "- Ethical and Legal Considerations: Scraping data from forums may raise concerns about user privacy and must adhere to the terms of service of the website.\n",
        "- Very Platform Dependent: Forum-specific solutions result in forum-specific data schemas that must be reverse engineered for successful text extraction.\n",
        "\n",
        "#### Forum-specific APIs\n",
        "**Tools:** Python (`requests` library for API calls and `json` library for handling responses)  \n",
        "**Pros:**  \n",
        "- Structured and Reliable Data Retrieval: APIs provide structured data, making it easier to process and integrate into your project.\n",
        "- Efficient and Direct Access: Directly accessing the forum's data through its API is efficient, bypassing the need for HTML parsing.\n",
        "- Compliance and Ethical Data Use: Utilizing APIs respects the forum's data policies and ensures access is in line with user agreements.  \n",
        "\n",
        "**Cons:**  \n",
        "- Rate Limiting: APIs often have limitations on the number of requests that can be made in a certain timeframe, which could slow down data collection.\n",
        "- API Changes: Dependence on the forum's API structure means that changes or deprecation could disrupt your data collection pipeline.\n",
        "- Access Restrictions: Some data or functionalities might be restricted or require authentication, posing additional challenges for comprehensive data collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lJ9umeIXNXy"
      },
      "source": [
        "## 2.2 Ingest Corpus from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y75VMgRttCo"
      },
      "outputs": [],
      "source": [
        "raise RuntimeError(\"Remove this line if you want to create a new corpus\")\n",
        "\n",
        "# Remove this line if you want to create a new corpus\n",
        "\n",
        "\n",
        "\n",
        "# Step 1 Create Corpus\n",
        "# Fetch and process forum threads\n",
        "# Corpus created in LDA notebook can be used.\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "def forum_thread_ids():\n",
        "    threads = 1  # Set the number of incremental threads to process here\n",
        "\n",
        "    file_path = os.path.join(BASE_PATH, 'e9_forum_thread_ids.csv')\n",
        "\n",
        "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "        e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "        last_thread_id = int(e9_forum_thread_ids['thread_id'].iloc[-1])\n",
        "    else:\n",
        "        e9_forum_thread_ids = pd.DataFrame(columns=['thread_id'])\n",
        "        last_thread_id = 0\n",
        "\n",
        "    next_thread_id = last_thread_id + 1\n",
        "    new_urls = [{'thread_id': thread_id} for thread_id in range(next_thread_id, next_thread_id + threads)]\n",
        "\n",
        "    new_df = pd.DataFrame(new_urls)\n",
        "    e9_forum_thread_ids = pd.concat([e9_forum_thread_ids, new_df], ignore_index=True)\n",
        "    e9_forum_thread_ids.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"Starting with thread_id {last_thread_id}\")\n",
        "    print(f\"Processing additional {threads} thread(s)\")\n",
        "    print(f\"Ending with thread_id {next_thread_id + threads - 1}\")\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def forum_thread_url(df):\n",
        "    if df.empty:\n",
        "        print(\"No new threads to process.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    pages = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "\n",
        "    df.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_url.csv'), index=False)\n",
        "    return df\n",
        "\n",
        "def forum_thread_first_post(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        post_content = first_post.get_text(strip=True) if first_post else \"No content found\"\n",
        "        data.append({'thread_id': thread_id, 'thread_first_post': post_content})\n",
        "\n",
        "    forum_first_post = pd.DataFrame(data)\n",
        "    forum_first_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_first_post.csv'), index=False)\n",
        "    return forum_first_post\n",
        "\n",
        "def forum_thread_all_post(df):\n",
        "    post_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')\n",
        "        for article in articles:\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "            post_data.append({'thread_id': row['thread_id'], 'post_raw': content})\n",
        "\n",
        "    e9_forum_posts = pd.DataFrame(post_data)\n",
        "    e9_forum_posts['thread_all_posts'] = e9_forum_posts['post_raw'].astype(str)\n",
        "    e9_forum_thread_all_post = e9_forum_posts.groupby('thread_id')['thread_all_posts'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "    e9_forum_thread_all_post.to_csv(os.path.join(BASE_PATH, 'e9_forum_thread_all_post.csv'), index=False)\n",
        "    return e9_forum_thread_all_post\n",
        "\n",
        "def forum_corpus(e9_forum_thread_url, e9_forum_thread_first_post, e9_forum_thread_all_post):\n",
        "    agg_df_1 = pd.merge(e9_forum_thread_url, e9_forum_thread_first_post, on='thread_id', how='left')\n",
        "    agg_df_2 = pd.merge(agg_df_1, e9_forum_thread_all_post, on='thread_id', how='left')\n",
        "\n",
        "    e9_forum_corpus = agg_df_2.dropna()\n",
        "    corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus.csv')\n",
        "    if os.path.exists(corpus_path) and os.path.getsize(corpus_path) > 0:\n",
        "        existing_corpus = pd.read_csv(corpus_path)\n",
        "        e9_forum_corpus = pd.concat([existing_corpus, e9_forum_corpus]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    e9_forum_corpus.columns = e9_forum_corpus.columns.str.upper()\n",
        "    e9_forum_corpus.to_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv'), index=False)\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def main():\n",
        "    e9_forum_thread_ids = forum_thread_ids()\n",
        "    e9_forum_thread_url_df = forum_thread_url(e9_forum_thread_ids)\n",
        "    e9_forum_thread_first_post_df = forum_thread_first_post(e9_forum_thread_url_df)\n",
        "    e9_forum_thread_all_post_df = forum_thread_all_post(e9_forum_thread_url_df)\n",
        "    e9_forum_corpus_df = forum_corpus(e9_forum_thread_url_df, e9_forum_thread_first_post_df, e9_forum_thread_all_post_df)\n",
        "    print(f\"Output saved to {os.path.join(BASE_PATH, 'e9_forum_corpus_dirty.csv')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65J-_EM5DTlM"
      },
      "source": [
        "## 2.3 Step Ingest corpus from LDA workbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dJ2e5CwqDYl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef3550a-aeb3-453c-97c5-2750a136e64d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records retrieved: 1000\n"
          ]
        }
      ],
      "source": [
        "# Data here is from corpus workbook stored in Snowflake\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT'),\n",
        "    )\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT THREAD_TITLE, THREAD_FIRST_POST FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    # Print the count of records retrieved\n",
        "    print(f\"Number of records retrieved: {len(e9_forum_corpus)}\")\n",
        "    return e9_forum_corpus\n",
        "\n",
        "# Main sequence\n",
        "path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "# Load credentials\n",
        "load_credentials(path_to_credentials)\n",
        "\n",
        "# Fetch data from Snowflake and print the count of records retrieved\n",
        "e9_forum_corpus = fetch_data_from_snowflake()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A97WWzNlTEmj"
      },
      "source": [
        "# 3 Preprocessing Text\n",
        "\n",
        "The collected text is very unstructured and needs a reasonable amount of pre-processing to make it usable for NLP. This will address values that are either not localized, use slang, or do not have value from an NLP perspective.\n",
        "\n",
        "- **Clean the Text:**\n",
        "  - Remove HTML tags, extra whitespace, non-printable characters, and other irrelevant elements.\n",
        "\n",
        "- **Standardize the Text:**\n",
        "  - Convert all characters to lowercase to ensure uniformity.\n",
        "\n",
        "- **Filter Out Common Stop Words:**\n",
        "  - Remove stop words to focus on more meaningful content.\n",
        "\n",
        "- **Remove Duplicate Entries:**\n",
        "  - Ensure the uniqueness of the data by eliminating duplicates.\n",
        "\n",
        "- **Lemmatization or Stemming:**\n",
        "  - Convert words to their base or dictionary form to consolidate similar forms of a word.\n",
        "\n",
        "- **Anonymize Personal Information:**\n",
        "  - Identify and anonymize personal information or specific entity names to maintain privacy.\n",
        "\n",
        "- **Remove Irrelevant Sections:**\n",
        "  - Remove sections of the text that do not contribute to the knowledge base or are off-topic.\n",
        "\n",
        "- **Tokenization:**\n",
        "  - Break down the text into smaller units called tokens. Use a tokenizer compatible with your chosen model, such as the BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO22BY4_rScA",
        "outputId": "04b4e7f6-ab98-432f-abe9-2720c11255c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records retrieved: 1000\n",
            "Cleaned data saved to /content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_clean.csv\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing Corpus\n",
        "# Clean and preprocess forum data\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "\n",
        "def fetch_data_from_snowflake():\n",
        "    conn = snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT'),\n",
        "    )\n",
        "\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT THREAD_ID, THREAD_TITLE, THREAD_FIRST_POST, THREAD_ALL_POSTS\n",
        "    FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "    \"\"\"\n",
        "    cur.execute(query)\n",
        "    e9_forum_corpus = cur.fetch_pandas_all()\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    # Print the count of records retrieved\n",
        "    print(f\"Number of records retrieved: {len(e9_forum_corpus)}\")\n",
        "    return e9_forum_corpus\n",
        "\n",
        "def remove_urls(df):\n",
        "    \"\"\"Removes URLs from the text.\"\"\"\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: url_pattern.sub(r'', str(text)))\n",
        "    return df\n",
        "\n",
        "def alpha_numeric(df):\n",
        "    \"\"\"Removes non-alphanumeric characters and unwanted patterns from text.\"\"\"\n",
        "    pattern_email = re.compile(r'\\S*@\\S*\\s?')\n",
        "    pattern_non_alpha = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: pattern_non_alpha.sub('', pattern_email.sub('', str(text))))\n",
        "        df[column] = df[column].apply(lambda text: re.sub(r'\\s+', ' ', text).strip())  # Remove extra spaces\n",
        "    return df\n",
        "\n",
        "def spell_check(df):\n",
        "    \"\"\"Corrects spelling errors in the text with caching.\"\"\"\n",
        "    spell = SpellChecker()\n",
        "    cache = {}\n",
        "\n",
        "    def correct_word(word):\n",
        "        if word in cache:\n",
        "            return cache[word]\n",
        "        else:\n",
        "            correction = spell.correction(word) or word\n",
        "            cache[word] = correction\n",
        "            return correction\n",
        "\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([correct_word(word) for word in text.split()]))\n",
        "    return df\n",
        "\n",
        "def remove_stop_words(df):\n",
        "    \"\"\"Removes stop words from the text.\"\"\"\n",
        "    stop_words_set = set(stopwords.words('english')).union({'car', 'csi', 'cs', 'csl', 'e9', 'coupe', 'http', 'https', 'www', 'ebay', 'bmw', 'html'})\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in stop_words_set and len(word) > 2]))\n",
        "    return df\n",
        "\n",
        "def tokenize_and_lemmatize(df):\n",
        "    \"\"\"Tokenizes and lemmatizes the text in specified columns using TensorFlow Text.\"\"\"\n",
        "    tokenizer = tf_text.WhitespaceTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def tokenize_text(text):\n",
        "        tokens = tokenizer.tokenize(text).numpy().astype(str).tolist()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].apply(tokenize_text)\n",
        "    return df\n",
        "\n",
        "def clean_nan_values(df):\n",
        "    \"\"\"Removes or replaces NaN values in the dataset and converts all entries to strings.\"\"\"\n",
        "    df.fillna('', inplace=True)\n",
        "    for column in ['THREAD_TITLE', 'THREAD_FIRST_POST', 'THREAD_ALL_POSTS']:\n",
        "        df[column] = df[column].astype(str)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the data processing pipeline.\"\"\"\n",
        "    # Load credentials\n",
        "    path_to_credentials = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "    load_credentials(path_to_credentials)\n",
        "\n",
        "    # Fetch data from Snowflake\n",
        "    e9_forum_corpus = fetch_data_from_snowflake()\n",
        "\n",
        "    # Process the data\n",
        "    df = remove_urls(e9_forum_corpus)\n",
        "    df = alpha_numeric(df)\n",
        "    # df = spell_check(df)  # Need to find a faster spell checking process\n",
        "    df = remove_stop_words(df)\n",
        "    df = tokenize_and_lemmatize(df)\n",
        "    df = clean_nan_values(df)  # Final NaN cleaning and type conversion step\n",
        "    df.columns = df.columns.str.upper()  # Convert column names to uppercase\n",
        "\n",
        "    # Save the cleaned data\n",
        "    output_path = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/e9_forum_corpus_clean.csv'\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Cleaned data saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQDl8Jpx-m_a"
      },
      "source": [
        "# 4 Clustering and Summarization\n",
        "\n",
        "Summarization in NLP involves condensing large texts into shorter versions, capturing the most critical information. This can be approached through multiple options. For this effort, the following solutions were scored to reduce the potential solution set:\n",
        "\n",
        "| Provider  | Specific Package | Key Strength              | CPU Compatibility | Ease of Use | Performance & Accuracy | Integration & Flexibility | Scalability | Total |\n",
        "|-----------|------------------|---------------------------|-------------------|-------------|------------------------|---------------------------|-------------|-------|\n",
        "| Meta      | DistilBART       | Optimized for CPU usage   | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Google    | T5 (t5-small)    | Efficient CPU usage       | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| Amazon    | AWS Comprehend   | Integrated AWS service    | 2                 | 2           | 2                      | 2                         | 2           | 10    |\n",
        "| OpenAI    | GPT (GPT-2)      | Optimal for CPUs          | 2                 | 2           | 2                      | 1                         | 1           | 8     |\n",
        "| Anthropic | Claude (Claude 3)| Enhanced safety features  | 2                 | 2           | 2                      | 1                         | 1           | 8     |\n",
        "\n",
        "\n",
        "\n",
        "A test of solutions can be found in the Appenix. T5 was chosen based on bettter ROUGE scores than BART. This also allows me to stick with Google centric stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "1f2335a360dc493e80e84497ea2fd7d0",
            "89ab4b91a3f44d11865617735eb6fc21",
            "3e6f81c5ff784dcc820cb159305d9c66",
            "6039aadeac5e44edbb50767eebfea081",
            "1ab1562a43a44b60b5b6dc29defc58f7",
            "f13348612de14f0fa883b23224225fdf",
            "081d569be12d41c283c6d79b64f1f052",
            "b4dcad77816f4029a1ee4a02fc3db813",
            "189af619a3154000aa9b70c4037a89d4",
            "d1b829d1d0cd4087aa1486a4b5a1eb2c",
            "f05fd91f7168447799ae75830941fe5a",
            "813dc0fe25cf4e8695657f49ae75953c",
            "c12d7c17da664cb38778bc37368665a5",
            "6d4fb8bb903148e8bcb756f3b9203bdb",
            "c55a126a337b4bb0989463573e2e29ca",
            "d03b8e666c5c4dd3b8394d3840285c4c",
            "4fe2fd27a24f47e1a7f07acfa0aa3b87",
            "52ae4ee67a494f60a6f6edd6a877c035",
            "8839db19183f4595904846635f2ee19e",
            "40f20485e99940ad9949ccf382bd6964",
            "a69063d4320b42b38b83894ef27d9cae",
            "62eab04215b945fbac8192f8294d775b",
            "0525271af699437e83e1ee25f1fb6766",
            "ba27e8a6dca74693a165eb1d18c511be",
            "a98b000427df41d4a3732b84308e1563",
            "86b0331b383c45289b981884dee08c50",
            "b762bff093604ca78e1c170f6cd26e91",
            "e340a70859d04d318106ed9557406eea",
            "2ffbb327a4e3441da83fe479b32c48c7",
            "fb73f91f111b44b18d459815706e9096",
            "2fec9b8f40b14fbcaa740ccb91fac7ab",
            "22fb1906c6034540bbf024b0ef6c881a",
            "e86903e3bdb24e9e8c26a590e17c64d1",
            "c2fdce18e7df45a695ce6cb9465451af",
            "3c9eadd541704f528794b0c8e60ca6b9",
            "830714b8e55b4209a615da4d04a16980",
            "76b6d174cc5a40ab848204802715c7ba",
            "412d1c037e7a48f2b773ea1d2657b33c",
            "348934ecc5c444b8b69da63efd5c0d8b",
            "10cfe3b9ec7e4c5bba5690fba1c7ed1e",
            "34476a8649834eb4bce9dcf9bd37e03a",
            "210334e27c7a4896b117ed15ee1daccd",
            "aefd38e6090646678f9099139cdb1501",
            "41d274e5608443e68d1d7680e2357c4a",
            "090c4728bfa34cf1b1137ca49da508cf",
            "03d4b91927854793a93ded643f6b1b30",
            "fb36e339cbe647d39df3bd815190ed70",
            "dd1d1db3a937471e8791dc517ebbf712",
            "357fa34619ae46d0a08f6345bcb2d394",
            "f9d1ce7fb5914fab8816a9f9719e23b9",
            "c8a2c9f8dae049feb5f60d8adc39cb59",
            "a8014bdf6c70410699f73ce00d1a44f4",
            "0d304b1091d947ac9e5569346d8ed93e",
            "888365f499ec4b1bbe143d170fdfe4f6",
            "aed2fa800e0644119b90cab85ea49a16",
            "f6334c872890464684841093b7fee63b",
            "b762f12de4044047a8f57ac1cdda8a19",
            "4c7c38a010c3415b96578a0da935d7ef",
            "d3938b6b7b8c4180bc6128ed757a52da",
            "e7c9d0adf1d3465bac7dfec5c45895a2",
            "dc96a637d35f4843b2ec4edac25636b9",
            "1327d2a4cf17483196c9d6b63754bca1",
            "c87601355ee145a88fb19ed7f9966ea8",
            "fa4f5e4f8fbc4cc39d100ef72ca30bdf",
            "27f7d802b7d1486e93936c0dd87c53cd",
            "2831acd49ae34e819f3d51e52b86d8c2"
          ]
        },
        "id": "2laEu8B7Yunw",
        "outputId": "b7038650-9c33-477d-ae5d-e2c919cf463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f2335a360dc493e80e84497ea2fd7d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "813dc0fe25cf4e8695657f49ae75953c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0525271af699437e83e1ee25f1fb6766"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2fdce18e7df45a695ce6cb9465451af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "090c4728bfa34cf1b1137ca49da508cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6334c872890464684841093b7fee63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 700\n",
            "Processing additional 300 thread(s)\n",
            "Ending with thread_id 1000\n"
          ]
        }
      ],
      "source": [
        "# Summarize forum data using T5 model\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Load e9_forum_corpus_clean DataFrame from the CSV\n",
        "e9_forum_corpus_clean = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_clean.csv'))\n",
        "\n",
        "# Load the existing summarized corpus if it exists, otherwise create it\n",
        "summarized_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv')\n",
        "e9_forum_corpus_summarized = pd.read_csv(summarized_corpus_path) if os.path.exists(summarized_corpus_path) and os.path.getsize(summarized_corpus_path) > 0 else pd.DataFrame(columns=e9_forum_corpus_clean.columns)\n",
        "\n",
        "# Calculate the starting THREAD_ID of the summarized corpus\n",
        "starting_thread_id = e9_forum_corpus_summarized['THREAD_ID'].max() if not e9_forum_corpus_summarized.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = e9_forum_corpus_clean[~e9_forum_corpus_clean['THREAD_ID'].isin(e9_forum_corpus_summarized['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def T5_summarize(text):\n",
        "    \"\"\"Summarization using T5.\"\"\"\n",
        "    try:\n",
        "        if text.strip() == \"\":\n",
        "            return text\n",
        "\n",
        "        unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            unformatted_text,\n",
        "            max_length=900,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=50,\n",
        "            min_length=10,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary if summary else text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def main():\n",
        "    # Check if 'THREAD_ALL_POSTS' column exists in new entries\n",
        "    if 'THREAD_ALL_POSTS' in new_entries.columns:\n",
        "        unique_texts = new_entries['THREAD_ALL_POSTS'].drop_duplicates()\n",
        "        summaries = unique_texts.apply(T5_summarize)\n",
        "        summary_map = dict(zip(unique_texts, summaries))\n",
        "        new_entries.loc[:, 'SUMMARIZED_THREAD'] = new_entries['THREAD_ALL_POSTS'].map(summary_map)\n",
        "\n",
        "        # Append the new summarized data to the existing summarized corpus\n",
        "        updated_summarized_corpus = pd.concat([e9_forum_corpus_summarized, new_entries], ignore_index=True)\n",
        "\n",
        "        # Save the results with the new summarized column\n",
        "        updated_summarized_corpus.to_csv(summarized_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Summarization completed and saved to {summarized_corpus_path}\")\n",
        "    else:\n",
        "        print(\"Error: Column 'THREAD_ALL_POSTS' does not exist in the dataset.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HjwEoXaR3y"
      },
      "source": [
        "# 5 Format Text for Training\n",
        "\n",
        "- Structure text into a question-answer format suitable for training a RAG model.\n",
        "- Ensure the question string ends with a question mark for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47CINHXsYvfh"
      },
      "outputs": [],
      "source": [
        "# Transform summarized data into QA format\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the dataset from Step 2\n",
        "df_summarized = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_summarized.csv'))\n",
        "\n",
        "# Load the existing QA corpus if it exists\n",
        "qa_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv')\n",
        "df_qa = pd.read_csv(qa_corpus_path) if os.path.exists(qa_corpus_path) and os.path.getsize(qa_corpus_path) > 0 else pd.DataFrame(columns=['THREAD_ID', 'QUESTION', 'ANSWER'])\n",
        "\n",
        "# Calculate the starting THREAD_ID of the QA corpus\n",
        "starting_thread_id = df_qa['THREAD_ID'].max() if not df_qa.empty else 0\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_summarized[~df_summarized['THREAD_ID'].isin(df_qa['THREAD_ID'])]\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "def create_qa_schema(df):\n",
        "    \"\"\"Creates a QA schema by renaming and dropping specific columns.\"\"\"\n",
        "    df.rename(columns={'SUMMARIZED_THREAD': 'ANSWER', 'THREAD_FIRST_POST': 'QUESTION'}, inplace=True)\n",
        "    df.drop(['THREAD_TITLE', 'THREAD_ALL_POSTS'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        # Process the new entries to create QA schema\n",
        "        df_qa_new = create_qa_schema(new_entries.dropna())\n",
        "\n",
        "        # Append the new QA data to the existing QA corpus\n",
        "        updated_qa_corpus = pd.concat([df_qa, df_qa_new], ignore_index=True)\n",
        "\n",
        "        # Save the updated QA corpus\n",
        "        updated_qa_corpus.to_csv(qa_corpus_path, index=False)\n",
        "\n",
        "        print(f\"Output saved to {qa_corpus_path}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_mp1BMbFXp"
      },
      "source": [
        "# 6 Embedding and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK6L99nQKA_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the DataFrame from your CSV file\n",
        "df_tok = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_qa.csv'))\n",
        "\n",
        "# Load the existing FAISS corpus if it exists, otherwise create an empty DataFrame\n",
        "faiss_corpus_path = os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv')\n",
        "df_faiss = pd.read_csv(faiss_corpus_path) if os.path.exists(faiss_corpus_path) and os.path.getsize(faiss_corpus_path) > 0 else pd.DataFrame(columns=df_tok.columns)\n",
        "\n",
        "\n",
        "df_faiss.info()\n",
        "\n",
        "\n",
        "# Identify new entries to be processed\n",
        "new_entries = df_tok[~df_tok['THREAD_ID'].isin(df_faiss['THREAD_ID'])].copy()\n",
        "\n",
        "# Calculate the starting THREAD_ID of the FAISS corpus\n",
        "starting_thread_id = df_faiss['THREAD_ID'].max() if not df_faiss.empty else 0\n",
        "\n",
        "# Calculate ending_thread_id and threads_processed\n",
        "ending_thread_id = new_entries['THREAD_ID'].max() if not new_entries.empty else starting_thread_id\n",
        "threads_processed = len(new_entries) if not new_entries.empty else 0\n",
        "\n",
        "print(f\"Starting with thread_id {starting_thread_id}\")\n",
        "print(f\"Processing additional {threads_processed} thread(s)\")\n",
        "print(f\"Ending with thread_id {ending_thread_id}\")\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to embed text using T5 encoder model\n",
        "def embed_text(tokens):\n",
        "    inputs = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return embeddings\n",
        "\n",
        "def process_new_entries(entries):\n",
        "    entries[\"Question_Tokens\"] = entries[\"QUESTION\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Answer_Tokens\"] = entries[\"ANSWER\"].apply(lambda x: tokenize_text(x).squeeze().tolist())\n",
        "    entries[\"Question_Embeddings\"] = entries[\"Question_Tokens\"].apply(embed_text)\n",
        "    entries[\"Answer_Embeddings\"] = entries[\"Answer_Tokens\"].apply(embed_text)\n",
        "    return entries\n",
        "\n",
        "def filter_embeddings(embeddings_list, expected_shape):\n",
        "    \"\"\"Filter out embeddings that do not match the expected shape.\"\"\"\n",
        "    return [embedding for embedding in embeddings_list if len(embedding) == expected_shape]\n",
        "\n",
        "def build_faiss_index(embeddings, index_path):\n",
        "    embeddings_np = np.array(embeddings).astype('float32')  # Convert to NumPy array of type float32\n",
        "    d = embeddings_np.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(d)  # Build the index\n",
        "    index.add(embeddings_np)  # Add vectors to the index\n",
        "\n",
        "    # Save the index\n",
        "    faiss.write_index(index, index_path)\n",
        "    return index\n",
        "\n",
        "def main():\n",
        "    if not new_entries.empty:\n",
        "        processed_entries = process_new_entries(new_entries)\n",
        "\n",
        "        # Append the new processed data to the existing FAISS corpus\n",
        "        updated_faiss_corpus = pd.concat([df_faiss, processed_entries], ignore_index=True)\n",
        "\n",
        "        # Save the updated FAISS corpus\n",
        "        updated_faiss_corpus.to_csv(faiss_corpus_path, index=False)\n",
        "\n",
        "        # Ensure all embeddings are of the same shape before saving\n",
        "        question_embeddings_list = updated_faiss_corpus[\"Question_Embeddings\"].to_list()\n",
        "        answer_embeddings_list = updated_faiss_corpus[\"Answer_Embeddings\"].to_list()\n",
        "\n",
        "        expected_shape = 512  # Expected embedding size (512 for T5 model)\n",
        "\n",
        "        question_embeddings_filtered = filter_embeddings(question_embeddings_list, expected_shape)\n",
        "        answer_embeddings_filtered = filter_embeddings(answer_embeddings_list, expected_shape)\n",
        "\n",
        "        question_embeddings = np.array(question_embeddings_filtered)\n",
        "        answer_embeddings = np.array(answer_embeddings_filtered)\n",
        "\n",
        "        np.save(os.path.join(BASE_PATH, 'question_embeddings_t5.npy'), question_embeddings)\n",
        "        np.save(os.path.join(BASE_PATH, 'answer_embeddings_t5.npy'), answer_embeddings)\n",
        "\n",
        "        # Build and save the FAISS index using the new answer embeddings\n",
        "        faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "        index = build_faiss_index(answer_embeddings, faiss_index_path)\n",
        "\n",
        "        print(f\"FAISS index has been rebuilt and saved to {faiss_index_path}\")\n",
        "        print(f\"Embeddings have been generated and saved to {BASE_PATH}\")\n",
        "    else:\n",
        "        print(\"No new entries to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kRXQtQdm-b"
      },
      "source": [
        "# 7 Query Processing and Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueakMTFdd4k3"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import os\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "\n",
        "# Load the FAISS index\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Load the pre-trained embeddings\n",
        "question_embeddings = np.load(os.path.join(BASE_PATH, 'question_embeddings_t5.npy'))\n",
        "answer_embeddings = np.load(os.path.join(BASE_PATH, 'answer_embeddings_t5.npy'))\n",
        "\n",
        "# Load the corpus DataFrame\n",
        "e9_forum_corpus = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv'))\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "def generate_query_embeddings(query, tokenizer, model):\n",
        "    tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return np.array(embeddings).astype('float32')  # Convert to float32 for FAISS\n",
        "\n",
        "def search_similar_questions(query_embeddings, index, top_k=5):\n",
        "    D, I = index.search(query_embeddings, top_k)\n",
        "    return I, D\n",
        "\n",
        "new_query = \"How do I fix the transmission issue in my car?\"\n",
        "query_embeddings = generate_query_embeddings(new_query, tokenizer, model).reshape(1, -1)\n",
        "\n",
        "top_k = 5\n",
        "I, D = search_similar_questions(query_embeddings, index, top_k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSiN7POcPZL"
      },
      "source": [
        "\n",
        "\n",
        "# 8 Retrieve and Rank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1Ql-pHeI-8"
      },
      "outputs": [],
      "source": [
        "def retrieve_and_rank(df, I, D):\n",
        "    results = []\n",
        "    for i, distances in zip(I, D):\n",
        "        for idx, distance in zip(i, distances):\n",
        "            result = {\n",
        "                'Thread ID': df.iloc[idx]['THREAD_ID'],\n",
        "                'Question': df.iloc[idx]['QUESTION'],  # Raw question text\n",
        "                'Answer': df.iloc[idx]['ANSWER'],  # Raw answer text\n",
        "                'Distance': distance\n",
        "            }\n",
        "            results.append(result)\n",
        "    return results\n",
        "\n",
        "# Example usage for step 8:\n",
        "query = \"I want a tool box?\"\n",
        "query_embeddings = generate_query_embedding(query).reshape(1, -1)\n",
        "\n",
        "top_k = 5\n",
        "I, D = search_similar_questions(query_embeddings, index, top_k)\n",
        "\n",
        "# Load the corpus DataFrame\n",
        "e9_forum_corpus = pd.read_csv(os.path.join(BASE_PATH, 'e9_forum_corpus_faiss.csv'))\n",
        "\n",
        "# Retrieve and rank results\n",
        "ranked_results = retrieve_and_rank(e9_forum_corpus, I, D)\n",
        "for result in ranked_results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okjxc-Mn5qXv"
      },
      "source": [
        "\n",
        "# 9 Answer Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n4dCgZYdZPs"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Initialize the T5 tokenizer and model for conditional generation\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "def generate_answer(query, ranked_results, tokenizer, model):\n",
        "    # Concatenate the retrieved contexts\n",
        "    concatenated_context = \" \".join([result['Answer'] for result in ranked_results])\n",
        "\n",
        "    # Generate the answer using the T5 model\n",
        "    input_ids = tokenizer(f\"answer: {query} context: {concatenated_context}\", return_tensors=\"pt\").input_ids\n",
        "    output_ids = model.generate(input_ids)\n",
        "    generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_answer\n",
        "\n",
        "# Example usage for step 9:\n",
        "query = \"How do I fix the transmission issue in my car?\"\n",
        "generated_answer = generate_answer(query, ranked_results, tokenizer, model)\n",
        "print(f\"Generated Answer: {generated_answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAmDDBpchI6"
      },
      "source": [
        "# 10 Evaluation and Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQuFH6wleDuc"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus_faiss WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBaTl_s-luFh"
      },
      "source": [
        "# 11 Deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YynK8Ccpzw3"
      },
      "source": [
        "\n",
        "\n",
        "1. **Hugging Face Spaces**\n",
        "   - **Pros:** Provides a simple and direct way to deploy and share machine learning models, including RAG models. It supports interactive web-based applications and API endpoints, making it ideal for showcasing projects.\n",
        "\n",
        "   - **Cons:** While convenient for prototypes and demonstrations, it might not offer the scalability and control needed for high-demand production environments.\n",
        "\n",
        "2. **AWS SageMaker**\n",
        "   - **Pros:** Offers a fully managed service that enables data scientists and developers to build, train, and deploy machine learning models at scale. SageMaker supports direct deployment of PyTorch models, including those built with the Hugging Face Transformers library, with robust monitoring and security features.  \n",
        "\n",
        "   - **Cons:** Can be more expensive and requires familiarity with AWS services. The setup and management might be complex for smaller projects or those new to cloud services.\n",
        "\n",
        "3. **Docker + Kubernetes**\n",
        "   - **Pros:** This combination offers flexibility and scalability for deploying machine learning models. Docker containers make it easy to package your RAG model with all its dependencies, while Kubernetes provides orchestration to manage and scale your deployment across multiple instances or cloud providers.  \n",
        "   \n",
        "   - **Cons:** Requires significant DevOps knowledge to setup, manage, and scale. It might be overkill for simple or one-off deployments.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoeXUPmISJ"
      },
      "source": [
        "# 12 Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bvouULWteB2"
      },
      "source": [
        "## Summarization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H0Xnn_aMnYq"
      },
      "outputs": [],
      "source": [
        "# Summarize corpus\n",
        "\n",
        "# Visual comparison of leading options\n",
        "\n",
        "# Sample text for summarization\n",
        "sample_text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board\".\"I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it\".\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours\".God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\".It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does\".\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "def t5_summarize(text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load the tokenizer and model for T5\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,  # Tokens\n",
        "        min_length=30,  # Optional: setting a minimum length for the output\n",
        "        length_penalty=2.0,  # Optional: encourage longer summaries if needed\n",
        "        num_beams=4,  # Optional: number of beams for beam search\n",
        "        early_stopping=True  # Stop when at least num_beams sentences are finished\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return summary, elapsed_time\n",
        "\n",
        "def distilbart_summarize(text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load the tokenizer and model for DistilBART\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "    # Remove new lines to treat the text as one long unformatted piece of text\n",
        "    unformatted_text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        unformatted_text,\n",
        "        max_length=900,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=500,  # Tokens\n",
        "        min_length=30,  # Optional: setting a minimum length for the output\n",
        "        length_penalty=2.0,  # Optional: encourage longer summaries if needed\n",
        "        num_beams=4,  # Optional: number of beams for beam search\n",
        "        early_stopping=True  # Stop when at least num_beams sentences are finished\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Return the summary text and the time taken\n",
        "    return summary, elapsed_time\n",
        "\n",
        "def main():\n",
        "    print(\"Original Text:\\n\", sample_text)\n",
        "\n",
        "    t5_summary, t5_time = t5_summarize(sample_text)\n",
        "    print(\"\\nT5 Summary:\\n\", t5_summary)\n",
        "    print(f\"T5 Time Taken: {t5_time:.2f} seconds\")\n",
        "\n",
        "    distilbart_summary, distilbart_time = distilbart_summarize(sample_text)\n",
        "    print(\"\\nDistilBART Summary:\\n\", distilbart_summary)\n",
        "    print(f\"DistilBART Time Taken: {distilbart_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0r8gc0eJQtF"
      },
      "outputs": [],
      "source": [
        "# Score summarization: T5\n",
        "# Implement an objective score: ROUGE\n",
        "\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def t5_summarize(text, max_length, min_length, num_beams):\n",
        "    # Prepend the text with the task-specific prefix for summarization\n",
        "    input_text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model_t5.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer_t5.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Define the reference summary (ground truth)\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = t5_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zal-aEkL_BFj"
      },
      "outputs": [],
      "source": [
        "# Score summarization: BART\n",
        "\n",
        "# Load the tokenizer and model for DistilBART\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def distilbart_summarize(text, max_length, min_length, num_beams):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,  # Adjusted to fit within the model's constraints\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_length\": [50, 100, 150],\n",
        "    \"min_length\": [10, 30, 50],\n",
        "    \"num_beams\": [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Example input text and reference summary\n",
        "text = \"\"\"\n",
        "And on the 8th day, God looked down on his planned paradise and said,\n",
        "\"I need a caretaker\". God said, \"I need somebody willing to get up before\n",
        "dawn, milk cows, work all day in the fields, milk cows again, eat supper, then\n",
        "go to town and stay past midnight at a meeting of the school board.\" I need\n",
        "somebody with arms strong enough to rustle a calf and yet gentle enough to\n",
        "deliver his own grandchild; somebody to call hogs, tame cantankerous machinery,\n",
        "come home hungry, have to wait lunch until his wife’s done feeding visiting\n",
        "ladies, then tell the ladies to be sure and come back real soon - and mean it.\n",
        "God said, \"I need somebody willing to sit up all night with a newborn colt,\n",
        "and watch it die, then dry his eyes and say, 'Maybe next year.' I need somebody\n",
        "who can shape an ax handle from a persimmon sprout, shoe a horse with a hunk\n",
        "of car tire, who can make harness out of haywire, feed sacks and shoe scraps;\n",
        "who, planting time and harvest season, will finish his forty-hour week by\n",
        "Tuesday noon, and then pain’n from tractor back, put in another seventy-two\n",
        "hours.\" God had to have somebody willing to ride the ruts at double speed to get\n",
        "the hay in ahead of the rain clouds, and yet stop in mid-field and race to help\n",
        "when he sees the first smoke from a neighbor’s place. God said, \"I need somebody\n",
        "strong enough to clear trees and heave bails, yet gentle enough to tame lambs\n",
        "and wean pigs and tend the pink-combed pullets, who will stop his mower for an\n",
        "hour to splint the broken leg of a meadow lark.\" It had to be somebody who’d\n",
        "plow deep and straight and not cut corners; somebody to seed, weed, feed, breed\n",
        "and rake and disc and plow and plant and tie the fleece and strain the milk and\n",
        "replenish the self-feeder and finish a hard week’s work with a five-mile drive\n",
        "to church; somebody who would bale a family together with the soft strong bonds\n",
        "of sharing, who would laugh, and then sigh, and then reply, with smiling eyes,\n",
        "when his son says that he wants to spend his life \"doing what dad does.\"\n",
        "-- so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"\n",
        "God needed someone to take care of the planet, so God made a Farmer.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize best scores and best params\n",
        "best_score = None\n",
        "best_params = None\n",
        "best_recall = None\n",
        "best_precision = None\n",
        "\n",
        "# Perform grid search\n",
        "for max_length, min_length, num_beams in itertools.product(param_grid[\"max_length\"], param_grid[\"min_length\"], param_grid[\"num_beams\"]):\n",
        "    generated_summary = distilbart_summarize(text, max_length, min_length, num_beams)\n",
        "    results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "\n",
        "    # Extract ROUGE-LSum scores\n",
        "    rougeLsum_precision = results['rougeLsum'].mid.precision\n",
        "    rougeLsum_recall = results['rougeLsum'].mid.recall\n",
        "    rougeLsum_fmeasure = results['rougeLsum'].mid.fmeasure\n",
        "\n",
        "    if best_score is None or rougeLsum_fmeasure > best_score:\n",
        "        best_score = rougeLsum_fmeasure\n",
        "        best_params = (max_length, min_length, num_beams)\n",
        "        best_recall = rougeLsum_recall\n",
        "        best_precision = rougeLsum_precision\n",
        "\n",
        "print(f\"Best ROUGE-Lsum F-measure: {best_score:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Recall: {best_recall:.4f}\")\n",
        "print(f\"Best ROUGE-Lsum Precision: {best_precision:.4f}\")\n",
        "print(f\"Best parameters: max_length={best_params[0]}, min_length={best_params[1]}, num_beams={best_params[2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RdIXKt4LPUB"
      },
      "source": [
        "Compare ROGE Summarization Scores:\n",
        "\n",
        "T5\n",
        "\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1778\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.1290\n",
        "*   Best parameters: max_length=50, min_length=10, num_beams=2\n",
        "\n",
        "\n",
        "\n",
        "BART\n",
        "\n",
        "\n",
        "*   Best ROUGE-Lsum F-measure: 0.1270\n",
        "*   Best ROUGE-Lsum Recall: 0.2857\n",
        "*   Best ROUGE-Lsum Precision: 0.0816\n",
        "*   Best parameters: max_length=100, min_length=10, num_beams=4\n",
        "\n",
        "\n",
        "\n",
        "Based on these scores, Ill be using T5 for training due to its higher Precision and no discernable differnce in Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVaVmP2DVcjM"
      },
      "source": [
        "# Parking lot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uptR5ODagddO"
      },
      "source": [
        "## 3.3 Data Storage and Database\n",
        "\n",
        "\n",
        "Efficient data storage and management are pivotal for the project, focusing on accommodating extensive unstructured data from various sources. The project explores two main classes of storage solutions: Cloud Storage and Local Storage, each offering unique benefits and challenges.\n",
        "\n",
        "### 3.3.1 Cloud Storage\n",
        "Cloud storage solutions offer scalability, reliability, and remote access, making them suitable for projects with dynamic data needs and global access requirements.\n",
        "\n",
        "- **Tools:** Snowflake (for relational data), MongoDB Atlas (for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Scalability:** Easily scales to meet growing data demands without the need for physical infrastructure management.\n",
        "        - **Accessibility:** Provides global access to the data, facilitating collaboration and remote work.\n",
        "        - **Maintenance and Security:** Cloud providers manage the security, backups, and maintenance, reducing the administrative burden.\n",
        "    - **Cons:**\n",
        "        - **Cost:** While scalable, costs can increase significantly with data volume and throughput.\n",
        "        - **Internet Dependence:** Requires consistent internet access, which might be a limitation in some scenarios.\n",
        "        - **Data Sovereignty:** Data stored in the cloud may be subject to the laws and regulations of the host country, raising concerns about compliance and privacy.\n",
        "\n",
        "\n",
        "### 3.3.2 Local Storage\n",
        "Local storage solutions rely on on-premises or personal hardware, providing full control over the data and its management but requiring more direct oversight.\n",
        "\n",
        "- **Tools:** MySQL (for relational data), MongoDB (Local installation for NoSQL data)\n",
        "    - **Pros:**\n",
        "        - **Control:** Complete control over the data storage environment and configurations.\n",
        "        - **Cost:** No ongoing costs related to data storage size or access rates, aside from initial hardware and setup.\n",
        "        - **Connectivity:** No reliance on internet connectivity for access, ensuring data availability even in offline scenarios.\n",
        "    - **Cons:**\n",
        "        - **Scalability:** Physical limits to scalability; expanding storage capacity requires additional hardware.\n",
        "        - **Maintenance:** Requires dedicated resources for maintenance, backups, and security, increasing the administrative burden.\n",
        "        - **Accessibility:** Data is not as easily accessible from remote locations, potentially hindering collaboration and remote access needs.\n",
        "\n",
        "\n",
        "**Conclusion: I will be using Snowflake to store my corpus.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3RpQ4jL_oEW"
      },
      "source": [
        "## Step 5 Tokenize and Embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HADiQJr4_WL"
      },
      "source": [
        "# Step 9 Score query result quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ir9PZxznFQ"
      },
      "outputs": [],
      "source": [
        "# Step 9:\n",
        "# Query Processing and Search of LDA derived topics\n",
        "# This step reuires LDA to have run first to generate topic sentences\n",
        "\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Data_sets/e9/'\n",
        "CREDENTIALS_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/snowflake_credentials'\n",
        "\n",
        "faiss_index_path = os.path.join(BASE_PATH, 'faiss_index_t5.index')\n",
        "representative_sentences_path = os.path.join(BASE_PATH, 'representative_sentences.csv')\n",
        "similarity_scores_output_path = os.path.join(BASE_PATH, 'similarity_scores_with_answers.csv')\n",
        "similarity_threshold = 0.01  # Set your threshold value here\n",
        "\n",
        "def load_credentials(path_to_credentials):\n",
        "    with open(path_to_credentials, 'r') as file:\n",
        "        for line_num, line in enumerate(file, start=1):\n",
        "            line = line.strip()\n",
        "            if line and '=' in line:\n",
        "                key, value = line.split('=')\n",
        "                os.environ[key] = value\n",
        "            else:\n",
        "                print(f\"Issue with line {line_num} in {path_to_credentials}: '{line}'\")\n",
        "                # Optionally raise an error or handle the issue as needed\n",
        "def connect_to_snowflake():\n",
        "    \"\"\"Establish a connection to the Snowflake database.\"\"\"\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.environ.get('USER'),\n",
        "        password=os.environ.get('PASSWORD'),\n",
        "        account=os.environ.get('ACCOUNT')\n",
        "    )\n",
        "\n",
        "# Load the rebuilt FAISS index\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "# Initialize the T5 tokenizer and encoder model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function to tokenize text using T5 tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
        "\n",
        "# Function to generate embeddings for a new query using the T5 model\n",
        "def generate_query_embedding(query):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(query_tokens)\n",
        "        query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Average pooling\n",
        "    return torch.tensor(query_embedding).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to search FAISS index for the most similar question\n",
        "def search_faiss_index(query_embedding, index, k=1):\n",
        "    query_embedding_np = query_embedding.numpy().astype('float32')  # Convert to NumPy array of type float32\n",
        "    D, I = index.search(query_embedding_np, k)  # Search\n",
        "    valid_indices = [idx for idx in I[0] if idx >= 0]\n",
        "    similarity_scores = D[0][:len(valid_indices)]  # Get similarity scores for valid indices\n",
        "    return valid_indices, similarity_scores  # Return only valid indices and their scores\n",
        "\n",
        "def fetch_answers_from_snowflake(indices):\n",
        "    \"\"\"Fetch answers and embeddings from Snowflake for given indices.\"\"\"\n",
        "    if not indices:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no valid indices\n",
        "\n",
        "    load_credentials(CREDENTIALS_PATH)\n",
        "    conn = connect_to_snowflake()\n",
        "    cur = conn.cursor()\n",
        "    query = f\"SELECT THREAD_ID, ANSWER FROM e9_corpus.e9_corpus_schema.e9_forum_corpus_faiss WHERE THREAD_ID IN ({','.join(map(str, indices))})\"\n",
        "    if indices:\n",
        "        cur.execute(query)\n",
        "        answers = cur.fetch_pandas_all()\n",
        "    else:\n",
        "        answers = pd.DataFrame()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return answers\n",
        "\n",
        "def process_representative_sentences():\n",
        "    # Load representative sentences\n",
        "    representative_sentences_df = pd.read_csv(representative_sentences_path)\n",
        "\n",
        "    # Generate embeddings and calculate similarity scores for each representative sentence\n",
        "    results = []\n",
        "\n",
        "    for idx, row in representative_sentences_df.iterrows():\n",
        "        topic = row['Topic']\n",
        "        sentence = row['Representative Sentence']\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = generate_query_embedding(sentence)\n",
        "\n",
        "        # Ensure the dimension matches\n",
        "        if query_embedding.shape[1] != index.d:\n",
        "            raise ValueError(f\"Embedding dimension mismatch: {query_embedding.shape[1]} vs {index.d}\")\n",
        "\n",
        "        # Search FAISS index\n",
        "        similar_indices, similarity_scores = search_faiss_index(query_embedding, index, k=1)\n",
        "\n",
        "        # Fetch answers from Snowflake\n",
        "        answer = None\n",
        "        score = None\n",
        "        if similar_indices:\n",
        "            answers = fetch_answers_from_snowflake(similar_indices)\n",
        "\n",
        "            if not answers.empty:\n",
        "                for idx, score in zip(similar_indices, similarity_scores):\n",
        "                    matching_answers = answers.loc[answers['THREAD_ID'] == idx, 'ANSWER'].values\n",
        "                    if len(matching_answers) > 0:\n",
        "                        answer = matching_answers[0]\n",
        "                        break\n",
        "        results.append({\n",
        "            'Representative Sentence': sentence,\n",
        "            'Answer': answer,\n",
        "            'Similarity Score': score\n",
        "        })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(similarity_scores_output_path, index=False)\n",
        "    print(\"Results saved.\")\n",
        "\n",
        "    # Output results\n",
        "    for result in results:\n",
        "        print(f\"Representative Sentence: {result['Representative Sentence']}\")\n",
        "        print(f\"Answer: {result['Answer']}\")\n",
        "        print(f\"Similarity Score: {result['Similarity Score']}\\n\")\n",
        "\n",
        "def main():\n",
        "    process_representative_sentences()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1A9uW-2OyR9fa3DZlxEvVAAVJPf7jpGrw",
      "authorship_tag": "ABX9TyPHDcQth36vZ7w8+MTS1jw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f2335a360dc493e80e84497ea2fd7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89ab4b91a3f44d11865617735eb6fc21",
              "IPY_MODEL_3e6f81c5ff784dcc820cb159305d9c66",
              "IPY_MODEL_6039aadeac5e44edbb50767eebfea081"
            ],
            "layout": "IPY_MODEL_1ab1562a43a44b60b5b6dc29defc58f7"
          }
        },
        "89ab4b91a3f44d11865617735eb6fc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f13348612de14f0fa883b23224225fdf",
            "placeholder": "​",
            "style": "IPY_MODEL_081d569be12d41c283c6d79b64f1f052",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3e6f81c5ff784dcc820cb159305d9c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4dcad77816f4029a1ee4a02fc3db813",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_189af619a3154000aa9b70c4037a89d4",
            "value": 2324
          }
        },
        "6039aadeac5e44edbb50767eebfea081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b829d1d0cd4087aa1486a4b5a1eb2c",
            "placeholder": "​",
            "style": "IPY_MODEL_f05fd91f7168447799ae75830941fe5a",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 120kB/s]"
          }
        },
        "1ab1562a43a44b60b5b6dc29defc58f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f13348612de14f0fa883b23224225fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "081d569be12d41c283c6d79b64f1f052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4dcad77816f4029a1ee4a02fc3db813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "189af619a3154000aa9b70c4037a89d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1b829d1d0cd4087aa1486a4b5a1eb2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05fd91f7168447799ae75830941fe5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "813dc0fe25cf4e8695657f49ae75953c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c12d7c17da664cb38778bc37368665a5",
              "IPY_MODEL_6d4fb8bb903148e8bcb756f3b9203bdb",
              "IPY_MODEL_c55a126a337b4bb0989463573e2e29ca"
            ],
            "layout": "IPY_MODEL_d03b8e666c5c4dd3b8394d3840285c4c"
          }
        },
        "c12d7c17da664cb38778bc37368665a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe2fd27a24f47e1a7f07acfa0aa3b87",
            "placeholder": "​",
            "style": "IPY_MODEL_52ae4ee67a494f60a6f6edd6a877c035",
            "value": "spiece.model: 100%"
          }
        },
        "6d4fb8bb903148e8bcb756f3b9203bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8839db19183f4595904846635f2ee19e",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40f20485e99940ad9949ccf382bd6964",
            "value": 791656
          }
        },
        "c55a126a337b4bb0989463573e2e29ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69063d4320b42b38b83894ef27d9cae",
            "placeholder": "​",
            "style": "IPY_MODEL_62eab04215b945fbac8192f8294d775b",
            "value": " 792k/792k [00:00&lt;00:00, 15.6MB/s]"
          }
        },
        "d03b8e666c5c4dd3b8394d3840285c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fe2fd27a24f47e1a7f07acfa0aa3b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ae4ee67a494f60a6f6edd6a877c035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8839db19183f4595904846635f2ee19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f20485e99940ad9949ccf382bd6964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a69063d4320b42b38b83894ef27d9cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62eab04215b945fbac8192f8294d775b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0525271af699437e83e1ee25f1fb6766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba27e8a6dca74693a165eb1d18c511be",
              "IPY_MODEL_a98b000427df41d4a3732b84308e1563",
              "IPY_MODEL_86b0331b383c45289b981884dee08c50"
            ],
            "layout": "IPY_MODEL_b762bff093604ca78e1c170f6cd26e91"
          }
        },
        "ba27e8a6dca74693a165eb1d18c511be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e340a70859d04d318106ed9557406eea",
            "placeholder": "​",
            "style": "IPY_MODEL_2ffbb327a4e3441da83fe479b32c48c7",
            "value": "tokenizer.json: 100%"
          }
        },
        "a98b000427df41d4a3732b84308e1563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb73f91f111b44b18d459815706e9096",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fec9b8f40b14fbcaa740ccb91fac7ab",
            "value": 1389353
          }
        },
        "86b0331b383c45289b981884dee08c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22fb1906c6034540bbf024b0ef6c881a",
            "placeholder": "​",
            "style": "IPY_MODEL_e86903e3bdb24e9e8c26a590e17c64d1",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 30.0MB/s]"
          }
        },
        "b762bff093604ca78e1c170f6cd26e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e340a70859d04d318106ed9557406eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ffbb327a4e3441da83fe479b32c48c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb73f91f111b44b18d459815706e9096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fec9b8f40b14fbcaa740ccb91fac7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22fb1906c6034540bbf024b0ef6c881a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86903e3bdb24e9e8c26a590e17c64d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2fdce18e7df45a695ce6cb9465451af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c9eadd541704f528794b0c8e60ca6b9",
              "IPY_MODEL_830714b8e55b4209a615da4d04a16980",
              "IPY_MODEL_76b6d174cc5a40ab848204802715c7ba"
            ],
            "layout": "IPY_MODEL_412d1c037e7a48f2b773ea1d2657b33c"
          }
        },
        "3c9eadd541704f528794b0c8e60ca6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_348934ecc5c444b8b69da63efd5c0d8b",
            "placeholder": "​",
            "style": "IPY_MODEL_10cfe3b9ec7e4c5bba5690fba1c7ed1e",
            "value": "config.json: 100%"
          }
        },
        "830714b8e55b4209a615da4d04a16980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34476a8649834eb4bce9dcf9bd37e03a",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_210334e27c7a4896b117ed15ee1daccd",
            "value": 1206
          }
        },
        "76b6d174cc5a40ab848204802715c7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aefd38e6090646678f9099139cdb1501",
            "placeholder": "​",
            "style": "IPY_MODEL_41d274e5608443e68d1d7680e2357c4a",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 75.3kB/s]"
          }
        },
        "412d1c037e7a48f2b773ea1d2657b33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348934ecc5c444b8b69da63efd5c0d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10cfe3b9ec7e4c5bba5690fba1c7ed1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34476a8649834eb4bce9dcf9bd37e03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210334e27c7a4896b117ed15ee1daccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aefd38e6090646678f9099139cdb1501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d274e5608443e68d1d7680e2357c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "090c4728bfa34cf1b1137ca49da508cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03d4b91927854793a93ded643f6b1b30",
              "IPY_MODEL_fb36e339cbe647d39df3bd815190ed70",
              "IPY_MODEL_dd1d1db3a937471e8791dc517ebbf712"
            ],
            "layout": "IPY_MODEL_357fa34619ae46d0a08f6345bcb2d394"
          }
        },
        "03d4b91927854793a93ded643f6b1b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9d1ce7fb5914fab8816a9f9719e23b9",
            "placeholder": "​",
            "style": "IPY_MODEL_c8a2c9f8dae049feb5f60d8adc39cb59",
            "value": "model.safetensors: 100%"
          }
        },
        "fb36e339cbe647d39df3bd815190ed70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8014bdf6c70410699f73ce00d1a44f4",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d304b1091d947ac9e5569346d8ed93e",
            "value": 242043056
          }
        },
        "dd1d1db3a937471e8791dc517ebbf712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_888365f499ec4b1bbe143d170fdfe4f6",
            "placeholder": "​",
            "style": "IPY_MODEL_aed2fa800e0644119b90cab85ea49a16",
            "value": " 242M/242M [00:03&lt;00:00, 40.6MB/s]"
          }
        },
        "357fa34619ae46d0a08f6345bcb2d394": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9d1ce7fb5914fab8816a9f9719e23b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a2c9f8dae049feb5f60d8adc39cb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8014bdf6c70410699f73ce00d1a44f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d304b1091d947ac9e5569346d8ed93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "888365f499ec4b1bbe143d170fdfe4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed2fa800e0644119b90cab85ea49a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6334c872890464684841093b7fee63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b762f12de4044047a8f57ac1cdda8a19",
              "IPY_MODEL_4c7c38a010c3415b96578a0da935d7ef",
              "IPY_MODEL_d3938b6b7b8c4180bc6128ed757a52da"
            ],
            "layout": "IPY_MODEL_e7c9d0adf1d3465bac7dfec5c45895a2"
          }
        },
        "b762f12de4044047a8f57ac1cdda8a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc96a637d35f4843b2ec4edac25636b9",
            "placeholder": "​",
            "style": "IPY_MODEL_1327d2a4cf17483196c9d6b63754bca1",
            "value": "generation_config.json: 100%"
          }
        },
        "4c7c38a010c3415b96578a0da935d7ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c87601355ee145a88fb19ed7f9966ea8",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa4f5e4f8fbc4cc39d100ef72ca30bdf",
            "value": 147
          }
        },
        "d3938b6b7b8c4180bc6128ed757a52da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27f7d802b7d1486e93936c0dd87c53cd",
            "placeholder": "​",
            "style": "IPY_MODEL_2831acd49ae34e819f3d51e52b86d8c2",
            "value": " 147/147 [00:00&lt;00:00, 7.09kB/s]"
          }
        },
        "e7c9d0adf1d3465bac7dfec5c45895a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc96a637d35f4843b2ec4edac25636b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1327d2a4cf17483196c9d6b63754bca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c87601355ee145a88fb19ed7f9966ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4f5e4f8fbc4cc39d100ef72ca30bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27f7d802b7d1486e93936c0dd87c53cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2831acd49ae34e819f3d51e52b86d8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}