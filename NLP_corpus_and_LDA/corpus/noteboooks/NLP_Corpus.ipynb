{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidelgas/DataSciencePortfolio/blob/main/NLP_corpus_and_LDA/corpus/noteboooks/NLP_Corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACjQwCY2Pej"
      },
      "source": [
        "# Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTZfGb8ciCXc"
      },
      "source": [
        "This focus of this project is on the creation of a corpus that will be utilized in several Natural Language Processing (NLP) effforts, including LDA, GRU, LSTM and Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkfw0hPnfuKI"
      },
      "source": [
        "## Corpus Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3tNUl3yfwv0"
      },
      "source": [
        "The corpus developed here was assembled by scraping a pubic forum specific to the BMW E9 automobile (www.e9coupe.com). This active forum has been in exsitence since 2003.\n",
        "The code was written in Python using Google Colab Notebooks and leveraging Beautiful Soup. Raw text was compiled and stored in a Snowflake database to support multiple NLP projects. Furture ideas include supplementing the forum corpus with an existing users guide specific to this car make and model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLQhNnniw35U"
      },
      "source": [
        "### Create Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIVXqhBsgBXP",
        "outputId": "f0ad6714-6352-450c-ad85-431c6632c8c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This seems to propagate credentials better from its own cell\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55VxBMZZom_d",
        "outputId": "10cc0601-d3f2-44a7-9922-1e74bad654fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (42.0.5)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (24.0.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.10.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.13.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Collecting platformdirs<4.0.0,>=2.6.0 (from snowflake-connector-python)\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.12.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.0.7)\n",
            "Installing collected packages: asn1crypto, tomlkit, platformdirs, snowflake-connector-python\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.2.0\n",
            "    Uninstalling platformdirs-4.2.0:\n",
            "      Successfully uninstalled platformdirs-4.2.0\n",
            "Successfully installed asn1crypto-1.5.1 platformdirs-3.11.0 snowflake-connector-python-3.7.1 tomlkit-0.12.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install requests\n",
        "import requests\n",
        "\n",
        "!pip3 install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip3 install pandas\n",
        "import pandas as pd\n",
        "\n",
        "!pip3 install numpy\n",
        "import numpy as np\n",
        "\n",
        "!pip install snowflake-connector-python\n",
        "import snowflake.connector\n",
        "\n",
        "import re\n",
        "\n",
        "import os\n",
        "\n",
        "import logging\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_OvjOpsd2j"
      },
      "source": [
        "## Create corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTEdShd_5KgC"
      },
      "source": [
        "Ill be scraping posts from my classic car forum for the corpus. Ill be limiting the data retreival while I build the model so I dont impact the site for users. Ill be using Beautiful Soup where possible to parse the content into a dataframe structure. The admin of the forum has been notified that I am experimenting with ways to improve the online community."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak2zFe-N8W3U",
        "outputId": "177c050d-3120-40a0-f689-8623a388e154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with thread_id 15000\n",
            "Processing additional 500 threads\n",
            "Ending with thread_id 15500\n"
          ]
        }
      ],
      "source": [
        "# Generate the list of thread_ids to scrape and parse\n",
        "# There are currently approximately 15k threads\n",
        "\n",
        "# Set the file path to save files\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_thread_ids.csv'\n",
        "\n",
        "# Set the number of incremental thread_ids to process\n",
        "threads = 500\n",
        "\n",
        "# Check if the file exists and has content. If it does, update last_thread_id\n",
        "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "    e9_forum_thread_ids = pd.read_csv(file_path)\n",
        "    last_thread_id = e9_forum_thread_ids['thread_id'].iloc[-1]\n",
        "    last_thread_id = int(last_thread_id)  # Convert to integer\n",
        "\n",
        "else:\n",
        "    last_thread_id = 0\n",
        "\n",
        "# Function to create URLs from the thread_ids\n",
        "def create_urls(threads, last_thread_id):\n",
        "    urls = []\n",
        "    for thread_id in range(last_thread_id + 1, last_thread_id + threads + 1):\n",
        "        urls.append({'thread_id': thread_id})\n",
        "    return urls\n",
        "\n",
        "urls = create_urls(threads, last_thread_id)\n",
        "\n",
        "last_thread_id_processed = urls[-1]['thread_id']\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "e9_forum_thread_ids = pd.DataFrame(urls)\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "e9_forum_thread_ids.to_csv(file_path, mode='a', header=['thread_id'], index=False)\n",
        "\n",
        "print(\"Starting with thread_id \" + str(last_thread_id))\n",
        "print(\"Processing additional \" + str(threads) + \" threads\")\n",
        "print(\"Ending with thread_id \" + str(last_thread_id_processed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y_iniU_XmWV"
      },
      "outputs": [],
      "source": [
        "# Generate the URL and title for each thread\n",
        "\n",
        "pages = 1\n",
        "\n",
        "def fetch_thread_data(df, pages=1):\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        thread_url = f\"https://e9coupe.com/forum/threads/{thread_id}\"\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{thread_url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.find('title').get_text()\n",
        "            thread_title = title.split('|')[0].strip()\n",
        "            df.at[index, 'thread_title'] = thread_title\n",
        "            df.at[index, 'thread_url'] = page_url\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fetch thread URLs and title\n",
        "e9_forum_threads = fetch_thread_data(e9_forum_thread_ids)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_threads.csv'\n",
        "\n",
        "header = ['thread_id', 'thread_title', 'thread_url']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU9i__qKWJhP",
        "outputId": "48e06f61-6c51-4d40-b444-3e7c8a3d2840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   thread_id     500 non-null    int64 \n",
            " 1   thread_title  500 non-null    object\n",
            " 2   thread_url    500 non-null    object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 11.8+ KB\n"
          ]
        }
      ],
      "source": [
        "e9_forum_threads.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcCYXQwhXjVw"
      },
      "outputs": [],
      "source": [
        "# Find the first post in the thread creation\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_first_post_content(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url, thread_title in zip(df['thread_id'], df['thread_url'], df['thread_title']):\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        if first_post:\n",
        "            post_content = first_post.get_text(strip=True)\n",
        "        else:\n",
        "            post_content = \"No content found\"  # Handle case where no post content is found\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_first_post': post_content})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Fetch first post content\n",
        "data = fetch_first_post_content(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_threads_decorated = pd.DataFrame(data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = '/content/drive/MyDrive/Data_sets/e9/e9_forum_threads_decorated.csv'\n",
        "\n",
        "header = not os.path.exists(file_path)\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads_decorated.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoCSlyqsXOWC"
      },
      "outputs": [],
      "source": [
        "# Find all posts associated with each thread\n",
        "\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')  # Correct class name as example\n",
        "        for article in articles:\n",
        "            # Extracting post timestamp instead of post ID\n",
        "            post_timestamp = article.find('time')['datetime'] if article.find('time') else 'N/A'\n",
        "            content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "            post_data.append({\n",
        "                'thread_id': row['thread_id'],\n",
        "                'post_timestamp': post_timestamp,\n",
        "                'post_raw': content\n",
        "            })\n",
        "\n",
        "    return post_data\n",
        "\n",
        "# Fetch all thread post content\n",
        "post_data = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "# Convert to DataFrame\n",
        "e9_forum_posts = pd.DataFrame(post_data)\n",
        "\n",
        "# Export and save result\n",
        "file_path = ('/content/drive/MyDrive/Data_sets/e9/e9_forum_posts.csv')\n",
        "\n",
        "header = ['thread_id', 'post_timestamp','post_raw']\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_posts.to_csv(file_path, mode='a', header=header, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyDiQVwTWNnj",
        "outputId": "9db7d0a4-a455-48e2-c0f6-34469add7ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3682 entries, 0 to 3681\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   thread_id       3682 non-null   int64 \n",
            " 1   post_timestamp  3682 non-null   object\n",
            " 2   post_raw        3682 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 86.4+ KB\n"
          ]
        }
      ],
      "source": [
        "e9_forum_posts.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9E7PB4BERNx"
      },
      "outputs": [],
      "source": [
        "# Create the corpus by aggregating all posts into one column\n",
        "# and merging with the threads df\n",
        "\n",
        "# Group by THREAD_ID and concatenate the POST_RAW values\n",
        "aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Rename the column to indicate that it contains concatenated post content\n",
        "aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "# Cast 'thread_id' column to int64 in both DataFrames\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype('int64')\n",
        "aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "# Merge the two DataFrames\n",
        "e9_forum_corpus = pd.merge(e9_forum_threads_decorated, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_corpus.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_corpus.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxP6Gu6pERQC",
        "outputId": "1a2add31-16dd-436b-c9e4-b40c3c7ba4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 500 entries, 0 to 499\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   thread_id          500 non-null    int64 \n",
            " 1   thread_title       500 non-null    object\n",
            " 2   thread_first_post  500 non-null    object\n",
            " 3   thread_all_posts   491 non-null    object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 19.5+ KB\n"
          ]
        }
      ],
      "source": [
        "e9_forum_corpus.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFEBuCRF9Rr"
      },
      "source": [
        "## Load tables into Snowflake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitYGdJqF9R3",
        "outputId": "e5c20151-8cd1-42b8-bc14-0555a5edc548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database and schema created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Create the db and schema\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create a database for the corpus and load the tables\n",
        "try:\n",
        "    # Create a new database\n",
        "    cur.execute(\"CREATE DATABASE IF NOT EXISTS e9_corpus\")\n",
        "\n",
        "    # Use the new database\n",
        "    cur.execute(\"USE DATABASE e9_corpus\")\n",
        "\n",
        "    # Create a new schema\n",
        "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS e9_corpus_schema\")\n",
        "\n",
        "    print(\"Database and schema created successfully.\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "cur.close()\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsgsobGIOvn",
        "outputId": "e6773829-3b7b-470a-ff68-0feb339886a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inserted into e9_forum_corpus table.\n"
          ]
        }
      ],
      "source": [
        "# Clean the file\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and symbols using regex\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    return cleaned_text\n",
        "\n",
        "e9_forum_threads = e9_forum_threads.applymap(clean_text)\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Check if the table exists\n",
        "try:\n",
        "    cur.execute(\"SELECT 1 FROM e9_corpus.e9_corpus_schema.e9_forum_corpus LIMIT 1\")\n",
        "    table_exists = True\n",
        "except snowflake.connector.errors.ProgrammingError:\n",
        "    table_exists = False\n",
        "\n",
        "# If the table does not exist, create it\n",
        "if not table_exists:\n",
        "    try:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE e9_corpus.e9_corpus_schema.e9_forum_corpus (\n",
        "            thread_id NUMBER(38,0),\n",
        "            thread_title VARCHAR(16777216),\n",
        "            thread_first_post VARCHAR(16777216),\n",
        "            thread_all_posts VARCHAR(16777216)\n",
        "        )\n",
        "        \"\"\")\n",
        "        print(\"e9_forum_corpus table created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# Insert data into e9_forum_corpus table\n",
        "for index, row in e9_forum_corpus.iterrows():\n",
        "\n",
        "    row = row.where(pd.notnull(row), None)\n",
        "\n",
        "    # Prepare the INSERT command with placeholders for the values\n",
        "    insert_command = \"\"\"\n",
        "    INSERT INTO e9_corpus.e9_corpus_schema.e9_forum_corpus\n",
        "    (thread_id, thread_title, thread_first_post, thread_all_posts)\n",
        "    VALUES\n",
        "    (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the row values as parameters to safely insert the data\n",
        "    cur.execute(insert_command, (row['thread_id'], row['thread_title'], row['thread_first_post'], row['thread_all_posts']))\n",
        "    conn.commit()\n",
        "\n",
        "print(\"Data inserted into e9_forum_corpus table.\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm dataset in Snowflake\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Select source data\n",
        "query = \"\"\"\n",
        "SELECT * FROM \"E9_CORPUS\".\"E9_CORPUS_SCHEMA\".\"E9_FORUM_CORPUS\";\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "# Load data into a df.\n",
        "e9_forum_corpus = cur.fetch_pandas_all()\n",
        "e9_forum_corpus.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOvcujDA47tw",
        "outputId": "89aa7605-22ec-4bbd-98bf-f117deb0bda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15437 entries, 0 to 15436\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   THREAD_ID          15437 non-null  int16 \n",
            " 1   THREAD_TITLE       15437 non-null  object\n",
            " 2   THREAD_FIRST_POST  15437 non-null  object\n",
            " 3   THREAD_ALL_POSTS   15153 non-null  object\n",
            "dtypes: int16(1), object(3)\n",
            "memory usage: 392.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "vlBbN3lXERWq",
        "outputId": "3fc6ff95-670c-4d68-d23a-f2b5823fa974"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopExecution",
          "evalue": "Execution stopped by user",
          "traceback": []
        }
      ],
      "source": [
        "# This code cell will stop execution of subsequent cells\n",
        "\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass  # This will prevent the traceback from being shown\n",
        "\n",
        "raise StopExecution(\"Execution stopped by user\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjNpAOZFF9R3"
      },
      "outputs": [],
      "source": [
        "# Clean threads df and add to snowflake\n",
        "\n",
        "# Clean the file\n",
        "def clean_text(text):\n",
        "\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "e9_forum_threads = e9_forum_threads.applymap(clean_text)\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create the e9_forum_threads table\n",
        "try:\n",
        "  cur.execute(\"\"\"\n",
        "  CREATE OR REPLACE TABLE E9_CORPUS.e9_corpus_schema.e9_forum_threads (\n",
        "    thread_id NUMBER(38,0),\n",
        "    thread_title VARCHAR(16777216),\n",
        "    thread_first_post VARCHAR(16777216)\n",
        "  )\n",
        "  \"\"\")\n",
        "\n",
        "  # Insert data into e9_forum_threads table\n",
        "  for index, row in e9_forum_threads.iterrows():\n",
        "      cur.execute(f\"\"\"\n",
        "      INSERT INTO E9_CORPUS.e9_corpus_schema.e9_forum_threads\n",
        "      (thread_id, thread_title, thread_first_post)\n",
        "      VALUES\n",
        "      ({row['thread_id']}, '{row['thread_title']}', '{row['thread_first_post']}')\n",
        "      \"\"\")\n",
        "      conn.commit()\n",
        "\n",
        "  print(\"e9_forum_threads created successfully.\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "cur.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHdzN9jYIiq_"
      },
      "outputs": [],
      "source": [
        "# Clean posts df and add to snowflake\n",
        "\n",
        "# Clean the file\n",
        "def clean_text(text):\n",
        "    # Remove special characters and symbols using regex\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "e9_forum_posts = e9_forum_posts.applymap(clean_text)\n",
        "\n",
        "\n",
        "# Set the snowflake account and login information\n",
        "path_to_credentials = '/content/drive/MyDrive/credentials/snowflake_credentials'\n",
        "\n",
        "# Load the credentials\n",
        "with open(path_to_credentials, 'r') as file:\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        os.environ[key] = value\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.environ.get('USER'),\n",
        "    password=os.environ.get('PASSWORD'),\n",
        "    account=os.environ.get('ACCOUNT'),\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create the e9_forum_posts table\n",
        "try:\n",
        "  cur.execute(\"\"\"\n",
        "  CREATE OR REPLACE TABLE E9_CORPUS.e9_corpus_schema.e9_forum_posts (\n",
        "    thread_id NUMBER(38,0),\n",
        "    post_timestamp VARCHAR(16777216),\n",
        "    post_raw VARCHAR(16777216)\n",
        "  )\n",
        "  \"\"\")\n",
        "\n",
        "  # Insert data into e9_forum_posts table\n",
        "  for index, row in e9_forum_posts.iterrows():\n",
        "      cur.execute(f\"\"\"\n",
        "      INSERT INTO E9_CORPUS.e9_corpus_schema.e9_forum_posts\n",
        "      (thread_id, post_timestamp, post_raw)\n",
        "      VALUES\n",
        "      ({row['thread_id']}, '{row['post_timestamp']}', '{row['post_raw']}')\n",
        "      \"\"\")\n",
        "      conn.commit()\n",
        "\n",
        "  print(\"e9_forum_posts created successfully.\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "cur.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL-OCMHHfglW"
      },
      "outputs": [],
      "source": [
        "e9_forum_posts.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNen5FkiwXUN"
      },
      "source": [
        "## Parking Lot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZJRhp0YdArx"
      },
      "source": [
        "## Create another corpus from user manual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpae-0NcZ5Nw"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Read in the manual\n",
        "file = open(\"/content/drive/MyDrive/Data_sets/e9_manual.txt\", \"r\")\n",
        "manual_raw = file.read()\n",
        "file.close()\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Load and prepare dataset\n",
        "file_path = '/content/drive/MyDrive/e9/nlp/df_table_3.csv'\n",
        "#file_path = '/content/drive/MyDrive/Data_sets/e9_manual.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()  # Assume each line is a separate data entry\n",
        "\n",
        "# Tokenize each line separately to treat each as a sample\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for line in lines:\n",
        "    tokens = tokenizer(line, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    input_ids.append(tokens['input_ids'])\n",
        "    attention_masks.append(tokens['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids)\n",
        "attention_masks = torch.cat(attention_masks)\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_inputs, val_inputs, train_masks, val_masks = train_test_split(input_ids, attention_masks, test_size=0.1, random_state=42)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_ids, masks):\n",
        "        self.input_ids = input_ids\n",
        "        self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.input_ids.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.masks[idx]\n",
        "        }\n",
        "\n",
        "# Create DataLoader for train and validation datasets\n",
        "train_dataset = TextDataset(train_inputs, train_masks)\n",
        "val_dataset = TextDataset(val_inputs, val_masks)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # Reduced batch size\n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "# Prepare for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define the training and evaluation loop\n",
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = input_ids.clone()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f'Epoch {epoch}, Validation Loss: {avg_val_loss}')\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model_path = '/content/drive/MyDrive/e9/pytorch'\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb4yLcE8Gx7n"
      },
      "source": [
        "# Below is code from an older effort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBMLUuGQ7_gw"
      },
      "outputs": [],
      "source": [
        "# Original effort\n",
        "# Create list of URLs to scrape and parse\n",
        "\n",
        "threads = 100\n",
        "urls = []\n",
        "\n",
        "def create_urls(threads, page_number=1):\n",
        "    base_url = 'https://e9coupe.com/forum/threads/'\n",
        "    # Iterate over thread IDs to generate URLs\n",
        "    for thread_id in range(1, threads + 1):\n",
        "        thread_url = f\"{base_url}{thread_id}\"\n",
        "        urls.append({'thread_id': thread_id, 'thread_url': thread_url})\n",
        "\n",
        "create_urls(threads)  # Using the 'threads' variable\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "e9_forum_urls = pd.DataFrame(urls)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "e9_forum_urls.head()\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_urls.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_urls.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeK3mLx-e2BQ"
      },
      "outputs": [],
      "source": [
        "# Original effort\n",
        "\n",
        "# Process URLs into a dataframe of thread ids and thread titles. This will be\n",
        "# the core table while I decorate the dataframe with additional metadata.\n",
        "\n",
        "# Each root URL can contain multiple pages. To limit any potential\n",
        "# impact the this production data source, the data here is limited to the\n",
        "# first page of each URL which contains as many as 20 individual member posts.\n",
        "\n",
        "pages = 1\n",
        "\n",
        "data = []\n",
        "\n",
        "df_threads = pd.DataFrame()\n",
        "\n",
        "def fetch_thread_data(df):\n",
        "    for index, row in df.iterrows():\n",
        "        thread_id = row['thread_id']\n",
        "        url = row['thread_url']\n",
        "        for i in range(1, pages + 1):\n",
        "            page_url = f\"{url}/?page={i}\"  # Construct the page URL\n",
        "            response = requests.get(page_url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                title = soup.find('title').get_text()\n",
        "                thread_title = title.split('|')[0].strip()  # Extract the thread title\n",
        "\n",
        "                data.append({'thread_id': thread_id, 'thread_title': thread_title, 'thread_url': page_url})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Fetch thread data\n",
        "data = fetch_thread_data(e9_forum_urls)\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "e9_forum_threads = pd.DataFrame(data)\n",
        "\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype(int)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "e9_forum_threads.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fLdgu0iJ2g_"
      },
      "outputs": [],
      "source": [
        "# Extract the inital content when the thread was created.\n",
        "# This text will be used to create a short description.\n",
        "\n",
        "def fetch_first_post_content(df):\n",
        "    data = []\n",
        "\n",
        "    for thread_id, thread_url in zip(df['thread_id'], df['thread_url']): # ensures pairing\n",
        "        response = requests.get(thread_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        first_post = soup.find('article', class_='message-body')\n",
        "        post_content = first_post.get_text(strip=True)\n",
        "\n",
        "        data.append({'thread_id': thread_id, 'thread_first_post': post_content})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Fetch first post content and convert to DataFrame\n",
        "data = fetch_first_post_content(e9_forum_threads)\n",
        "first_post_df = pd.DataFrame(data)\n",
        "\n",
        "# Casting the values I want to join on\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype(int)\n",
        "first_post_df['thread_id'] = first_post_df['thread_id'].astype(int)\n",
        "\n",
        "# Update the df\n",
        "e9_forum_threads = pd.merge(e9_forum_threads, first_post_df, on='thread_id', how='left')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "e9_forum_threads.head()\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_threads.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_threads.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWwiLTr0mKHO"
      },
      "outputs": [],
      "source": [
        "# Fetch all post data from each thread\n",
        "\n",
        "# As written this will fetch all the posts on the first page, which is 20\n",
        "# This might need to be updated to iterate through all page values (1 through n)\n",
        "\n",
        "def fetch_and_parse_thread(df):\n",
        "    post_data = []\n",
        "    processed_posts = set()\n",
        "    for index, row in df.iterrows():\n",
        "        response = requests.get(row['thread_url'])\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message--post')  # Correct class name as example\n",
        "        for article in articles:\n",
        "            post_id = article.get('id', 'N/A')\n",
        "            numeric_post_id = re.findall(r'\\d+', post_id)[0] if re.findall(r'\\d+', post_id) else 'N/A'\n",
        "\n",
        "            if numeric_post_id not in processed_posts:\n",
        "                processed_posts.add(numeric_post_id)\n",
        "                content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "                #timestamp = article.find('time', class_='u-dt').get_text(strip=True) if article.find('time', class_='u-dt') else 'N/A'\n",
        "                #post_number_element = article.find('ul', class_='message-attribution-opposite').find('li').find_next_sibling('li')\n",
        "                #post_number = post_number_element.get_text(strip=True) if post_number_element else 'N/A'\n",
        "                #post_number = post_number.lstrip('#') if post_number != 'N/A' else post_number\n",
        "\n",
        "                post_data.append({\n",
        "                    'thread_id': row['thread_id'],  # Corrected to use row's data\n",
        "                    'post_id': numeric_post_id,\n",
        "                    'post_raw': content\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(post_data, columns=['thread_id', 'post_id','post_raw'])\n",
        "\n",
        "# Fetch thread URLs and titles, and store in a DataFrame\n",
        "e9_forum_posts = fetch_and_parse_thread(e9_forum_threads)\n",
        "\n",
        "e9_forum_posts['thread_id'] = e9_forum_posts['thread_id'].astype(int)\n",
        "e9_forum_posts['post_id'] = e9_forum_posts['post_id'].astype(int)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "e9_forum_posts.head()\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_posts.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_posts.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOGif4rFpAE5"
      },
      "outputs": [],
      "source": [
        "# Define a function to clean text\n",
        "# The problem is that in removes the // from URLs in e9_forum_threads\n",
        "\n",
        "\n",
        "# Remove URL from e9_forum_threads\n",
        "\n",
        "e9_forum_threads.drop(columns=['thread_url'], inplace=True)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and symbols using regex\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "e9_forum_threads = e9_forum_threads.applymap(clean_text)\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "#e9_forum_posts = e9_forum_posts.applymap(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fZUP0i6Hubn"
      },
      "outputs": [],
      "source": [
        "# Aggregate threads and posts into one df\n",
        "\n",
        "# Group by THREAD_ID and concatenate the POST_RAW values\n",
        "aggregated_data = e9_forum_posts.groupby('thread_id')['post_raw'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Rename the column to indicate that it contains concatenated post content\n",
        "aggregated_data.rename(columns={'post_raw': 'thread_all_posts'}, inplace=True)\n",
        "\n",
        "\n",
        "# Convert 'thread_id' column to int64 in both DataFrames\n",
        "e9_forum_threads['thread_id'] = e9_forum_threads['thread_id'].astype('int64')\n",
        "aggregated_data['thread_id'] = aggregated_data['thread_id'].astype('int64')\n",
        "\n",
        "# Merge the two DataFrames\n",
        "e9_forum_corpus = pd.merge(e9_forum_threads, aggregated_data, on='thread_id', how='left')\n",
        "\n",
        "\n",
        "# Export and save result\n",
        "e9_forum_corpus.to_csv('/content/drive/MyDrive/Data_sets/e9/e9_forum_corpus.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ87fuRiotXR"
      },
      "outputs": [],
      "source": [
        "# Define a function to clean text\n",
        "# The problem is that in removes the // from URLs in e9_forum_threads\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and symbols using regex\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "e9_forum_corpus = e9_forum_corpus.applymap(clean_text)\n",
        "\n",
        "# Apply the clean_text function to all columns in the DataFrame\n",
        "#e9_forum_posts = e9_forum_posts.applymap(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTlqkHV0NgmK"
      },
      "outputs": [],
      "source": [
        "# This is a summarization of posts\n",
        "# This includes tokenization of the text\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Initialize the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "def summarize_text(df):\n",
        "    sum_text = []  # Initialize the list to hold summaries\n",
        "    for text in df['post_concat']:\n",
        "        # Ensure the text is a string and not empty\n",
        "        #if not isinstance(text, str) or not text.strip():\n",
        "        #    sum_text.append(\"\")  # Append an empty string for non-valid entries\n",
        "        #   continue\n",
        "\n",
        "        # Prefixing the input text with \"summarize: \" as T5 expects\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decode the generated ids to get the summary text\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        sum_text.append(summary)\n",
        "\n",
        "    return sum_text\n",
        "\n",
        "df_threads['post_summary'] = summarize_text(df_threads)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWGj_PDvEb40"
      },
      "outputs": [],
      "source": [
        "# Find the key words of all posts for a given thread\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract keywords using BERT\n",
        "def bert_extract_keywords(text, tokenizer, model, top_n=5):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Compute word importance by summing up the embeddings\n",
        "    word_importance = torch.sum(embeddings, dim=1)\n",
        "\n",
        "    # Get the indices of the top n important words\n",
        "    top_n_indices = word_importance.argsort(descending=True)[:top_n]\n",
        "\n",
        "    # Filter out indices that are out of range of input_ids\n",
        "    top_n_indices = [idx for idx in top_n_indices if idx < len(input_ids)]\n",
        "\n",
        "    # Decode the top n words\n",
        "    keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_n_indices]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "df_threads['post_keywords'] = df_threads['post_summary'].apply(lambda x: bert_extract_keywords(x, tokenizer, model))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_threads.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t83z6mu7cvm"
      },
      "outputs": [],
      "source": [
        "# Export and save result\n",
        "df_threads.to_csv('/content/drive/MyDrive/e9/nlp/df_threads.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "531Fj7AM9wPx"
      },
      "source": [
        "## Create Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqBySX4n2S7E"
      },
      "source": [
        "####Table 1: Issues\n",
        "\n",
        "*   ID (Unique ID)\n",
        "*   Issue\n",
        "*   Short Description\n",
        "*   Keywords\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk-nZAgAsvkR"
      },
      "outputs": [],
      "source": [
        "#Table 1 Issues\n",
        "\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_id\n",
        "#*   Issue Should be the thread title\n",
        "#*   Short Description Using the thread title for now. Should be the post of the originating thread\n",
        "#*   Keywords: Should be from the thread post\n",
        "\n",
        "# Table 1\n",
        "df_table_1 = df_threads[['thread_id','thread_title','thread_first_post_summary','thread_first_post_keywords']]\n",
        "\n",
        "# Export and save result\n",
        "df_table_1.to_csv('/content/drive/MyDrive/Data_sets/df_table_1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9T12lBX6m6J"
      },
      "source": [
        "####Table 2: Solutions\n",
        "\n",
        "*   ID (Unique Note ID)\n",
        "*   Issue\n",
        "*   Issue ID (Foreign Key linking to ID in Issues table)\n",
        "*   Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg_0l-eCsvmz"
      },
      "outputs": [],
      "source": [
        "#Table 2\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue Should be the thread title\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "#*   Detailed Solution Should be the concatinated post_raw per thread_id\n",
        "#*   Keywords Should be from be the concatinated post_raw per thread_id\n",
        "\n",
        "df_table_2 = df_threads[['thread_title','thread_id','post_concat','post_keywords']]\n",
        "\n",
        "# Export and save result\n",
        "df_table_2.to_csv('/content/drive/MyDrive/Data_sets/df_table_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKTQy1yCF51R"
      },
      "source": [
        "####Table 3: Notes\n",
        "\n",
        "Key Should be from pandas\n",
        "Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "Unstructured Note Content Should be from the thread posts\n",
        "\n",
        "\n",
        "*   ID (Unique Note ID)\n",
        "*   Issue ID (Foreign Key linking to ID in Issues table)\n",
        "*   Unstructured Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF92Opgbsvpp"
      },
      "outputs": [],
      "source": [
        "# Table 3: Notes\n",
        "\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "#*   Unstructured Content: Should be from the raw thread posts\n",
        "\n",
        "df_table_3 = df_threads[['thread_id','post_concat']]\n",
        "\n",
        "\n",
        "# Export and save result\n",
        "df_table_3.to_csv('/content/drive/MyDrive/Data_sets/df_table_3.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZjYoWFrG1ug"
      },
      "outputs": [],
      "source": [
        "df_table_3.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXqdpjX3hSFk"
      },
      "outputs": [],
      "source": [
        "# Old code on determining post vs thread\n",
        "\n",
        "\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "#        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "#        results = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "#        for result in results:\n",
        "#            title_element = result.find('h3', class_='contentRow-title')\n",
        "#            if title_element and title_element.find('a'):\n",
        "#                title = title_element.get_text(strip=True)\n",
        "#                url = title_element.find('a')['href']\n",
        "\n",
        "                # Check if it's a thread or a post\n",
        "#                post_info = result.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "#                if \"Thread\" in post_info:\n",
        "#                    type = \"Thread\"\n",
        "\n",
        " #                   print(f\"Title: {title}, URL: {url}, Type: {type}\")\n",
        " #                   print('--------------------------------------------------')\n",
        "                # Append the information to a list\n",
        "#                    url = 'https://e9coupe.com'+url\n",
        "#                    thread_urls.append({'url': url})\n",
        "#                    #thread_urls.append({'title': title, 'url': url, 'type': type})\n",
        "\n",
        "\n",
        "# Create a DataFrame for the URLs\n",
        "#df_threads = pd.DataFrame(thread_urls)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}